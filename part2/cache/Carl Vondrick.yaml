interactions:
- request:
    body: null
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      User-Agent:
      - python-requests/2.31.0
    method: GET
    uri: https://api.semanticscholar.org/graph/v1/author/search?query=Carl%20Vondrick&fields=affiliations,aliases,authorId,citationCount,externalIds,hIndex,homepage,name,paperCount,papers,papers.abstract,papers.authors,papers.citationCount,papers.corpusId,papers.externalIds,papers.fieldsOfStudy,papers.influentialCitationCount,papers.isOpenAccess,papers.journal,papers.openAccessPdf,papers.paperId,papers.publicationDate,papers.publicationTypes,papers.publicationVenue,papers.referenceCount,papers.s2FieldsOfStudy,papers.title,papers.url,papers.venue,papers.year,url&offset=0&limit=1000
  response:
    body:
      string: '{"total": 3, "offset": 0, "data": [{"authorId": "148281620", "externalIds":
        {}, "url": "https://www.semanticscholar.org/author/148281620", "name": "V.
        Carl", "aliases": ["Vondrick Carl", "V Thompson Carl"], "affiliations": [],
        "homepage": null, "paperCount": 2, "citationCount": 0, "hIndex": 0, "papers":
        [{"paperId": "b2dfa5e94b55dec38f9ace6eac00815dbb6e375e", "externalIds": {"MAG":
        "2743208974", "CorpusId": 103456795}, "corpusId": 103456795, "publicationVenue":
        null, "url": "https://www.semanticscholar.org/paper/b2dfa5e94b55dec38f9ace6eac00815dbb6e375e",
        "title": "\u539f\u5b50\u5c64\u5806\u7a4d\u306b\u3088\u308b\u9178\u5316\u30eb\u30c6\u30cb\u30a6\u30e0\u3067\u88ab\u8986\u3055\u308c\u305fSi\u30ca\u30ce\u30ef\u30a4\u30e4\u306b\u57fa\u3065\u304f\u9ad8\u6027\u80fd\u56fa\u4f53\u30aa\u30f3\u30c1\u30c3\u30d7\u30b9\u30fc\u30d1\u30fc\u30ad\u30e3\u30d1\u30b7\u30bf\u3010Powered
        by NICT\u3011", "abstract": null, "venue": "", "year": 2017, "referenceCount":
        0, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category":
        "Chemistry", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate":
        null, "journal": {"volume": "341", "pages": "10", "name": "Journal of Power
        Sources"}, "authors": [{"authorId": "2067933486", "name": "Zheng Wen"}, {"authorId":
        "2081308954", "name": "Cheng Qingmei"}, {"authorId": "96847898", "name": "Wang
        Dunwei"}, {"authorId": "148281620", "name": "V. Carl"}]}, {"paperId": "175db9391d94c32acb10a190c4829422150b76b8",
        "externalIds": {"MAG": "2748933726", "CorpusId": 189039287}, "corpusId": 189039287,
        "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/175db9391d94c32acb10a190c4829422150b76b8",
        "title": "\u5f31\u304f\u6574\u5217\u3057\u305f\u30c7\u30fc\u30bf\u304b\u3089\u306e\u6574\u5217\u3057\u305f\u30af\u30ed\u30b9\u30e2\u30fc\u30c0\u30eb\u8868\u73fe\u306e\u5b66\u7fd2\u3010Powered
        by NICT\u3011", "abstract": null, "venue": "", "year": 2016, "referenceCount":
        0, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy": [], "publicationTypes":
        null, "publicationDate": null, "journal": {"volume": "2016", "pages": "2949",
        "name": ""}, "authors": [{"authorId": "146391833", "name": "Castrejon Lluis"},
        {"authorId": "146225685", "name": "Aytar Yusuf"}, {"authorId": "148281620",
        "name": "V. Carl"}, {"authorId": "101886885", "name": "Pirsiavash Hamed"},
        {"authorId": "148178143", "name": "T. Antonio"}]}]}, {"authorId": "2252069989",
        "externalIds": {"DBLP": ["Carl Vondrick"]}, "url": "https://www.semanticscholar.org/author/2252069989",
        "name": "Carl Vondrick", "aliases": null, "affiliations": [], "homepage":
        null, "paperCount": 1, "citationCount": 2, "hIndex": 1, "papers": [{"paperId":
        "68c5e320725c26d94d69a61e42e5a7e88bd9a242", "externalIds": {"DBLP": "journals/scirobotics/ChenKVL22",
        "DOI": "10.1126/scirobotics.abn1944", "CorpusId": 263512982, "PubMed": "35857575"},
        "corpusId": 263512982, "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/68c5e320725c26d94d69a61e42e5a7e88bd9a242",
        "title": "Fully body visual self-modeling of robot morphologies", "abstract":
        "Internal computational models of physical bodies are fundamental to the ability
        of robots and animals alike to plan and control their actions. These \"self-models\"
        allow robots to consider outcomes of multiple possible future actions without
        trying them out in physical reality. Recent progress in fully data-driven
        self-modeling has enabled machines to learn their own forward kinematics directly
        from task-agnostic interaction data. However, forward kinematic models can
        only predict limited aspects of the morphology, such as the position of end
        effectors or velocity of joints and masses. A key challenge is to model the
        entire morphology and kinematics without prior knowledge of what aspects of
        the morphology will be relevant to future tasks. Here, we propose that instead
        of directly modeling forward kinematics, a more useful form of self-modeling
        is one that could answer space occupancy queries, conditioned on the robot''s
        state. Such query-driven self-models are continuous in the spatial domain,
        memory efficient, fully differentiable, and kinematic aware and can be used
        across a broader range of tasks. In physical experiments, we demonstrate how
        a visual self-model is accurate to about 1% of the workspace, enabling the
        robot to perform various motion planning and control tasks. Visual self-modeling
        can also allow the robot to detect, localize, and recover from real-world
        damage, leading to improved machine resiliency.", "venue": "Sci. Robotics",
        "year": 2022, "referenceCount": 10, "citationCount": 5, "influentialCitationCount":
        1, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science", "Medicine"], "s2FieldsOfStudy": [{"category": "Computer Science",
        "source": "external"}, {"category": "Medicine", "source": "external"}, {"category":
        "Biology", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
        "publicationDate": "2022-07-13", "journal": {"volume": "7 68", "pages": "\n          eabn1944\n        ",
        "name": "Science robotics"}, "authors": [{"authorId": "2252193265", "name":
        "Boyuan Chen"}, {"authorId": "1420209643", "name": "Robert Kwiatkowski"},
        {"authorId": "2252069989", "name": "Carl Vondrick"}, {"authorId": "2251487966",
        "name": "Hod Lipson"}]}]}, {"authorId": "1856025", "externalIds": {"DBLP":
        ["Carl Vondrick"]}, "url": "https://www.semanticscholar.org/author/1856025",
        "name": "Carl Vondrick", "aliases": ["C. Vondrick"], "affiliations": [], "homepage":
        null, "paperCount": 104, "citationCount": 9417, "hIndex": 35, "papers": [{"paperId":
        "0c1a64d547a93b206f0488366ca765ee2a711593", "externalIds": {"DBLP": "journals/corr/abs-2304-06197",
        "ArXiv": "2304.06197", "DOI": "10.48550/arXiv.2304.06197", "CorpusId": 258108053},
        "corpusId": 258108053, "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
        "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url":
        "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/0c1a64d547a93b206f0488366ca765ee2a711593",
        "title": "SURFSUP: Learning Fluid Simulation for Novel Surfaces", "abstract":
        "Modeling the mechanics of fluid in complex scenes is vital to applications
        in design, graphics, and robotics. Learning-based methods provide fast and
        differentiable fluid simulators, however most prior work is unable to accurately
        model how fluids interact with genuinely novel surfaces not seen during training.
        We introduce SURFSUP, a framework that represents objects implicitly using
        signed distance functions (SDFs), rather than an explicit representation of
        meshes or particles. This continuous representation of geometry enables more
        accurate simulation of fluid-object interactions over long time periods while
        simultaneously making computation more efficient. Moreover, SURFSUP trained
        on simple shape primitives generalizes considerably out-of-distribution, even
        to complex real-world scenes and objects. Finally, we show we can invert our
        model to design simple objects to manipulate fluid flow.", "venue": "arXiv.org",
        "year": 2023, "referenceCount": 33, "citationCount": 0, "influentialCitationCount":
        0, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science", "Physics"], "s2FieldsOfStudy": [{"category": "Computer Science",
        "source": "external"}, {"category": "Physics", "source": "external"}, {"category":
        "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
        "publicationDate": "2023-04-13", "journal": {"volume": "abs/2304.06197", "name":
        "ArXiv"}, "authors": [{"authorId": "20657367", "name": "Arjun Mani"}, {"authorId":
        "2146376367", "name": "I. Chandratreya"}, {"authorId": "3422145", "name":
        "Elliot Creager"}, {"authorId": "1856025", "name": "Carl Vondrick"}, {"authorId":
        "1804104", "name": "R. Zemel"}]}, {"paperId": "111ef44f5c3cf2f8a93a124ef152ebc116135def",
        "externalIds": {"DBLP": "journals/corr/abs-2306-08754", "ArXiv": "2306.08754",
        "DOI": "10.48550/arXiv.2306.08754", "CorpusId": 259164511}, "corpusId": 259164511,
        "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name":
        "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"},
        "url": "https://www.semanticscholar.org/paper/111ef44f5c3cf2f8a93a124ef152ebc116135def",
        "title": "ClimSim: An open large-scale dataset for training high-resolution
        physics emulators in hybrid multi-scale climate simulators", "abstract": "Modern
        climate projections lack adequate spatial and temporal resolution due to computational
        constraints. A consequence is inaccurate and imprecise predictions of critical
        processes such as storms. Hybrid methods that combine physics with machine
        learning (ML) have introduced a new generation of higher fidelity climate
        simulators that can sidestep Moore''s Law by outsourcing compute-hungry, short,
        high-resolution simulations to ML emulators. However, this hybrid ML-physics
        simulation approach requires domain-specific treatment and has been inaccessible
        to ML experts because of lack of training data and relevant, easy-to-use workflows.
        We present ClimSim, the largest-ever dataset designed for hybrid ML-physics
        research. It comprises multi-scale climate simulations, developed by a consortium
        of climate scientists and ML researchers. It consists of 5.7 billion pairs
        of multivariate input and output vectors that isolate the influence of locally-nested,
        high-resolution, high-fidelity physics on a host climate simulator''s macro-scale
        physical state. The dataset is global in coverage, spans multiple years at
        high sampling frequency, and is designed such that resulting emulators are
        compatible with downstream coupling into operational climate simulators. We
        implement a range of deterministic and stochastic regression baselines to
        highlight the ML challenges and their scoring. The data (https://huggingface.co/datasets/LEAP/ClimSim_high-res,
        https://huggingface.co/datasets/LEAP/ClimSim_low-res, and https://huggingface.co/datasets/LEAP/ClimSim_low-res_aqua-planet)
        and code (https://leap-stc.github.io/ClimSim) are released openly to support
        the development of hybrid ML-physics and high-fidelity climate simulations
        for the benefit of science and society.", "venue": "arXiv.org", "year": 2023,
        "referenceCount": 98, "citationCount": 4, "influentialCitationCount": 0, "isOpenAccess":
        false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Physics"],
        "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
        {"category": "Physics", "source": "external"}, {"category": "Computer Science",
        "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
        "2023-06-14", "journal": {"volume": "abs/2306.08754", "name": "ArXiv"}, "authors":
        [{"authorId": "90474628", "name": "Sungduk Yu"}, {"authorId": "49324141",
        "name": "W. Hannah"}, {"authorId": "113643144", "name": "Liran Peng"}, {"authorId":
        "3437232", "name": "Mohamed Aziz Bhouri"}, {"authorId": "81270041", "name":
        "Ritwik Gupta"}, {"authorId": "2220140479", "name": "Jerry Lin"}, {"authorId":
        "2047341717", "name": "Bjorn Lutjens"}, {"authorId": "2220097282", "name":
        "Justus C. Will"}, {"authorId": "102947589", "name": "T. Beucler"}, {"authorId":
        "102654158", "name": "B. Harrop"}, {"authorId": "34894520", "name": "B. Hillman"},
        {"authorId": "39592416", "name": "A. Jenney"}, {"authorId": "2220098655",
        "name": "Savannah L. Ferretti"}, {"authorId": "2220703809", "name": "Nana
        Liu"}, {"authorId": "2047844", "name": "Anima Anandkumar"}, {"authorId": "2138925448",
        "name": "Noah Brenowitz"}, {"authorId": "91154210", "name": "V. Eyring"},
        {"authorId": "15572823", "name": "P. Gentine"}, {"authorId": "1783468", "name":
        "S. Mandt"}, {"authorId": "35620505", "name": "Jaideep Pathak"}, {"authorId":
        "1856025", "name": "Carl Vondrick"}, {"authorId": "2023052", "name": "Rose
        Yu"}, {"authorId": "103206782", "name": "L. Zanna"}, {"authorId": "69353867",
        "name": "R. Abernathey"}, {"authorId": "50879037", "name": "F. Ahmed"}, {"authorId":
        "93900910", "name": "D. Bader"}, {"authorId": "2075424199", "name": "P. Baldi"},
        {"authorId": "49004366", "name": "E. Barnes"}, {"authorId": "2058631810",
        "name": "G. Behrens"}, {"authorId": "67038210", "name": "C. Bretherton"},
        {"authorId": "69394374", "name": "Julius J. M. Busecke"}, {"authorId": "2060294224",
        "name": "P. Caldwell"}, {"authorId": "6586625", "name": "W. Chuang"}, {"authorId":
        "2112846436", "name": "Yilun Han"}, {"authorId": "2164725519", "name": "Yu
        Huang"}, {"authorId": "1403715027", "name": "Fernando Iglesias\u2010Suarez"},
        {"authorId": "1974470911", "name": "Sanket R. Jantre"}, {"authorId": "3410191",
        "name": "K. Kashinath"}, {"authorId": "70224669", "name": "M. Khairoutdinov"},
        {"authorId": "1476814684", "name": "T. Kurth"}, {"authorId": "102455838",
        "name": "N. Lutsko"}, {"authorId": "37700717", "name": "P. Ma"}, {"authorId":
        "113393992", "name": "G. Mooers"}, {"authorId": "144312277", "name": "J. Neelin"},
        {"authorId": "2285593", "name": "D. Randall"}, {"authorId": "1405073990",
        "name": "S. Shamekh"}, {"authorId": "31388055", "name": "Akshay Subramaniam"},
        {"authorId": "153517508", "name": "M. Taylor"}, {"authorId": "2123986387",
        "name": "Nathan M. Urban"}, {"authorId": "5276672", "name": "J. Yuval"}, {"authorId":
        "49288108", "name": "G. Zhang"}, {"authorId": "2220097886", "name": "Tian
        Zheng"}, {"authorId": "46303219", "name": "M. Pritchard"}]}, {"paperId": "16a122b84bab5e61775869967b1822b71dd453a9",
        "externalIds": {"DBLP": "journals/corr/abs-2305-01652", "ArXiv": "2305.01652",
        "DOI": "10.1109/CVPR52729.2023.01206", "CorpusId": 258437027}, "corpusId":
        258437027, "publicationVenue": {"id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
        "name": "Computer Vision and Pattern Recognition", "type": "conference", "alternate_names":
        ["CVPR", "Comput Vis Pattern Recognit"], "issn": "1063-6919", "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
        "alternate_urls": ["https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"]},
        "url": "https://www.semanticscholar.org/paper/16a122b84bab5e61775869967b1822b71dd453a9",
        "title": "Humans as Light Bulbs: 3D Human Reconstruction from Thermal Reflection",
        "abstract": "The relatively hot temperature of the human body causes people
        to turn into long-wave infrared light sources. Since this emitted light has
        a larger wavelength than visible light, many surfaces in typical scenes act
        as infrared mirrors with strong specular reflections. We exploit the thermal
        reflections of a person onto objects in order to locate their position and
        reconstruct their pose, even if they are not visible to a normal camera. We
        propose an analysis-by-synthesis framework that jointly models the objects,
        people, and their thermal reflections, which combines generative models with
        differentiable rendering of reflections. Quantitative and qualitative experiments
        show our approach works in highly challenging cases, such as with curved mirrors
        or when the person is completely unseen by a normal camera.", "venue": "Computer
        Vision and Pattern Recognition", "year": 2023, "referenceCount": 86, "citationCount":
        3, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url":
        "https://arxiv.org/pdf/2305.01652", "status": null}, "fieldsOfStudy": ["Computer
        Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
        "external"}, {"category": "Physics", "source": "s2-fos-model"}], "publicationTypes":
        ["JournalArticle", "Conference"], "publicationDate": "2023-05-02", "journal":
        {"pages": "12531-12542", "name": "2023 IEEE/CVF Conference on Computer Vision
        and Pattern Recognition (CVPR)"}, "authors": [{"authorId": "2143183492", "name":
        "Ruoshi Liu"}, {"authorId": "1856025", "name": "Carl Vondrick"}]}, {"paperId":
        "1a96f901dad704fafa88fa7525174b15c720bcf1", "externalIds": {"DBLP": "journals/corr/abs-2305-03052",
        "ArXiv": "2305.03052", "DOI": "10.1109/CVPR52729.2023.01326", "CorpusId":
        258480002}, "corpusId": 258480002, "publicationVenue": {"id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
        "name": "Computer Vision and Pattern Recognition", "type": "conference", "alternate_names":
        ["CVPR", "Comput Vis Pattern Recognit"], "issn": "1063-6919", "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
        "alternate_urls": ["https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"]},
        "url": "https://www.semanticscholar.org/paper/1a96f901dad704fafa88fa7525174b15c720bcf1",
        "title": "Tracking Through Containers and Occluders in the Wild", "abstract":
        "Tracking objects with persistence in cluttered and dynamic environments remains
        a difficult challenge for computer vision systems. In this paper, we introduce
        TCOW, a new benchmark and model for visual tracking through heavy occlusion
        and containment. We set up a task where the goal is to, given a video sequence,
        segment both the projected extent of the target object, as well as the surrounding
        container or occluder whenever one exists. To study this task, we create a
        mixture of synthetic and annotated real datasets to support both supervised
        learning and structured evaluation of model performance under various forms
        of task variation, such as moving or nested containment. We evaluate two recent
        transformer-based video models and find that while they can be surprisingly
        capable of tracking targets under certain settings of task variation, there
        remains a considerable performance gap before we can claim a tracking model
        to have acquired a true notion of object permanence.", "venue": "Computer
        Vision and Pattern Recognition", "year": 2023, "referenceCount": 78, "citationCount":
        1, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url":
        "https://arxiv.org/pdf/2305.03052", "status": null}, "fieldsOfStudy": ["Computer
        Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
        "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2023-05-04",
        "journal": {"pages": "13802-13812", "name": "2023 IEEE/CVF Conference on Computer
        Vision and Pattern Recognition (CVPR)"}, "authors": [{"authorId": "1470838102",
        "name": "Basile Van Hoorick"}, {"authorId": "2931554", "name": "P. Tokmakov"},
        {"authorId": "2307158", "name": "Simon Stent"}, {"authorId": "49298718", "name":
        "Jie Li"}, {"authorId": "1856025", "name": "Carl Vondrick"}]}, {"paperId":
        "1b90e9e9734bed6b379ae87d688cb3b887baf597", "externalIds": {"DBLP": "journals/corr/abs-2307-05663",
        "ArXiv": "2307.05663", "DOI": "10.48550/arXiv.2307.05663", "CorpusId": 259836993},
        "corpusId": 259836993, "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
        "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url":
        "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/1b90e9e9734bed6b379ae87d688cb3b887baf597",
        "title": "Objaverse-XL: A Universe of 10M+ 3D Objects", "abstract": "Natural
        language processing and 2D vision models have attained remarkable proficiency
        on many tasks primarily by escalating the scale of training data. However,
        3D vision tasks have not seen the same progress, in part due to the challenges
        of acquiring high-quality 3D data. In this work, we present Objaverse-XL,
        a dataset of over 10 million 3D objects. Our dataset comprises deduplicated
        3D objects from a diverse set of sources, including manually designed objects,
        photogrammetry scans of landmarks and everyday items, and professional scans
        of historic and antique artifacts. Representing the largest scale and diversity
        in the realm of 3D datasets, Objaverse-XL enables significant new possibilities
        for 3D vision. Our experiments demonstrate the improvements enabled with the
        scale provided by Objaverse-XL. We show that by training Zero123 on novel
        view synthesis, utilizing over 100 million multi-view rendered images, we
        achieve strong zero-shot generalization abilities. We hope that releasing
        Objaverse-XL will enable further innovations in the field of 3D vision at
        scale.", "venue": "arXiv.org", "year": 2023, "referenceCount": 68, "citationCount":
        10, "influentialCitationCount": 1, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
        "Computer Science", "source": "external"}, {"category": "Computer Science",
        "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
        "2023-07-11", "journal": {"volume": "abs/2307.05663", "name": "ArXiv"}, "authors":
        [{"authorId": "1632916259", "name": "Matt Deitke"}, {"authorId": "2143183492",
        "name": "Ruoshi Liu"}, {"authorId": "1632957174", "name": "Matthew Wallingford"},
        {"authorId": "2223118889", "name": "Huong Ngo"}, {"authorId": "2196005933",
        "name": "Oscar Michel"}, {"authorId": "52207562", "name": "Aditya Kusupati"},
        {"authorId": "2219305108", "name": "Alan Fan"}, {"authorId": "2223133147",
        "name": "Christian Laforte"}, {"authorId": "2961618", "name": "Vikram S. Voleti"},
        {"authorId": "1387466862", "name": "S. Gadre"}, {"authorId": "1632920625",
        "name": "Eli VanderBilt"}, {"authorId": "2684226", "name": "Aniruddha Kembhavi"},
        {"authorId": "1856025", "name": "Carl Vondrick"}, {"authorId": "2082991",
        "name": "Georgia Gkioxari"}, {"authorId": "2883417", "name": "Kiana Ehsani"},
        {"authorId": "152772922", "name": "Ludwig Schmidt"}, {"authorId": "143787583",
        "name": "Ali Farhadi"}]}, {"paperId": "2c70684973bc4d7b6f8404a647b8031c4d3c8383",
        "externalIds": {"DBLP": "journals/corr/abs-2303-11328", "ArXiv": "2303.11328",
        "DOI": "10.48550/arXiv.2303.11328", "CorpusId": 257631738}, "corpusId": 257631738,
        "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name":
        "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"},
        "url": "https://www.semanticscholar.org/paper/2c70684973bc4d7b6f8404a647b8031c4d3c8383",
        "title": "Zero-1-to-3: Zero-shot One Image to 3D Object", "abstract": "We
        introduce Zero-1-to-3, a framework for changing the camera viewpoint of an
        object given just a single RGB image. To perform novel view synthesis in this
        under-constrained setting, we capitalize on the geometric priors that large-scale
        diffusion models learn about natural images. Our conditional diffusion model
        uses a synthetic dataset to learn controls of the relative camera viewpoint,
        which allow new images to be generated of the same object under a specified
        camera transformation. Even though it is trained on a synthetic dataset, our
        model retains a strong zero-shot generalization ability to out-of-distribution
        datasets as well as in-the-wild images, including impressionist paintings.
        Our viewpoint-conditioned diffusion approach can further be used for the task
        of 3D reconstruction from a single image. Qualitative and quantitative experiments
        show that our method significantly outperforms state-of-the-art single-view
        3D reconstruction and novel view synthesis models by leveraging Internet-scale
        pre-training.", "venue": "arXiv.org", "year": 2023, "referenceCount": 68,
        "citationCount": 73, "influentialCitationCount": 19, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
        "publicationDate": "2023-03-20", "journal": {"volume": "abs/2303.11328", "name":
        "ArXiv"}, "authors": [{"authorId": "2143183492", "name": "Ruoshi Liu"}, {"authorId":
        "1406236938", "name": "Rundi Wu"}, {"authorId": "1470838102", "name": "Basile
        Van Hoorick"}, {"authorId": "2931554", "name": "P. Tokmakov"}, {"authorId":
        "144506587", "name": "Sergey Zakharov"}, {"authorId": "1856025", "name": "Carl
        Vondrick"}]}, {"paperId": "34a4229372313f3741c579c9f48c8687e40f1f1b", "externalIds":
        {"ArXiv": "2310.10591", "CorpusId": 264145927}, "corpusId": 264145927, "publicationVenue":
        null, "url": "https://www.semanticscholar.org/paper/34a4229372313f3741c579c9f48c8687e40f1f1b",
        "title": "Interpreting and Controlling Vision Foundation Models via Text Explanations",
        "abstract": "Large-scale pre-trained vision foundation models, such as CLIP,
        have become de facto backbones for various vision tasks. However, due to their
        black-box nature, understanding the underlying rules behind these models''
        predictions and controlling model behaviors have remained open challenges.
        We present a framework for interpreting vision transformer''s latent tokens
        with natural language. Given a latent token, our framework retains its semantic
        information to the final layer using transformer''s local operations and retrieves
        the closest text for explanation. Our approach enables understanding of model
        visual reasoning procedure without needing additional model training or data
        collection. Based on the obtained interpretations, our framework allows for
        model editing that controls model reasoning behaviors and improves model robustness
        against biases and spurious correlations.", "venue": "", "year": 2023, "referenceCount":
        63, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate":
        "2023-10-16", "journal": null, "authors": [{"authorId": "2259331717", "name":
        "Haozhe Chen"}, {"authorId": "2110694456", "name": "Junfeng Yang"}, {"authorId":
        "1856025", "name": "Carl Vondrick"}, {"authorId": "7700460", "name": "Chengzhi
        Mao"}]}, {"paperId": "35f5483fa6c1816739b604a5bb57719fadd79249", "externalIds":
        {"ArXiv": "2301.10939", "DBLP": "journals/corr/abs-2301-10939", "DOI": "10.48550/arXiv.2301.10939",
        "CorpusId": 256274727}, "corpusId": 256274727, "publicationVenue": {"id":
        "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names":
        ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/35f5483fa6c1816739b604a5bb57719fadd79249",
        "title": "Affective Faces for Goal-Driven Dyadic Communication", "abstract":
        "We introduce a video framework for modeling the association between verbal
        and non-verbal communication during dyadic conversation. Given the input speech
        of a speaker, our approach retrieves a video of a listener, who has facial
        expressions that would be socially appropriate given the context. Our approach
        further allows the listener to be conditioned on their own goals, personalities,
        or backgrounds. Our approach models conversations through a composition of
        large language models and vision-language models, creating internal representations
        that are interpretable and controllable. To study multimodal communication,
        we propose a new video dataset of unscripted conversations covering diverse
        topics and demographics. Experiments and visualizations show our approach
        is able to output listeners that are significantly more socially appropriate
        than baselines. However, many challenges remain, and we release our dataset
        publicly to spur further progress. See our website for video results, data,
        and code: https://realtalk.cs.columbia.edu.", "venue": "arXiv.org", "year":
        2023, "referenceCount": 60, "citationCount": 4, "influentialCitationCount":
        0, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
        "external"}, {"category": "Psychology", "source": "s2-fos-model"}], "publicationTypes":
        ["JournalArticle"], "publicationDate": "2023-01-26", "journal": {"volume":
        "abs/2301.10939", "name": "ArXiv"}, "authors": [{"authorId": "2187059392",
        "name": "Scott Geng"}, {"authorId": "1396817932", "name": "Revant Teotia"},
        {"authorId": "2163453087", "name": "Purva Tendulkar"}, {"authorId": "46245898",
        "name": "Sachit Menon"}, {"authorId": "1856025", "name": "Carl Vondrick"}]},
        {"paperId": "4cb4303e79acef4c276a87615641f94cd77a3e4a", "externalIds": {"DBLP":
        "journals/corr/abs-2305-15399", "ArXiv": "2305.15399", "DOI": "10.48550/arXiv.2305.15399",
        "CorpusId": 258866164}, "corpusId": 258866164, "publicationVenue": {"id":
        "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names":
        ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/4cb4303e79acef4c276a87615641f94cd77a3e4a",
        "title": "Sin3DM: Learning a Diffusion Model from a Single 3D Textured Shape",
        "abstract": "Synthesizing novel 3D models that resemble the input example
        has long been pursued by researchers and artists in computer graphics. In
        this paper, we present Sin3DM, a diffusion model that learns the internal
        patch distribution from a single 3D textured shape and generates high-quality
        variations with fine geometry and texture details. Training a diffusion model
        directly in 3D would induce large memory and computational cost. Therefore,
        we first compress the input into a lower-dimensional latent space and then
        train a diffusion model on it. Specifically, we encode the input 3D textured
        shape into triplane feature maps that represent the signed distance and texture
        fields of the input. The denoising network of our diffusion model has a limited
        receptive field to avoid overfitting, and uses triplane-aware 2D convolution
        blocks to improve the result quality. Aside from randomly generating new samples,
        our model also facilitates applications such as retargeting, outpainting and
        local editing. Through extensive qualitative and quantitative evaluation,
        we show that our model can generate 3D shapes of various types with better
        quality than prior methods.", "venue": "arXiv.org", "year": 2023, "referenceCount":
        99, "citationCount": 2, "influentialCitationCount": 0, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
        "publicationDate": "2023-05-24", "journal": {"volume": "abs/2305.15399", "name":
        "ArXiv"}, "authors": [{"authorId": "1406236938", "name": "Rundi Wu"}, {"authorId":
        "2143183492", "name": "Ruoshi Liu"}, {"authorId": "1856025", "name": "Carl
        Vondrick"}, {"authorId": "39294084", "name": "Changxi Zheng"}]}, {"paperId":
        "6e754273d54a91371efbc928cd6b156364d517da", "externalIds": {"DBLP": "journals/corr/abs-2303-08128",
        "ArXiv": "2303.08128", "DOI": "10.48550/arXiv.2303.08128", "CorpusId": 257505358},
        "corpusId": 257505358, "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
        "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url":
        "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/6e754273d54a91371efbc928cd6b156364d517da",
        "title": "ViperGPT: Visual Inference via Python Execution for Reasoning",
        "abstract": "Answering visual queries is a complex task that requires both
        visual processing and reasoning. End-to-end models, the dominant approach
        for this task, do not explicitly differentiate between the two, limiting interpretability
        and generalization. Learning modular programs presents a promising alternative,
        but has proven challenging due to the difficulty of learning both the programs
        and modules simultaneously. We introduce ViperGPT, a framework that leverages
        code-generation models to compose vision-and-language models into subroutines
        to produce a result for any query. ViperGPT utilizes a provided API to access
        the available modules, and composes them by generating Python code that is
        later executed. This simple approach requires no further training, and achieves
        state-of-the-art results across various complex visual tasks.", "venue": "arXiv.org",
        "year": 2023, "referenceCount": 73, "citationCount": 79, "influentialCitationCount":
        11, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
        "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle"], "publicationDate": "2023-03-14", "journal":
        {"volume": "abs/2303.08128", "name": "ArXiv"}, "authors": [{"authorId": "35552695",
        "name": "D''idac Sur''is"}, {"authorId": "46245898", "name": "Sachit Menon"},
        {"authorId": "1856025", "name": "Carl Vondrick"}]}, {"paperId": "7f368c2255a3dbd1356c21af574f493b8e90174a",
        "externalIds": {"DBLP": "conf/cvpr/LiuMMPSV23", "DOI": "10.1109/CVPR52729.2023.01636",
        "CorpusId": 259257339}, "corpusId": 259257339, "publicationVenue": {"id":
        "768b87bb-8a18-4d9c-a161-4d483c776bcf", "name": "Computer Vision and Pattern
        Recognition", "type": "conference", "alternate_names": ["CVPR", "Comput Vis
        Pattern Recognit"], "issn": "1063-6919", "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
        "alternate_urls": ["https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"]},
        "url": "https://www.semanticscholar.org/paper/7f368c2255a3dbd1356c21af574f493b8e90174a",
        "title": "What You Can Reconstruct from a Shadow", "abstract": "3D reconstruction
        is a fundamental problem in computer vision, and the task is especially challenging
        when the object to reconstruct is partially or fully occluded. We introduce
        a method that uses the shadows cast by an unobserved object in order to infer
        the possible 3D volumes under occlusion. We create a differentiable image
        formation model that allows us to jointly infer the 3D shape of an object,
        its pose, and the position of a light source. Since the approach is end-to-end
        differentiable, we are able to integrate learned priors of object geometry
        in order to generate realistic 3D shapes of different object categories. Experiments
        and visualizations show that the method is able to generate multiple possible
        solutions that are consistent with the observation of the shadow. Our approach
        works even when the position of the light source and object pose are both
        unknown. Our approach is also robust to real-world images where ground-truth
        shadow mask is unknown.", "venue": "Computer Vision and Pattern Recognition",
        "year": 2023, "referenceCount": 48, "citationCount": 0, "influentialCitationCount":
        0, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
        "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2023-06-01",
        "journal": {"pages": "17059-17068", "name": "2023 IEEE/CVF Conference on Computer
        Vision and Pattern Recognition (CVPR)"}, "authors": [{"authorId": "2143183492",
        "name": "Ruoshi Liu"}, {"authorId": "46245898", "name": "Sachit Menon"}, {"authorId":
        "7700460", "name": "Chengzhi Mao"}, {"authorId": "37432086", "name": "Dennis
        Park"}, {"authorId": "2307158", "name": "Simon Stent"}, {"authorId": "1856025",
        "name": "Carl Vondrick"}]}, {"paperId": "dcd0410e49db0aeac786a4a8700ab27b91f50d52",
        "externalIds": {"ArXiv": "2309.05810", "DBLP": "journals/corr/abs-2309-05810",
        "DOI": "10.48550/arXiv.2309.05810", "CorpusId": 261696545}, "corpusId": 261696545,
        "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name":
        "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"},
        "url": "https://www.semanticscholar.org/paper/dcd0410e49db0aeac786a4a8700ab27b91f50d52",
        "title": "SHIFT3D: Synthesizing Hard Inputs For Tricking 3D Detectors", "abstract":
        "We present SHIFT3D, a differentiable pipeline for generating 3D shapes that
        are structurally plausible yet challenging to 3D object detectors. In safety-critical
        applications like autonomous driving, discovering such novel challenging objects
        can offer insight into unknown vulnerabilities of 3D detectors. By representing
        objects with a signed distanced function (SDF), we show that gradient error
        signals allow us to smoothly deform the shape or pose of a 3D object in order
        to confuse a downstream 3D detector. Importantly, the objects generated by
        SHIFT3D physically differ from the baseline object yet retain a semantically
        recognizable shape. Our approach provides interpretable failure modes for
        modern 3D object detectors, and can aid in preemptive discovery of potential
        safety risks within 3D perception systems before these risks become critical
        failures.", "venue": "arXiv.org", "year": 2023, "referenceCount": 33, "citationCount":
        0, "influentialCitationCount": 0, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
        "Computer Science", "source": "external"}, {"category": "Computer Science",
        "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
        "2023-09-11", "journal": {"volume": "abs/2309.05810", "name": "ArXiv"}, "authors":
        [{"authorId": "2239162474", "name": "Hongge Chen"}, {"authorId": "2239158293",
        "name": "Zhao Chen"}, {"authorId": "2239094232", "name": "Gregory P. Meyer"},
        {"authorId": "2239400681", "name": "Dennis Park"}, {"authorId": "1856025",
        "name": "Carl Vondrick"}, {"authorId": "2239093905", "name": "Ashish Shrivastava"},
        {"authorId": "2239094430", "name": "Yuning Chai"}]}, {"paperId": "02fff38be9c6caa03a0bfb0de61090971fe2c072",
        "externalIds": {"DBLP": "conf/acl/FuZCVR22", "ACL": "2022.acl-long.81", "ArXiv":
        "2203.00758", "DOI": "10.48550/arXiv.2203.00758", "CorpusId": 247218503},
        "corpusId": 247218503, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
        "name": "Annual Meeting of the Association for Computational Linguistics",
        "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc
        Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"},
        "url": "https://www.semanticscholar.org/paper/02fff38be9c6caa03a0bfb0de61090971fe2c072",
        "title": "There\u2019s a Time and Place for Reasoning Beyond the Image", "abstract":
        "Images are often more significant than only the pixels to human eyes, as
        we can infer, associate, and reason with contextual information from other
        sources to establish a more complete picture. For example, in Figure 1, we
        can find a way to identify the news articles related to the picture through
        segment-wise understandings of the signs, the buildings, the crowds, and more.
        This reasoning could provide the time and place the image was taken, which
        will help us in subsequent tasks, such as automatic storyline construction,
        correction of image source in intended effect photographs, and upper-stream
        processing such as image clustering for certain location or time.In this work,
        we formulate this problem and introduce TARA: a dataset with 16k images with
        their associated news, time, and location, automatically extracted from New
        York Times, and an additional 61k examples as distant supervision from WIT.
        On top of the extractions, we present a crowdsourced subset in which we believe
        it is possible to find the images\u2019 spatio-temporal information for evaluation
        purpose. We show that there exists a 70% gap between a state-of-the-art joint
        model and human performance, which is slightly filled by our proposed model
        that uses segment-wise reasoning, motivating higher-level vision-language
        joint models that can conduct open-ended reasoning with world knowledge.The
        data and code are publicly available at https://github.com/zeyofu/TARA.",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2022, "referenceCount": 25, "citationCount": 6, "influentialCitationCount":
        3, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
        "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2022-03-01",
        "journal": {"volume": "abs/2203.00758", "name": "ArXiv"}, "authors": [{"authorId":
        "2078360", "name": "Xingyu Fu"}, {"authorId": "2108536188", "name": "Ben Zhou"},
        {"authorId": "2146376367", "name": "I. Chandratreya"}, {"authorId": "1856025",
        "name": "Carl Vondrick"}, {"authorId": "144590225", "name": "D. Roth"}]},
        {"paperId": "16596dd03fa40ba278f9533ea9986982dcc81fb6", "externalIds": {"DBLP":
        "conf/iclr/MaoGYWV23", "ArXiv": "2212.07016", "DOI": "10.48550/arXiv.2212.07016",
        "CorpusId": 254636202}, "corpusId": 254636202, "publicationVenue": {"id":
        "939c6e1d-0d17-4d6e-8a82-66d960df0e40", "name": "International Conference
        on Learning Representations", "type": "conference", "alternate_names": ["Int
        Conf Learn Represent", "ICLR"], "url": "https://iclr.cc/"}, "url": "https://www.semanticscholar.org/paper/16596dd03fa40ba278f9533ea9986982dcc81fb6",
        "title": "Understanding Zero-Shot Adversarial Robustness for Large-Scale Models",
        "abstract": "Pretrained large-scale vision-language models like CLIP have
        exhibited strong generalization over unseen tasks. Yet imperceptible adversarial
        perturbations can significantly reduce CLIP''s performance on new tasks. In
        this work, we identify and explore the problem of \\emph{adapting large-scale
        models for zero-shot adversarial robustness}. We first identify two key factors
        during model adaption -- training losses and adaptation methods -- that affect
        the model''s zero-shot adversarial robustness. We then propose a text-guided
        contrastive adversarial training loss, which aligns the text embeddings and
        the adversarial visual features with contrastive learning on a small set of
        training data. We apply this training loss to two adaption methods, model
        finetuning and visual prompt tuning. We find that visual prompt tuning is
        more effective in the absence of texts, while finetuning wins in the existence
        of text guidance. Overall, our approach significantly improves the zero-shot
        adversarial robustness over CLIP, seeing an average improvement of over 31
        points over ImageNet and 15 zero-shot datasets. We hope this work can shed
        light on understanding the zero-shot adversarial robustness of large-scale
        models.", "venue": "International Conference on Learning Representations",
        "year": 2022, "referenceCount": 75, "citationCount": 13, "influentialCitationCount":
        1, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
        "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle"], "publicationDate": "2022-12-14", "journal":
        {"volume": "abs/2212.07016", "name": "ArXiv"}, "authors": [{"authorId": "7700460",
        "name": "Chengzhi Mao"}, {"authorId": "2187059392", "name": "Scott Geng"},
        {"authorId": "2110694456", "name": "Junfeng Yang"}, {"authorId": "48631993",
        "name": "Xin Eric Wang"}, {"authorId": "1856025", "name": "Carl Vondrick"}]},
        {"paperId": "17aaad12138347e3f1a40b6be69b892ad455a00c", "externalIds": {"DBLP":
        "journals/corr/abs-2212-00912", "ArXiv": "2212.00912", "DOI": "10.48550/arXiv.2212.00912",
        "CorpusId": 253380981}, "corpusId": 253380981, "publicationVenue": {"id":
        "d9720b90-d60b-48bc-9df8-87a30b9a60dd", "name": "Neural Information Processing
        Systems", "type": "conference", "alternate_names": ["Neural Inf Process Syst",
        "NeurIPS", "NIPS"], "url": "http://neurips.cc/"}, "url": "https://www.semanticscholar.org/paper/17aaad12138347e3f1a40b6be69b892ad455a00c",
        "title": "Private Multiparty Perception for Navigation", "abstract": "We introduce
        a framework for navigating through cluttered environments by connecting multiple
        cameras together while simultaneously preserving privacy. Occlusions and obstacles
        in large environments are often challenging situations for navigation agents
        because the environment is not fully observable from a single camera view.
        Given multiple camera views of an environment, our approach learns to produce
        a multiview scene representation that can only be used for navigation, provably
        preventing one party from inferring anything beyond the output task. On a
        new navigation dataset that we will publicly release, experiments show that
        private multiparty representations allow navigation through complex scenes
        and around obstacles while jointly preserving privacy. Our approach scales
        to an arbitrary number of camera viewpoints. We believe developing visual
        representations that preserve privacy is increasingly important for many applications
        such as navigation.", "venue": "Neural Information Processing Systems", "year":
        2022, "referenceCount": 40, "citationCount": 0, "influentialCitationCount":
        0, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
        "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle"], "publicationDate": "2022-12-02", "journal":
        {"volume": "abs/2212.00912", "name": "ArXiv"}, "authors": [{"authorId": "2187501149",
        "name": "Hui Lu"}, {"authorId": "2093237935", "name": "Mia Chiquier"}, {"authorId":
        "1856025", "name": "Carl Vondrick"}]}, {"paperId": "1c2a3eec0d09ff66266c4484e19fe279aedba3c0",
        "externalIds": {"DBLP": "journals/corr/abs-2210-01322", "ArXiv": "2210.01322",
        "DOI": "10.48550/arXiv.2210.01322", "CorpusId": 252693262}, "corpusId": 252693262,
        "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd", "name":
        "Neural Information Processing Systems", "type": "conference", "alternate_names":
        ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
        "url": "https://www.semanticscholar.org/paper/1c2a3eec0d09ff66266c4484e19fe279aedba3c0",
        "title": "Representing Spatial Trajectories as Distributions", "abstract":
        "We introduce a representation learning framework for spatial trajectories.
        We represent partial observations of trajectories as probability distributions
        in a learned latent space, which characterize the uncertainty about unobserved
        parts of the trajectory. Our framework allows us to obtain samples from a
        trajectory for any continuous point in time, both interpolating and extrapolating.
        Our flexible approach supports directly modifying specific attributes of a
        trajectory, such as its pace, as well as combining different partial observations
        into single representations. Experiments show our method''s advantage over
        baselines in prediction tasks.", "venue": "Neural Information Processing Systems",
        "year": 2022, "referenceCount": 64, "citationCount": 1, "influentialCitationCount":
        0, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
        "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle"], "publicationDate": "2022-10-04", "journal":
        {"volume": "abs/2210.01322", "name": "ArXiv"}, "authors": [{"authorId": "35552695",
        "name": "D''idac Sur''is"}, {"authorId": "1856025", "name": "Carl Vondrick"}]},
        {"paperId": "1f1fd049a174e521e417596946e64a37290ec251", "externalIds": {"DBLP":
        "journals/corr/abs-2211-11903", "ArXiv": "2211.11903", "DOI": "10.1109/CVPR52729.2023.02029",
        "CorpusId": 253761353}, "corpusId": 253761353, "publicationVenue": {"id":
        "768b87bb-8a18-4d9c-a161-4d483c776bcf", "name": "Computer Vision and Pattern
        Recognition", "type": "conference", "alternate_names": ["CVPR", "Comput Vis
        Pattern Recognit"], "issn": "1063-6919", "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
        "alternate_urls": ["https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"]},
        "url": "https://www.semanticscholar.org/paper/1f1fd049a174e521e417596946e64a37290ec251",
        "title": "FLEX: Full-Body Grasping Without Full-Body Grasps", "abstract":
        "Synthesizing 3D human avatars interacting realistically with a scene is an
        important problem with applications in AR/VR, video games, and robotics. Towards
        this goal, we address the task of generating a virtual human \u2013 hands
        and full body \u2013 grasping everyday objects. Existing methods approach
        this problem by collecting a 3D dataset of humans interacting with objects
        and training on this data. However, 1) these methods do not generalize to
        different object positions and orientations or to the presence of furniture
        in the scene, and 2) the diversity of their generated full-body poses is very
        limited. In this work, we address all the above challenges to generate realistic,
        diverse full-body grasps in everyday scenes without requiring any 3D full-body
        grasping data. Our key insight is to leverage the existence of both full-body
        pose and hand-grasping priors, composing them using 3D geometrical constraints
        to obtain full-body grasps. We empirically validate that these constraints
        can generate a variety of feasible human grasps that are superior to baselines
        both quantitatively and qualitatively. See our webpage for more details: flex.cs.columbia.edu.",
        "venue": "Computer Vision and Pattern Recognition", "year": 2022, "referenceCount":
        86, "citationCount": 8, "influentialCitationCount": 0, "isOpenAccess": true,
        "openAccessPdf": {"url": "https://arxiv.org/pdf/2211.11903", "status": null},
        "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer
        Science", "source": "external"}, {"category": "Computer Science", "source":
        "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate":
        "2022-11-22", "journal": {"pages": "21179-21189", "name": "2023 IEEE/CVF Conference
        on Computer Vision and Pattern Recognition (CVPR)"}, "authors": [{"authorId":
        "2163453087", "name": "Purva Tendulkar"}, {"authorId": "35552695", "name":
        "D''idac Sur''is"}, {"authorId": "1856025", "name": "Carl Vondrick"}]}, {"paperId":
        "326cbf7c9891dff720bd1f57bcbe6dc0b5d327a1", "externalIds": {"ArXiv": "2212.02978",
        "DBLP": "journals/corr/abs-2212-02978", "DOI": "10.48550/arXiv.2212.02978",
        "CorpusId": 254275276}, "corpusId": 254275276, "publicationVenue": {"id":
        "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names":
        ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/326cbf7c9891dff720bd1f57bcbe6dc0b5d327a1",
        "title": "Muscles in Action", "abstract": "Human motion is created by, and
        constrained by, our muscles. We take a first step at building computer vision
        methods that represent the internal muscle activity that causes motion. We
        present a new dataset, Muscles in Action (MIA), to learn to incorporate muscle
        activity into human motion representations. The dataset consists of 12.5 hours
        of synchronized video and surface electromyography (sEMG) data of 10 subjects
        performing various exercises. Using this dataset, we learn a bidirectional
        representation that predicts muscle activation from video, and conversely,
        reconstructs motion from muscle activation. We evaluate our model on in-distribution
        subjects and exercises, as well as on out-of-distribution subjects and exercises.
        We demonstrate how advances in modeling both modalities jointly can serve
        as conditioning for muscularly consistent motion generation. Putting muscles
        into computer vision systems will enable richer models of virtual humans,
        with applications in sports, fitness, and AR/VR.", "venue": "arXiv.org", "year":
        2022, "referenceCount": 75, "citationCount": 0, "influentialCitationCount":
        0, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science", "Biology"], "s2FieldsOfStudy": [{"category": "Computer Science",
        "source": "external"}, {"category": "Biology", "source": "external"}, {"category":
        "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
        "publicationDate": "2022-12-05", "journal": {"volume": "abs/2212.02978", "name":
        "ArXiv"}, "authors": [{"authorId": "2093237935", "name": "Mia Chiquier"},
        {"authorId": "1856025", "name": "Carl Vondrick"}]}, {"paperId": "384d527447f96c7efec56a9bd0a352f9a764ed63",
        "externalIds": {"DBLP": "journals/corr/abs-2212-07815", "ArXiv": "2212.07815",
        "DOI": "10.48550/arXiv.2212.07815", "CorpusId": 254685465}, "corpusId": 254685465,
        "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name":
        "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"},
        "url": "https://www.semanticscholar.org/paper/384d527447f96c7efec56a9bd0a352f9a764ed63",
        "title": "Adversarially Robust Video Perception by Seeing Motion", "abstract":
        "Despite their excellent performance, state-of-the-art computer vision models
        often fail when they encounter adversarial examples. Video perception models
        tend to be more fragile under attacks, because the adversary has more places
        to manipulate in high-dimensional data. In this paper, we find one reason
        for video models'' vulnerability is that they fail to perceive the correct
        motion under adversarial perturbations. Inspired by the extensive evidence
        that motion is a key factor for the human visual system, we propose to correct
        what the model sees by restoring the perceived motion information. Since motion
        information is an intrinsic structure of the video data, recovering motion
        signals can be done at inference time without any human annotation, which
        allows the model to adapt to unforeseen, worst-case inputs. Visualizations
        and empirical experiments on UCF-101 and HMDB-51 datasets show that restoring
        motion information in deep vision models improves adversarial robustness.
        Even under adaptive attacks where the adversary knows our defense, our algorithm
        is still effective. Our work provides new insight into robust video perception
        algorithms by using intrinsic structures from the data. Our webpage is available
        at https://motion4robust.cs.columbia.edu.", "venue": "arXiv.org", "year":
        2022, "referenceCount": 62, "citationCount": 1, "influentialCitationCount":
        0, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
        "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle"], "publicationDate": "2022-12-13", "journal":
        {"volume": "abs/2212.07815", "name": "ArXiv"}, "authors": [{"authorId": "2196168322",
        "name": "Lingyu Zhang"}, {"authorId": "7700460", "name": "Chengzhi Mao"},
        {"authorId": "2110694456", "name": "Junfeng Yang"}, {"authorId": "1856025",
        "name": "Carl Vondrick"}]}, {"paperId": "396833983f6d7d77957e12c3839e5da05feb053a",
        "externalIds": {"ACL": "2022.naacl-demo.7", "DOI": "10.18653/v1/2022.naacl-demo.7",
        "CorpusId": 249010869}, "corpusId": 249010869, "publicationVenue": {"id":
        "01103732-3808-4930-b8e4-7e9e68d5c68d", "name": "North American Chapter of
        the Association for Computational Linguistics", "type": "conference", "alternate_names":
        ["North Am Chapter Assoc Comput Linguistics", "NAACL"], "url": "https://www.aclweb.org/portal/naacl"},
        "url": "https://www.semanticscholar.org/paper/396833983f6d7d77957e12c3839e5da05feb053a",
        "title": "RESIN-11: Schema-guided Event Prediction for 11 Newsworthy Scenarios",
        "abstract": "We introduce RESIN-11, a new schema-guided event extraction&prediction
        framework that can be applied to a large variety of newsworthy scenarios.
        The framework consists of two parts: (1) an open-domain end-to-end multimedia
        multilingual information extraction system with weak-supervision and zero-shot
        learningbased techniques. (2) schema matching and schema-guided event prediction
        based on our curated schema library. We build a demo website based on our
        dockerized system and schema library publicly available for installation (https://github.com/RESIN-KAIROS/RESIN-11).
        We also include a video demonstrating the system.", "venue": "North American
        Chapter of the Association for Computational Linguistics", "year": 2022, "referenceCount":
        59, "citationCount": 13, "influentialCitationCount": 0, "isOpenAccess": true,
        "openAccessPdf": {"url": "https://aclanthology.org/2022.naacl-demo.7.pdf",
        "status": null}, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["Conference"],
        "publicationDate": null, "journal": {"name": "Proceedings of the 2022 Conference
        of the North American Chapter of the Association for Computational Linguistics:
        Human Language Technologies: System Demonstrations"}, "authors": [{"authorId":
        "13728923", "name": "Xinya Du"}, {"authorId": "2116461591", "name": "Zixuan
        Zhang"}, {"authorId": "2109154767", "name": "Sha Li"}, {"authorId": "1390880371",
        "name": "Ziqi Wang"}, {"authorId": "144808890", "name": "Pengfei Yu"}, {"authorId":
        "2108986414", "name": "Hongwei Wang"}, {"authorId": "145242558", "name": "T.
        Lai"}, {"authorId": "48030192", "name": "Xudong Lin"}, {"authorId": "2166112050",
        "name": "Iris Liu"}, {"authorId": "2108536188", "name": "Ben Zhou"}, {"authorId":
        "4428136", "name": "Haoyang Wen"}, {"authorId": "2118482058", "name": "Manling
        Li"}, {"authorId": "153060461", "name": "Darryl Hannan"}, {"authorId": "46665218",
        "name": "Jie Lei"}, {"authorId": "51270689", "name": "Hyounghun Kim"}, {"authorId":
        "3372941", "name": "Rotem Dror"}, {"authorId": "34269118", "name": "Haoyu
        Wang"}, {"authorId": "145666891", "name": "Michael Regan"}, {"authorId": "145653969",
        "name": "Qi Zeng"}, {"authorId": "1904906987", "name": "QING LYU"}, {"authorId":
        "2110963190", "name": "Charles Yu"}, {"authorId": "48870109", "name": "Carl
        N. Edwards"}, {"authorId": "2149111828", "name": "Xiaomeng Jin"}, {"authorId":
        "1381900594", "name": "Yizhu Jiao"}, {"authorId": "51203051", "name": "Ghazaleh
        Kazeminejad"}, {"authorId": "2052036545", "name": "Zhenhailong Wang"}, {"authorId":
        "1763608", "name": "Chris Callison-Burch"}, {"authorId": "1856025", "name":
        "Carl Vondrick"}, {"authorId": "143977268", "name": "Mohit Bansal"}, {"authorId":
        "144590225", "name": "D. Roth"}, {"authorId": "2111759643", "name": "Jiawei
        Han"}, {"authorId": "2122374530", "name": "Shih-Fu Chang"}, {"authorId": "145755155",
        "name": "Martha Palmer"}, {"authorId": "2113323573", "name": "Heng Ji"}]},
        {"paperId": "4b57f6eb0c1a69349dd3f446d114f2e8301bfcbe", "externalIds": {"DBLP":
        "conf/cvpr/SurisVRS22", "ArXiv": "2206.07148", "DOI": "10.1109/CVPR52688.2022.01031",
        "CorpusId": 249674596}, "corpusId": 249674596, "publicationVenue": {"id":
        "768b87bb-8a18-4d9c-a161-4d483c776bcf", "name": "Computer Vision and Pattern
        Recognition", "type": "conference", "alternate_names": ["CVPR", "Comput Vis
        Pattern Recognit"], "issn": "1063-6919", "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
        "alternate_urls": ["https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"]},
        "url": "https://www.semanticscholar.org/paper/4b57f6eb0c1a69349dd3f446d114f2e8301bfcbe",
        "title": "It''s Time for Artistic Correspondence in Music and Video", "abstract":
        "We present an approach for recommending a music track for a given video,
        and vice versa, based on both their temporal alignment and their correspondence
        at an artistic level. We propose a self-supervised approach that learns this
        correspondence directly from data, without any need of human annotations.
        In order to capture the high-level concepts that are required to solve the
        task, we propose modeling the long-term temporal context of both the video
        and the music signals, using Transformer networks for each modality. Experiments
        show that this approach strongly outperforms alternatives that do not exploit
        the temporal context. The combination of our contributions improve retrieval
        accuracy up to 10\u00d7 over prior state of the art. This strong improvement
        allows us to introduce a wide range of analyses and applications. For instance,
        we can condition music retrieval based on visually defined attributes.", "venue":
        "Computer Vision and Pattern Recognition", "year": 2022, "referenceCount":
        71, "citationCount": 20, "influentialCitationCount": 6, "isOpenAccess": true,
        "openAccessPdf": {"url": "https://arxiv.org/pdf/2206.07148", "status": null},
        "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer
        Science", "source": "external"}, {"category": "Computer Science", "source":
        "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate":
        "2022-06-01", "journal": {"pages": "10554-10564", "name": "2022 IEEE/CVF Conference
        on Computer Vision and Pattern Recognition (CVPR)"}, "authors": [{"authorId":
        "35399640", "name": "D\u00eddac Sur\u00eds"}, {"authorId": "1856025", "name":
        "Carl Vondrick"}, {"authorId": "145160921", "name": "Bryan C. Russell"}, {"authorId":
        "1786276", "name": "J. Salamon"}]}, {"paperId": "4eb5198062f78ecf844ff48bcaefe4c1c0f395cc",
        "externalIds": {"ArXiv": "2212.06202", "DBLP": "conf/cvpr/MaoTSMYWV23", "DOI":
        "10.1109/CVPR52729.2023.00267", "CorpusId": 254591376}, "corpusId": 254591376,
        "publicationVenue": {"id": "768b87bb-8a18-4d9c-a161-4d483c776bcf", "name":
        "Computer Vision and Pattern Recognition", "type": "conference", "alternate_names":
        ["CVPR", "Comput Vis Pattern Recognit"], "issn": "1063-6919", "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
        "alternate_urls": ["https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"]},
        "url": "https://www.semanticscholar.org/paper/4eb5198062f78ecf844ff48bcaefe4c1c0f395cc",
        "title": "Doubly Right Object Recognition: A Why Prompt for Visual Rationales",
        "abstract": "Many visual recognition models are evaluated only on their classification
        accuracy, a metric for which they obtain strong performance. In this paper,
        we investigate whether computer vision models can also provide correct rationales
        for their predictions. We propose a \u201cdoubly right\u201d object recognition
        benchmark, where the metric requires the model to simultaneously produce both
        the right labels as well as the right rationales. We find that state-of-the-art
        visual models, such as CLIP, often provide incorrect rationales for their
        categorical predictions. However, by transferring the rationales from language
        models into visual representations through a tailored dataset, we show that
        we can learn a \u201cwhy prompt,\u201d which adapts large visual representations
        to produce correct rationales. Visualizations and empirical experiments show
        that our prompts significantly improve performance on doubly right object
        recognition, in addition to zero-shot transfer to unseen tasks and datasets.",
        "venue": "Computer Vision and Pattern Recognition", "year": 2022, "referenceCount":
        67, "citationCount": 10, "influentialCitationCount": 1, "isOpenAccess": true,
        "openAccessPdf": {"url": "https://arxiv.org/pdf/2212.06202", "status": null},
        "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer
        Science", "source": "external"}, {"category": "Computer Science", "source":
        "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate":
        "2022-12-12", "journal": {"pages": "2722-2732", "name": "2023 IEEE/CVF Conference
        on Computer Vision and Pattern Recognition (CVPR)"}, "authors": [{"authorId":
        "7700460", "name": "Chengzhi Mao"}, {"authorId": "1396817932", "name": "Revant
        Teotia"}, {"authorId": "2196554360", "name": "Amrutha Sundar"}, {"authorId":
        "46245898", "name": "Sachit Menon"}, {"authorId": "2110694456", "name": "Junfeng
        Yang"}, {"authorId": "48631993", "name": "Xin Eric Wang"}, {"authorId": "1856025",
        "name": "Carl Vondrick"}]}, {"paperId": "571cfbddba05cd997c27dff0b0ae9b1e02f7eb17",
        "externalIds": {"ArXiv": "2206.08990", "DBLP": "journals/corr/abs-2206-08990",
        "DOI": "10.48550/arXiv.2206.08990", "CorpusId": 249889933}, "corpusId": 249889933,
        "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name":
        "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"},
        "url": "https://www.semanticscholar.org/paper/571cfbddba05cd997c27dff0b0ae9b1e02f7eb17",
        "title": "Shadows Shed Light on 3D Objects", "abstract": "3D reconstruction
        is a fundamental problem in computer vision, and the task is especially challenging
        when the object to reconstruct is partially or fully occluded. We introduce
        a method that uses the shadows cast by an unobserved object in order to infer
        the possible 3D volumes behind the occlusion. We create a differentiable image
        formation model that allows us to jointly infer the 3D shape of an object,
        its pose, and the position of a light source. Since the approach is end-to-end
        differentiable, we are able to integrate learned priors of object geometry
        in order to generate realistic 3D shapes of different object categories. Experiments
        and visualizations show that the method is able to generate multiple possible
        solutions that are consistent with the observation of the shadow. Our approach
        works even when the position of the light source and object pose are both
        unknown. Our approach is also robust to real-world images where ground-truth
        shadow mask is unknown.", "venue": "arXiv.org", "year": 2022, "referenceCount":
        43, "citationCount": 6, "influentialCitationCount": 0, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
        "publicationDate": "2022-06-17", "journal": {"volume": "abs/2206.08990", "name":
        "ArXiv"}, "authors": [{"authorId": "2143183492", "name": "Ruoshi Liu"}, {"authorId":
        "46245898", "name": "Sachit Menon"}, {"authorId": "7700460", "name": "Chengzhi
        Mao"}, {"authorId": "37432086", "name": "Dennis Park"}, {"authorId": "2307158",
        "name": "Simon Stent"}, {"authorId": "1856025", "name": "Carl Vondrick"}]},
        {"paperId": "5dad3748e8d4d8c659005903062e5d8e855fa86c", "externalIds": {"DBLP":
        "journals/corr/abs-2206-09027", "ArXiv": "2206.09027", "DOI": "10.48550/arXiv.2206.09027",
        "CorpusId": 249889676}, "corpusId": 249889676, "publicationVenue": {"id":
        "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names":
        ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/5dad3748e8d4d8c659005903062e5d8e855fa86c",
        "title": "Landscape Learning for Neural Network Inversion", "abstract": "Many
        machine learning methods operate by inverting a neural network at inference
        time, which has become a popular technique for solving inverse problems in
        computer vision, robotics, and graphics. However, these methods often involve
        gradient descent through a highly non-convex loss landscape, causing the optimization
        process to be unstable and slow. We introduce a method that learns a loss
        landscape where gradient descent is efficient, bringing massive improvement
        and acceleration to the inversion process. We demonstrate this advantage on
        a number of methods for both generative and discriminative tasks, including
        GAN inversion, adversarial defense, and 3D human pose reconstruction.", "venue":
        "arXiv.org", "year": 2022, "referenceCount": 63, "citationCount": 3, "influentialCitationCount":
        0, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
        "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle"], "publicationDate": "2022-06-17", "journal":
        {"volume": "abs/2206.09027", "name": "ArXiv"}, "authors": [{"authorId": "2143183492",
        "name": "Ruoshi Liu"}, {"authorId": "2075339580", "name": "Chen-Guang Mao"},
        {"authorId": "2163453087", "name": "Purva Tendulkar"}, {"authorId": "2359832",
        "name": "Hongya Wang"}, {"authorId": "1856025", "name": "Carl Vondrick"}]},
        {"paperId": "82d19ceba300875f108a91539ca555dfad142a99", "externalIds": {"ArXiv":
        "2212.06079", "DBLP": "conf/icml/MaoZJYWV23", "DOI": "10.48550/arXiv.2212.06079",
        "CorpusId": 254563940}, "corpusId": 254563940, "publicationVenue": {"id":
        "fc0a208c-acb7-47dc-a0d4-af8190e21d29", "name": "International Conference
        on Machine Learning", "type": "conference", "alternate_names": ["ICML", "Int
        Conf Mach Learn"], "url": "https://icml.cc/"}, "url": "https://www.semanticscholar.org/paper/82d19ceba300875f108a91539ca555dfad142a99",
        "title": "Robust Perception through Equivariance", "abstract": "Deep networks
        for computer vision are not reliable when they encounter adversarial examples.
        In this paper, we introduce a framework that uses the dense intrinsic constraints
        in natural images to robustify inference. By introducing constraints at inference
        time, we can shift the burden of robustness from training to the inference
        algorithm, thereby allowing the model to adjust dynamically to each individual
        image''s unique and potentially novel characteristics at inference time. Among
        different constraints, we find that equivariance-based constraints are most
        effective, because they allow dense constraints in the feature space without
        overly constraining the representation at a fine-grained level. Our theoretical
        results validate the importance of having such dense constraints at inference
        time. Our empirical experiments show that restoring feature equivariance at
        inference time defends against worst-case adversarial perturbations. The method
        obtains improved adversarial robustness on four datasets (ImageNet, Cityscapes,
        PASCAL VOC, and MS-COCO) on image recognition, semantic segmentation, and
        instance segmentation tasks. Project page is available at equi4robust.cs.columbia.edu.",
        "venue": "International Conference on Machine Learning", "year": 2022, "referenceCount":
        65, "citationCount": 4, "influentialCitationCount": 0, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
        "Conference"], "publicationDate": "2022-12-12", "journal": {"volume": "abs/2212.06079",
        "name": "ArXiv"}, "authors": [{"authorId": "7700460", "name": "Chengzhi Mao"},
        {"authorId": "2196168322", "name": "Lingyu Zhang"}, {"authorId": "2066732439",
        "name": "Abhishek Joshi"}, {"authorId": "2110694456", "name": "Junfeng Yang"},
        {"authorId": "2359832", "name": "Hongya Wang"}, {"authorId": "1856025", "name":
        "Carl Vondrick"}]}, {"paperId": "a42b091adaf29b06a092b67192ac07cb93312f2a",
        "externalIds": {"DBLP": "conf/iclr/MenonV23", "ArXiv": "2210.07183", "DOI":
        "10.48550/arXiv.2210.07183", "CorpusId": 252872997}, "corpusId": 252872997,
        "publicationVenue": {"id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40", "name":
        "International Conference on Learning Representations", "type": "conference",
        "alternate_names": ["Int Conf Learn Represent", "ICLR"], "url": "https://iclr.cc/"},
        "url": "https://www.semanticscholar.org/paper/a42b091adaf29b06a092b67192ac07cb93312f2a",
        "title": "Visual Classification via Description from Large Language Models",
        "abstract": "Vision-language models (VLMs) such as CLIP have shown promising
        performance on a variety of recognition tasks using the standard zero-shot
        classification procedure -- computing similarity between the query image and
        the embedded words for each category. By only using the category name, they
        neglect to make use of the rich context of additional information that language
        affords. The procedure gives no intermediate understanding of why a category
        is chosen, and furthermore provides no mechanism for adjusting the criteria
        used towards this decision. We present an alternative framework for classification
        with VLMs, which we call classification by description. We ask VLMs to check
        for descriptive features rather than broad categories: to find a tiger, look
        for its stripes; its claws; and more. By basing decisions on these descriptors,
        we can provide additional cues that encourage using the features we want to
        be used. In the process, we can get a clear idea of what features the model
        uses to construct its decision; it gains some level of inherent explainability.
        We query large language models (e.g., GPT-3) for these descriptors to obtain
        them in a scalable way. Extensive experiments show our framework has numerous
        advantages past interpretability. We show improvements in accuracy on ImageNet
        across distribution shifts; demonstrate the ability to adapt VLMs to recognize
        concepts unseen during training; and illustrate how descriptors can be edited
        to effectively mitigate bias compared to the baseline.", "venue": "International
        Conference on Learning Representations", "year": 2022, "referenceCount": 51,
        "citationCount": 63, "influentialCitationCount": 11, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
        "publicationDate": "2022-10-13", "journal": {"volume": "abs/2210.07183", "name":
        "ArXiv"}, "authors": [{"authorId": "46245898", "name": "Sachit Menon"}, {"authorId":
        "1856025", "name": "Carl Vondrick"}]}, {"paperId": "b400b066929e8070842b33b450fe69698c5ed826",
        "externalIds": {"DBLP": "journals/corr/abs-2204-12363", "ArXiv": "2204.12363",
        "DOI": "10.1109/CVPR52688.2022.00737", "CorpusId": 248392254}, "corpusId":
        248392254, "publicationVenue": {"id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
        "name": "Computer Vision and Pattern Recognition", "type": "conference", "alternate_names":
        ["CVPR", "Comput Vis Pattern Recognit"], "issn": "1063-6919", "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
        "alternate_urls": ["https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"]},
        "url": "https://www.semanticscholar.org/paper/b400b066929e8070842b33b450fe69698c5ed826",
        "title": "Causal Transportability for Visual Recognition", "abstract": "Visual
        representations underlie object recognition tasks, but they often contain
        both robust and non-robust features. Our main observation is that image classifiers
        may perform poorly on out-of-distribution samples because spurious correlations
        between non-robust features and labels can be changed in a new environment.
        By analyzing procedures for out-of-distribution generalization with a causal
        graph, we show that standard classifiers fail because the association between
        images and labels is not transportable across settings. However, we then show
        that the causal effect, which severs all sources of confounding, remains invariant
        across domains. This motivates us to develop an algorithm to estimate the
        causal effect for image classification, which is transportable (i.e., invariant)
        across source and target environments. Without observing additional variables,
        we show that we can derive an estimand for the causal effect under empirical
        assumptions using representations in deep models as proxies. Theoretical analysis,
        empirical results, and visualizations show that our approach captures causal
        invariances and improves overall generalization.", "venue": "Computer Vision
        and Pattern Recognition", "year": 2022, "referenceCount": 59, "citationCount":
        19, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf":
        {"url": "https://arxiv.org/pdf/2204.12363", "status": null}, "fieldsOfStudy":
        ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science",
        "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2022-04-26",
        "journal": {"pages": "7511-7521", "name": "2022 IEEE/CVF Conference on Computer
        Vision and Pattern Recognition (CVPR)"}, "authors": [{"authorId": "7700460",
        "name": "Chengzhi Mao"}, {"authorId": "2125225990", "name": "Kevin Xia"},
        {"authorId": "2163667482", "name": "James Wang"}, {"authorId": "2359832",
        "name": "Hongya Wang"}, {"authorId": "2110694456", "name": "Junfeng Yang"},
        {"authorId": "2778721", "name": "E. Bareinboim"}, {"authorId": "1856025",
        "name": "Carl Vondrick"}]}, {"paperId": "b4a2c2ae8bcefb389a7f9aeab38b90d6f4583fa5",
        "externalIds": {"ArXiv": "2204.10916", "DBLP": "journals/corr/abs-2204-10916",
        "DOI": "10.1109/CVPR52688.2022.00302", "CorpusId": 248377574}, "corpusId":
        248377574, "publicationVenue": {"id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
        "name": "Computer Vision and Pattern Recognition", "type": "conference", "alternate_names":
        ["CVPR", "Comput Vis Pattern Recognit"], "issn": "1063-6919", "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
        "alternate_urls": ["https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"]},
        "url": "https://www.semanticscholar.org/paper/b4a2c2ae8bcefb389a7f9aeab38b90d6f4583fa5",
        "title": "Revealing Occlusions with 4D Neural Fields", "abstract": "For computer
        vision systems to operate in dynamic situations, they need to be able to represent
        and reason about object permanence. We introduce a framework for learning
        to estimate 4D visual representations from monocular RGB-D video, which is
        able to persist objects, even once they become obstructed by occlusions. Unlike
        traditional video representations, we encode point clouds into a continuous
        representation, which permits the model to attend across the spatiotemporal
        context to resolve occlusions. On two large video datasets that we release
        along with this paper, our experiments show that the representation is able
        to successfully reveal occlusions for several tasks, without any architectural
        changes. Visualizations show that the attention mechanism automatically learns
        to follow occluded objects. Since our approach can be trained end-to-end and
        is easily adaptable, we believe it will be useful for handling occlusions
        in many video understanding tasks. Data, code, and models are available at
        occ1usions. cs. co1umbia. edu.", "venue": "Computer Vision and Pattern Recognition",
        "year": 2022, "referenceCount": 70, "citationCount": 7, "influentialCitationCount":
        0, "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/2204.10916",
        "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
        "Conference"], "publicationDate": "2022-04-22", "journal": {"pages": "3001-3011",
        "name": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition
        (CVPR)"}, "authors": [{"authorId": "1470838102", "name": "Basile Van Hoorick"},
        {"authorId": "2163453087", "name": "Purva Tendulkar"}, {"authorId": "35399640",
        "name": "D\u00eddac Sur\u00eds"}, {"authorId": "37432086", "name": "Dennis
        Park"}, {"authorId": "2307158", "name": "Simon Stent"}, {"authorId": "1856025",
        "name": "Carl Vondrick"}]}, {"paperId": "d857fa1e47283da17b72f8e59c377c7aa72bae78",
        "externalIds": {"DBLP": "journals/corr/abs-2212-04412", "ArXiv": "2212.04412",
        "DOI": "10.48550/arXiv.2212.04412", "CorpusId": 254408708}, "corpusId": 254408708,
        "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name":
        "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"},
        "url": "https://www.semanticscholar.org/paper/d857fa1e47283da17b72f8e59c377c7aa72bae78",
        "title": "Task Bias in Vision-Language Models", "abstract": "Incidental supervision
        from language has become a popular approach for learning generic visual representations
        that can be prompted to perform many recognition tasks in computer vision.
        We conduct an in-depth exploration of the CLIP model and show that its visual
        representation is often strongly biased towards solving some tasks more than
        others. Moreover, which task the representation will be biased towards is
        unpredictable, with little consistency across images. To resolve this task
        bias, we show how to learn a visual prompt that guides the representation
        towards features relevant to their task of interest. Our results show that
        these visual prompts can be independent of the input image and still effectively
        provide a conditioning mechanism to steer visual representations towards the
        desired task.", "venue": "arXiv.org", "year": 2022, "referenceCount": 37,
        "citationCount": 3, "influentialCitationCount": 0, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}, {"category": "Psychology", "source":
        "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
        "2022-12-08", "journal": {"volume": "abs/2212.04412", "name": "ArXiv"}, "authors":
        [{"authorId": "46245898", "name": "Sachit Menon"}, {"authorId": "2146376367",
        "name": "I. Chandratreya"}, {"authorId": "1856025", "name": "Carl Vondrick"}]},
        {"paperId": "ea6d6c92458bebb3adc39cf48dc976b462ee0794", "externalIds": {"CorpusId":
        252757932}, "corpusId": 252757932, "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/ea6d6c92458bebb3adc39cf48dc976b462ee0794",
        "title": "Finding Spuriously Correlated Visual Attributes", "abstract": "Deep
        neural models often learn to use spurious features in image datasets, which
        raises concerns when the models are deployed to critical applications, such
        as medical imaging. Identifying spurious features is essential to developing
        robust models. Existing methods to find spurious features do not give semantic
        meaning to the features and rely on human interpretation to decide if they
        are spurious or not. In this paper, we propose to find spurious visual attributes
        in the dataset. We first linearly transform the latent features into visual
        attributes and then learn correlations between the attributes and object classes
        by training a simple linear classifier. Correlated visual attributes are easily
        interpretable because they are in natural language having well defined meanings
        which makes it easier to find if they are spurious or not. Through visualizations
        and experiments, we show how to find spurious visual attributes, their ex-tent
        in existing dataset and failure mode examples showing negative impact of learned
        spurious correlations on out-of-distribution generalization.", "venue": "",
        "year": 2022, "referenceCount": 13, "citationCount": 0, "influentialCitationCount":
        0, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
        null, "publicationDate": null, "journal": null, "authors": [{"authorId": "1396817932",
        "name": "Revant Teotia"}, {"authorId": "7700460", "name": "Chengzhi Mao"},
        {"authorId": "1856025", "name": "Carl Vondrick"}]}, {"paperId": "f354354d32e820ce55f26f2cb6508599df8cc698",
        "externalIds": {"DBLP": "conf/uai/MenonBV22", "ArXiv": "2207.09535", "DOI":
        "10.48550/arXiv.2207.09535", "CorpusId": 249917642}, "corpusId": 249917642,
        "publicationVenue": {"id": "f9af8000-42f8-410d-a622-e8811e41660a", "name":
        "Conference on Uncertainty in Artificial Intelligence", "type": "conference",
        "alternate_names": ["Uncertainty in Artificial Intelligence", "UAI", "Conf
        Uncertain Artif Intell", "Uncertain Artif Intell"], "url": "http://www.auai.org/"},
        "url": "https://www.semanticscholar.org/paper/f354354d32e820ce55f26f2cb6508599df8cc698",
        "title": "Forget-me-not! Contrastive Critics for Mitigating Posterior Collapse",
        "abstract": "Variational autoencoders (VAEs) suffer from posterior collapse,
        where the powerful neural networks used for modeling and inference optimize
        the objective without meaningfully using the latent representation. We introduce
        inference critics that detect and incentivize against posterior collapse by
        requiring correspondence between latent variables and the observations. By
        connecting the critic''s objective to the literature in self-supervised contrastive
        representation learning, we show both theoretically and empirically that optimizing
        inference critics increases the mutual information between observations and
        latents, mitigating posterior collapse. This approach is straightforward to
        implement and requires significantly less training time than prior methods,
        yet obtains competitive results on three established datasets. Overall, the
        approach lays the foundation to bridge the previously disconnected frameworks
        of contrastive learning and probabilistic modeling with variational autoencoders,
        underscoring the benefits both communities may find at their intersection.",
        "venue": "Conference on Uncertainty in Artificial Intelligence", "year": 2022,
        "referenceCount": 50, "citationCount": 1, "influentialCitationCount": 1, "isOpenAccess":
        false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
        "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
        {"category": "Mathematics", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
        "Conference"], "publicationDate": "2022-07-19", "journal": {"volume": "abs/2207.09535",
        "name": "ArXiv"}, "authors": [{"authorId": "46245898", "name": "Sachit Menon"},
        {"authorId": "1796335", "name": "D. Blei"}, {"authorId": "1856025", "name":
        "Carl Vondrick"}]}, {"paperId": "039ce73659332c12168de439e3f79e7039b636af",
        "externalIds": {"ACL": "2021.naacl-demos.16", "DBLP": "conf/naacl/WenLLPLLZLWZYDW21",
        "DOI": "10.18653/v1/2021.naacl-demos.16", "CorpusId": 235097376}, "corpusId":
        235097376, "publicationVenue": {"id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
        "name": "North American Chapter of the Association for Computational Linguistics",
        "type": "conference", "alternate_names": ["North Am Chapter Assoc Comput Linguistics",
        "NAACL"], "url": "https://www.aclweb.org/portal/naacl"}, "url": "https://www.semanticscholar.org/paper/039ce73659332c12168de439e3f79e7039b636af",
        "title": "RESIN: A Dockerized Schema-Guided Cross-document Cross-lingual Cross-media
        Information Extraction and Event Tracking System", "abstract": "We present
        a new information extraction system that can automatically construct temporal
        event graphs from a collection of news documents from multiple sources, multiple
        languages (English and Spanish for our experiment), and multiple data modalities
        (speech, text, image and video). The system advances state-of-the-art from
        two aspects: (1) extending from sentence-level event extraction to cross-document
        cross-lingual cross-media event extraction, coreference resolution and temporal
        event tracking; (2) using human curated event schema library to match and
        enhance the extraction output. We have made the dockerlized system publicly
        available for research purpose at GitHub, with a demo video.", "venue": "North
        American Chapter of the Association for Computational Linguistics", "year":
        2021, "referenceCount": 64, "citationCount": 40, "influentialCitationCount":
        3, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.naacl-demos.16.pdf",
        "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
        "Conference"], "publicationDate": null, "journal": {"pages": "133-143"}, "authors":
        [{"authorId": "4428136", "name": "Haoyang Wen"}, {"authorId": "2117032681",
        "name": "Ying Lin"}, {"authorId": "145242558", "name": "T. Lai"}, {"authorId":
        "34741133", "name": "Xiaoman Pan"}, {"authorId": "2109154767", "name": "Sha
        Li"}, {"authorId": "48030192", "name": "Xudong Lin"}, {"authorId": "2108536188",
        "name": "Ben Zhou"}, {"authorId": "2118482058", "name": "Manling Li"}, {"authorId":
        "34269118", "name": "Haoyu Wang"}, {"authorId": "2111112132", "name": "Hongming
        Zhang"}, {"authorId": "3099583", "name": "Xiaodong Yu"}, {"authorId": "2101316346",
        "name": "Alexander Dong"}, {"authorId": "2108330537", "name": "Zhenhailong
        Wang"}, {"authorId": "51135899", "name": "Y. Fung"}, {"authorId": "51234098",
        "name": "Piyush Mishra"}, {"authorId": "1904906987", "name": "QING LYU"},
        {"authorId": "35399640", "name": "D\u00eddac Sur\u00eds"}, {"authorId": "2108342501",
        "name": "Brian Chen"}, {"authorId": "1783500", "name": "S. Brown"}, {"authorId":
        "145755155", "name": "Martha Palmer"}, {"authorId": "1763608", "name": "Chris
        Callison-Burch"}, {"authorId": "1856025", "name": "Carl Vondrick"}, {"authorId":
        "153034701", "name": "Jiawei Han"}, {"authorId": "144590225", "name": "D.
        Roth"}, {"authorId": "9546964", "name": "Shih-Fu Chang"}, {"authorId": "144016781",
        "name": "Heng Ji"}]}, {"paperId": "0fb956fe2bc1272ed53fecb47060c0c3dcff739c",
        "externalIds": {"ArXiv": "2112.10194", "DBLP": "journals/corr/abs-2112-10194",
        "DOI": "10.1109/CVPR52688.2022.01340", "CorpusId": 245335320}, "corpusId":
        245335320, "publicationVenue": {"id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
        "name": "Computer Vision and Pattern Recognition", "type": "conference", "alternate_names":
        ["CVPR", "Comput Vis Pattern Recognit"], "issn": "1063-6919", "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
        "alternate_urls": ["https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"]},
        "url": "https://www.semanticscholar.org/paper/0fb956fe2bc1272ed53fecb47060c0c3dcff739c",
        "title": "UnweaveNet: Unweaving Activity Stories", "abstract": "Our lives
        can be seen as a complex weaving of activities; we switch from one activity
        to another, to maximise our achievements or in reaction to demands placed
        upon us. Observing a video of unscripted daily activities, we parse the video
        into its constituent activity threads through a process we call unweaving.
        To accomplish this, we introduce a video representation explicitly capturing
        activity threads called a thread bank, along with a neural controller capable
        of detecting goal changes and resuming of past activities, together forming
        UnweaveNet. We train and evaluate UnweaveNet on sequences from the unscripted
        egocentric dataset EPIC-KITCHENS. We propose and showcase the efficacy of
        pretraining UnweaveNet in a self-supervised manner.", "venue": "Computer Vision
        and Pattern Recognition", "year": 2021, "referenceCount": 53, "citationCount":
        8, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url":
        "https://arxiv.org/pdf/2112.10194", "status": null}, "fieldsOfStudy": ["Computer
        Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
        "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-12-19",
        "journal": {"pages": "13760-13769", "name": "2022 IEEE/CVF Conference on Computer
        Vision and Pattern Recognition (CVPR)"}, "authors": [{"authorId": "50065546",
        "name": "Will Price"}, {"authorId": "1856025", "name": "Carl Vondrick"}, {"authorId":
        "145089978", "name": "D. Damen"}]}, {"paperId": "357411070c0779e6c29db11532e690db2bb8a64c",
        "externalIds": {"DBLP": "conf/aaai/BoultGPSHAJADLC21", "DOI": "10.1609/aaai.v35i17.17766",
        "CorpusId": 235363448}, "corpusId": 235363448, "publicationVenue": {"id":
        "bdc2e585-4e48-4e36-8af1-6d859763d405", "name": "AAAI Conference on Artificial
        Intelligence", "type": "conference", "alternate_names": ["National Conference
        on Artificial Intelligence", "National Conf Artif Intell", "AAAI Conf Artif
        Intell", "AAAI"], "url": "http://www.aaai.org/"}, "url": "https://www.semanticscholar.org/paper/357411070c0779e6c29db11532e690db2bb8a64c",
        "title": "Towards a Unifying Framework for Formal Theories of Novelty", "abstract":
        "Managing inputs that are novel, unknown, or out-of-distribution is critical
        as an agent moves from the lab to the open world. Novelty-related problems
        include being tolerant to novel perturbations of the normal input, detecting
        when the input includes novel items, and adapting to novel inputs. While significant
        research has been undertaken in these areas, a noticeable gap exists in the
        lack of a formalized definition of novelty that transcends problem domains.
        As a team of researchers spanning multiple research groups and different domains,
        we have seen, first hand, the difficulties that arise from ill-specified novelty
        problems, as well as inconsistent definitions and terminology. Therefore,
        we present the first unified framework for formal theories of novelty and
        use the framework to formally define a family of novelty types. Our framework
        can be applied across a wide range of domains, from symbolic AI to reinforcement
        learning, and beyond to open world image recognition. Thus, it can be used
        to help kick-start new research efforts and accelerate ongoing work on these
        important novelty-related problems.", "venue": "AAAI Conference on Artificial
        Intelligence", "year": 2021, "referenceCount": 9, "citationCount": 31, "influentialCitationCount":
        2, "isOpenAccess": true, "openAccessPdf": {"url": "https://ojs.aaai.org/index.php/AAAI/article/download/17766/17573",
        "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
        "Conference"], "publicationDate": "2021-05-18", "journal": {"pages": "15047-15052"},
        "authors": [{"authorId": "32163276", "name": "T. Boult"}, {"authorId": "1907673",
        "name": "Przemyslaw A. Grabowicz"}, {"authorId": "1395598766", "name": "D.
        Prijatelj"}, {"authorId": "37300633", "name": "Roni Stern"}, {"authorId":
        "1726417", "name": "L. Holder"}, {"authorId": "3280194", "name": "J. Alspector"},
        {"authorId": "8021024", "name": "Mohsen Jafarzadeh"}, {"authorId": "122774749",
        "name": "T. Ahmad"}, {"authorId": "24021418", "name": "A. Dhamija"}, {"authorId":
        "2109426091", "name": "Chunchun Li"}, {"authorId": "2066063009", "name": "S.
        Cruz"}, {"authorId": "1781242", "name": "Abhinav Shrivastava"}, {"authorId":
        "1856025", "name": "Carl Vondrick"}, {"authorId": "2613438", "name": "W. Scheirer"}]},
        {"paperId": "42908b31098fd6760fc35e23e77c9c6d3b03e4f9", "externalIds": {"DBLP":
        "journals/corr/abs-2111-06389", "ArXiv": "2111.06389", "CorpusId": 243985830},
        "corpusId": 243985830, "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
        "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url":
        "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/42908b31098fd6760fc35e23e77c9c6d3b03e4f9",
        "title": "Full-Body Visual Self-Modeling of Robot Morphologies", "abstract":
        "Internal computational models of physical bodies are fundamental to the ability
        of robots and animals alike to plan and control their actions. These\"self-models\"allow
        robots to consider outcomes of multiple possible future actions, without trying
        them out in physical reality. Recent progress in fully data-driven self-modeling
        has enabled machines to learn their own forward kinematics directly from task-agnostic
        interaction data. However, forward-kinema\\-tics models can only predict limited
        aspects of the morphology, such as the position of end effectors or velocity
        of joints and masses. A key challenge is to model the entire morphology and
        kinematics, without prior knowledge of what aspects of the morphology will
        be relevant to future tasks. Here, we propose that instead of directly modeling
        forward-kinematics, a more useful form of self-modeling is one that could
        answer space occupancy queries, conditioned on the robot''s state. Such query-driven
        self models are continuous in the spatial domain, memory efficient, fully
        differentiable and kinematic aware. In physical experiments, we demonstrate
        how a visual self-model is accurate to about one percent of the workspace,
        enabling the robot to perform various motion planning and control tasks. Visual
        self-modeling can also allow the robot to detect, localize and recover from
        real-world damage, leading to improved machine resiliency. Our project website
        is at: https://robot-morphology.cs.columbia.edu/", "venue": "arXiv.org", "year":
        2021, "referenceCount": 32, "citationCount": 12, "influentialCitationCount":
        0, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science", "Engineering"], "s2FieldsOfStudy": [{"category": "Computer Science",
        "source": "external"}, {"category": "Engineering", "source": "external"},
        {"category": "Biology", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
        "publicationDate": "2021-11-11", "journal": {"volume": "abs/2111.06389", "name":
        "ArXiv"}, "authors": [{"authorId": "8786274", "name": "Boyuan Chen"}, {"authorId":
        "1420209643", "name": "Robert Kwiatkowski"}, {"authorId": "1856025", "name":
        "Carl Vondrick"}, {"authorId": "1747909", "name": "Hod Lipson"}]}, {"paperId":
        "4e2dfe2b54bcd5d5c8178aca868959568298f0c8", "externalIds": {"DBLP": "journals/corr/abs-2112-07076",
        "ArXiv": "2112.07076", "CorpusId": 245131182}, "corpusId": 245131182, "publicationVenue":
        {"id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40", "name": "International Conference
        on Learning Representations", "type": "conference", "alternate_names": ["Int
        Conf Learn Represent", "ICLR"], "url": "https://iclr.cc/"}, "url": "https://www.semanticscholar.org/paper/4e2dfe2b54bcd5d5c8178aca868959568298f0c8",
        "title": "Real-Time Neural Voice Camouflage", "abstract": "Automatic speech
        recognition systems have created exciting possibilities for applications,
        however they also enable opportunities for systematic eavesdropping. We propose
        a method to camouflage a person''s voice over-the-air from these systems without
        inconveniencing the conversation between people in the room. Standard adversarial
        attacks are not effective in real-time streaming situations because the characteristics
        of the signal will have changed by the time the attack is executed. We introduce
        predictive attacks, which achieve real-time performance by forecasting the
        attack that will be the most effective in the future. Under real-time constraints,
        our method jams the established speech recognition system DeepSpeech 3.9x
        more than baselines as measured through word error rate, and 6.6x more as
        measured through character error rate. We furthermore demonstrate our approach
        is practically effective in realistic environments over physical distances.",
        "venue": "International Conference on Learning Representations", "year": 2021,
        "referenceCount": 42, "citationCount": 4, "influentialCitationCount": 1, "isOpenAccess":
        false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Engineering"],
        "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
        {"category": "Engineering", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
        "publicationDate": "2021-12-14", "journal": {"volume": "abs/2112.07076", "name":
        "ArXiv"}, "authors": [{"authorId": "2093237935", "name": "Mia Chiquier"},
        {"authorId": "7700460", "name": "Chengzhi Mao"}, {"authorId": "1856025", "name":
        "Carl Vondrick"}]}, {"paperId": "601ab36b6f077ff57472f4a0cf2e061dd05b9b85",
        "externalIds": {"ArXiv": "2111.10493", "DBLP": "journals/corr/abs-2111-10493",
        "CorpusId": 244478155}, "corpusId": 244478155, "publicationVenue": {"id":
        "939c6e1d-0d17-4d6e-8a82-66d960df0e40", "name": "International Conference
        on Learning Representations", "type": "conference", "alternate_names": ["Int
        Conf Learn Represent", "ICLR"], "url": "https://iclr.cc/"}, "url": "https://www.semanticscholar.org/paper/601ab36b6f077ff57472f4a0cf2e061dd05b9b85",
        "title": "Discrete Representations Strengthen Vision Transformer Robustness",
        "abstract": "Vision Transformer (ViT) is emerging as the state-of-the-art
        architecture for image recognition. While recent studies suggest that ViTs
        are more robust than their convolutional counterparts, our experiments find
        that ViTs trained on ImageNet are overly reliant on local textures and fail
        to make adequate use of shape information. ViTs thus have difficulties generalizing
        to out-of-distribution, real-world data. To address this deficiency, we present
        a simple and effective architecture modification to ViT''s input layer by
        adding discrete tokens produced by a vector-quantized encoder. Different from
        the standard continuous pixel tokens, discrete tokens are invariant under
        small perturbations and contain less information individually, which promote
        ViTs to learn global information that is invariant. Experimental results demonstrate
        that adding discrete representation on four architecture variants strengthens
        ViT robustness by up to 12% across seven ImageNet robustness benchmarks while
        maintaining the performance on ImageNet.", "venue": "International Conference
        on Learning Representations", "year": 2021, "referenceCount": 52, "citationCount":
        25, "influentialCitationCount": 1, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
        "Computer Science", "source": "external"}, {"category": "Computer Science",
        "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
        "2021-11-20", "journal": {"volume": "abs/2111.10493", "name": "ArXiv"}, "authors":
        [{"authorId": "7700460", "name": "Chengzhi Mao"}, {"authorId": "39978626",
        "name": "Lu Jiang"}, {"authorId": "3226635", "name": "Mostafa Dehghani"},
        {"authorId": "1856025", "name": "Carl Vondrick"}, {"authorId": "1694199",
        "name": "R. Sukthankar"}, {"authorId": "145955800", "name": "Irfan Essa"}]},
        {"paperId": "868381eea54ec0b8a96fb525e22836cd756a08b7", "externalIds": {"DOI":
        "10.1038/s41598-020-77918-x", "CorpusId": 257043656}, "corpusId": 257043656,
        "publicationVenue": {"id": "f99f77b7-b1b6-44d3-984a-f288e9884b9b", "name":
        "Scientific Reports", "type": "journal", "alternate_names": ["Sci Rep"], "issn":
        "2045-2322", "url": "http://www.nature.com/srep/", "alternate_urls": ["http://www.nature.com/srep/index.html"]},
        "url": "https://www.semanticscholar.org/paper/868381eea54ec0b8a96fb525e22836cd756a08b7",
        "title": "Visual behavior modelling for robotic theory of mind", "abstract":
        null, "venue": "Scientific Reports", "year": 2021, "referenceCount": 0, "citationCount":
        0, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url":
        "https://www.nature.com/articles/s41598-020-77918-x.pdf", "status": null},
        "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category": "Psychology", "source":
        "s2-fos-model"}], "publicationTypes": null, "publicationDate": "2021-01-11",
        "journal": {"volume": "11", "name": "Scientific Reports"}, "authors": [{"authorId":
        "8786274", "name": "Boyuan Chen"}, {"authorId": "1856025", "name": "Carl Vondrick"},
        {"authorId": "51022452", "name": "H. Lipson"}]}, {"paperId": "8ce65937232b5083f0e9da47ce1d6220bc385c49",
        "externalIds": {"ArXiv": "2101.01600", "MAG": "3173655578", "DBLP": "journals/corr/abs-2101-01600",
        "DOI": "10.1109/CVPR46437.2021.01242", "CorpusId": 230524097}, "corpusId":
        230524097, "publicationVenue": {"id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
        "name": "Computer Vision and Pattern Recognition", "type": "conference", "alternate_names":
        ["CVPR", "Comput Vis Pattern Recognit"], "issn": "1063-6919", "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
        "alternate_urls": ["https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"]},
        "url": "https://www.semanticscholar.org/paper/8ce65937232b5083f0e9da47ce1d6220bc385c49",
        "title": "Learning the Predictability of the Future", "abstract": "We introduce
        a framework for learning from unlabeled video what is predictable in the future.
        Instead of committing up front to features to predict, our approach learns
        from data which features are predictable. Based on the observation that hyperbolic
        geometry naturally and compactly encodes hierarchical structure, we propose
        a predictive model in hyperbolic space. When the model is most confident,
        it will predict at a concrete level of the hierarchy, but when the model is
        not confident, it learns to automatically select a higher level of abstraction.
        Experiments on two established datasets show the key role of hierarchical
        representations for action prediction. Although our representation is trained
        with unlabeled video, visualizations show that action hierarchies emerge in
        the representation.", "venue": "Computer Vision and Pattern Recognition",
        "year": 2021, "referenceCount": 81, "citationCount": 46, "influentialCitationCount":
        8, "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/2101.01600",
        "status": null}, "fieldsOfStudy": ["Computer Science", "Engineering"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Engineering",
        "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-01-01",
        "journal": {"pages": "12602-12612", "name": "2021 IEEE/CVF Conference on Computer
        Vision and Pattern Recognition (CVPR)"}, "authors": [{"authorId": "35552695",
        "name": "D''idac Sur''is"}, {"authorId": "2143183492", "name": "Ruoshi Liu"},
        {"authorId": "1856025", "name": "Carl Vondrick"}]}, {"paperId": "9fab147d338fe2b7fa3ffb6ecefed75bed7cf824",
        "externalIds": {"PubMedCentral": "7801744", "DOI": "10.1038/s41598-020-77918-x",
        "CorpusId": 231580621, "PubMed": "33431917"}, "corpusId": 231580621, "publicationVenue":
        {"id": "f99f77b7-b1b6-44d3-984a-f288e9884b9b", "name": "Scientific Reports",
        "type": "journal", "alternate_names": ["Sci Rep"], "issn": "2045-2322", "url":
        "http://www.nature.com/srep/", "alternate_urls": ["http://www.nature.com/srep/index.html"]},
        "url": "https://www.semanticscholar.org/paper/9fab147d338fe2b7fa3ffb6ecefed75bed7cf824",
        "title": "Visual behavior modelling for robotic theory of mind", "abstract":
        null, "venue": "Scientific Reports", "year": 2021, "referenceCount": 62, "citationCount":
        13, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf":
        {"url": "https://www.nature.com/articles/s41598-020-77918-x.pdf", "status":
        null}, "fieldsOfStudy": ["Medicine"], "s2FieldsOfStudy": [{"category": "Medicine",
        "source": "external"}, {"category": "Psychology", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle"], "publicationDate": "2021-01-11", "journal":
        {"volume": "11", "name": "Scientific Reports"}, "authors": [{"authorId": "8786274",
        "name": "Boyuan Chen"}, {"authorId": "1856025", "name": "Carl Vondrick"},
        {"authorId": "1747909", "name": "Hod Lipson"}]}, {"paperId": "d59fb7b76578e725d3179aa236ba8a26c5e7b844",
        "externalIds": {"ArXiv": "2103.14222", "DBLP": "journals/corr/abs-2103-14222",
        "DOI": "10.1109/ICCV48922.2021.00070", "CorpusId": 232380311}, "corpusId":
        232380311, "publicationVenue": {"id": "7654260e-79f9-45c5-9663-d72027cf88f3",
        "name": "IEEE International Conference on Computer Vision", "type": "conference",
        "alternate_names": ["ICCV", "IEEE Int Conf Comput Vis", "ICCV Workshops",
        "ICCV Work"], "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"},
        "url": "https://www.semanticscholar.org/paper/d59fb7b76578e725d3179aa236ba8a26c5e7b844",
        "title": "Adversarial Attacks are Reversible with Natural Supervision", "abstract":
        "We find that images contain intrinsic structure that enables the reversal
        of many adversarial attacks. Attack vectors cause not only image classifiers
        to fail, but also collaterally disrupt incidental structure in the image.
        We demonstrate that modifying the attacked image to restore the natural structure
        will reverse many types of attacks, providing a defense. Experiments demonstrate
        significantly improved robustness for several state-of-the-art models across
        the CIFAR-10, CIFAR-100, SVHN, and ImageNet datasets. Our results show that
        our defense is still effective even if the attacker is aware of the defense
        mechanism. Since our defense is deployed during inference instead of training,
        it is compatible with pre-trained networks as well as most other defenses.
        Our results suggest deep networks are vulnerable to adversarial examples partly
        because their representations do not enforce the natural structure of images.",
        "venue": "IEEE International Conference on Computer Vision", "year": 2021,
        "referenceCount": 71, "citationCount": 33, "influentialCitationCount": 4,
        "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/2103.14222",
        "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
        "Conference"], "publicationDate": "2021-03-26", "journal": {"pages": "641-651",
        "name": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)"},
        "authors": [{"authorId": "7700460", "name": "Chengzhi Mao"}, {"authorId":
        "2060209971", "name": "Mia Chiquer"}, {"authorId": "2144220750", "name": "Hao
        Wang"}, {"authorId": "152211006", "name": "Junfeng Yang"}, {"authorId": "1856025",
        "name": "Carl Vondrick"}]}, {"paperId": "d666e22aa939a262bcf2a7985bcc3eb16dcb85e4",
        "externalIds": {"ArXiv": "2105.08052", "DBLP": "journals/corr/abs-2105-08052",
        "CorpusId": 234742488}, "corpusId": 234742488, "publicationVenue": {"id":
        "fbfbf10a-faa4-4d2a-85be-3ac660454ce3", "name": "Conference on Robot Learning",
        "type": "conference", "alternate_names": ["CoRL", "Conf Robot Learn"]}, "url":
        "https://www.semanticscholar.org/paper/d666e22aa939a262bcf2a7985bcc3eb16dcb85e4",
        "title": "The Boombox: Visual Reconstruction from Acoustic Vibrations", "abstract":
        "Interacting with bins and containers is a fundamental task in robotics, making
        state estimation of the objects inside the bin critical. While robots often
        use cameras for state estimation, the visual modality is not always ideal
        due to occlusions and poor illumination. We introduce The Boombox, a container
        that uses sound to estimate the state of the contents inside a box. Based
        on the observation that the collision between objects and its containers will
        cause an acoustic vibration, we present a convolutional network for learning
        to reconstruct visual scenes. Although we use low-cost and low-power contact
        microphones to detect the vibrations, our results show that learning from
        multimodal data enables state estimation from affordable audio sensors. Due
        to the many ways that robots use containers, we believe the box will have
        a number of applications in robotics. Our project website is at: boombox.cs.columbia.edu",
        "venue": "Conference on Robot Learning", "year": 2021, "referenceCount": 55,
        "citationCount": 4, "influentialCitationCount": 0, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Engineering"],
        "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
        {"category": "Engineering", "source": "external"}, {"category": "Physics",
        "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
        "2021-05-17", "journal": {"volume": "abs/2105.08052", "name": "ArXiv"}, "authors":
        [{"authorId": "8786274", "name": "Boyuan Chen"}, {"authorId": "2093237935",
        "name": "Mia Chiquier"}, {"authorId": "1747909", "name": "Hod Lipson"}, {"authorId":
        "1856025", "name": "Carl Vondrick"}]}, {"paperId": "1193fc121f9be24a5da562745dd3a07a1e1a7269",
        "externalIds": {"DBLP": "conf/cvpr/EpsteinV21", "MAG": "3112785400", "DOI":
        "10.1109/CVPR46437.2021.01104", "CorpusId": 229285731}, "corpusId": 229285731,
        "publicationVenue": {"id": "768b87bb-8a18-4d9c-a161-4d483c776bcf", "name":
        "Computer Vision and Pattern Recognition", "type": "conference", "alternate_names":
        ["CVPR", "Comput Vis Pattern Recognit"], "issn": "1063-6919", "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
        "alternate_urls": ["https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"]},
        "url": "https://www.semanticscholar.org/paper/1193fc121f9be24a5da562745dd3a07a1e1a7269",
        "title": "Learning Goals from Failure", "abstract": "We introduce a framework
        that predicts the goals behind observable human action in video. Motivated
        by evidence in developmental psychology, we leverage video of unintentional
        action to learn video representations of goals without direct supervision.
        Our approach models videos as contextual trajectories that represent both
        low-level motion and high-level action features. Experiments and visualizations
        show our trained model is able to predict the underlying goals in video of
        unintentional action. We also propose a method to \"automatically correct\"
        unintentional action by leveraging gradient signals of our model to adjust
        latent trajectories. Although the model is trained with minimal supervision,
        it is competitive with or outperforms baselines trained on large (supervised)
        datasets of successfully executed goals, showing that observing unintentional
        action is crucial to learning about goals in video.", "venue": "Computer Vision
        and Pattern Recognition", "year": 2020, "referenceCount": 67, "citationCount":
        7, "influentialCitationCount": 1, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
        "Computer Science", "source": "external"}, {"category": "Computer Science",
        "source": "s2-fos-model"}, {"category": "Psychology", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-12-16",
        "journal": {"pages": "11189-11199", "name": "2021 IEEE/CVF Conference on Computer
        Vision and Pattern Recognition (CVPR)"}, "authors": [{"authorId": "32486555",
        "name": "Dave Epstein"}, {"authorId": "1856025", "name": "Carl Vondrick"}]},
        {"paperId": "33c723f096c3fd156e32295325afc2e6081afac4", "externalIds": {"MAG":
        "3048450692", "DBLP": "journals/corr/abs-2008-05596", "ArXiv": "2008.05596",
        "DOI": "10.1007/978-3-030-58523-5_2", "CorpusId": 221112090}, "corpusId":
        221112090, "publicationVenue": {"id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
        "name": "European Conference on Computer Vision", "type": "conference", "alternate_names":
        ["ECCV", "Eur Conf Comput Vis"], "url": "https://link.springer.com/conference/eccv"},
        "url": "https://www.semanticscholar.org/paper/33c723f096c3fd156e32295325afc2e6081afac4",
        "title": "We Have So Much In Common: Modeling Semantic Relational Set Abstractions
        in Videos", "abstract": null, "venue": "European Conference on Computer Vision",
        "year": 2020, "referenceCount": 43, "citationCount": 8, "influentialCitationCount":
        0, "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/2008.05596",
        "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
        "Conference"], "publicationDate": "2020-08-12", "journal": {"volume": "abs/2008.05596",
        "name": "ArXiv"}, "authors": [{"authorId": "50112310", "name": "A. Andonian"},
        {"authorId": "1482544048", "name": "Camilo Luciano Fosco"}, {"authorId": "95743023",
        "name": "Mathew Monfort"}, {"authorId": "2132497979", "name": "Allen Lee"},
        {"authorId": "1723233", "name": "R. Feris"}, {"authorId": "1856025", "name":
        "Carl Vondrick"}, {"authorId": "143868587", "name": "A. Oliva"}]}, {"paperId":
        "5f4cb14efef53e82b116f999b829c108104e8670", "externalIds": {"DBLP": "journals/corr/abs-2011-11831",
        "ArXiv": "2011.11831", "MAG": "3108140514", "DOI": "10.1109/ICCV48922.2021.00960",
        "CorpusId": 227151523}, "corpusId": 227151523, "publicationVenue": {"id":
        "7654260e-79f9-45c5-9663-d72027cf88f3", "name": "IEEE International Conference
        on Computer Vision", "type": "conference", "alternate_names": ["ICCV", "IEEE
        Int Conf Comput Vis", "ICCV Workshops", "ICCV Work"], "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"},
        "url": "https://www.semanticscholar.org/paper/5f4cb14efef53e82b116f999b829c108104e8670",
        "title": "Dissecting Image Crops", "abstract": "The elementary operation of
        cropping underpins nearly every computer vision system, ranging from data
        augmentation and translation invariance to computational photography and representation
        learning. This paper investigates the subtle traces introduced by this operation.
        For example, despite refinements to camera optics, lenses will leave behind
        certain clues, notably chromatic aberration and vignetting. Photographers
        also leave behind other clues relating to image aesthetics and scene composition.
        We study how to detect these traces, and investigate the impact that cropping
        has on the image distribution. While our aim is to dissect the fundamental
        impact of spatial crops, there are also a number of practical implications
        to our work, such as revealing faulty photojournalism and equipping neural
        network researchers with a better understanding of shortcut learning. Code
        is available at https://github.com/basilevh/dissecting-image-crops.", "venue":
        "IEEE International Conference on Computer Vision", "year": 2020, "referenceCount":
        62, "citationCount": 9, "influentialCitationCount": 3, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
        "Conference"], "publicationDate": "2020-11-24", "journal": {"pages": "9721-9730",
        "name": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)"},
        "authors": [{"authorId": "1470838102", "name": "Basile Van Hoorick"}, {"authorId":
        "1856025", "name": "Carl Vondrick"}]}, {"paperId": "8320ea909c38a616f9daccff4e5a49cfce4d9735",
        "externalIds": {"ArXiv": "2007.11668", "DBLP": "journals/corr/abs-2007-11668",
        "MAG": "3043845016", "CorpusId": 220713182}, "corpusId": 220713182, "publicationVenue":
        {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names":
        ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/8320ea909c38a616f9daccff4e5a49cfce4d9735",
        "title": "Analogical Reasoning for Visually Grounded Language Acquisition",
        "abstract": "Children acquire language subconsciously by observing the surrounding
        world and listening to descriptions. They can discover the meaning of words
        even without explicit language knowledge, and generalize to novel compositions
        effortlessly. In this paper, we bring this ability to AI, by studying the
        task of Visually grounded Language Acquisition (VLA). We propose a multimodal
        transformer model augmented with a novel mechanism for analogical reasoning,
        which approximates novel compositions by learning semantic mapping and reasoning
        operations from previously seen compositions. Our proposed method, Analogical
        Reasoning Transformer Networks (ARTNet), is trained on raw multimedia data
        (video frames and transcripts), and after observing a set of compositions
        such as \"washing apple\" or \"cutting carrot\", it can generalize and recognize
        new compositions in new video frames, such as \"washing carrot\" or \"cutting
        apple\". To this end, ARTNet refers to relevant instances in the training
        data and uses their visual features and captions to establish analogies with
        the query image. Then it chooses the suitable verb and noun to create a new
        composition that describes the new image best. Extensive experiments on an
        instructional video dataset demonstrate that the proposed method achieves
        significantly better generalization capability and recognition accuracy compared
        to state-of-the-art transformer models.", "venue": "arXiv.org", "year": 2020,
        "referenceCount": 60, "citationCount": 6, "influentialCitationCount": 0, "isOpenAccess":
        false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
        "publicationDate": "2020-07-22", "journal": {"volume": "abs/2007.11668", "name":
        "ArXiv"}, "authors": [{"authorId": "1993581583", "name": "Bo Wu"}, {"authorId":
        "71660806", "name": "Haoyu Qin"}, {"authorId": "2778637", "name": "Alireza
        Zareian"}, {"authorId": "1856025", "name": "Carl Vondrick"}, {"authorId":
        "9546964", "name": "Shih-Fu Chang"}]}, {"paperId": "945aa2eb4b7ceecebf0562dfc12fcadb8fd38970",
        "externalIds": {"DBLP": "journals/corr/abs-2012-12265", "ArXiv": "2012.12265",
        "DOI": "10.1109/CVPR46437.2021.00394", "CorpusId": 229363762}, "corpusId":
        229363762, "publicationVenue": {"id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
        "name": "Computer Vision and Pattern Recognition", "type": "conference", "alternate_names":
        ["CVPR", "Comput Vis Pattern Recognit"], "issn": "1063-6919", "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
        "alternate_urls": ["https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"]},
        "url": "https://www.semanticscholar.org/paper/945aa2eb4b7ceecebf0562dfc12fcadb8fd38970",
        "title": "Generative Interventions for Causal Learning", "abstract": "We introduce
        a framework for learning robust visual representations that generalize to
        new viewpoints, backgrounds, and scene contexts. Discriminative models often
        learn naturally occurring spurious correlations, which cause them to fail
        on images outside of the training distribution. In this paper, we show that
        we can steer generative models to manufacture interventions on features caused
        by confounding factors. Experiments, visualizations, and theoretical results
        show this method learns robust representations more consistent with the underlying
        causal relationships. Our approach improves performance on multiple datasets
        demanding out-of-distribution generalization, and we demonstrate state-of-the-art
        performance generalizing from ImageNet to ObjectNet dataset.", "venue": "Computer
        Vision and Pattern Recognition", "year": 2020, "referenceCount": 54, "citationCount":
        43, "influentialCitationCount": 5, "isOpenAccess": true, "openAccessPdf":
        {"url": "https://arxiv.org/pdf/2012.12265", "status": null}, "fieldsOfStudy":
        ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science",
        "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-12-22",
        "journal": {"pages": "3946-3955", "name": "2021 IEEE/CVF Conference on Computer
        Vision and Pattern Recognition (CVPR)"}, "authors": [{"authorId": "7700460",
        "name": "Chengzhi Mao"}, {"authorId": "1813517446", "name": "Amogh Gupta"},
        {"authorId": "51380124", "name": "Augustine Cha"}, {"authorId": "2359832",
        "name": "Hongya Wang"}, {"authorId": "152211006", "name": "Junfeng Yang"},
        {"authorId": "1856025", "name": "Carl Vondrick"}]}, {"paperId": "9a6dd28c5449ddcad7b079c0dca6ea6a518c3eb0",
        "externalIds": {"ArXiv": "2006.15657", "MAG": "3037985126", "DBLP": "journals/corr/abs-2006-15657",
        "CorpusId": 220250263}, "corpusId": 220250263, "publicationVenue": {"id":
        "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names":
        ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/9a6dd28c5449ddcad7b079c0dca6ea6a518c3eb0",
        "title": "Video Representations of Goals Emerge from Watching Failure", "abstract":
        "We introduce a video representation learning framework that models the latent
        goals behind observable human action. Motivated by how children learn to reason
        about goals and intentions by experiencing failure, we leverage unconstrained
        video of unintentional action to learn without direct supervision. Our approach
        models videos as contextual trajectories that represent both low-level motion
        and high-level action features. Experiments and visualizations show the model
        is able to predict underlying goals, detect when action switches from intentional
        to unintentional, and automatically correct unintentional action. Although
        the model is trained with minimal supervision, it is competitive with highly-supervised
        baselines, underscoring the role of failure examples for learning goal-oriented
        video representations. The project website is available at this https URL",
        "venue": "arXiv.org", "year": 2020, "referenceCount": 56, "citationCount":
        3, "influentialCitationCount": 0, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
        "Computer Science", "source": "external"}, {"category": "Computer Science",
        "source": "s2-fos-model"}, {"category": "Psychology", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle"], "publicationDate": "2020-06-28", "journal":
        {"volume": "abs/2006.15657", "name": "ArXiv"}, "authors": [{"authorId": "32486555",
        "name": "Dave Epstein"}, {"authorId": "1856025", "name": "Carl Vondrick"}]},
        {"paperId": "cc8a78b43b5e14643dbf8991b585314cdee6a341", "externalIds": {"CorpusId":
        219411507}, "corpusId": 219411507, "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/cc8a78b43b5e14643dbf8991b585314cdee6a341",
        "title": "What\u2019s Missing From Self-Supervised Representation Learning?",
        "abstract": "Despite tremendous effort to train visual recognition systems
        without human supervision, there is still no substitute for large, labeled
        training datasets. We perform a large-scale analysis to quantitatively understand
        the difference between the representations learned by self-supervised learning
        and supervised learning. Adopting a large collection of trained models for
        different computer vision tasks, we probe for functional similarities between
        visual recognition systems. Experiments and visualizations suggest that two
        key differences between self-supervised and supervised models are its representations
        for 3D geometry and deformable objects, which also substantially contribute
        to its failures. Our hope is that such analysis will expose future research
        directions in self-supervised visual learning.", "venue": "", "year": 2020,
        "referenceCount": 44, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess":
        false, "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category":
        "Computer Science", "source": "s2-fos-model"}, {"category": "Psychology",
        "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": null,
        "journal": null, "authors": [{"authorId": "32486555", "name": "Dave Epstein"},
        {"authorId": "2118897254", "name": "Yiliang Shi"}, {"authorId": "2056843177",
        "name": "Eugene Wu"}, {"authorId": "1856025", "name": "Carl Vondrick"}]},
        {"paperId": "ec19b41534219677864c473a379067d18b3c0908", "externalIds": {"DBLP":
        "conf/cvpr/SurisEV22", "ArXiv": "2012.04631", "DOI": "10.1109/CVPR52688.2022.01598",
        "CorpusId": 247594717}, "corpusId": 247594717, "publicationVenue": {"id":
        "768b87bb-8a18-4d9c-a161-4d483c776bcf", "name": "Computer Vision and Pattern
        Recognition", "type": "conference", "alternate_names": ["CVPR", "Comput Vis
        Pattern Recognit"], "issn": "1063-6919", "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
        "alternate_urls": ["https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"]},
        "url": "https://www.semanticscholar.org/paper/ec19b41534219677864c473a379067d18b3c0908",
        "title": "Globetrotter: Connecting Languages by Connecting Images", "abstract":
        "Machine translation between many languages at once is highly challenging,
        since training with ground truth re-quires supervision between all language
        pairs, which is dif-ficult to obtain. Our key insight is that, while languages
        may vary drastically, the underlying visual appearance of the world remains
        consistent. We introduce a method that uses visual observations to bridge
        the gap between languages, rather than relying on parallel corpora or topo-logical
        properties of the representations. We train a model that aligns segments of
        text from different languages if and only if the images associated with them
        are similar and each image in turn is well-aligned with its textual description.
        We train our model from scratch on a new dataset of text in over fifty languages
        with accompanying images. Experiments show that our method outperforms previous
        work on unsupervised word and sentence translation using retrieval. Code,
        models and data are available on globetrotter.cs.columbia.edu", "venue": "Computer
        Vision and Pattern Recognition", "year": 2020, "referenceCount": 56, "citationCount":
        4, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url":
        "https://arxiv.org/pdf/2012.04631", "status": null}, "fieldsOfStudy": ["Computer
        Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
        "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-12-08",
        "journal": {"pages": "16453-16463", "name": "2022 IEEE/CVF Conference on Computer
        Vision and Pattern Recognition (CVPR)"}, "authors": [{"authorId": "35552695",
        "name": "D''idac Sur''is"}, {"authorId": "32486555", "name": "Dave Epstein"},
        {"authorId": "1856025", "name": "Carl Vondrick"}]}, {"paperId": "f1085830042fdc07961f4bf4a7e6ff60cd534fd9",
        "externalIds": {"DBLP": "journals/corr/abs-2012-04631", "MAG": "3111865597",
        "CorpusId": 227736819}, "corpusId": 227736819, "publicationVenue": {"id":
        "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names":
        ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/f1085830042fdc07961f4bf4a7e6ff60cd534fd9",
        "title": "Globetrotter: Unsupervised Multilingual Translation from Visual
        Alignment", "abstract": "Multi-language machine translation without parallel
        corpora is challenging because there is no explicit supervision between languages.
        Existing unsupervised methods typically rely on topological properties of
        the language representations. We introduce a framework that instead uses the
        visual modality to align multiple languages, using images as the bridge between
        them. We estimate the cross-modal alignment between language and images, and
        use this estimate to guide the learning of cross-lingual representations.
        Our language representations are trained jointly in one model with a single
        stage. Experiments with fifty-two languages show that our method outperforms
        baselines on unsupervised word-level and sentence-level translation using
        retrieval.", "venue": "arXiv.org", "year": 2020, "referenceCount": 54, "citationCount":
        9, "influentialCitationCount": 3, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
        "Computer Science", "source": "external"}, {"category": "Computer Science",
        "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
        "2020-12-08", "journal": {"volume": "abs/2012.04631", "name": "ArXiv"}, "authors":
        [{"authorId": "35552695", "name": "D''idac Sur''is"}, {"authorId": "32486555",
        "name": "Dave Epstein"}, {"authorId": "1856025", "name": "Carl Vondrick"}]},
        {"paperId": "f1d16d4a122e3a6d6db5e959dad03055e3955444", "externalIds": {"DBLP":
        "conf/nips/0001WIVZ20", "ArXiv": "2010.12013", "MAG": "3094040572", "CorpusId":
        225062163}, "corpusId": 225062163, "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
        "name": "Neural Information Processing Systems", "type": "conference", "alternate_names":
        ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
        "url": "https://www.semanticscholar.org/paper/f1d16d4a122e3a6d6db5e959dad03055e3955444",
        "title": "Listening to Sounds of Silence for Speech Denoising", "abstract":
        "We introduce a deep learning model for speech denoising, a long-standing
        challenge in audio analysis arising in numerous applications. Our approach
        is based on a key observation about human speech: there is often a short pause
        between each sentence or word. In a recorded speech signal, those pauses introduce
        a series of time periods during which only noise is present. We leverage these
        incidental silent intervals to learn a model for automatic speech denoising
        given only mono-channel audio. Detected silent intervals over time expose
        not just pure noise but its time-varying features, allowing the model to learn
        noise dynamics and suppress it from the speech signal. Experiments on multiple
        datasets confirm the pivotal role of silent interval detection for speech
        denoising, and our method outperforms several state-of-the-art denoising methods,
        including those that accept only audio input (like ours) and those that denoise
        based on audiovisual input (and hence require more information). We also show
        that our method enjoys excellent generalization properties, such as denoising
        spoken languages not seen during training.", "venue": "Neural Information
        Processing Systems", "year": 2020, "referenceCount": 109, "citationCount":
        23, "influentialCitationCount": 1, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": ["Computer Science", "Psychology", "Engineering"],
        "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
        {"category": "Psychology", "source": "external"}, {"category": "Engineering",
        "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle"], "publicationDate": "2020-10-22", "journal":
        {"volume": "abs/2010.12013", "name": "ArXiv"}, "authors": [{"authorId": "47462777",
        "name": "Ruilin Xu"}, {"authorId": "1406236938", "name": "Rundi Wu"}, {"authorId":
        "2018840", "name": "Y. Ishiwaka"}, {"authorId": "1856025", "name": "Carl Vondrick"},
        {"authorId": "39294084", "name": "Changxi Zheng"}]}, {"paperId": "f2d1cbb25b1c58b89b6a8140f775493817f73752",
        "externalIds": {"MAG": "3111255553", "DBLP": "journals/corr/abs-2012-04226",
        "ArXiv": "2012.04226", "CorpusId": 227744935}, "corpusId": 227744935, "publicationVenue":
        {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names":
        ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/f2d1cbb25b1c58b89b6a8140f775493817f73752",
        "title": "A Unifying Framework for Formal Theories of Novelty: Framework,
        Examples and Discussion", "abstract": "Managing inputs that are novel, unknown,
        or out-of-distribution is critical as an agent moves from the lab to the open
        world. Novelty-related problems include being tolerant to novel perturbations
        of the normal input, detecting when the input includes novel items, and adapting
        to novel inputs. While significant research has been undertaken in these areas,
        a noticeable gap exists in the lack of a formalized definition of novelty
        that transcends problem domains. As a team of researchers spanning multiple
        research groups and different domains, we have seen, first hand, the difficulties
        that arise from ill-specified novelty problems, as well as inconsistent definitions
        and terminology. Therefore, we present the first unified framework for formal
        theories of novelty and use the framework to formally define a family of novelty
        types. Our framework can be applied across a wide range of domains, from symbolic
        AI to reinforcement learning, and beyond to open world image recognition.
        Thus, it can be used to help kick-start new research efforts and accelerate
        ongoing work on these important novelty-related problems. This extended version
        of our AAAI 2021 paper included more details and examples in multiple domains.",
        "venue": "arXiv.org", "year": 2020, "referenceCount": 9, "citationCount":
        10, "influentialCitationCount": 0, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
        "Computer Science", "source": "external"}, {"category": "Computer Science",
        "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
        "2020-12-08", "journal": {"volume": "abs/2012.04226", "name": "ArXiv"}, "authors":
        [{"authorId": "32163276", "name": "T. Boult"}, {"authorId": "1907673", "name":
        "Przemyslaw A. Grabowicz"}, {"authorId": "1395598766", "name": "D. Prijatelj"},
        {"authorId": "21458831", "name": "R. Stern"}, {"authorId": "144468194", "name":
        "L. Holder"}, {"authorId": "3280194", "name": "J. Alspector"}, {"authorId":
        "8021024", "name": "Mohsen Jafarzadeh"}, {"authorId": "122774749", "name":
        "T. Ahmad"}, {"authorId": "24021418", "name": "A. Dhamija"}, {"authorId":
        "146455613", "name": "C.Li"}, {"authorId": "2066063009", "name": "S. Cruz"},
        {"authorId": "2075394544", "name": "A. Shrivastava"}, {"authorId": "1856025",
        "name": "Carl Vondrick"}, {"authorId": "2613438", "name": "W. Scheirer"}]},
        {"paperId": "fb1166ada6c98d4c6abed81d9995eb515c95f341", "externalIds": {"CorpusId":
        228095096}, "corpusId": 228095096, "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/fb1166ada6c98d4c6abed81d9995eb515c95f341",
        "title": "Formalizing Novelty", "abstract": "Managing inputs that are novel,
        unknown, or out-ofdistribution is critical as an agent moves from the lab
        to the open world. Novelty-related problems include being tolerant to novel
        perturbations of the normal input, detecting when the input includes novel
        items, and adapting to novel inputs. While significant research has been undertaken
        in these areas, a noticeable gap exists in the lack of a formalized definition
        of novelty that transcends problem domains. As a team of researchers spanning
        multiple research groups and different domains, we have seen, first hand,
        the difficulties that arise from ill-specified novelty problems, as well as
        inconsistent definitions and terminology. Therefore, we present the first
        unified framework for formal theories of novelty and use the framework to
        formally define a family of novelty types. Our framework can be applied across
        a wide range of domains, from symbolic AI to reinforcement learning, and beyond
        to open world image recognition. Thus, it can be used to help kick-start new
        research efforts and accelerate ongoing work on these important novelty-related
        problems.", "venue": "", "year": 2020, "referenceCount": 9, "citationCount":
        0, "influentialCitationCount": 0, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category": "Computer Science",
        "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": null,
        "journal": null, "authors": [{"authorId": "32163276", "name": "T. Boult"},
        {"authorId": "1907673", "name": "Przemyslaw A. Grabowicz"}, {"authorId": "1395598766",
        "name": "D. Prijatelj"}, {"authorId": "21458831", "name": "R. Stern"}, {"authorId":
        "144468194", "name": "L. Holder"}, {"authorId": "3280194", "name": "J. Alspector"},
        {"authorId": "8021024", "name": "Mohsen Jafarzadeh"}, {"authorId": "122774749",
        "name": "T. Ahmad"}, {"authorId": "24021418", "name": "A. Dhamija"}, {"authorId":
        "2266223", "name": "C. Li"}, {"authorId": "2066063009", "name": "S. Cruz"},
        {"authorId": "2075394544", "name": "A. Shrivastava"}, {"authorId": "1856025",
        "name": "Carl Vondrick"}, {"authorId": "2613438", "name": "W. Scheirer"}]},
        {"paperId": "007ca8ca7a68451c32da034c72a06238434843c1", "externalIds": {"DBLP":
        "conf/eccv/SurisEJCV20", "MAG": "3010976655", "DOI": "10.1007/978-3-030-58526-6_26",
        "CorpusId": 215907940}, "corpusId": 215907940, "publicationVenue": {"id":
        "167fa0ca-e88a-4ef7-a16f-bc66c457c806", "name": "European Conference on Computer
        Vision", "type": "conference", "alternate_names": ["ECCV", "Eur Conf Comput
        Vis"], "url": "https://link.springer.com/conference/eccv"}, "url": "https://www.semanticscholar.org/paper/007ca8ca7a68451c32da034c72a06238434843c1",
        "title": "Learning to Learn Words from Visual Scenes", "abstract": null, "venue":
        "European Conference on Computer Vision", "year": 2019, "referenceCount":
        69, "citationCount": 10, "influentialCitationCount": 1, "isOpenAccess": true,
        "openAccessPdf": {"url": "https://arxiv.org/pdf/1911.11237", "status": null},
        "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer
        Science", "source": "external"}, {"category": "Computer Science", "source":
        "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate":
        "2019-11-25", "journal": {"pages": "434-452"}, "authors": [{"authorId": "35399640",
        "name": "D\u00eddac Sur\u00eds"}, {"authorId": "32486555", "name": "Dave Epstein"},
        {"authorId": "2113323573", "name": "Heng Ji"}, {"authorId": "9546964", "name":
        "Shih-Fu Chang"}, {"authorId": "1856025", "name": "Carl Vondrick"}]}, {"paperId":
        "1178245776864b2c5286992e61822e50bf772220", "externalIds": {"ArXiv": "1911.11237",
        "DBLP": "journals/corr/abs-1911-11237", "MAG": "2990129575", "CorpusId": 208291181},
        "corpusId": 208291181, "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
        "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url":
        "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/1178245776864b2c5286992e61822e50bf772220",
        "title": "Learning to Learn Words from Narrated Video", "abstract": "When
        we travel, we often encounter new scenarios we have never experienced before,
        with new sights and new words that describe them. We can use our language-learning
        ability to quickly learn these new words and correlate them with the visual
        world. In contrast, language models often do not robustly generalize to novel
        words and compositions. We propose a framework that learns how to learn text
        representations from visual context. Experiments show that our approach significantly
        outperforms the state-of-the-art in visual language modeling for acquiring
        new words and predicting new compositions. Model ablations and visualizations
        suggest that the visual modality helps our approach more robustly generalize
        at these tasks. Project webpage is available at https://expert.cs.columbia.edu/",
        "venue": "arXiv.org", "year": 2019, "referenceCount": 55, "citationCount":
        5, "influentialCitationCount": 0, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
        "Computer Science", "source": "external"}, {"category": "Computer Science",
        "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
        "2019-11-25", "journal": {"volume": "abs/1911.11237", "name": "ArXiv"}, "authors":
        [{"authorId": "35552695", "name": "D''idac Sur''is"}, {"authorId": "32486555",
        "name": "Dave Epstein"}, {"authorId": "2113323573", "name": "Heng Ji"}, {"authorId":
        "9546964", "name": "Shih-Fu Chang"}, {"authorId": "1856025", "name": "Carl
        Vondrick"}]}, {"paperId": "1657065a6f3933a8fcfa7ab99b8bcafb0e610e2d", "externalIds":
        {"MAG": "2964433888", "DBLP": "journals/sigops/PeiWTWVCRJY19", "DOI": "10.1145/3352020.3352030",
        "CorpusId": 198986596}, "corpusId": 198986596, "publicationVenue": {"id":
        "b9083d6f-6a0b-458f-b91b-ac82b9afdf3c", "name": "ACM SIGOPS Operating Systems
        Review", "type": "journal", "alternate_names": ["ACM SIGOPS Oper Syst Rev",
        "Operating Systems Review", "Oper Syst Rev"], "issn": "0163-5980", "url":
        "http://portal.acm.org/sigops/newsletter"}, "url": "https://www.semanticscholar.org/paper/1657065a6f3933a8fcfa7ab99b8bcafb0e610e2d",
        "title": "Bringing Engineering Rigor to Deep Learning", "abstract": "Deep
        learning (DL) systems are increasingly deployed in safety- and security-critical
        domains including autonomous driving, robotics, and malware detection, where
        the correctness and predictability of a system on corner-case inputs are of
        great importance. Unfortunately, the common practice to validating a deep
        neural network (DNN) - measuring overall accuracy on a randomly selected test
        set - is not designed to surface corner-case errors. As recent work shows,
        even DNNs with state-of-the-art accuracy are easily fooled by human-imperceptible,
        adversarial perturbations to the inputs. Questions such as how to test corner-case
        behaviors more thoroughly and whether all adversarial samples have been found
        remain unanswered. In the last few years, we have been working on bringing
        more engineering rigor into deep learning. Towards this goal, we have built
        five systems to test DNNs more thoroughly and verify the absence of adversarial
        samples for given datasets. These systems check a broad spectrum of properties
        (e.g., rotating an image should never change its classification) and find
        thousands of error-inducing samples for popular DNNs in critical domains (e.g.,
        ImageNet, autonomous driving, and malware detection). Our DNN verifiers are
        also orders of magnitude (e.g., 5,000\u00d7) faster than similar tools. This
        article overviews our systems and discusses three open research challenges
        to hopefully inspire more future research towards testing and verifying DNNs.",
        "venue": "ACM SIGOPS Operating Systems Review", "year": 2019, "referenceCount":
        85, "citationCount": 4, "influentialCitationCount": 0, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
        "Review"], "publicationDate": "2019-07-25", "journal": {"volume": "53", "pages":
        "59 - 67", "name": "ACM SIGOPS Operating Systems Review"}, "authors": [{"authorId":
        "40428350", "name": "Kexin Pei"}, {"authorId": "51257183", "name": "Shiqi
        Wang"}, {"authorId": "22472831", "name": "Yuchi Tian"}, {"authorId": "2042956146",
        "name": "J. Whitehouse"}, {"authorId": "1856025", "name": "Carl Vondrick"},
        {"authorId": "3139121", "name": "Yinzhi Cao"}, {"authorId": "31631000", "name":
        "Baishakhi Ray"}, {"authorId": "39400201", "name": "S. Jana"}, {"authorId":
        "152211006", "name": "Junfeng Yang"}]}, {"paperId": "2324d55ec54b9a12c4ac5353c51bcfa8440f7b6a",
        "externalIds": {"DBLP": "conf/cvpr/EpsteinCV20", "ArXiv": "1911.11206", "MAG":
        "3034630387", "DOI": "10.1109/cvpr42600.2020.00100", "CorpusId": 208291335},
        "corpusId": 208291335, "publicationVenue": {"id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
        "name": "Computer Vision and Pattern Recognition", "type": "conference", "alternate_names":
        ["CVPR", "Comput Vis Pattern Recognit"], "issn": "1063-6919", "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
        "alternate_urls": ["https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"]},
        "url": "https://www.semanticscholar.org/paper/2324d55ec54b9a12c4ac5353c51bcfa8440f7b6a",
        "title": "Oops! Predicting Unintentional Action in Video", "abstract": "From
        just a short glance at a video, we can often tell whether a person''s action
        is intentional or not. Can we train a model to recognize this? We introduce
        a dataset of in-the-wild videos of unintentional action, as well as a suite
        of tasks for recognizing, localizing, and anticipating its onset. We train
        a supervised neural network as a baseline and analyze its performance compared
        to human consistency on the tasks. We also investigate self-supervised representations
        that leverage natural signals in our dataset, and show the effectiveness of
        an approach that uses the intrinsic speed of video to perform competitively
        with highly-supervised pretraining. However, a significant gap between machine
        and human performance remains.", "venue": "Computer Vision and Pattern Recognition",
        "year": 2019, "referenceCount": 86, "citationCount": 71, "influentialCitationCount":
        13, "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/1911.11206",
        "status": null}, "fieldsOfStudy": ["Computer Science", "Engineering"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Engineering",
        "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-11-25",
        "journal": {"pages": "916-926", "name": "2020 IEEE/CVF Conference on Computer
        Vision and Pattern Recognition (CVPR)"}, "authors": [{"authorId": "32486555",
        "name": "Dave Epstein"}, {"authorId": "8786274", "name": "Boyuan Chen"}, {"authorId":
        "1856025", "name": "Carl Vondrick"}]}, {"paperId": "2af4c764352d911641e96c4a52093b2fdeba6a61",
        "externalIds": {"MAG": "3042776260", "DBLP": "conf/isalalife/ChenSLV20", "ArXiv":
        "1910.07882", "DOI": "10.1162/isal_a_00269", "CorpusId": 204744022}, "corpusId":
        204744022, "publicationVenue": {"id": "43397d64-0997-46ca-8396-7ca707887a5c",
        "name": "IEEE Symposium on Artificial Life", "type": "conference", "alternate_names":
        ["ALIFE", "Workshop on the Synthesis and Simulation of Living Systems", "Workshop
        Synth Simul Living Syst", "IEEE Symp Artif Life"]}, "url": "https://www.semanticscholar.org/paper/2af4c764352d911641e96c4a52093b2fdeba6a61",
        "title": "Visual Hide and Seek", "abstract": "We train embodied agents to
        play Visual Hide and Seek where a prey must navigate in a simulated environment
        in order to avoid capture from a predator. We place a variety of obstacles
        in the environment for the prey to hide behind, and we only give the agents
        partial observations of their environment using an egocentric perspective.
        Although we train the model to play this game from scratch, experiments and
        visualizations suggest that the agent learns to predict its own visibility
        in the environment. Furthermore, we quantitatively analyze how agent weaknesses,
        such as slower speed, effect the learned policy. Our results suggest that,
        although agent weaknesses make the learning problem more challenging, they
        also cause more useful features to be learned. Our project website is available
        at: this http URL ~bchen/visualhideseek/.", "venue": "IEEE Symposium on Artificial
        Life", "year": 2019, "referenceCount": 71, "citationCount": 16, "influentialCitationCount":
        0, "isOpenAccess": true, "openAccessPdf": {"url": "https://direct.mit.edu/isal/proceedings-pdf/isal2020/32/645/1908628/isal_a_00269.pdf",
        "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
        "publicationDate": "2019-09-25", "journal": {"volume": "abs/1910.07882", "name":
        "ArXiv"}, "authors": [{"authorId": "8786274", "name": "Boyuan Chen"}, {"authorId":
        "3340170", "name": "Shuran Song"}, {"authorId": "51022452", "name": "H. Lipson"},
        {"authorId": "1856025", "name": "Carl Vondrick"}]}, {"paperId": "6edfe8350da54cd563158b0d7d0c664f16cb91a8",
        "externalIds": {"ArXiv": "1904.04231", "MAG": "2950620323", "DBLP": "journals/corr/abs-1904-04231",
        "DOI": "10.1109/CVPR.2019.00036", "CorpusId": 102352873}, "corpusId": 102352873,
        "publicationVenue": {"id": "768b87bb-8a18-4d9c-a161-4d483c776bcf", "name":
        "Computer Vision and Pattern Recognition", "type": "conference", "alternate_names":
        ["CVPR", "Comput Vis Pattern Recognit"], "issn": "1063-6919", "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
        "alternate_urls": ["https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"]},
        "url": "https://www.semanticscholar.org/paper/6edfe8350da54cd563158b0d7d0c664f16cb91a8",
        "title": "Relational Action Forecasting", "abstract": "This paper focuses
        on multi-person action forecasting in videos. More precisely, given a history
        of H previous frames, the goal is to detect actors and to predict their future
        actions for the next T frames. Our approach jointly models temporal and spatial
        interactions among different actors by constructing a recurrent graph, using
        actor proposals obtained with Faster R-CNN as nodes. Our method learns to
        select a subset of discriminative relations without requiring explicit supervision,
        thus enabling us to tackle challenging visual data. We refer to our model
        as Discriminative Relational Recurrent Network (DRRN). Evaluation of action
        prediction on AVA demonstrates the effectiveness of our proposed method compared
        to simpler baselines. Furthermore, we significantly improve performance on
        the task of early action classification on J-HMDB, from the previous SOTA
        of 48% to 60%.", "venue": "Computer Vision and Pattern Recognition", "year":
        2019, "referenceCount": 81, "citationCount": 73, "influentialCitationCount":
        5, "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/1904.04231",
        "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
        "Conference"], "publicationDate": "2019-04-08", "journal": {"pages": "273-283",
        "name": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition
        (CVPR)"}, "authors": [{"authorId": "1491624845", "name": "Chen Sun"}, {"authorId":
        "1781242", "name": "Abhinav Shrivastava"}, {"authorId": "1856025", "name":
        "Carl Vondrick"}, {"authorId": "1694199", "name": "R. Sukthankar"}, {"authorId":
        "1702318", "name": "K. Murphy"}, {"authorId": "2462253", "name": "C. Schmid"}]},
        {"paperId": "c41a11c0e9b8b92b4faaf97749841170b760760a", "externalIds": {"ArXiv":
        "1904.01766", "MAG": "2981851019", "DBLP": "journals/corr/abs-1904-01766",
        "DOI": "10.1109/ICCV.2019.00756", "CorpusId": 102483628}, "corpusId": 102483628,
        "publicationVenue": {"id": "7654260e-79f9-45c5-9663-d72027cf88f3", "name":
        "IEEE International Conference on Computer Vision", "type": "conference",
        "alternate_names": ["ICCV", "IEEE Int Conf Comput Vis", "ICCV Workshops",
        "ICCV Work"], "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"},
        "url": "https://www.semanticscholar.org/paper/c41a11c0e9b8b92b4faaf97749841170b760760a",
        "title": "VideoBERT: A Joint Model for Video and Language Representation Learning",
        "abstract": "Self-supervised learning has become increasingly important to
        leverage the abundance of unlabeled data available on platforms like YouTube.
        Whereas most existing approaches learn low-level representations, we propose
        a joint visual-linguistic model to learn high-level features without any explicit
        supervision. In particular, inspired by its recent success in language modeling,
        we build upon the BERT model to learn bidirectional joint distributions over
        sequences of visual and linguistic tokens, derived from vector quantization
        of video data and off-the-shelf speech recognition outputs, respectively.
        We use VideoBERT in numerous tasks, including action classification and video
        captioning. We show that it can be applied directly to open-vocabulary classification,
        and confirm that large amounts of training data and cross-modal information
        are critical to performance. Furthermore, we outperform the state-of-the-art
        on video captioning, and quantitative results verify that the model learns
        high-level semantic features.", "venue": "IEEE International Conference on
        Computer Vision", "year": 2019, "referenceCount": 40, "citationCount": 937,
        "influentialCitationCount": 83, "isOpenAccess": true, "openAccessPdf": {"url":
        "https://arxiv.org/pdf/1904.01766", "status": null}, "fieldsOfStudy": ["Computer
        Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
        "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-04-03",
        "journal": {"pages": "7463-7472", "name": "2019 IEEE/CVF International Conference
        on Computer Vision (ICCV)"}, "authors": [{"authorId": "1491624845", "name":
        "Chen Sun"}, {"authorId": "49588480", "name": "Austin Myers"}, {"authorId":
        "1856025", "name": "Carl Vondrick"}, {"authorId": "1702318", "name": "K. Murphy"},
        {"authorId": "2462253", "name": "C. Schmid"}]}, {"paperId": "e1dea4c733ee7c98aaa42972452f545821b5d3b5",
        "externalIds": {"DBLP": "journals/corr/abs-1909-00900", "MAG": "2972267366",
        "ArXiv": "1909.00900", "CorpusId": 202540581}, "corpusId": 202540581, "publicationVenue":
        {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd", "name": "Neural Information
        Processing Systems", "type": "conference", "alternate_names": ["Neural Inf
        Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"}, "url": "https://www.semanticscholar.org/paper/e1dea4c733ee7c98aaa42972452f545821b5d3b5",
        "title": "Metric Learning for Adversarial Robustness", "abstract": "Deep networks
        are well-known to be fragile to adversarial attacks. We conduct an empirical
        analysis of deep representations under the state-of-the-art attack method
        called PGD, and find that the attack causes the internal representation to
        shift closer to the ``false'''' class. Motivated by this observation, we propose
        to regularize the representation space under attack with metric learning to
        produce more robust classifiers. By carefully sampling examples for metric
        learning, our learned representation not only increases robustness, but also
        detects previously unseen adversarial samples. Quantitative experiments show
        improvement of robustness accuracy by up to 4% and detection efficiency by
        up to 6% according to Area Under Curve score over prior work. The code of
        our work is available at https://github.com/columbia/Metric_Learning_Adversarial_Robustness.",
        "venue": "Neural Information Processing Systems", "year": 2019, "referenceCount":
        53, "citationCount": 143, "influentialCitationCount": 17, "isOpenAccess":
        false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
        "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
        {"category": "Mathematics", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
        "publicationDate": "2019-09-01", "journal": {"pages": "478-489"}, "authors":
        [{"authorId": "7700460", "name": "Chengzhi Mao"}, {"authorId": "30188141",
        "name": "Ziyuan Zhong"}, {"authorId": "152211006", "name": "Junfeng Yang"},
        {"authorId": "1856025", "name": "Carl Vondrick"}, {"authorId": "31631000",
        "name": "Baishakhi Ray"}]}, {"paperId": "07174c2f209f15cacf9ad3422b48652df286be69",
        "externalIds": {"DBLP": "journals/pami/MonfortVOAZRBYB20", "ArXiv": "1801.03150",
        "MAG": "2952992253", "DOI": "10.1109/TPAMI.2019.2901464", "CorpusId": 11868155,
        "PubMed": "30802849"}, "corpusId": 11868155, "publicationVenue": {"id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
        "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
        "type": "journal", "alternate_names": ["IEEE Trans Pattern Anal Mach Intell"],
        "issn": "0162-8828", "url": "http://www.computer.org/tpami/", "alternate_urls":
        ["http://www.computer.org/portal/web/tpami", "http://ieeexplore.ieee.org/servlet/opac?punumber=34"]},
        "url": "https://www.semanticscholar.org/paper/07174c2f209f15cacf9ad3422b48652df286be69",
        "title": "Moments in Time Dataset: One Million Videos for Event Understanding",
        "abstract": "We present the Moments in Time Dataset, a large-scale human-annotated
        collection of one million short videos corresponding to dynamic events unfolding
        within three seconds. Modeling the spatial-audio-temporal dynamics even for
        actions occurring in 3 second videos poses many challenges: meaningful events
        do not include only people, but also objects, animals, and natural phenomena;
        visual and auditory events can be symmetrical in time (\u201copening\u201d
        is \u201cclosing\u201d in reverse), and either transient or sustained. We
        describe the annotation process of our dataset (each video is tagged with
        one action or activity label among 339 different classes), analyze its scale
        and diversity in comparison to other large-scale video datasets for action
        recognition, and report results of several baseline models addressing separately,
        and jointly, three modalities: spatial, temporal and auditory. The Moments
        in Time dataset, designed to have a large coverage and diversity of events
        in both visual and auditory modalities, can serve as a new challenge to develop
        models that scale to the level of complexity and abstract reasoning that a
        human processes on a daily basis.", "venue": "IEEE Transactions on Pattern
        Analysis and Machine Intelligence", "year": 2018, "referenceCount": 60, "citationCount":
        444, "influentialCitationCount": 69, "isOpenAccess": true, "openAccessPdf":
        {"url": "https://arxiv.org/pdf/1801.03150", "status": null}, "fieldsOfStudy":
        ["Computer Science", "Medicine"], "s2FieldsOfStudy": [{"category": "Computer
        Science", "source": "external"}, {"category": "Medicine", "source": "external"},
        {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
        ["JournalArticle"], "publicationDate": "2018-01-09", "journal": {"volume":
        "42", "pages": "502-508", "name": "IEEE Transactions on Pattern Analysis and
        Machine Intelligence"}, "authors": [{"authorId": "95743023", "name": "Mathew
        Monfort"}, {"authorId": "145291669", "name": "Bolei Zhou"}, {"authorId": "3298267",
        "name": "Sarah Adel Bargal"}, {"authorId": "50112310", "name": "A. Andonian"},
        {"authorId": "2059614890", "name": "Tom Yan"}, {"authorId": "40544169", "name":
        "K. Ramakrishnan"}, {"authorId": "49860655", "name": "L. Brown"}, {"authorId":
        "33421444", "name": "Quanfu Fan"}, {"authorId": "1891570", "name": "Dan Gutfreund"},
        {"authorId": "1856025", "name": "Carl Vondrick"}, {"authorId": "143868587",
        "name": "A. Oliva"}]}, {"paperId": "1d913f05044382ddc1fea170a739fffa189c9c88",
        "externalIds": {"MAG": "2949118208", "DBLP": "conf/sigmod/SellamLHYV019",
        "ArXiv": "1808.04486", "DOI": "10.1145/3299869.3300073", "CorpusId": 52006707},
        "corpusId": 52006707, "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/1d913f05044382ddc1fea170a739fffa189c9c88",
        "title": "DeepBase: Deep Inspection of Neural Networks", "abstract": "Although
        deep learning models perform remarkably well across a range of tasks such
        as language translation and object recognition, it remains unclear what high-level
        logic, if any, they follow. Understanding this logic may lead to more transparency,
        better model design, and faster experimentation. Recent machine learning research
        has leveraged statistical methods to identify hidden units that behave (e.g.,
        activate) similarly to human understandable logic, but those analyses require
        considerable manual effort. Our insight is that many of those studies follow
        a common analysis pattern, and therefore there is opportunity to provide a
        declarative abstraction to easily express, execute and optimize them. This
        paper describes DeepBase, a system to inspect neural network behaviors through
        a unified interface. We model logic with user-provided hypothesis functions
        that annotate the data with high-level labels (e.g., part-of-speech tags,
        image captions). DeepBase lets users quickly identify individual or groups
        of units that have strong statistical dependencies with desired hypotheses.
        We discuss how DeepBase can express existing analyses, propose a set of simple
        and effective optimizations to speed up a standard Python implementation by
        up to 72x, and reproduce recent studies from the NLP literature.", "venue":
        "SIGMOD Conference", "year": 2018, "referenceCount": 70, "citationCount":
        19, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf":
        {"url": "https://dl.acm.org/doi/pdf/10.1145/3299869.3300073", "status": null},
        "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer
        Science", "source": "external"}, {"category": "Computer Science", "source":
        "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Book", "Conference"],
        "publicationDate": "2018-08-13", "journal": {"name": "Proceedings of the 2019
        International Conference on Management of Data"}, "authors": [{"authorId":
        "145450400", "name": "Thibault Sellam"}, {"authorId": "48085802", "name":
        "Kevin Lin"}, {"authorId": "48612187", "name": "I. Huang"}, {"authorId": "2110603262",
        "name": "Michelle Yang"}, {"authorId": "1856025", "name": "Carl Vondrick"},
        {"authorId": "2056843177", "name": "Eugene Wu"}]}, {"paperId": "2718cd594d2aa09315da52594877cd71d377dfcf",
        "externalIds": {"MAG": "2954830955", "DBLP": "journals/corr/abs-1811-11683",
        "ArXiv": "1811.11683", "DOI": "10.1109/CVPR.2019.01276", "CorpusId": 53845347},
        "corpusId": 53845347, "publicationVenue": {"id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
        "name": "Computer Vision and Pattern Recognition", "type": "conference", "alternate_names":
        ["CVPR", "Comput Vis Pattern Recognit"], "issn": "1063-6919", "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
        "alternate_urls": ["https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"]},
        "url": "https://www.semanticscholar.org/paper/2718cd594d2aa09315da52594877cd71d377dfcf",
        "title": "Multi-Level Multimodal Common Semantic Space for Image-Phrase Grounding",
        "abstract": "We address the problem of phrase grounding by learning a multi-level
        common semantic space shared by the textual and visual modalities. We exploit
        multiple levels of feature maps of a Deep Convolutional Neural Network, as
        well as contextualized word and sentence embeddings extracted from a character-based
        language model. Following dedicated non-linear mappings for visual features
        at each level, word, and sentence embeddings, we obtain multiple instantiations
        of our common semantic space in which comparisons between any target text
        and the visual content is performed with cosine similarity. We guide the model
        by a multi-level multimodal attention mechanism which outputs attended visual
        features at each level. The best level is chosen to be compared with text
        content for maximizing the pertinence scores of image-sentence pairs of the
        ground truth. Experiments conducted on three publicly available datasets show
        significant performance gains (20%-60% relative) over the state-of-the-art
        in phrase localization and set a new performance record on those datasets.
        We provide a detailed ablation study to show the contribution of each element
        of our approach and release our code on GitHub.", "venue": "Computer Vision
        and Pattern Recognition", "year": 2018, "referenceCount": 63, "citationCount":
        59, "influentialCitationCount": 9, "isOpenAccess": true, "openAccessPdf":
        {"url": "https://arxiv.org/pdf/1811.11683", "status": null}, "fieldsOfStudy":
        ["Computer Science", "Engineering"], "s2FieldsOfStudy": [{"category": "Computer
        Science", "source": "external"}, {"category": "Engineering", "source": "external"},
        {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
        ["JournalArticle", "Conference"], "publicationDate": "2018-11-28", "journal":
        {"pages": "12468-12478", "name": "2019 IEEE/CVF Conference on Computer Vision
        and Pattern Recognition (CVPR)"}, "authors": [{"authorId": "153769937", "name":
        "Hassan Akbari"}, {"authorId": "35862299", "name": "Svebor Karaman"}, {"authorId":
        "1754397", "name": "Surabhi Bhargava"}, {"authorId": "2108342501", "name":
        "Brian Chen"}, {"authorId": "1856025", "name": "Carl Vondrick"}, {"authorId":
        "9546964", "name": "Shih-Fu Chang"}]}, {"paperId": "2c6442460c325f8267c75002d77a72035e36f14b",
        "externalIds": {"MAG": "2786517654", "CorpusId": 196143856}, "corpusId": 196143856,
        "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/2c6442460c325f8267c75002d77a72035e36f14b",
        "title": "Counterfactual Image Networks", "abstract": "We capitalize on the
        natural compositional structure of images in order to learn object segmentation
        with weakly labeled images. The intuition behind our approach is that removing
        objects from images will yield natural images, however removing random patches
        will yield unnatural images. We leverage this signal to develop a generative
        model that decomposes an image into layers, and when all layers are combined,
        it reconstructs the input image. However, when a layer is removed, the model
        learns to produce a different image that still looks natural to an adversary,
        which is possible by removing objects. Experiments and visualizations suggest
        that this model automatically learns object segmentation on images labeled
        only by scene better than baselines.", "venue": "", "year": 2018, "referenceCount":
        0, "citationCount": 3, "influentialCitationCount": 0, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate":
        "2018-02-15", "journal": {"volume": "", "name": ""}, "authors": [{"authorId":
        "4781463", "name": "Deniz Oktay"}, {"authorId": "1856025", "name": "Carl Vondrick"},
        {"authorId": "143805211", "name": "A. Torralba"}]}, {"paperId": "360ef12906a531733b66e7e15c3d51771e7126d3",
        "externalIds": {"MAG": "2963426332", "DBLP": "journals/corr/abs-1806-09594",
        "ArXiv": "1806.09594", "DOI": "10.1007/978-3-030-01261-8_24", "CorpusId":
        49405781}, "corpusId": 49405781, "publicationVenue": {"id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
        "name": "European Conference on Computer Vision", "type": "conference", "alternate_names":
        ["ECCV", "Eur Conf Comput Vis"], "url": "https://link.springer.com/conference/eccv"},
        "url": "https://www.semanticscholar.org/paper/360ef12906a531733b66e7e15c3d51771e7126d3",
        "title": "Tracking Emerges by Colorizing Videos", "abstract": null, "venue":
        "European Conference on Computer Vision", "year": 2018, "referenceCount":
        67, "citationCount": 332, "influentialCitationCount": 28, "isOpenAccess":
        true, "openAccessPdf": {"url": "http://arxiv.org/pdf/1806.09594", "status":
        null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
        "Computer Science", "source": "external"}, {"category": "Computer Science",
        "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"],
        "publicationDate": "2018-06-25", "journal": {"volume": "abs/1806.09594", "name":
        "ArXiv"}, "authors": [{"authorId": "1856025", "name": "Carl Vondrick"}, {"authorId":
        "1781242", "name": "Abhinav Shrivastava"}, {"authorId": "50706340", "name":
        "A. Fathi"}, {"authorId": "1687120", "name": "S. Guadarrama"}, {"authorId":
        "1702318", "name": "K. Murphy"}]}, {"paperId": "4974f16af328e774d0a9e53a366bfdd738488b90",
        "externalIds": {"CorpusId": 201797908}, "corpusId": 201797908, "publicationVenue":
        null, "url": "https://www.semanticscholar.org/paper/4974f16af328e774d0a9e53a366bfdd738488b90",
        "title": "\u201c I Like the Way You Think ! \u201d Inspecting the Internal
        Logic of Recurrent Neural Networks", "abstract": null, "venue": "", "year":
        2018, "referenceCount": 24, "citationCount": 2, "influentialCitationCount":
        0, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy":
        [], "publicationTypes": null, "publicationDate": null, "journal": null, "authors":
        [{"authorId": "145450400", "name": "Thibault Sellam"}, {"authorId": "48085802",
        "name": "Kevin Lin"}, {"authorId": "48612187", "name": "I. Huang"}, {"authorId":
        "1856025", "name": "Carl Vondrick"}, {"authorId": "2056843177", "name": "Eugene
        Wu"}]}, {"paperId": "6b325af08a63bc8f94c39ae1c12509dce23d755c", "externalIds":
        {"MAG": "2883275382", "DBLP": "conf/eccv/SunSVMSS18", "ArXiv": "1807.10982",
        "DOI": "10.1007/978-3-030-01252-6_20", "CorpusId": 51876625}, "corpusId":
        51876625, "publicationVenue": {"id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
        "name": "European Conference on Computer Vision", "type": "conference", "alternate_names":
        ["ECCV", "Eur Conf Comput Vis"], "url": "https://link.springer.com/conference/eccv"},
        "url": "https://www.semanticscholar.org/paper/6b325af08a63bc8f94c39ae1c12509dce23d755c",
        "title": "Actor-Centric Relation Network", "abstract": null, "venue": "European
        Conference on Computer Vision", "year": 2018, "referenceCount": 66, "citationCount":
        179, "influentialCitationCount": 18, "isOpenAccess": true, "openAccessPdf":
        {"url": "https://arxiv.org/pdf/1807.10982", "status": null}, "fieldsOfStudy":
        ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science",
        "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2018-07-28",
        "journal": {"volume": "abs/1807.10982", "name": "ArXiv"}, "authors": [{"authorId":
        "1491624845", "name": "Chen Sun"}, {"authorId": "1781242", "name": "Abhinav
        Shrivastava"}, {"authorId": "1856025", "name": "Carl Vondrick"}, {"authorId":
        "1702318", "name": "K. Murphy"}, {"authorId": "1694199", "name": "R. Sukthankar"},
        {"authorId": "2462253", "name": "C. Schmid"}]}, {"paperId": "a990eee6c47ded320f8f85f54febba8ea11ee908",
        "externalIds": {"MAG": "2916446864", "CorpusId": 187720685}, "corpusId": 187720685,
        "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/a990eee6c47ded320f8f85f54febba8ea11ee908",
        "title": "Learning from Noisy Demonstration Sets via Meta-Learned Suitability
        Assessor", "abstract": null, "venue": "", "year": 2018, "referenceCount":
        0, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}], "publicationTypes":
        null, "publicationDate": "2018-09-27", "journal": {"volume": "", "name": ""},
        "authors": [{"authorId": "2015467", "name": "Te-Lin Wu"}, {"authorId": "6157177",
        "name": "Jaedong Hwang"}, {"authorId": "2003805955", "name": "Jingyun Yang"},
        {"authorId": "31746321", "name": "Shaofan Lai"}, {"authorId": "1856025", "name":
        "Carl Vondrick"}, {"authorId": "35198686", "name": "Joseph J. Lim"}]}, {"paperId":
        "b263ccf4c5bb19a578165ed731ca24f9ea3653cf", "externalIds": {"MAG": "2893481810",
        "DOI": "10.1167/18.10.753", "CorpusId": 149690414}, "corpusId": 149690414,
        "publicationVenue": {"id": "c3faa921-3f7d-4435-906f-25cdb7d6a885", "name":
        "Journal of Vision", "type": "journal", "alternate_names": ["J Vis", "Journal
        of Visualization"], "issn": "1534-7362", "alternate_issns": ["1343-8875"],
        "url": "http://www.journalofvision.org/4/6/", "alternate_urls": ["https://link.springer.com/journal/12650",
        "http://journalofvision.org/", "https://www.iospress.nl/html/13438875.php"]},
        "url": "https://www.semanticscholar.org/paper/b263ccf4c5bb19a578165ed731ca24f9ea3653cf",
        "title": "A Large Scale Video Dataset for Event Recognition", "abstract":
        null, "venue": "Journal of Vision", "year": 2018, "referenceCount": 0, "citationCount":
        0, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null,
        "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer
        Science", "source": "external"}, {"category": "Computer Science", "source":
        "s2-fos-model"}], "publicationTypes": null, "publicationDate": "2018-09-01",
        "journal": {"name": "Journal of Vision"}, "authors": [{"authorId": "95743023",
        "name": "Mathew Monfort"}, {"authorId": "145291669", "name": "Bolei Zhou"},
        {"authorId": "3298267", "name": "Sarah Adel Bargal"}, {"authorId": "50112310",
        "name": "A. Andonian"}, {"authorId": "40544169", "name": "K. Ramakrishnan"},
        {"authorId": "1856025", "name": "Carl Vondrick"}, {"authorId": "143868587",
        "name": "A. Oliva"}]}, {"paperId": "dad2459b0488560ec9ce351e6b797e4beee29f85",
        "externalIds": {"CorpusId": 160032212}, "corpusId": 160032212, "publicationVenue":
        null, "url": "https://www.semanticscholar.org/paper/dad2459b0488560ec9ce351e6b797e4beee29f85",
        "title": "Deep Neural Inspection Using DeepBase", "abstract": "There is currently
        excellent software and hardware infrastructure for every part of the neural
        network (NN) development lifecycle\u2014creating models, training them, evaluating
        their accuracy, and deploying them. This has helped drive the excitement towards
        developing and deploying NN models in nearly every discipline and industry.
        Although neural networks today are largely evaluated on held-out test data,
        this does not guarantee models will behave reliably and correctly when deployed
        in practice. Models may encounter new situations that are statistically different
        from their training set and testing set, and researchers need to understand
        how their trained models will behave.", "venue": "", "year": 2018, "referenceCount":
        23, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category":
        "Computer Science", "source": "s2-fos-model"}], "publicationTypes": null,
        "publicationDate": null, "journal": null, "authors": [{"authorId": "2108952064",
        "name": "Yiru Chen"}, {"authorId": "2118897254", "name": "Yiliang Shi"}, {"authorId":
        "8786274", "name": "Boyuan Chen"}, {"authorId": "145450400", "name": "Thibault
        Sellam"}, {"authorId": "1856025", "name": "Carl Vondrick"}, {"authorId": "2056843177",
        "name": "Eugene Wu"}]}, {"paperId": "fe018f22600d07cbd0452a070e03708886470015",
        "externalIds": {"DBLP": "journals/corr/abs-1804-03160", "ArXiv": "1804.03160",
        "MAG": "2952943485", "DOI": "10.1007/978-3-030-01246-5_35", "CorpusId": 4748509},
        "corpusId": 4748509, "publicationVenue": {"id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
        "name": "European Conference on Computer Vision", "type": "conference", "alternate_names":
        ["ECCV", "Eur Conf Comput Vis"], "url": "https://link.springer.com/conference/eccv"},
        "url": "https://www.semanticscholar.org/paper/fe018f22600d07cbd0452a070e03708886470015",
        "title": "The Sound of Pixels", "abstract": null, "venue": "European Conference
        on Computer Vision", "year": 2018, "referenceCount": 55, "citationCount":
        419, "influentialCitationCount": 67, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": ["Computer Science", "Engineering"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Engineering",
        "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2018-04-09",
        "journal": {"pages": "587-604"}, "authors": [{"authorId": "47940821", "name":
        "Hang Zhao"}, {"authorId": "144158271", "name": "Chuang Gan"}, {"authorId":
        "41020711", "name": "Andrew Rouditchenko"}, {"authorId": "1856025", "name":
        "Carl Vondrick"}, {"authorId": "2324658", "name": "Josh H. McDermott"}, {"authorId":
        "143805211", "name": "A. Torralba"}]}, {"paperId": "241b86d3c71d14b8cc6044a425b047a0724cfdc9",
        "externalIds": {"DBLP": "conf/iccv/RecasensVK017", "MAG": "2776312359", "DOI":
        "10.1109/ICCV.2017.160", "CorpusId": 9531279}, "corpusId": 9531279, "publicationVenue":
        {"id": "7654260e-79f9-45c5-9663-d72027cf88f3", "name": "IEEE International
        Conference on Computer Vision", "type": "conference", "alternate_names": ["ICCV",
        "IEEE Int Conf Comput Vis", "ICCV Workshops", "ICCV Work"], "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"},
        "url": "https://www.semanticscholar.org/paper/241b86d3c71d14b8cc6044a425b047a0724cfdc9",
        "title": "Following Gaze in Video", "abstract": "Following the gaze of people
        inside videos is an important signal for understanding people and their actions.
        In this paper, we present an approach for following gaze in video by predicting
        where a person (in the video) is looking even when the object is in a different
        frame. We collect VideoGaze, a new dataset which we use as a benchmark to
        both train and evaluate models. Given one frame with a person in it, our model
        estimates a density for gaze location in every frame and the probability that
        the person is looking in that particular frame. A key aspect of our approach
        is an end-to-end model that jointly estimates: saliency, gaze pose, and geometric
        relationships between views while only using gaze as supervision. Visualizations
        suggest that the model learns to internally solve these intermediate tasks
        automatically without additional supervision. Experiments show that our approach
        follows gaze in video better than existing approaches, enabling a richer understanding
        of human activities in video.", "venue": "IEEE International Conference on
        Computer Vision", "year": 2017, "referenceCount": 34, "citationCount": 61,
        "influentialCitationCount": 9, "isOpenAccess": false, "openAccessPdf": null,
        "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer
        Science", "source": "external"}, {"category": "Computer Science", "source":
        "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate":
        "2017-10-01", "journal": {"pages": "1444-1452", "name": "2017 IEEE International
        Conference on Computer Vision (ICCV)"}, "authors": [{"authorId": "39257069",
        "name": "Adri\u00e0 Recasens"}, {"authorId": "1856025", "name": "Carl Vondrick"},
        {"authorId": "2556428", "name": "A. Khosla"}, {"authorId": "143805211", "name":
        "A. Torralba"}]}, {"paperId": "4b95428093db0021a19d6b47751dcc7dd2978717",
        "externalIds": {"DBLP": "phd/ndltd/Vondrick17", "CorpusId": 41764345}, "corpusId":
        41764345, "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/4b95428093db0021a19d6b47751dcc7dd2978717",
        "title": "Predictive vision", "abstract": null, "venue": "", "year": 2017,
        "referenceCount": 0, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess":
        false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}], "publicationTypes":
        null, "publicationDate": null, "journal": null, "authors": [{"authorId": "1856025",
        "name": "Carl Vondrick"}]}, {"paperId": "52ed3b634c302af93ee2e70b7c28e4b2128a5947",
        "externalIds": {"DBLP": "journals/corr/AytarVT17", "ArXiv": "1706.00932",
        "MAG": "2623327532", "CorpusId": 31547460}, "corpusId": 31547460, "publicationVenue":
        {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names":
        ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/52ed3b634c302af93ee2e70b7c28e4b2128a5947",
        "title": "See, Hear, and Read: Deep Aligned Representations", "abstract":
        "We capitalize on large amounts of readily-available, synchronous data to
        learn a deep discriminative representations shared across three major natural
        modalities: vision, sound and language. By leveraging over a year of sound
        from video and millions of sentences paired with images, we jointly train
        a deep convolutional network for aligned representation learning. Our experiments
        suggest that this representation is useful for several tasks, such as cross-modal
        retrieval or transferring classifiers between modalities. Moreover, although
        our network is only trained with image+text and image+sound pairs, it can
        transfer between text and sound as well, a transfer the network never observed
        during training. Visualizations of our representation reveal many hidden units
        which automatically emerge to detect concepts, independent of the modality.",
        "venue": "arXiv.org", "year": 2017, "referenceCount": 48, "citationCount":
        113, "influentialCitationCount": 7, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
        "Computer Science", "source": "external"}, {"category": "Computer Science",
        "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
        "2017-06-03", "journal": {"volume": "abs/1706.00932", "name": "ArXiv"}, "authors":
        [{"authorId": "3152281", "name": "Y. Aytar"}, {"authorId": "1856025", "name":
        "Carl Vondrick"}, {"authorId": "143805211", "name": "A. Torralba"}]}, {"paperId":
        "6d2892f82a89bfc81f9924adb8bd070fe007adf7", "externalIds": {"DBLP": "conf/cvpr/Vondrick017",
        "MAG": "2738136547", "DOI": "10.1109/CVPR.2017.319", "CorpusId": 8234308},
        "corpusId": 8234308, "publicationVenue": {"id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
        "name": "Computer Vision and Pattern Recognition", "type": "conference", "alternate_names":
        ["CVPR", "Comput Vis Pattern Recognit"], "issn": "1063-6919", "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
        "alternate_urls": ["https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"]},
        "url": "https://www.semanticscholar.org/paper/6d2892f82a89bfc81f9924adb8bd070fe007adf7",
        "title": "Generating the Future with Adversarial Transformers", "abstract":
        "We learn models to generate the immediate future in video. This problem has
        two main challenges. Firstly, since the future is uncertain, models should
        be multi-modal, which can be difficult to learn. Secondly, since the future
        is similar to the past, models store low-level details, which complicates
        learning of high-level semantics. We propose a framework to tackle both of
        these challenges. We present a model that generates the future by transforming
        pixels in the past. Our approach explicitly disentangles the models memory
        from the prediction, which helps the model learn desirable invariances. Experiments
        suggest that this model can generate short videos of plausible futures. We
        believe predictive models have many applications in robotics, health-care,
        and video understanding.", "venue": "Computer Vision and Pattern Recognition",
        "year": 2017, "referenceCount": 51, "citationCount": 165, "influentialCitationCount":
        6, "isOpenAccess": true, "openAccessPdf": {"url": "https://dspace.mit.edu/bitstream/1721.1/123483/2/transformer.pdf",
        "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
        "Conference"], "publicationDate": "2017-07-01", "journal": {"pages": "2992-3000",
        "name": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"},
        "authors": [{"authorId": "1856025", "name": "Carl Vondrick"}, {"authorId":
        "143805211", "name": "A. Torralba"}]}, {"paperId": "1145d3d5c96157a9b19c1bedb090a5157537ad97",
        "externalIds": {"ArXiv": "1612.03094", "MAG": "2567215512", "DBLP": "journals/corr/RecasensVKT16",
        "CorpusId": 7607847}, "corpusId": 7607847, "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
        "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url":
        "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/1145d3d5c96157a9b19c1bedb090a5157537ad97",
        "title": "Following Gaze Across Views", "abstract": "Following the gaze of
        people inside videos is an important signal for understanding people and their
        actions. In this paper, we present an approach for following gaze across views
        by predicting where a particular person is looking throughout a scene. We
        collect VideoGaze, a new dataset which we use as a benchmark to both train
        and evaluate models. Given one view with a person in it and a second view
        of the scene, our model estimates a density for gaze location in the second
        view. A key aspect of our approach is an end-to-end model that solves the
        following sub-problems: saliency, gaze pose, and geometric relationships between
        views. Although our model is supervised only with gaze, we show that the model
        learns to solve these subproblems automatically without supervision. Experiments
        suggest that our approach follows gaze better than standard baselines and
        produces plausible results for everyday situations.", "venue": "arXiv.org",
        "year": 2016, "referenceCount": 29, "citationCount": 5, "influentialCitationCount":
        0, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
        "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle"], "publicationDate": "2016-12-09", "journal":
        {"volume": "abs/1612.03094", "name": "ArXiv"}, "authors": [{"authorId": "39257069",
        "name": "Adri\u00e0 Recasens"}, {"authorId": "1856025", "name": "Carl Vondrick"},
        {"authorId": "2556428", "name": "A. Khosla"}, {"authorId": "143805211", "name":
        "A. Torralba"}]}, {"paperId": "2e68190ebda2db8fb690e378fa213319ca915cf8",
        "externalIds": {"CorpusId": 260554932}, "corpusId": 260554932, "publicationVenue":
        null, "url": "https://www.semanticscholar.org/paper/2e68190ebda2db8fb690e378fa213319ca915cf8",
        "title": "Stream 2 D convolutions Foreground Stream 3 D convolutions Noise
        100 dim Mask Foreground Background", "abstract": "We capitalize on large amounts
        of unlabeled video in order to learn a model of scene dynamics for both video
        recognition tasks (e.g. action classification) and video generation tasks
        (e.g. future prediction). We propose a generative adversarial network for
        video with a spatio-temporal convolutional architecture that untangles the
        scene\u2019s foreground from the background. Experiments suggest this model
        can generate tiny videos up to a second at full frame rate better than simple
        baselines, and we show its utility at predicting plausible futures of static
        images. Moreover, experiments and visualizations show the model internally
        learns useful features for recognizing actions with minimal supervision, suggesting
        scene dynamics are a promising signal for representation learning. We believe
        generative video models can impact many applications in video understanding
        and simulation.", "venue": "", "year": 2016, "referenceCount": 49, "citationCount":
        0, "influentialCitationCount": 0, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category": "Computer Science",
        "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": null,
        "journal": null, "authors": [{"authorId": "1856025", "name": "Carl Vondrick"},
        {"authorId": "2367683", "name": "H. Pirsiavash"}, {"authorId": "143805211",
        "name": "A. Torralba"}]}, {"paperId": "48b5289aa08beb10fca58c6a542a597afc359e5d",
        "externalIds": {"CorpusId": 62879857}, "corpusId": 62879857, "publicationVenue":
        null, "url": "https://www.semanticscholar.org/paper/48b5289aa08beb10fca58c6a542a597afc359e5d",
        "title": "Shared '' Cross + Modal '' Representation religious , * church ,
        * plants", "abstract": null, "venue": "", "year": 2016, "referenceCount":
        44, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category":
        "Philosophy", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate":
        null, "journal": null, "authors": [{"authorId": "3436589", "name": "Llu\u00eds
        Castrej\u00f3n"}, {"authorId": "3152281", "name": "Y. Aytar"}, {"authorId":
        "51346163", "name": "Mit Csail"}, {"authorId": "1856025", "name": "Carl Vondrick"},
        {"authorId": "2367683", "name": "H. Pirsiavash"}, {"authorId": "143805211",
        "name": "A. Torralba"}]}, {"paperId": "7ab8d3af6f78f9c9f64a2f2d38471401ad0988a9",
        "externalIds": {"MAG": "2950452517", "DBLP": "conf/nips/AytarVT16", "ArXiv":
        "1610.09001", "CorpusId": 2915490}, "corpusId": 2915490, "publicationVenue":
        {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd", "name": "Neural Information
        Processing Systems", "type": "conference", "alternate_names": ["Neural Inf
        Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"}, "url": "https://www.semanticscholar.org/paper/7ab8d3af6f78f9c9f64a2f2d38471401ad0988a9",
        "title": "SoundNet: Learning Sound Representations from Unlabeled Video",
        "abstract": "We learn rich natural sound representations by capitalizing on
        large amounts of unlabeled sound data collected in the wild. We leverage the
        natural synchronization between vision and sound to learn an acoustic representation
        using two-million unlabeled videos. Unlabeled video has the advantage that
        it can be economically acquired at massive scales, yet contains useful signals
        about natural sound. We propose a student-teacher training procedure which
        transfers discriminative visual knowledge from well established visual recognition
        models into the sound modality using unlabeled video as a bridge. Our sound
        representation yields significant performance improvements over the state-of-the-art
        results on standard benchmarks for acoustic scene/object classification. Visualizations
        suggest some high-level semantics automatically emerge in the sound network,
        even though it is trained without ground truth labels.", "venue": "Neural
        Information Processing Systems", "year": 2016, "referenceCount": 43, "citationCount":
        915, "influentialCitationCount": 134, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
        "Computer Science", "source": "external"}, {"category": "Computer Science",
        "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"],
        "publicationDate": "2016-10-27", "journal": {"volume": "abs/1610.09001", "name":
        "ArXiv"}, "authors": [{"authorId": "3152281", "name": "Y. Aytar"}, {"authorId":
        "1856025", "name": "Carl Vondrick"}, {"authorId": "143805211", "name": "A.
        Torralba"}]}, {"paperId": "7e64992091458256f438fbe1bd44fffcc197b76c", "externalIds":
        {"ArXiv": "1607.07295", "DBLP": "conf/cvpr/CastrejonAVPT16", "MAG": "2474574787",
        "DOI": "10.1109/CVPR.2016.321", "CorpusId": 2560991}, "corpusId": 2560991,
        "publicationVenue": {"id": "768b87bb-8a18-4d9c-a161-4d483c776bcf", "name":
        "Computer Vision and Pattern Recognition", "type": "conference", "alternate_names":
        ["CVPR", "Comput Vis Pattern Recognit"], "issn": "1063-6919", "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
        "alternate_urls": ["https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"]},
        "url": "https://www.semanticscholar.org/paper/7e64992091458256f438fbe1bd44fffcc197b76c",
        "title": "Learning Aligned Cross-Modal Representations from Weakly Aligned
        Data", "abstract": "People can recognize scenes across many different modalities
        beyond natural images. In this paper, we investigate how to learn cross-modal
        scene representations that transfer across modalities. To study this problem,
        we introduce a new cross-modal scene dataset. While convolutional neural networks
        can categorize cross-modal scenes well, they also learn an intermediate representation
        not aligned across modalities, which is undesirable for crossmodal transfer
        applications. We present methods to regularize cross-modal convolutional neural
        networks so that they have a shared representation that is agnostic of the
        modality. Our experiments suggest that our scene representation can help transfer
        representations across modalities for retrieval. Moreover, our visualizations
        suggest that units emerge in the shared representation that tend to activate
        on consistent concepts independently of the modality.", "venue": "Computer
        Vision and Pattern Recognition", "year": 2016, "referenceCount": 47, "citationCount":
        154, "influentialCitationCount": 14, "isOpenAccess": true, "openAccessPdf":
        {"url": "http://arxiv.org/pdf/1607.07295", "status": null}, "fieldsOfStudy":
        ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science",
        "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"},
        {"category": "Psychology", "source": "s2-fos-model"}], "publicationTypes":
        ["JournalArticle", "Conference"], "publicationDate": "2016-06-27", "journal":
        {"pages": "2940-2949", "name": "2016 IEEE Conference on Computer Vision and
        Pattern Recognition (CVPR)"}, "authors": [{"authorId": "3436589", "name":
        "Llu\u00eds Castrej\u00f3n"}, {"authorId": "3152281", "name": "Y. Aytar"},
        {"authorId": "1856025", "name": "Carl Vondrick"}, {"authorId": "2367683",
        "name": "H. Pirsiavash"}, {"authorId": "143805211", "name": "A. Torralba"}]},
        {"paperId": "a4162e328aacba376ea95a7654378423e504ca3d", "externalIds": {"ArXiv":
        "1610.09003", "MAG": "2949074159", "DBLP": "journals/pami/AytarCVP018", "DOI":
        "10.1109/TPAMI.2017.2753232", "CorpusId": 8278386, "PubMed": "28922114"},
        "corpusId": 8278386, "publicationVenue": {"id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
        "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
        "type": "journal", "alternate_names": ["IEEE Trans Pattern Anal Mach Intell"],
        "issn": "0162-8828", "url": "http://www.computer.org/tpami/", "alternate_urls":
        ["http://www.computer.org/portal/web/tpami", "http://ieeexplore.ieee.org/servlet/opac?punumber=34"]},
        "url": "https://www.semanticscholar.org/paper/a4162e328aacba376ea95a7654378423e504ca3d",
        "title": "Cross-Modal Scene Networks", "abstract": "People can recognize scenes
        across many different modalities beyond natural images. In this paper, we
        investigate how to learn cross-modal scene representations that transfer across
        modalities. To study this problem, we introduce a new cross-modal scene dataset.
        While convolutional neural networks can categorize scenes well, they also
        learn an intermediate representation not aligned across modalities, which
        is undesirable for cross-modal transfer applications. We present methods to
        regularize cross-modal convolutional neural networks so that they have a shared
        representation that is agnostic of the modality. Our experiments suggest that
        our scene representation can help transfer representations across modalities
        for retrieval. Moreover, our visualizations suggest that units emerge in the
        shared representation that tend to activate on consistent concepts independently
        of the modality.", "venue": "IEEE Transactions on Pattern Analysis and Machine
        Intelligence", "year": 2016, "referenceCount": 55, "citationCount": 106, "influentialCitationCount":
        2, "isOpenAccess": true, "openAccessPdf": {"url": "http://arxiv.org/pdf/1610.09003",
        "status": null}, "fieldsOfStudy": ["Computer Science", "Medicine"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Medicine",
        "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle"], "publicationDate": "2016-10-27", "journal":
        {"volume": "40", "pages": "2303-2314", "name": "IEEE Transactions on Pattern
        Analysis and Machine Intelligence"}, "authors": [{"authorId": "3152281", "name":
        "Y. Aytar"}, {"authorId": "3436589", "name": "Llu\u00eds Castrej\u00f3n"},
        {"authorId": "1856025", "name": "Carl Vondrick"}, {"authorId": "2367683",
        "name": "H. Pirsiavash"}, {"authorId": "143805211", "name": "A. Torralba"}]},
        {"paperId": "d274a0c1c383bc2440f9d90cab61df4ff934efdb", "externalIds": {"DOI":
        "10.1007/s11263-016-0884-7", "CorpusId": 255104771}, "corpusId": 255104771,
        "publicationVenue": {"id": "939ee07c-6009-43f8-b884-69238b40659e", "name":
        "International Journal of Computer Vision", "type": "journal", "alternate_names":
        ["Int J Comput Vis"], "issn": "0920-5691", "url": "https://www.springer.com/computer/image+processing/journal/11263",
        "alternate_urls": ["https://link.springer.com/journal/11263", "http://link.springer.com/journal/11263"]},
        "url": "https://www.semanticscholar.org/paper/d274a0c1c383bc2440f9d90cab61df4ff934efdb",
        "title": "Visualizing Object Detection Features", "abstract": null, "venue":
        "International Journal of Computer Vision", "year": 2016, "referenceCount":
        0, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": true,
        "openAccessPdf": {"url": "http://dspace.mit.edu/bitstream/1721.1/82370/2/862074378-MIT.pdf",
        "status": null}, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate":
        "2016-03-01", "journal": {"volume": "119", "pages": "145 - 158", "name": "International
        Journal of Computer Vision"}, "authors": [{"authorId": "1856025", "name":
        "Carl Vondrick"}, {"authorId": "2556428", "name": "A. Khosla"}, {"authorId":
        "2367683", "name": "H. Pirsiavash"}, {"authorId": "3045340", "name": "Tomasz
        Malisiewicz"}, {"authorId": "143805211", "name": "A. Torralba"}]}, {"paperId":
        "ee091ccf24c4f053c5c3dfbefe4a7975ed3447c1", "externalIds": {"MAG": "2520707650",
        "DBLP": "conf/nips/VondrickPT16", "ArXiv": "1609.02612", "DOI": "10.13016/M26GIH-TNYZ",
        "CorpusId": 9933254}, "corpusId": 9933254, "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
        "name": "Neural Information Processing Systems", "type": "conference", "alternate_names":
        ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
        "url": "https://www.semanticscholar.org/paper/ee091ccf24c4f053c5c3dfbefe4a7975ed3447c1",
        "title": "Generating Videos with Scene Dynamics", "abstract": "We capitalize
        on large amounts of unlabeled video in order to learn a model of scene dynamics
        for both video recognition tasks (e.g. action classification) and video generation
        tasks (e.g. future prediction). We propose a generative adversarial network
        for video with a spatio-temporal convolutional architecture that untangles
        the scene''s foreground from the background. Experiments suggest this model
        can generate tiny videos up to a second at full frame rate better than simple
        baselines, and we show its utility at predicting plausible futures of static
        images. Moreover, experiments and visualizations show the model internally
        learns useful features for recognizing actions with minimal supervision, suggesting
        scene dynamics are a promising signal for representation learning. We believe
        generative video models can impact many applications in video understanding
        and simulation.", "venue": "Neural Information Processing Systems", "year":
        2016, "referenceCount": 60, "citationCount": 1294, "influentialCitationCount":
        139, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
        "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2016-09-08",
        "journal": {"pages": "613-621"}, "authors": [{"authorId": "1856025", "name":
        "Carl Vondrick"}, {"authorId": "2367683", "name": "H. Pirsiavash"}, {"authorId":
        "143805211", "name": "A. Torralba"}]}, {"paperId": "fac17d4bb7268bbf33290de2915c33e6e57647da",
        "externalIds": {"DBLP": "journals/corr/EysenbachVT16", "MAG": "2560404968",
        "ArXiv": "1612.01175", "CorpusId": 2277274}, "corpusId": 2277274, "publicationVenue":
        {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names":
        ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/fac17d4bb7268bbf33290de2915c33e6e57647da",
        "title": "Who is Mistaken?", "abstract": "Recognizing when people have false
        beliefs is crucial for understanding their actions. We introduce the novel
        problem of identifying when people in abstract scenes have incorrect beliefs.
        We present a dataset of scenes, each visually depicting an 8-frame story in
        which a character has a mistaken belief. We then create a representation of
        characters'' beliefs for two tasks in human action understanding: predicting
        who is mistaken, and when they are mistaken. Experiments suggest that our
        method for identifying mistaken characters performs better on these tasks
        than simple baselines. Diagnostics on our model suggest it learns important
        cues for recognizing mistaken beliefs, such as gaze. We believe models of
        people''s beliefs will have many applications in action understanding, robotics,
        and healthcare.", "venue": "arXiv.org", "year": 2016, "referenceCount": 39,
        "citationCount": 10, "influentialCitationCount": 0, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
        "publicationDate": "2016-12-04", "journal": {"volume": "abs/1612.01175", "name":
        "ArXiv"}, "authors": [{"authorId": "8140754", "name": "Benjamin Eysenbach"},
        {"authorId": "1856025", "name": "Carl Vondrick"}, {"authorId": "143805211",
        "name": "A. Torralba"}]}, {"paperId": "0b07f20c2037a6ca5fcc1dd022092fd5c57dd647",
        "externalIds": {"CorpusId": 7567473}, "corpusId": 7567473, "publicationVenue":
        null, "url": "https://www.semanticscholar.org/paper/0b07f20c2037a6ca5fcc1dd022092fd5c57dd647",
        "title": "Kitchen Units for Food Units for Table . . . Past Future Predictor",
        "abstract": "In many computer vision applications, machines will need to reason
        beyond the present, and predict the future. This task is challenging because
        it requires leveraging extensive commonsense knowledge of the world that is
        difficult to write down. We believe that a promising resource for efficiently
        obtaining this knowledge is through the massive amounts of readily available
        unlabeled video. In this paper, we present a large scale framework that capitalizes
        on temporal structure in unlabeled video to learn to anticipate both actions
        and objects in the future. The key idea behind our approach is that we can
        train deep networks to predict the visual representation of images in the
        future. We experimentally validate this idea on two challenging \u201cin the
        wild\u201d video datasets, and our results suggest that learning with unlabeled
        videos significantly helps forecast actions and anticipate objects.", "venue":
        "", "year": 2015, "referenceCount": 51, "citationCount": 0, "influentialCitationCount":
        0, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
        null, "publicationDate": null, "journal": null, "authors": [{"authorId": "1856025",
        "name": "Carl Vondrick"}, {"authorId": "2367683", "name": "H. Pirsiavash"},
        {"authorId": "143805211", "name": "A. Torralba"}]}, {"paperId": "0fb3b63090f95af97723efe565893eb25ea9188c",
        "externalIds": {"MAG": "1599058448", "ArXiv": "1504.08023", "DBLP": "journals/corr/VondrickPT15",
        "CorpusId": 8596971}, "corpusId": 8596971, "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
        "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url":
        "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/0fb3b63090f95af97723efe565893eb25ea9188c",
        "title": "Anticipating the future by watching unlabeled video", "abstract":
        "In many computer vision applications, machines will need to reason beyond
        the present, and predict the future. This task is challenging because it requires
        leveraging extensive commonsense knowledge of the world that is difficult
        to write down. We believe that a promising resource for efficiently obtaining
        this knowledge is through the massive amounts of readily available unlabeled
        video. In this paper, we present a large scale framework that capitalizes
        on temporal structure in unlabeled video to learn to anticipate both actions
        and objects in the future. The key idea behind our approach is that we can
        train deep networks to predict the visual representation of images in the
        future. We experimentally validate this idea on two challenging \"in the wild\"
        video datasets, and our results suggest that learning with unlabeled videos
        significantly helps forecast actions and anticipate objects.", "venue": "arXiv.org",
        "year": 2015, "referenceCount": 52, "citationCount": 141, "influentialCitationCount":
        8, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
        "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle"], "publicationDate": "2015-04-29", "journal":
        {"volume": "abs/1504.08023", "name": "ArXiv"}, "authors": [{"authorId": "1856025",
        "name": "Carl Vondrick"}, {"authorId": "2367683", "name": "H. Pirsiavash"},
        {"authorId": "143805211", "name": "A. Torralba"}]}, {"paperId": "36961e595e31dea127bfa49124b5cc4f2100710f",
        "externalIds": {"DBLP": "journals/corr/ZhuVFR15", "MAG": "1999404243", "ArXiv":
        "1503.01508", "DOI": "10.1007/s11263-015-0812-2", "CorpusId": 877989}, "corpusId":
        877989, "publicationVenue": {"id": "939ee07c-6009-43f8-b884-69238b40659e",
        "name": "International Journal of Computer Vision", "type": "journal", "alternate_names":
        ["Int J Comput Vis"], "issn": "0920-5691", "url": "https://www.springer.com/computer/image+processing/journal/11263",
        "alternate_urls": ["https://link.springer.com/journal/11263", "http://link.springer.com/journal/11263"]},
        "url": "https://www.semanticscholar.org/paper/36961e595e31dea127bfa49124b5cc4f2100710f",
        "title": "Do We Need More Training Data?", "abstract": null, "venue": "International
        Journal of Computer Vision", "year": 2015, "referenceCount": 37, "citationCount":
        187, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf":
        {"url": "http://arxiv.org/pdf/1503.01508", "status": null}, "fieldsOfStudy":
        ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science",
        "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle"], "publicationDate": "2015-03-01", "journal":
        {"volume": "119", "pages": "76 - 92", "name": "International Journal of Computer
        Vision"}, "authors": [{"authorId": "32015491", "name": "Xiangxin Zhu"}, {"authorId":
        "1856025", "name": "Carl Vondrick"}, {"authorId": "143800213", "name": "Charless
        C. Fowlkes"}, {"authorId": "1770537", "name": "Deva Ramanan"}]}, {"paperId":
        "54bf134b47bdadbf2dd04956cf51076d4f26ce01", "externalIds": {"MAG": "2597049056",
        "ArXiv": "1502.05461", "DBLP": "journals/corr/VondrickKPMT15", "DOI": "10.1007/s11263-016-0884-7",
        "CorpusId": 13967139}, "corpusId": 13967139, "publicationVenue": {"id": "939ee07c-6009-43f8-b884-69238b40659e",
        "name": "International Journal of Computer Vision", "type": "journal", "alternate_names":
        ["Int J Comput Vis"], "issn": "0920-5691", "url": "https://www.springer.com/computer/image+processing/journal/11263",
        "alternate_urls": ["https://link.springer.com/journal/11263", "http://link.springer.com/journal/11263"]},
        "url": "https://www.semanticscholar.org/paper/54bf134b47bdadbf2dd04956cf51076d4f26ce01",
        "title": "Visualizing Object Detection Features", "abstract": null, "venue":
        "International Journal of Computer Vision", "year": 2015, "referenceCount":
        73, "citationCount": 55, "influentialCitationCount": 1, "isOpenAccess": true,
        "openAccessPdf": {"url": "http://dspace.mit.edu/bitstream/1721.1/82370/2/862074378-MIT.pdf",
        "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
        "publicationDate": "2015-02-18", "journal": {"volume": "119", "pages": "145-158",
        "name": "International Journal of Computer Vision"}, "authors": [{"authorId":
        "1856025", "name": "Carl Vondrick"}, {"authorId": "2556428", "name": "A. Khosla"},
        {"authorId": "2367683", "name": "H. Pirsiavash"}, {"authorId": "3045340",
        "name": "Tomasz Malisiewicz"}, {"authorId": "143805211", "name": "A. Torralba"}]},
        {"paperId": "7294d3ac0001e4b36c67aeb5c31d1db8ba1da23a", "externalIds": {"DBLP":
        "conf/nips/RecasensKVT15", "MAG": "2184540135", "CorpusId": 286832, "PubMed":
        "5812264"}, "corpusId": 286832, "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
        "name": "Neural Information Processing Systems", "type": "conference", "alternate_names":
        ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
        "url": "https://www.semanticscholar.org/paper/7294d3ac0001e4b36c67aeb5c31d1db8ba1da23a",
        "title": "Where are they looking?", "abstract": "Humans have the remarkable
        ability to follow the gaze of other people to identify what they are looking
        at. Following eye gaze, or gaze-following, is an important ability that allows
        us to understand what other people are thinking, the actions they are performing,
        and even predict what they might do next. Despite the importance of this topic,
        this problem has only been studied in limited scenarios within the computer
        vision community. In this paper, we propose a deep neural network-based approach
        for gaze-following and a new benchmark dataset, GazeFollow, for thorough evaluation.
        Given an image and the location of a head, our approach follows the gaze of
        the person and identifies the object being looked at. Our deep network is
        able to discover how to extract head pose and gaze orientation, and to select
        objects in the scene that are in the predicted line of sight and likely to
        be looked at (such as televisions, balls and food). The quantitative evaluation
        shows that our approach produces reliable results, even when viewing only
        the back of the head. While our method outperforms several baseline approaches,
        we are still far from reaching human performance on this task. Overall, we
        believe that gaze-following is a challenging and important problem that deserves
        more attention from the community.", "venue": "Neural Information Processing
        Systems", "year": 2015, "referenceCount": 28, "citationCount": 173, "influentialCitationCount":
        43, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Medicine",
        "Computer Science"], "s2FieldsOfStudy": [{"category": "Medicine", "source":
        "external"}, {"category": "Computer Science", "source": "external"}, {"category":
        "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
        "Conference"], "publicationDate": "2015-12-07", "journal": {"volume": "65
        40", "pages": "\n          1251\n        ", "name": "Nursing times"}, "authors":
        [{"authorId": "39257069", "name": "Adri\u00e0 Recasens"}, {"authorId": "2556428",
        "name": "A. Khosla"}, {"authorId": "1856025", "name": "Carl Vondrick"}, {"authorId":
        "143805211", "name": "A. Torralba"}]}, {"paperId": "8073bb5b5d26430c3d6ca55504c7c224a69736f0",
        "externalIds": {"CorpusId": 15507564}, "corpusId": 15507564, "publicationVenue":
        null, "url": "https://www.semanticscholar.org/paper/8073bb5b5d26430c3d6ca55504c7c224a69736f0",
        "title": "Goals Inferring the Why in Images", "abstract": "Humans have the
        remarkable capability to infer the motivations of other people\u2019s actions,
        likely due to cognitive skills known in psychophysics as the theory of mind.
        In this paper, we strive to build a computational model that predicts the
        motivation behind the actions of people from images. To our knowledge, this
        challenging problem has not yet been extensively explored in computer vision.
        We present a novel learning based framework that uses high-level visual recognition
        to infer why people are performing an actions in images. However, the information
        in an image alone may not be sufficient to automatically solve this task.
        Since humans can rely on their own experiences to infer motivation, we propose
        to give computer vision systems access to some of these experiences by using
        recently developed natural language models to mine knowledge stored in massive
        amounts of text. While we are still far away from automatically inferring
        motivation, our results suggest that transferring knowledge from language
        into vision can help machines understand why a person might be performing
        an action in an image.", "venue": "", "year": 2015, "referenceCount": 3, "citationCount":
        0, "influentialCitationCount": 0, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category": "Computer Science",
        "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": null,
        "journal": null, "authors": [{"authorId": "2367683", "name": "H. Pirsiavash"},
        {"authorId": "1856025", "name": "Carl Vondrick"}, {"authorId": "143805211",
        "name": "A. Torralba"}]}, {"paperId": "932ac3707e1ed84ab67526692a1ef8f064f24ab5",
        "externalIds": {"MAG": "2789198060", "DBLP": "conf/cvpr/VondrickPT16", "DOI":
        "10.1109/CVPR.2016.18", "CorpusId": 10533233}, "corpusId": 10533233, "publicationVenue":
        {"id": "768b87bb-8a18-4d9c-a161-4d483c776bcf", "name": "Computer Vision and
        Pattern Recognition", "type": "conference", "alternate_names": ["CVPR", "Comput
        Vis Pattern Recognit"], "issn": "1063-6919", "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
        "alternate_urls": ["https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"]},
        "url": "https://www.semanticscholar.org/paper/932ac3707e1ed84ab67526692a1ef8f064f24ab5",
        "title": "Anticipating Visual Representations from Unlabeled Video", "abstract":
        "Anticipating actions and objects before they start or appear is a difficult
        problem in computer vision with several real-world applications. This task
        is challenging partly because it requires leveraging extensive knowledge of
        the world that is difficult to write down. We believe that a promising resource
        for efficiently learning this knowledge is through readily available unlabeled
        video. We present a framework that capitalizes on temporal structure in unlabeled
        video to learn to anticipate human actions and objects. The key idea behind
        our approach is that we can train deep networks to predict the visual representation
        of images in the future. Visual representations are a promising prediction
        target because they encode images at a higher semantic level than pixels yet
        are automatic to compute. We then apply recognition algorithms on our predicted
        representation to anticipate objects and actions. We experimentally validate
        this idea on two datasets, anticipating actions one second in the future and
        objects five seconds in the future.", "venue": "Computer Vision and Pattern
        Recognition", "year": 2015, "referenceCount": 45, "citationCount": 462, "influentialCitationCount":
        35, "isOpenAccess": true, "openAccessPdf": {"url": "http://arxiv.org/pdf/1504.08023",
        "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
        "Conference"], "publicationDate": "2015-04-29", "journal": {"pages": "98-106",
        "name": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"},
        "authors": [{"authorId": "1856025", "name": "Carl Vondrick"}, {"authorId":
        "2367683", "name": "H. Pirsiavash"}, {"authorId": "143805211", "name": "A.
        Torralba"}]}, {"paperId": "03c48850373b40f32b2bc0b1fbf7c13ccf0c8063", "externalIds":
        {"MAG": "2787052186", "DBLP": "conf/cvpr/VondrickOPT16", "DOI": "10.1109/CVPR.2016.327",
        "CorpusId": 2237901}, "corpusId": 2237901, "publicationVenue": {"id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
        "name": "Computer Vision and Pattern Recognition", "type": "conference", "alternate_names":
        ["CVPR", "Comput Vis Pattern Recognit"], "issn": "1063-6919", "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
        "alternate_urls": ["https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"]},
        "url": "https://www.semanticscholar.org/paper/03c48850373b40f32b2bc0b1fbf7c13ccf0c8063",
        "title": "Predicting Motivations of Actions by Leveraging Text", "abstract":
        "Understanding human actions is a key problem in computer vision. However,
        recognizing actions is only the first step of understanding what a person
        is doing. In this paper, we introduce the problem of predicting why a person
        has performed an action in images. This problem has many applications in human
        activity understanding, such as anticipating or explaining an action. To study
        this problem, we introduce a new dataset of people performing actions annotated
        with likely motivations. However, the information in an image alone may not
        be sufficient to automatically solve this task. Since humans can rely on their
        lifetime of experiences to infer motivation, we propose to give computer vision
        systems access to some of these experiences by using recently developed natural
        language models to mine knowledge stored in massive amounts of text. While
        we are still far away from fully understanding motivation, our results suggest
        that transferring knowledge from language into vision can help machines understand
        why people in images might be performing an action.", "venue": "Computer Vision
        and Pattern Recognition", "year": 2014, "referenceCount": 49, "citationCount":
        38, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf":
        {"url": "https://arxiv.org/pdf/1406.5472", "status": null}, "fieldsOfStudy":
        ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science",
        "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2014-06-20",
        "journal": {"pages": "2997-3005", "name": "2016 IEEE Conference on Computer
        Vision and Pattern Recognition (CVPR)"}, "authors": [{"authorId": "1856025",
        "name": "Carl Vondrick"}, {"authorId": "4781463", "name": "Deniz Oktay"},
        {"authorId": "2367683", "name": "H. Pirsiavash"}, {"authorId": "143805211",
        "name": "A. Torralba"}]}, {"paperId": "480f8aa54b19e7b6b31be09aa2124cb0f159ae9b",
        "externalIds": {"MAG": "1934133592", "DBLP": "conf/nips/VondrickPOT15", "CorpusId":
        1639049}, "corpusId": 1639049, "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
        "name": "Neural Information Processing Systems", "type": "conference", "alternate_names":
        ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
        "url": "https://www.semanticscholar.org/paper/480f8aa54b19e7b6b31be09aa2124cb0f159ae9b",
        "title": "Learning visual biases from human imagination", "abstract": "Although
        the human visual system can recognize many concepts under challenging conditions,
        it still has some biases. In this paper, we investigate whether we can extract
        these biases and transfer them into a machine recognition system. We introduce
        a novel method that, inspired by well-known tools in human psychophysics,
        estimates the biases that the human visual system might use for recognition,
        but in computer vision feature spaces. Our experiments are surprising, and
        suggest that classifiers from the human visual system can be transferred into
        a machine with some success. Since these classifiers seem to capture favorable
        biases in the human visual system, we further present an SVM formulation that
        constrains the orientation of the SVM hyperplane to agree with the bias from
        human visual system. Our results suggest that transferring this human bias
        into machines may help object recognition systems generalize across datasets
        and perform better when very little training data is available.", "venue":
        "Neural Information Processing Systems", "year": 2014, "referenceCount": 48,
        "citationCount": 22, "influentialCitationCount": 2, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
        "Conference"], "publicationDate": "2014-10-17", "journal": {"pages": "289-297"},
        "authors": [{"authorId": "1856025", "name": "Carl Vondrick"}, {"authorId":
        "2367683", "name": "H. Pirsiavash"}, {"authorId": "143868587", "name": "A.
        Oliva"}, {"authorId": "143805211", "name": "A. Torralba"}]}, {"paperId": "6bb60de0e93c9df260ef3d20dbff0716ed1ec711",
        "externalIds": {"DBLP": "conf/eccv/PirsiavashVT14", "MAG": "2260521078", "DOI":
        "10.1007/978-3-319-10599-4_36", "CorpusId": 123520}, "corpusId": 123520, "publicationVenue":
        {"id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806", "name": "European Conference
        on Computer Vision", "type": "conference", "alternate_names": ["ECCV", "Eur
        Conf Comput Vis"], "url": "https://link.springer.com/conference/eccv"}, "url":
        "https://www.semanticscholar.org/paper/6bb60de0e93c9df260ef3d20dbff0716ed1ec711",
        "title": "Assessing the Quality of Actions", "abstract": null, "venue": "European
        Conference on Computer Vision", "year": 2014, "referenceCount": 43, "citationCount":
        165, "influentialCitationCount": 40, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
        "Computer Science", "source": "external"}, {"category": "Computer Science",
        "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"],
        "publicationDate": "2014-09-06", "journal": {"pages": "556-571"}, "authors":
        [{"authorId": "2367683", "name": "H. Pirsiavash"}, {"authorId": "1856025",
        "name": "Carl Vondrick"}, {"authorId": "143805211", "name": "A. Torralba"}]},
        {"paperId": "dfe448d6297ea0a3d4deba21fbf1006bc35877d7", "externalIds": {"MAG":
        "46519926", "ArXiv": "1406.5472", "DBLP": "journals/corr/PirsiavashVT14",
        "DOI": "10.21236/ada612444", "CorpusId": 16732164}, "corpusId": 16732164,
        "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name":
        "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"},
        "url": "https://www.semanticscholar.org/paper/dfe448d6297ea0a3d4deba21fbf1006bc35877d7",
        "title": "Inferring the Why in Images", "abstract": "Abstract : Humans have
        the remarkable capability to infer the motivations of other people''s actions,
        likely due to cognitive skills known in psychophysics as the theory of mind.
        In this paper, we strive to build a computational model that predicts the
        motivation behind the actions of people from images. To our knowledge, this
        challenging problem has not yet been extensively explored in computer vision.
        We present a novel learning based framework that uses high-level visual recognition
        to infer why people are performing an actions in images. However, the information
        in an image alone may not be sufficient to automatically solve this task.
        Since humans can rely on their own experiences to infer motivation, we propose
        to give computer vision systems access to some of these experiences by using
        recently developed natural language models to mine knowledge stored in massive
        amounts of text. While we are still far away from automatically inferring
        motivation, our results suggest that transferring knowledge from language
        into vision can help machines understand why a person might be performing
        an action in an image.", "venue": "arXiv.org", "year": 2014, "referenceCount":
        39, "citationCount": 35, "influentialCitationCount": 5, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
        "publicationDate": "2014-06-20", "journal": {"volume": "abs/1406.5472", "name":
        "ArXiv"}, "authors": [{"authorId": "2367683", "name": "H. Pirsiavash"}, {"authorId":
        "1856025", "name": "Carl Vondrick"}, {"authorId": "143805211", "name": "A.
        Torralba"}]}, {"paperId": "e30a0e7a798831f8ab8b82ea3c51ba68201b27d1", "externalIds":
        {"DBLP": "journals/corr/VondrickPOT14", "MAG": "2190806670", "ArXiv": "1410.4627",
        "DOI": "10.21236/ada612443", "CorpusId": 6963502}, "corpusId": 6963502, "publicationVenue":
        {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names":
        ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/e30a0e7a798831f8ab8b82ea3c51ba68201b27d1",
        "title": "Acquiring Visual Classifiers from Human Imagination", "abstract":
        "Abstract : The human mind can remarkably imagine objects that it has never
        seen, touched, or heard, all in vivid detail. Motivated by the desire to harness
        this rich source of information from the human mind, this paper investigates
        how to extract classifiers from the human visual system and leverage them
        in a machine. We introduce a method that, inspired by wellknown tools in human
        psychophysics, estimates the classifier that the human visual system might
        use for recognition but in computer vision feature spaces. Our experiments
        are surprising, and suggest that classifiers from the human visual system
        can be transferred into a machine with some success. Since these classifiers
        seem to capture favorable biases in the human visual system, we present a
        novel SVM formulation that constrains the orientation of the SVM hyperplane
        to agree with the human visual system. Our results suggest that transferring
        this human bias into machines can help object recognition systems generalize
        across datasets. Moreover, we found that people''s culture may subtly vary
        the objects that people imagine, which influences this bias. Overall, human
        imagination can be an interesting resource for future visual recognition systems.",
        "venue": "arXiv.org", "year": 2014, "referenceCount": 42, "citationCount":
        5, "influentialCitationCount": 1, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": ["Computer Science", "Psychology"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Psychology",
        "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle"], "publicationDate": "2014-10-16", "journal":
        {"volume": "abs/1410.4627", "name": "ArXiv"}, "authors": [{"authorId": "1856025",
        "name": "Carl Vondrick"}, {"authorId": "2367683", "name": "H. Pirsiavash"},
        {"authorId": "143868587", "name": "A. Oliva"}, {"authorId": "143805211", "name":
        "A. Torralba"}]}, {"paperId": "854994253119aa3dbae827a42ca3a6d91d46f215",
        "externalIds": {"MAG": "1982428585", "DBLP": "conf/iccv/VondrickKMT13", "DOI":
        "10.1109/ICCV.2013.8", "CorpusId": 11891893}, "corpusId": 11891893, "publicationVenue":
        {"id": "7654260e-79f9-45c5-9663-d72027cf88f3", "name": "IEEE International
        Conference on Computer Vision", "type": "conference", "alternate_names": ["ICCV",
        "IEEE Int Conf Comput Vis", "ICCV Workshops", "ICCV Work"], "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"},
        "url": "https://www.semanticscholar.org/paper/854994253119aa3dbae827a42ca3a6d91d46f215",
        "title": "HOGgles: Visualizing Object Detection Features", "abstract": "We
        introduce algorithms to visualize feature spaces used by object detectors.
        The tools in this paper allow a human to put on ''HOG goggles'' and perceive
        the visual world as a HOG based object detector sees it. We found that these
        visualizations allow us to analyze object detection systems in new ways and
        gain new insight into the detector''s failures. For example, when we visualize
        the features for high scoring false alarms, we discovered that, although they
        are clearly wrong in image space, they do look deceptively similar to true
        positives in feature space. This result suggests that many of these false
        alarms are caused by our choice of feature space, and indicates that creating
        a better learning algorithm or building bigger datasets is unlikely to correct
        these errors. By visualizing feature spaces, we can gain a more intuitive
        understanding of our detection systems.", "venue": "IEEE International Conference
        on Computer Vision", "year": 2013, "referenceCount": 25, "citationCount":
        306, "influentialCitationCount": 24, "isOpenAccess": true, "openAccessPdf":
        {"url": "http://people.csail.mit.edu/tomasz/papers/vondrick_iccv2013.pdf",
        "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
        "Conference"], "publicationDate": "2013-12-01", "journal": {"pages": "1-8",
        "name": "2013 IEEE International Conference on Computer Vision"}, "authors":
        [{"authorId": "1856025", "name": "Carl Vondrick"}, {"authorId": "2556428",
        "name": "A. Khosla"}, {"authorId": "3045340", "name": "Tomasz Malisiewicz"},
        {"authorId": "143805211", "name": "A. Torralba"}]}, {"paperId": "4cb1494e547f1eaf51bab8038c8fab904dda9026",
        "externalIds": {"MAG": "2046875449", "DBLP": "conf/bmvc/ZhuVRF12", "DOI":
        "10.5244/C.26.80", "CorpusId": 7582143}, "corpusId": 7582143, "publicationVenue":
        {"id": "78a7fbcc-41c5-4258-b633-04b8637d4a9f", "name": "British Machine Vision
        Conference", "type": "conference", "alternate_names": ["Br Mach Vis Conf",
        "BMVC"], "url": "http://www.bmva.org/bmvc/"}, "url": "https://www.semanticscholar.org/paper/4cb1494e547f1eaf51bab8038c8fab904dda9026",
        "title": "Do We Need More Training Data or Better Models for Object Detection?",
        "abstract": "Datasets for training object recognition systems are steadily
        growing in size. This paper investigates the question of whether existing
        detectors will continue to improve as data grows, or if models are close to
        saturating due to limited model complexity and the Bayes risk associated with
        the feature spaces in which they operate. We focus on the popular paradigm
        of scanning-window templates defined on oriented gradient features, trained
        with discriminative classifiers. We investigate the performance of mixtures
        of templates as a function of the number of templates (complexity) and the
        amount of training data. We find that additional data does help, but only
        with correct regularization and treatment of noisy examples or \u201coutliers\u201d
        in the training data. Surprisingly, the performance of problem domain-agnostic
        mixture models appears to saturate quickly (\u223c10 templates and \u223c100
        positive training examples per template). However, compositional mixtures
        (implemented via composed parts) give much better performance because they
        share parameters among templates, and can synthesize new templates not encountered
        during training. This suggests there is still room to improve performance
        with linear classifiers and the existing feature space by improved representations
        and learning algorithms.", "venue": "British Machine Vision Conference", "year":
        2012, "referenceCount": 18, "citationCount": 183, "influentialCitationCount":
        8, "isOpenAccess": true, "openAccessPdf": {"url": "http://www.bmva.org/bmvc/2012/BMVC/paper080/paper080.pdf",
        "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
        "Conference"], "publicationDate": null, "journal": {"pages": "1-11"}, "authors":
        [{"authorId": "32015491", "name": "Xiangxin Zhu"}, {"authorId": "1856025",
        "name": "Carl Vondrick"}, {"authorId": "1770537", "name": "Deva Ramanan"},
        {"authorId": "143800213", "name": "Charless C. Fowlkes"}]}, {"paperId": "981e7c22aaeb7756e2f7bb33186d44b4929bd76e",
        "externalIds": {"MAG": "1506491340", "DBLP": "journals/ijcv/VondrickPR13",
        "DOI": "10.1007/s11263-012-0564-1", "CorpusId": 2315620}, "corpusId": 2315620,
        "publicationVenue": {"id": "939ee07c-6009-43f8-b884-69238b40659e", "name":
        "International Journal of Computer Vision", "type": "journal", "alternate_names":
        ["Int J Comput Vis"], "issn": "0920-5691", "url": "https://www.springer.com/computer/image+processing/journal/11263",
        "alternate_urls": ["https://link.springer.com/journal/11263", "http://link.springer.com/journal/11263"]},
        "url": "https://www.semanticscholar.org/paper/981e7c22aaeb7756e2f7bb33186d44b4929bd76e",
        "title": "Efficiently Scaling up Crowdsourced Video Annotation", "abstract":
        null, "venue": "International Journal of Computer Vision", "year": 2012, "referenceCount":
        48, "citationCount": 542, "influentialCitationCount": 38, "isOpenAccess":
        false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
        "publicationDate": "2012-09-05", "journal": {"volume": "101", "pages": "184
        - 204", "name": "International Journal of Computer Vision"}, "authors": [{"authorId":
        "1856025", "name": "Carl Vondrick"}, {"authorId": "2247333970", "name": "Donald
        J. Patterson"}, {"authorId": "2247329199", "name": "Deva Ramanan"}]}, {"paperId":
        "dc4ea16406f985b7a045cf0fa6254c8b12accf9d", "externalIds": {"DBLP": "journals/corr/abs-1212-2278",
        "ArXiv": "1212.2278", "MAG": "1737613372", "CorpusId": 6013731}, "corpusId":
        6013731, "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
        "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url":
        "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/dc4ea16406f985b7a045cf0fa6254c8b12accf9d",
        "title": "Inverting and Visualizing Features for Object Detection", "abstract":
        "Abstract : This paper presents methods to visualize feature spaces commonly
        used in object detection. The tools in this paper allow a human to put on
        feature space glasses and see the visual world as a computer might see it.
        We found that these glasses allow us to gain insight into the behavior of
        computer vision systems. We show a variety of experiments with our visualizations,
        such as examining the linear separability of recognition in HOG space, generating
        high scoring super objects for an object detector, and diagnosing false positives.
        We pose the visualization problem as one of feature inversion, i.e. recovering
        the natural image that generated a feature descriptor. We describe four algorithms
        to tackle this task, with different trade-offs in speed accuracy, and scalability.
        Our most successful algorithm uses ideas from sparse coding to learn a pair
        of dictionaries that enable regression between HOG features and natural images,
        and can invert features at interactive rates. We believe these visualizations
        are useful tools to add to an object detector researcher''s toolbox, and code
        is available.", "venue": "arXiv.org", "year": 2012, "referenceCount": 36,
        "citationCount": 18, "influentialCitationCount": 1, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
        "publicationDate": "2012-12-10", "journal": {"volume": "abs/1212.2278", "name":
        "ArXiv"}, "authors": [{"authorId": "1856025", "name": "Carl Vondrick"}, {"authorId":
        "2556428", "name": "A. Khosla"}, {"authorId": "3045340", "name": "Tomasz Malisiewicz"},
        {"authorId": "143805211", "name": "A. Torralba"}]}, {"paperId": "2c305fa65fed336e6be1d15a6567075c6ea6e51b",
        "externalIds": {"MAG": "2146997834", "DBLP": "conf/avss/OhHPCCLMALDSWJRSVPRYTSFCD11",
        "DOI": "10.1109/AVSS.2011.6027400", "CorpusId": 1826131}, "corpusId": 1826131,
        "publicationVenue": {"id": "827334c9-c744-4f75-9b28-571cb89ad45f", "name":
        "Advanced Video and Signal Based Surveillance", "type": "conference", "alternate_names":
        ["AVSS", "Adv Video Signal Based Surveill"], "url": "http://www.wikicfp.com/cfp/program?id=266"},
        "url": "https://www.semanticscholar.org/paper/2c305fa65fed336e6be1d15a6567075c6ea6e51b",
        "title": "AVSS 2011 demo session: A large-scale benchmark dataset for event
        recognition in surveillance video", "abstract": "Summary form only given.
        We present a concept for automatic construction site monitoring by taking
        into account 4D information (3D over time), that is acquired from highly-overlapping
        digital aerial images. On the one hand today''s maturity of flying micro aerial
        vehicles (MAVs) enables a low-cost and an efficient image acquisition of high-quality
        data that maps construction sites entirely from many varying viewpoints. On
        the other hand, due to low-noise sensors and high redundancy in the image
        data, recent developments in 3D reconstruction workflows have benefited the
        automatic computation of accurate and dense 3D scene information. Having both
        an inexpensive high-quality image acquisition and an efficient 3D analysis
        workflow enables monitoring, documentation and visualization of observed sites
        over time with short intervals. Relating acquired 4D site observations, composed
        of color, texture, geometry over time, largely supports automated methods
        toward full scene understanding, the acquisition of both the change and the
        construction site''s progress.", "venue": "Advanced Video and Signal Based
        Surveillance", "year": 2011, "referenceCount": 1, "citationCount": 706, "influentialCitationCount":
        77, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
        "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle"], "publicationDate": "2011-08-30", "journal":
        {"pages": "527-528"}, "authors": [{"authorId": "2457612", "name": "Sangmin
        Oh"}, {"authorId": "1397590190", "name": "Anthony J. Hoogs"}, {"authorId":
        "145895163", "name": "A. Perera"}, {"authorId": "1803047", "name": "Naresh
        P. Cuntoor"}, {"authorId": "1786891", "name": "Chia-Chih Chen"}, {"authorId":
        "2108613995", "name": "J. T. Lee"}, {"authorId": "2972072", "name": "Saurajit
        Mukherjee"}, {"authorId": "1705627", "name": "J. Aggarwal"}, {"authorId":
        "2445131", "name": "Hyungtae Lee"}, {"authorId": "1693428", "name": "L. Davis"},
        {"authorId": "2754027", "name": "E. Swears"}, {"authorId": "48631781", "name":
        "Xiaoyang Wang"}, {"authorId": "50426357", "name": "Q. Ji"}, {"authorId":
        "145096334", "name": "K. Reddy"}, {"authorId": "145103012", "name": "M. Shah"},
        {"authorId": "1856025", "name": "Carl Vondrick"}, {"authorId": "2367683",
        "name": "H. Pirsiavash"}, {"authorId": "1770537", "name": "Deva Ramanan"},
        {"authorId": "143738177", "name": "Jenny Yuen"}, {"authorId": "143805211",
        "name": "A. Torralba"}, {"authorId": "145178895", "name": "Bi Song"}, {"authorId":
        "1409113695", "name": "Anesco Fong"}, {"authorId": "1404727582", "name": "A.
        Roy-Chowdhury"}, {"authorId": "46988196", "name": "M. Desai"}]}, {"paperId":
        "5f5d107016990cb297c26fbd7bee083c6df3aa62", "externalIds": {"DBLP": "conf/nips/VondrickR11",
        "MAG": "2117907414", "CorpusId": 5787213}, "corpusId": 5787213, "publicationVenue":
        {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd", "name": "Neural Information
        Processing Systems", "type": "conference", "alternate_names": ["Neural Inf
        Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"}, "url": "https://www.semanticscholar.org/paper/5f5d107016990cb297c26fbd7bee083c6df3aa62",
        "title": "Video Annotation and Tracking with Active Learning", "abstract":
        "We introduce a novel active learning framework for video annotation. By judiciously
        choosing which frames a user should annotate, we can obtain highly accurate
        tracks with minimal user effort. We cast this problem as one of active learning,
        and show that we can obtain excellent performance by querying frames that,
        if annotated, would produce a large expected change in the estimated object
        track. We implement a constrained tracker and compute the expected change
        for putative annotations with efficient dynamic programming algorithms. We
        demonstrate our framework on four datasets, including two benchmark datasets
        constructed with key frame annotations obtained by Amazon Mechanical Turk.
        Our results indicate that we could obtain equivalent labels for a small fraction
        of the original cost.", "venue": "Neural Information Processing Systems",
        "year": 2011, "referenceCount": 18, "citationCount": 116, "influentialCitationCount":
        5, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
        "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2011-12-12",
        "journal": {"pages": "28-36"}, "authors": [{"authorId": "1856025", "name":
        "Carl Vondrick"}, {"authorId": "1770537", "name": "Deva Ramanan"}]}, {"paperId":
        "89d80d34bc0cf1cc9f62a76c0ffa5b01f05b9ee7", "externalIds": {"MAG": "2142996775",
        "DBLP": "conf/cvpr/OhHPCCLMALDSWJRSVPRYTSFRD11", "DOI": "10.1109/CVPR.2011.5995586",
        "CorpusId": 263882069}, "corpusId": 263882069, "publicationVenue": {"id":
        "768b87bb-8a18-4d9c-a161-4d483c776bcf", "name": "Computer Vision and Pattern
        Recognition", "type": "conference", "alternate_names": ["CVPR", "Comput Vis
        Pattern Recognit"], "issn": "1063-6919", "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
        "alternate_urls": ["https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"]},
        "url": "https://www.semanticscholar.org/paper/89d80d34bc0cf1cc9f62a76c0ffa5b01f05b9ee7",
        "title": "A large-scale benchmark dataset for event recognition in surveillance
        video", "abstract": "We introduce a new large-scale video dataset designed
        to assess the performance of diverse visual event recognition algorithms with
        a focus on continuous visual event recognition (CVER) in outdoor areas with
        wide coverage. Previous datasets for action recognition are unrealistic for
        real-world surveillance because they consist of short clips showing one action
        by one individual [15, 8]. Datasets have been developed for movies [11] and
        sports [12], but, these actions and scene conditions do not apply effectively
        to surveillance videos. Our dataset consists of many outdoor scenes with actions
        occurring naturally by non-actors in continuously captured videos of the real
        world. The dataset includes large numbers of instances for 23 event types
        distributed throughout 29 hours of video. This data is accompanied by detailed
        annotations which include both moving object tracks and event examples, which
        will provide solid basis for large-scale evaluation. Additionally, we propose
        different types of evaluation modes for visual recognition tasks and evaluation
        metrics along with our preliminary experimental results. We believe that this
        dataset will stimulate diverse aspects of computer vision research and help
        us to advance the CVER tasks in the years ahead.", "venue": "Computer Vision
        and Pattern Recognition", "year": 2011, "referenceCount": 21, "citationCount":
        16, "influentialCitationCount": 1, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
        "Computer Science", "source": "external"}, {"category": "Computer Science",
        "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
        "2011-06-20", "journal": {"pages": "3153-3160", "name": "CVPR 2011"}, "authors":
        [{"authorId": "2240396630", "name": "Sangmin Oh"}, {"authorId": "1397590190",
        "name": "Anthony J. Hoogs"}, {"authorId": "145895163", "name": "A. Perera"},
        {"authorId": "1803047", "name": "Naresh P. Cuntoor"}, {"authorId": "1786891",
        "name": "Chia-Chih Chen"}, {"authorId": "2257769679", "name": "Jong Taek Lee"},
        {"authorId": "2972072", "name": "Saurajit Mukherjee"}, {"authorId": "2244745632",
        "name": "Jake K. Aggarwal"}, {"authorId": "2257367677", "name": "Hyungtae
        Lee"}, {"authorId": "2257236826", "name": "Larry S. Davis"}, {"authorId":
        "2754027", "name": "E. Swears"}, {"authorId": "2258332678", "name": "Xiaoyang
        Wang"}, {"authorId": "2257292253", "name": "Qiang Ji"}, {"authorId": "2257351000",
        "name": "Kishore K. Reddy"}, {"authorId": "2242383769", "name": "Mubarak Shah"},
        {"authorId": "1856025", "name": "Carl Vondrick"}, {"authorId": "2257286345",
        "name": "Hamed Pirsiavash"}, {"authorId": "2249174292", "name": "Deva Ramanan"},
        {"authorId": "2242266673", "name": "J. Yuen"}, {"authorId": "2257247342",
        "name": "Antonio Torralba"}, {"authorId": "145178895", "name": "Bi Song"},
        {"authorId": "2257289563", "name": "Anesco Fong"}, {"authorId": "1404727582",
        "name": "A. Roy-Chowdhury"}, {"authorId": "46988196", "name": "M. Desai"}]},
        {"paperId": "09dd01e19b247a33162d71f07491781bdf4bfd00", "externalIds": {"DBLP":
        "conf/eccv/VondrickRP10", "MAG": "1514923623", "DOI": "10.1007/978-3-642-15561-1_44",
        "CorpusId": 386910}, "corpusId": 386910, "publicationVenue": {"id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
        "name": "European Conference on Computer Vision", "type": "conference", "alternate_names":
        ["ECCV", "Eur Conf Comput Vis"], "url": "https://link.springer.com/conference/eccv"},
        "url": "https://www.semanticscholar.org/paper/09dd01e19b247a33162d71f07491781bdf4bfd00",
        "title": "Efficiently Scaling Up Video Annotation with Crowdsourced Marketplaces",
        "abstract": null, "venue": "European Conference on Computer Vision", "year":
        2010, "referenceCount": 16, "citationCount": 101, "influentialCitationCount":
        7, "isOpenAccess": true, "openAccessPdf": {"url": "https://link.springer.com/content/pdf/10.1007/978-3-642-15561-1_44.pdf",
        "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
        "Conference"], "publicationDate": "2010-09-05", "journal": {"pages": "610-623"},
        "authors": [{"authorId": "1856025", "name": "Carl Vondrick"}, {"authorId":
        "1770537", "name": "Deva Ramanan"}, {"authorId": "143712289", "name": "Donald
        J. Patterson"}]}]}]}

        '
    headers:
      Access-Control-Allow-Origin:
      - '*'
      Connection:
      - keep-alive
      Content-Length:
      - '248296'
      Content-Type:
      - application/json
      Date:
      - Thu, 19 Oct 2023 20:32:07 GMT
      Via:
      - 1.1 1dbafa627b28576f280c85d24d02a0c2.cloudfront.net (CloudFront)
      X-Amz-Cf-Id:
      - CXpuYisiMuGj6Hi1IoW9bUbNjUeDYIiwVWQFKmdHQRfsvjTbZTPo-g==
      X-Amz-Cf-Pop:
      - JFK52-P2
      X-Cache:
      - Miss from cloudfront
      x-amz-apigw-id:
      - NEPLJFp3vHcFSiA=
      x-amzn-Remapped-Connection:
      - keep-alive
      x-amzn-Remapped-Content-Length:
      - '248296'
      x-amzn-Remapped-Date:
      - Thu, 19 Oct 2023 20:32:07 GMT
      x-amzn-Remapped-Server:
      - gunicorn
      x-amzn-RequestId:
      - ad7d9bc2-a162-40b5-9f8c-7e752cd6141b
    status:
      code: 200
      message: OK
version: 1
