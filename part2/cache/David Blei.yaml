interactions:
- request:
    body: null
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      User-Agent:
      - python-requests/2.31.0
    method: GET
    uri: https://api.semanticscholar.org/graph/v1/author/search?query=David%20Blei&fields=affiliations,aliases,authorId,citationCount,externalIds,hIndex,homepage,name,paperCount,papers,papers.abstract,papers.authors,papers.citationCount,papers.corpusId,papers.externalIds,papers.fieldsOfStudy,papers.influentialCitationCount,papers.isOpenAccess,papers.journal,papers.openAccessPdf,papers.paperId,papers.publicationDate,papers.publicationTypes,papers.publicationVenue,papers.referenceCount,papers.s2FieldsOfStudy,papers.title,papers.url,papers.venue,papers.year,url&offset=0&limit=1000
  response:
    body:
      string: '{"total": 2, "offset": 0, "data": [{"authorId": "1796335", "externalIds":
        {"DBLP": ["David Blei", "David M. Blei"]}, "url": "https://www.semanticscholar.org/author/1796335",
        "name": "D. Blei", "aliases": ["David M. Blei", "David Blei", "D. Blei", "David.
        Blei", "David M Blei"], "affiliations": [], "homepage": null, "paperCount":
        356, "citationCount": 90466, "hIndex": 94, "papers": [{"paperId": "0926bffd052d95e54db0ba8b38ca36dd1ec387e8",
        "externalIds": {"ArXiv": "2301.00537", "DBLP": "conf/nips/WangBC21", "DOI":
        "10.48550/arXiv.2301.00537", "CorpusId": 248498415}, "corpusId": 248498415,
        "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd", "name":
        "Neural Information Processing Systems", "type": "conference", "alternate_names":
        ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
        "url": "https://www.semanticscholar.org/paper/0926bffd052d95e54db0ba8b38ca36dd1ec387e8",
        "title": "Posterior Collapse and Latent Variable Non-identifiability", "abstract":
        "Variational autoencoders model high-dimensional data by positing low-dimensional
        latent variables that are mapped through a flexible distribution parametrized
        by a neural network. Unfortunately, variational autoencoders often suffer
        from posterior collapse: the posterior of the latent variables is equal to
        its prior, rendering the variational autoencoder useless as a means to produce
        meaningful representations. Existing approaches to posterior collapse often
        attribute it to the use of neural networks or optimization issues due to variational
        approximation. In this paper, we consider posterior collapse as a problem
        of latent variable non-identifiability. We prove that the posterior collapses
        if and only if the latent variables are non-identifiable in the generative
        model. This fact implies that posterior collapse is not a phenomenon specific
        to the use of flexible distributions or approximate inference. Rather, it
        can occur in classical probabilistic models even with exact inference, which
        we also demonstrate. Based on these results, we propose a class of latent-identifiable
        variational autoencoders, deep generative models which enforce identifiability
        without sacrificing flexibility. This model class resolves the problem of
        latent variable non-identifiability by leveraging bijective Brenier maps and
        parameterizing them with input convex neural networks, without special variational
        inference objectives or optimization tricks. Across synthetic and real datasets,
        latent-identifiable variational autoencoders outperform existing methods in
        mitigating posterior collapse and providing meaningful representations of
        the data.", "venue": "Neural Information Processing Systems", "year": 2023,
        "referenceCount": 64, "citationCount": 33, "influentialCitationCount": 7,
        "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science",
        "source": "external"}, {"category": "Mathematics", "source": "external"},
        {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
        ["JournalArticle"], "publicationDate": "2023-01-02", "journal": {"volume":
        "abs/2301.00537", "name": "ArXiv"}, "authors": [{"authorId": "2108734693",
        "name": "Yixin Wang"}, {"authorId": "1796335", "name": "D. Blei"}, {"authorId":
        "2575774", "name": "J. Cunningham"}]}, {"paperId": "12acdfc7e32e9d603dc108008bb15e65439e7c79",
        "externalIds": {"DBLP": "journals/corr/abs-2307-14324", "ArXiv": "2307.14324",
        "DOI": "10.48550/arXiv.2307.14324", "CorpusId": 260164518}, "corpusId": 260164518,
        "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name":
        "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"},
        "url": "https://www.semanticscholar.org/paper/12acdfc7e32e9d603dc108008bb15e65439e7c79",
        "title": "Evaluating the Moral Beliefs Encoded in LLMs", "abstract": "This
        paper presents a case study on the design, administration, post-processing,
        and evaluation of surveys on large language models (LLMs). It comprises two
        components: (1) A statistical method for eliciting beliefs encoded in LLMs.
        We introduce statistical measures and evaluation metrics that quantify the
        probability of an LLM\"making a choice\", the associated uncertainty, and
        the consistency of that choice. (2) We apply this method to study what moral
        beliefs are encoded in different LLMs, especially in ambiguous cases where
        the right choice is not obvious. We design a large-scale survey comprising
        680 high-ambiguity moral scenarios (e.g.,\"Should I tell a white lie?\") and
        687 low-ambiguity moral scenarios (e.g.,\"Should I stop for a pedestrian on
        the road?\"). Each scenario includes a description, two possible actions,
        and auxiliary labels indicating violated rules (e.g.,\"do not kill\"). We
        administer the survey to 28 open- and closed-source LLMs. We find that (a)
        in unambiguous scenarios, most models\"choose\"actions that align with commonsense.
        In ambiguous cases, most models express uncertainty. (b) Some models are uncertain
        about choosing the commonsense action because their responses are sensitive
        to the question-wording. (c) Some models reflect clear preferences in ambiguous
        scenarios. Specifically, closed-source models tend to agree with each other.",
        "venue": "arXiv.org", "year": 2023, "referenceCount": 73, "citationCount":
        6, "influentialCitationCount": 0, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
        "Computer Science", "source": "external"}, {"category": "Economics", "source":
        "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Review"], "publicationDate":
        "2023-07-26", "journal": {"volume": "abs/2307.14324", "name": "ArXiv"}, "authors":
        [{"authorId": "1742339548", "name": "Nino Scherrer"}, {"authorId": "133797256",
        "name": "Claudia Shi"}, {"authorId": "46609506", "name": "Amir Feder"}, {"authorId":
        "1796335", "name": "D. Blei"}]}, {"paperId": "376f88d661e1b299bfdef13c9a404cbc76b9566b",
        "externalIds": {"DBLP": "journals/corr/abs-2307-11018", "ArXiv": "2307.11018",
        "DOI": "10.48550/arXiv.2307.11018", "CorpusId": 259991495}, "corpusId": 259991495,
        "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name":
        "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"},
        "url": "https://www.semanticscholar.org/paper/376f88d661e1b299bfdef13c9a404cbc76b9566b",
        "title": "Amortized Variational Inference: When and Why?", "abstract": "Amortized
        variational inference (A-VI) is a method for approximating the intractable
        posterior distributions that arise in probabilistic models. The defining feature
        of A-VI is that it learns a global inference function that maps each observation
        to its local latent variable''s approximate posterior. This stands in contrast
        to the more classical factorized (or mean-field) variational inference (F-VI),
        which directly learns the parameters of the approximating distribution for
        each latent variable. In deep generative models, A-VI is used as a computational
        trick to speed up inference for local latent variables. In this paper, we
        study A-VI as a general alternative to F-VI for approximate posterior inference.
        A-VI cannot produce an approximation with a lower Kullback-Leibler divergence
        than F-VI''s optimal solution, because the amortized family is a subset of
        the factorized family. Thus a central theoretical problem is to characterize
        when A-VI still attains F-VI''s optimal solution. We derive conditions on
        both the model and the inference function under which A-VI can theoretically
        achieve F-VI''s optimum. We show that for a broad class of hierarchical models,
        including deep generative models, it is possible to close the gap between
        A-VI and F-VI. Further, for an even broader class of models, we establish
        when and how to expand the domain of the inference function to make amortization
        a feasible strategy. Finally, we prove that for certain models -- including
        hidden Markov models and Gaussian processes -- A-VI cannot match F-VI''s solution,
        no matter how expressive the inference function is. We also study A-VI empirically.
        On several examples, we corroborate our theoretical results and investigate
        the performance of A-VI when varying the complexity of the inference function.
        When the gap between A-VI and F-VI can be closed, we find that the required
        complexity of the function need not scale with the number of observations,
        and that A-VI often converges faster than F-VI.", "venue": "arXiv.org", "year":
        2023, "referenceCount": 33, "citationCount": 0, "influentialCitationCount":
        0, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Mathematics",
        "Computer Science"], "s2FieldsOfStudy": [{"category": "Mathematics", "source":
        "external"}, {"category": "Computer Science", "source": "external"}, {"category":
        "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
        "publicationDate": "2023-07-20", "journal": {"volume": "abs/2307.11018", "name":
        "ArXiv"}, "authors": [{"authorId": "51889795", "name": "C. Margossian"}, {"authorId":
        "1796335", "name": "D. Blei"}]}, {"paperId": "3cb746ee9ab49920ead4bd832d94ca0bc1ee5d3b",
        "externalIds": {"ArXiv": "2306.12497", "DBLP": "journals/corr/abs-2306-12497",
        "DOI": "10.48550/arXiv.2306.12497", "CorpusId": 259224545}, "corpusId": 259224545,
        "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name":
        "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"},
        "url": "https://www.semanticscholar.org/paper/3cb746ee9ab49920ead4bd832d94ca0bc1ee5d3b",
        "title": "Density Uncertainty Layers for Reliable Uncertainty Estimation",
        "abstract": "Assessing the predictive uncertainty of deep neural networks
        is crucial for safety-related applications of deep learning. Although Bayesian
        deep learning offers a principled framework for estimating model uncertainty,
        the approaches that are commonly used to approximate the posterior often fail
        to deliver reliable estimates of predictive uncertainty. In this paper we
        propose a novel criterion for predictive uncertainty, that a model''s predictive
        variance should be grounded in the empirical density of the input. It should
        produce higher uncertainty for inputs that are improbable in the training
        data and lower uncertainty for those inputs that are more probable. To operationalize
        this criterion, we develop the density uncertainty layer, an architectural
        element for a stochastic neural network that guarantees that the density uncertain
        criterion is satisfied. We study neural networks with density uncertainty
        layers on the CIFAR-10 and CIFAR-100 uncertainty benchmarks. Compared to existing
        approaches, we find that density uncertainty layers provide reliable uncertainty
        estimates and robust out-of-distribution detection performance.", "venue":
        "arXiv.org", "year": 2023, "referenceCount": 45, "citationCount": 0, "influentialCitationCount":
        0, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science",
        "source": "external"}, {"category": "Mathematics", "source": "external"},
        {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
        ["JournalArticle"], "publicationDate": "2023-06-21", "journal": {"volume":
        "abs/2306.12497", "name": "ArXiv"}, "authors": [{"authorId": "134893586",
        "name": "Yookoon Park"}, {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId":
        "4707587fc67fd590c0f8c767869bb7ba73f3e56e", "externalIds": {"DBLP": "journals/corr/abs-2307-07849",
        "ArXiv": "2307.07849", "DOI": "10.48550/arXiv.2307.07849", "CorpusId": 259936958},
        "corpusId": 259936958, "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
        "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url":
        "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/4707587fc67fd590c0f8c767869bb7ba73f3e56e",
        "title": "Variational Inference with Gaussian Score Matching", "abstract":
        "Variational inference (VI) is a method to approximate the computationally
        intractable posterior distributions that arise in Bayesian statistics. Typically,
        VI fits a simple parametric distribution to the target posterior by minimizing
        an appropriate objective such as the evidence lower bound (ELBO). In this
        work, we present a new approach to VI based on the principle of score matching,
        that if two distributions are equal then their score functions (i.e., gradients
        of the log density) are equal at every point on their support. With this,
        we develop score matching VI, an iterative algorithm that seeks to match the
        scores between the variational approximation and the exact posterior. At each
        iteration, score matching VI solves an inner optimization, one that minimally
        adjusts the current variational estimate to match the scores at a newly sampled
        value of the latent variables. We show that when the variational family is
        a Gaussian, this inner optimization enjoys a closed form solution, which we
        call Gaussian score matching VI (GSM-VI). GSM-VI is also a ``black box''''
        variational algorithm in that it only requires a differentiable joint distribution,
        and as such it can be applied to a wide class of models. We compare GSM-VI
        to black box variational inference (BBVI), which has similar requirements
        but instead optimizes the ELBO. We study how GSM-VI behaves as a function
        of the problem dimensionality, the condition number of the target covariance
        matrix (when the target is Gaussian), and the degree of mismatch between the
        approximating and exact posterior distribution. We also study GSM-VI on a
        collection of real-world Bayesian inference problems from the posteriorDB
        database of datasets and models. In all of our studies we find that GSM-VI
        is faster than BBVI, but without sacrificing accuracy. It requires 10-100x
        fewer gradient evaluations to obtain a comparable quality of approximation.",
        "venue": "arXiv.org", "year": 2023, "referenceCount": 31, "citationCount":
        0, "influentialCitationCount": 0, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics",
        "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle"], "publicationDate": "2023-07-15", "journal":
        {"volume": "abs/2307.07849", "name": "ArXiv"}, "authors": [{"authorId": "5113077",
        "name": "C. Modi"}, {"authorId": "51889795", "name": "C. Margossian"}, {"authorId":
        "34767391", "name": "Yuling Yao"}, {"authorId": "11043660", "name": "Robert
        Mansel Gower"}, {"authorId": "1796335", "name": "D. Blei"}, {"authorId": "1796044",
        "name": "L. Saul"}]}, {"paperId": "58c4a25181c962df905bbf32f23a047d57549163",
        "externalIds": {"DBLP": "journals/corr/abs-2306-00542", "ArXiv": "2306.00542",
        "DOI": "10.48550/arXiv.2306.00542", "CorpusId": 258999192}, "corpusId": 258999192,
        "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name":
        "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"},
        "url": "https://www.semanticscholar.org/paper/58c4a25181c962df905bbf32f23a047d57549163",
        "title": "Nonparametric Identifiability of Causal Representations from Unknown
        Interventions", "abstract": "We study causal representation learning, the
        task of inferring latent causal variables and their causal relations from
        high-dimensional functions (\"mixtures\") of the variables. Prior work relies
        on weak supervision, in the form of counterfactual pre- and post-intervention
        views or temporal structure; places restrictive assumptions, such as linearity,
        on the mixing function or latent causal model; or requires partial knowledge
        of the generative process, such as the causal graph or the intervention targets.
        We instead consider the general setting in which both the causal model and
        the mixing function are nonparametric. The learning signal takes the form
        of multiple datasets, or environments, arising from unknown interventions
        in the underlying causal model. Our goal is to identify both the ground truth
        latents and their causal graph up to a set of ambiguities which we show to
        be irresolvable from interventional data. We study the fundamental setting
        of two causal variables and prove that the observational distribution and
        one perfect intervention per node suffice for identifiability, subject to
        a genericity condition. This condition rules out spurious solutions that involve
        fine-tuning of the intervened and observational distributions, mirroring similar
        conditions for nonlinear cause-effect inference. For an arbitrary number of
        variables, we show that two distinct paired perfect interventions per node
        guarantee identifiability. Further, we demonstrate that the strengths of causal
        influences among the latent variables are preserved by all equivalent solutions,
        rendering the inferred representation appropriate for drawing causal conclusions
        from new data. Our study provides the first identifiability results for the
        general nonparametric setting with unknown interventions, and elucidates what
        is possible and impossible for causal representation learning without more
        direct supervision.", "venue": "arXiv.org", "year": 2023, "referenceCount":
        114, "citationCount": 5, "influentialCitationCount": 1, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
        "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
        {"category": "Mathematics", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
        "publicationDate": "2023-06-01", "journal": {"volume": "abs/2306.00542", "name":
        "ArXiv"}, "authors": [{"authorId": "51135567", "name": "Julius von K\u00fcgelgen"},
        {"authorId": "2599082", "name": "M. Besserve"}, {"authorId": "153667762",
        "name": "Wendong Liang"}, {"authorId": "31821560", "name": "Luigi Gresele"},
        {"authorId": "2078911543", "name": "Armin Keki''c"}, {"authorId": "2778721",
        "name": "E. Bareinboim"}, {"authorId": "1796335", "name": "D. Blei"}, {"authorId":
        "1707625", "name": "B. Scholkopf"}]}, {"paperId": "79531b47bb27cb18022891eb2ab1fcb41745fca6",
        "externalIds": {"ArXiv": "2306.17775", "DBLP": "journals/corr/abs-2306-17775",
        "DOI": "10.48550/arXiv.2306.17775", "CorpusId": 259309049}, "corpusId": 259309049,
        "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name":
        "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"},
        "url": "https://www.semanticscholar.org/paper/79531b47bb27cb18022891eb2ab1fcb41745fca6",
        "title": "Practical and Asymptotically Exact Conditional Sampling in Diffusion
        Models", "abstract": "Diffusion models have been successful on a range of
        conditional generation tasks including molecular design and text-to-image
        generation. However, these achievements have primarily depended on task-specific
        conditional training or error-prone heuristic approximations. Ideally, a conditional
        generation method should provide exact samples for a broad range of conditional
        distributions without requiring task-specific training. To this end, we introduce
        the Twisted Diffusion Sampler, or TDS. TDS is a sequential Monte Carlo (SMC)
        algorithm that targets the conditional distributions of diffusion models.
        The main idea is to use twisting, an SMC technique that enjoys good computational
        efficiency, to incorporate heuristic approximations without compromising asymptotic
        exactness. We first find in simulation and on MNIST image inpainting and class-conditional
        generation tasks that TDS provides a computational statistical trade-off,
        yielding more accurate approximations with many particles but with empirical
        improvements over heuristics with as few as two particles. We then turn to
        motif-scaffolding, a core task in protein design, using a TDS extension to
        Riemannian diffusion models. On benchmark test cases, TDS allows flexible
        conditioning criteria and often outperforms the state of the art.", "venue":
        "arXiv.org", "year": 2023, "referenceCount": 38, "citationCount": 4, "influentialCitationCount":
        0, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science", "Mathematics", "Biology"], "s2FieldsOfStudy": [{"category": "Computer
        Science", "source": "external"}, {"category": "Mathematics", "source": "external"},
        {"category": "Biology", "source": "external"}, {"category": "Computer Science",
        "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
        "2023-06-30", "journal": {"volume": "abs/2306.17775", "name": "ArXiv"}, "authors":
        [{"authorId": "1410052582", "name": "Luhuan Wu"}, {"authorId": "80066929",
        "name": "Brian L. Trippe"}, {"authorId": "2328322", "name": "C. A. Naesseth"},
        {"authorId": "1796335", "name": "D. Blei"}, {"authorId": "2196193764", "name":
        "J. Cunningham"}]}, {"paperId": "b7a9a235112ad1b3712d97a740704f742ffbfac1",
        "externalIds": {"ArXiv": "2302.12777", "CorpusId": 257206105}, "corpusId":
        257206105, "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/b7a9a235112ad1b3712d97a740704f742ffbfac1",
        "title": "On the Misspecification of Linear Assumptions in Synthetic Control",
        "abstract": "The synthetic control (SC) method is a popular approach for estimating
        treatment effects from observational panel data. It rests on a crucial assumption
        that we can write the treated unit as a linear combination of the untreated
        units. This linearity assumption, however, can be unlikely to hold in practice
        and, when violated, the resulting SC estimates are incorrect. In this paper
        we examine two questions: (1) How large can the misspecification error be?
        (2) How can we limit it? First, we provide theoretical bounds to quantify
        the misspecification error. The bounds are comforting: small misspecifications
        induce small errors. With these bounds in hand, we then develop new SC estimators
        that are specially designed to minimize misspecification error. The estimators
        are based on additional data about each unit, which is used to produce the
        SC weights. (For example, if the units are countries then the additional data
        might be demographic information about each.) We study our estimators on synthetic
        data; we find they produce more accurate causal estimates than standard synthetic
        controls. We then re-analyze the California tobacco-program data of the original
        SC paper, now including additional data from the US census about per-state
        demographics. Our estimators show that the observations in the pre-treatment
        period lie within the bounds of misspecification error, and that the observations
        post-treatment lie outside of those bounds. This is evidence that our SC methods
        have uncovered a true effect.", "venue": "", "year": 2023, "referenceCount":
        36, "citationCount": 1, "influentialCitationCount": 1, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Mathematics", "Economics"], "s2FieldsOfStudy":
        [{"category": "Mathematics", "source": "external"}, {"category": "Economics",
        "source": "external"}, {"category": "Economics", "source": "s2-fos-model"}],
        "publicationTypes": null, "publicationDate": "2023-02-24", "journal": null,
        "authors": [{"authorId": "116310873", "name": "Achille Nazaret"}, {"authorId":
        "133797256", "name": "Claudia Shi"}, {"authorId": "1796335", "name": "D. Blei"}]},
        {"paperId": "d95b441c2838888d7ac1af73b5f9c800f22fad3a", "externalIds": {"ACL":
        "2023.acl-long.179", "DBLP": "journals/corr/abs-2306-00198", "ArXiv": "2306.00198",
        "DOI": "10.48550/arXiv.2306.00198", "CorpusId": 254223269}, "corpusId": 254223269,
        "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name":
        "Annual Meeting of the Association for Computational Linguistics", "type":
        "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting
        of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput
        Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url":
        "https://www.semanticscholar.org/paper/d95b441c2838888d7ac1af73b5f9c800f22fad3a",
        "title": "An Invariant Learning Characterization of Controlled Text Generation",
        "abstract": "Controlled generation refers to the problem of creating text
        that contains stylistic or semantic attributes of interest. Many approaches
        reduce this problem to training a predictor of the desired attribute. For
        example, researchers hoping to deploy a large language model to produce non-toxic
        content may use a toxicity classifier to filter generated text. In practice,
        the generated text to classify, which is determined by user prompts, may come
        from a wide range of distributions.In this paper, we show that the performance
        of controlled generation may be poor if the distributions of text in response
        to user prompts differ from the distribution the predictor was trained on.
        To address this problem, we cast controlled generation under distribution
        shift as an invariant learning problem: the most effective predictor should
        be invariant across multiple text environments. We then discuss a natural
        solution that arises from this characterization and propose heuristics for
        selecting natural environments.We study this characterization and the proposed
        method empirically using both synthetic and real data. Experiments demonstrate
        both the challenge of distribution shift in controlled generation and the
        potential of invariance methods in this setting.", "venue": "Annual Meeting
        of the Association for Computational Linguistics", "year": 2023, "referenceCount":
        72, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
        "Conference"], "publicationDate": "2023-05-31", "journal": {"volume": "abs/2306.00198",
        "name": "ArXiv"}, "authors": [{"authorId": "133797256", "name": "Claudia Shi"},
        {"authorId": "2113919506", "name": "Carolina Zheng"}, {"authorId": "46609506",
        "name": "Amir Feder"}, {"authorId": "70025184", "name": "Keyon Vafa"}, {"authorId":
        "1796335", "name": "D. Blei"}]}, {"paperId": "41aead90e62b111616e21b4e4eb3af38faa1310e",
        "externalIds": {"ArXiv": "2206.15433", "DOI": "10.1088/1475-7516/2023/03/059",
        "CorpusId": 250144405}, "corpusId": 250144405, "publicationVenue": {"id":
        "fcdf5d7f-8654-4d3c-b1a4-0d86cc3f93cb", "name": "Journal of Cosmology and
        Astroparticle Physics", "type": "journal", "alternate_names": ["J Cosmol Astropart
        Phys"], "issn": "1475-7516", "url": "http://www.iop.org/EJ/journal/JCAP",
        "alternate_urls": ["http://iopscience.iop.org/1475-7516", "https://iopscience.iop.org/journal/1475-7516"]},
        "url": "https://www.semanticscholar.org/paper/41aead90e62b111616e21b4e4eb3af38faa1310e",
        "title": "Reconstructing the universe with variational self-boosted sampling",
        "abstract": "Forward modeling approaches in cosmology have made it possible
        to reconstruct the initial conditions at the beginning of the Universe from
        the observed survey data. However the high dimensionality of the parameter
        space still poses a challenge to explore the full posterior, with traditional
        algorithms such as Hamiltonian Monte Carlo (HMC) being computationally inefficient
        due to generating correlated samples and the performance of variational inference
        being highly dependent on the choice of divergence (loss) function. Here we
        develop a hybrid scheme, called variational self-boosted sampling (VBS) to
        mitigate the drawbacks of both these algorithms by learning a variational
        approximation for the proposal distribution of Monte Carlo sampling and combine
        it with HMC. The variational distribution is parameterized as a normalizing
        flow and learnt with samples generated on the fly, while proposals drawn from
        it reduce auto-correlation length in MCMC chains. Our normalizing flow uses
        Fourier space convolutions and element-wise operations to scale to high dimensions.
        We show that after a short initial warm-up and training phase, VBS generates
        better quality of samples than simple VI approaches and in the hybrid sampling
        phase, reduces the correlation length in the sampling phase by a factor of
        10\u201350 over using only HMC to explore the posterior of initial conditions
        in 643 and 1283 dimensional problems, with larger gains for high signal-to-noise
        data observations. Hybrid sampling with online training of the variational
        distribution violates Markov property, and to retain the asymptotic guarantees
        of HMC, in the final phase we use a fixed variational distribution as proposal
        distribution and propagate these samples to the posterior distribution.",
        "venue": "Journal of Cosmology and Astroparticle Physics", "year": 2022, "referenceCount":
        32, "citationCount": 3, "influentialCitationCount": 0, "isOpenAccess": true,
        "openAccessPdf": {"url": "http://arxiv.org/pdf/2206.15433", "status": null},
        "fieldsOfStudy": ["Physics", "Mathematics"], "s2FieldsOfStudy": [{"category":
        "Physics", "source": "external"}, {"category": "Mathematics", "source": "external"},
        {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
        ["Review"], "publicationDate": "2022-06-28", "journal": {"volume": "2023",
        "name": "Journal of Cosmology and Astroparticle Physics"}, "authors": [{"authorId":
        "5113077", "name": "C. Modi"}, {"authorId": null, "name": "Yin Li"}, {"authorId":
        "1796335", "name": "D. Blei"}]}, {"paperId": "57ceadbb37da24ce24b3ab8ff826ddcae717c0e2",
        "externalIds": {"DBLP": "journals/corr/abs-2202-08370", "CorpusId": 246904808},
        "corpusId": 246904808, "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
        "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url":
        "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/57ceadbb37da24ce24b3ab8ff826ddcae717c0e2",
        "title": "Learning Transferrable Representations of Career Trajectories for
        Economic Prediction", "abstract": "Understanding career trajectories\u2014the
        sequences of jobs that individuals hold over their working lives\u2014is important
        to economists for studying labor markets. In the past, economists have estimated
        relevant quantities by \ufb01tting predictive models to small surveys, but
        in recent years large datasets of online resumes have also become available.
        These new datasets provide job sequences of many more individuals, but they
        are too large and complex for standard econometric modeling. To this end,
        we adapt ideas from modern language modeling to the analysis of large-scale
        job sequence data. We develop CAREER, a transformer-based model that learns
        a low-dimensional representation of an individual\u2019s job history. This
        representation can be used to predict jobs directly on a large dataset, or
        can be \u201ctransferred\u201d to represent jobs in smaller and better-curated
        datasets. We \ufb01t the model to a large dataset of resumes, 24 million people
        who are involved in more than a thousand unique occupations. It forms accurate
        predictions on held-out data, and it learns useful career representations
        that can be \ufb01ne-tuned to make accurate predictions on common economics
        datasets. for predicting and forecasting career", "venue": "arXiv.org", "year":
        2022, "referenceCount": 57, "citationCount": 2, "influentialCitationCount":
        0, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
        "external"}, {"category": "Economics", "source": "s2-fos-model"}], "publicationTypes":
        ["JournalArticle", "Review"], "publicationDate": null, "journal": {"volume":
        "abs/2202.08370", "name": "ArXiv"}, "authors": [{"authorId": "70025184", "name":
        "Keyon Vafa"}, {"authorId": "2083101971", "name": "Emil Palikot"}, {"authorId":
        null, "name": "Tianyu Du"}, {"authorId": "2661277", "name": "Ayush Kanodia"},
        {"authorId": "2631417", "name": "S. Athey"}, {"authorId": "1796335", "name":
        "D. Blei"}]}, {"paperId": "5de7813ce5bc9f361f2b6fd09f884b468dcb43bf", "externalIds":
        {"DBLP": "journals/corr/abs-2202-01841", "ArXiv": "2202.01841", "CorpusId":
        246607993}, "corpusId": 246607993, "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
        "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url":
        "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/5de7813ce5bc9f361f2b6fd09f884b468dcb43bf",
        "title": "Transport Score Climbing: Variational Inference Using Forward KL
        and Adaptive Neural Transport", "abstract": "Variational inference often minimizes
        the\"reverse\"Kullbeck-Leibler (KL) KL(q||p) from the approximate distribution
        q to the posterior p. Recent work studies the\"forward\"KL KL(p||q), which
        unlike reverse KL does not lead to variational approximations that underestimate
        uncertainty. This paper introduces Transport Score Climbing (TSC), a method
        that optimizes KL(p||q) by using Hamiltonian Monte Carlo (HMC) and a novel
        adaptive transport map. The transport map improves the trajectory of HMC by
        acting as a change of variable between the latent variable space and a warped
        space. TSC uses HMC samples to dynamically train the transport map while optimizing
        KL(p||q). TSC leverages synergies, where better transport maps lead to better
        HMC sampling, which then leads to better transport maps. We demonstrate TSC
        on synthetic and real data. We find that TSC achieves competitive performance
        when training variational autoencoders on large-scale data.", "venue": "arXiv.org",
        "year": 2022, "referenceCount": 59, "citationCount": 3, "influentialCitationCount":
        0, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Mathematics",
        "Computer Science"], "s2FieldsOfStudy": [{"category": "Mathematics", "source":
        "external"}, {"category": "Computer Science", "source": "external"}, {"category":
        "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
        "publicationDate": "2022-02-03", "journal": {"volume": "abs/2202.01841", "name":
        "ArXiv"}, "authors": [{"authorId": "2108871879", "name": "Liyi Zhang"}, {"authorId":
        "2328322", "name": "C. A. Naesseth"}, {"authorId": "1796335", "name": "D.
        Blei"}]}, {"paperId": "60c6c0fbae81caee615f328cbc46c4a9a6f0cd22", "externalIds":
        {"ArXiv": "2211.11183", "DBLP": "journals/corr/abs-2211-11183", "DOI": "10.48550/arXiv.2211.11183",
        "CorpusId": 253734863}, "corpusId": 253734863, "publicationVenue": {"id":
        "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names":
        ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/60c6c0fbae81caee615f328cbc46c4a9a6f0cd22",
        "title": "A Bayesian Causal Inference Approach for Assessing Fairness in Clinical
        Decision-Making", "abstract": "Fairness in clinical decision-making is a critical
        element of health equity, but assessing fairness of clinical decisions from
        observational data is challenging. Recently, many fairness notions have been
        proposed to quantify fairness in decision-making, among which causality-based
        fairness notions have gained increasing attention due to its potential in
        adjusting for confounding and reasoning about bias. However, causal fairness
        notions remain under-explored in the context of clinical decision-making with
        large-scale healthcare data. In this work, we propose a Bayesian causal inference
        approach for assessing a causal fairness notion called principal fairness
        in clinical settings. We demonstrate our approach using both simulated data
        and electronic health records (EHR) data.", "venue": "arXiv.org", "year":
        2022, "referenceCount": 45, "citationCount": 0, "influentialCitationCount":
        0, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
        "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle"], "publicationDate": "2022-11-21", "journal":
        {"volume": "abs/2211.11183", "name": "ArXiv"}, "authors": [{"authorId": "3414608",
        "name": "Linying Zhang"}, {"authorId": "37706581", "name": "L. R. Richter"},
        {"authorId": "2108734693", "name": "Yixin Wang"}, {"authorId": "82276238",
        "name": "A. Ostropolets"}, {"authorId": "134509639", "name": "N. Elhadad"},
        {"authorId": "1796335", "name": "D. Blei"}, {"authorId": "1686114", "name":
        "G. Hripcsak"}]}, {"paperId": "6eac963198c68e8e54d3f95c7dc87dedd4c34ea0",
        "externalIds": {"DBLP": "conf/icml/NazaretB22", "ArXiv": "2209.10091", "DOI":
        "10.48550/arXiv.2209.10091", "CorpusId": 250340748}, "corpusId": 250340748,
        "publicationVenue": {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29", "name":
        "International Conference on Machine Learning", "type": "conference", "alternate_names":
        ["ICML", "Int Conf Mach Learn"], "url": "https://icml.cc/"}, "url": "https://www.semanticscholar.org/paper/6eac963198c68e8e54d3f95c7dc87dedd4c34ea0",
        "title": "Variational Inference for Infinitely Deep Neural Networks", "abstract":
        "We introduce the unbounded depth neural network (UDN), an infinitely deep
        probabilistic model that adapts its complexity to the training data. The UDN
        contains an infinite sequence of hidden layers and places an unbounded prior
        on a truncation L, the layer from which it produces its data. Given a dataset
        of observations, the posterior UDN provides a conditional distribution of
        both the parameters of the infinite neural network and its truncation. We
        develop a novel variational inference algorithm to approximate this posterior,
        optimizing a distribution of the neural network weights and of the truncation
        depth L, and without any upper limit on L. To this end, the variational family
        has a special structure: it models neural network weights of arbitrary depth,
        and it dynamically creates or removes free variational parameters as its distribution
        of the truncation is optimized. (Unlike heuristic approaches to model search,
        it is solely through gradient-based optimization that this algorithm explores
        the space of truncations.) We study the UDN on real and synthetic data. We
        find that the UDN adapts its posterior depth to the dataset complexity; it
        outperforms standard neural networks of similar computational complexity;
        and it outperforms other approaches to infinite-depth neural networks.", "venue":
        "International Conference on Machine Learning", "year": 2022, "referenceCount":
        59, "citationCount": 3, "influentialCitationCount": 1, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
        "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
        {"category": "Mathematics", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
        "Conference"], "publicationDate": "2022-09-21", "journal": {"pages": "16447-16461"},
        "authors": [{"authorId": "116310873", "name": "Achille Nazaret"}, {"authorId":
        "1796335", "name": "D. Blei"}]}, {"paperId": "76057f3f3a5816e004477837b25e742b1fa7d7f1",
        "externalIds": {"DOI": "10.1101/2022.11.21.517420", "CorpusId": 253967941},
        "corpusId": 253967941, "publicationVenue": {"id": "027ffd21-ebb0-4af8-baf5-911124292fd0",
        "name": "bioRxiv", "type": "journal", "url": "http://biorxiv.org/"}, "url":
        "https://www.semanticscholar.org/paper/76057f3f3a5816e004477837b25e742b1fa7d7f1",
        "title": "Starfysh reveals heterogeneous spatial dynamics in the breast tumor
        microenvironment", "abstract": "Spatially-resolved gene expression profiling
        provides valuable insight into tissue organization and cell-cell crosstalk;
        however, spatial transcriptomics (ST) lacks single-cell resolution. Current
        ST analysis methods require single-cell RNA sequencing data as a reference
        for a rigorous interpretation of cell states and do not utilize associated
        histology images. Significant sample variation further complicates the integration
        of ST datasets, which is essential for identifying commonalities across tissues
        or altered cellular wiring in disease. Here, we present Starfysh, the first
        comprehensive computational toolbox for joint modeling of ST and histology
        data, dissection of refined cell states, and systematic integration of multiple
        ST datasets from complex tissues. Starfysh uses an auxiliary deep generative
        model that incorporates archetypal analysis and any known cell state markers
        to avoid the need for a single-cell-resolution reference in characterizing
        known or novel tissue-specific cell states. Additionally, Starfysh improves
        the characterization of spatial dynamics in complex tissues by leveraging
        histology images and enables the comparison of niches as spatial \u201chubs\u201d
        across tissues. Integrative analysis of primary estrogen receptor-positive
        (ER+) breast cancer, triple-negative breast cancer (TNBC), and metaplastic
        breast cancer (MBC) tumors using Starfysh led to the identification of heterogeneous
        patient- and disease-specific hubs as well as a shared stromal hub with varying
        spatial orientation. Our results show the ability to delineate the spatial
        co-evolution of tumor and immune cell states and their crosstalk underlying
        intratumoral heterogeneity in TNBC and revealed metabolic reprogramming shaping
        immunosuppressive hubs in aggressive MBC. Starfysh is publicly available (https://github.com/azizilab/starfysh).",
        "venue": "bioRxiv", "year": 2022, "referenceCount": 90, "citationCount": 1,
        "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url":
        "https://www.biorxiv.org/content/biorxiv/early/2022/11/24/2022.11.21.517420.full.pdf",
        "status": null}, "fieldsOfStudy": ["Biology"], "s2FieldsOfStudy": [{"category":
        "Biology", "source": "external"}, {"category": "Biology", "source": "s2-fos-model"}],
        "publicationTypes": null, "publicationDate": "2022-11-24", "journal": {"name":
        "bioRxiv"}, "authors": [{"authorId": "2109380974", "name": "Siyu He"}, {"authorId":
        "2110836656", "name": "Yinuo Jin"}, {"authorId": "116310873", "name": "Achille
        Nazaret"}, {"authorId": "48138148", "name": "Lingting Shi"}, {"authorId":
        "32372467", "name": "Xueer Chen"}, {"authorId": "26624833", "name": "Sham
        Rampersaud"}, {"authorId": "2192361845", "name": "Bahawar S. Dhillon"}, {"authorId":
        "2140698223", "name": "Izabella Valdez"}, {"authorId": "2192361706", "name":
        "Lauren E Friend"}, {"authorId": "2156228016", "name": "Joy Linyue Fan"},
        {"authorId": "2115187813", "name": "Cameron Y Park"}, {"authorId": "48439626",
        "name": "Rachel L. Mintz"}, {"authorId": "49353539", "name": "Yeh-Hsing Lao"},
        {"authorId": "2064201428", "name": "David Carrera"}, {"authorId": "2192362327",
        "name": "Kaylee W Fang"}, {"authorId": "2192361346", "name": "Kaleem Mehdi"},
        {"authorId": "2192361590", "name": "Madeline Rohde"}, {"authorId": "1411274831",
        "name": "Jos\u00e9 L. McFaline-Figueroa"}, {"authorId": "1796335", "name":
        "D. Blei"}, {"authorId": "2073384044", "name": "Kam W. Leong"}, {"authorId":
        "5146760", "name": "A. Rudensky"}, {"authorId": "5601483", "name": "G. Plitas"},
        {"authorId": "146577765", "name": "E. Azizi"}]}, {"paperId": "79d4464f1b0244e0527f10c290767152067ea5ca",
        "externalIds": {"DOI": "10.1214/21-aoas1485", "CorpusId": 235309237}, "corpusId":
        235309237, "publicationVenue": {"id": "8b03733e-25e3-49b5-803c-58c0e9e11180",
        "name": "Annals of Applied Statistics", "type": "journal", "alternate_names":
        ["Ann Appl Stat", "The Annals of Applied Statistics"], "issn": "1932-6157",
        "url": "http://www.imstat.org/aoas/", "alternate_urls": ["http://www.jstor.org/action/showPublication?journalCode=annaapplstat",
        "https://www.imstat.org/journals-and-publications/annals-of-applied-statistics/",
        "https://projecteuclid.org/DPubS?handle=euclid.aoas&service=UI&verb=Display&version=1.0",
        "https://www.jstor.org/journal/annaapplstat"]}, "url": "https://www.semanticscholar.org/paper/79d4464f1b0244e0527f10c290767152067ea5ca",
        "title": "A Bayesian model of dose-response for cancer drug studies", "abstract":
        "Exploratory cancer drug studies test multiple tumor cell lines against multiple
        candidate drugs. The goal in each paired (cell line, drug) experiment is to
        map out the dose-response curve of the cell line as the dose level of the
        drug increases. We propose Bayesian tensor \ufb01ltering (BTF), a hierarchical
        Bayesian model for dose-response modeling in multisample, mul-titreatment
        cancer drug studies. BTF uses low-dimensional embeddings to share statistical
        strength between similar drugs and similar cell lines. Structured shrinkage
        priors in BTF encourage smoothness in the dose-response curves while remaining
        adaptive to sharp jumps when the data call for it. We focus on a pair of cancer
        drug studies exhibiting a particular pathol-ogy in their experimental design,
        leading us to a nonconjugate monotone mixture-of-gammas likelihood. To perform
        posterior inference, we develop a variant of the elliptical slice sampling
        algorithm for sampling from linearly-constrained multivariate normal priors
        with nonconjugate likelihoods. In benchmarks, BTF outperforms state-of-the-art
        methods for covariance regression and dynamic Poisson matrix factorization.
        On the two cancer drug studies, BTF outperforms the current standard approach
        in biology and reveals potential new biomarkers of drug sensitivity in cancer.
        Code is available at https://github.com/tansey/functionalmf. a Bayesian trend
        \ufb01ltering prior (Faulkner and Minin on top of a linear dynamical system
        with P\u00f3lya-gamma augmentation and Windle for binomial observations. and
        develop Poisson-gamma dynamical systems (PGDS), a dynamic matrix factorization
        model speci\ufb01cally for Poisson-distributed observations; we compare BTF
        with a tensor extension of PGDS in Section 6.", "venue": "Annals of Applied
        Statistics", "year": 2022, "referenceCount": 69, "citationCount": 3, "influentialCitationCount":
        1, "isOpenAccess": true, "openAccessPdf": {"url": "http://arxiv.org/pdf/1906.04072",
        "status": null}, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category": "Biology",
        "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": "2022-06-01",
        "journal": {"name": "The Annals of Applied Statistics"}, "authors": [{"authorId":
        "145305182", "name": "Wesley Tansey"}, {"authorId": "2603895", "name": "Christopher
        Tosh"}, {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId": "aef22855ca3475e324d92768cbc0d1d84110b786",
        "externalIds": {"DBLP": "journals/corr/abs-2204-01633", "ArXiv": "2204.01633",
        "DOI": "10.48550/arXiv.2204.01633", "CorpusId": 247517539}, "corpusId": 247517539,
        "publicationVenue": {"id": "3d07319c-4f2a-4f30-b619-c295ccd29367", "name":
        "CLEaR", "type": "conference", "alternate_names": ["Classification of Events,
        Activities and Relationships", "CLEAR", "CLeaR", "Conf Causal Learn Reason",
        "Classif Event Act Relatsh", "Conference on Causal Learning and Reasoning"],
        "issn": "2453-7128", "url": "http://www.jolace.com/publications/clear/", "alternate_urls":
        ["https://www.cclear.cc/"]}, "url": "https://www.semanticscholar.org/paper/aef22855ca3475e324d92768cbc0d1d84110b786",
        "title": "Estimating Social Influence from Observational Data", "abstract":
        "We consider the problem of estimating social influence, the effect that a
        person''s behavior has on the future behavior of their peers. The key challenge
        is that shared behavior between friends could be equally explained by influence
        or by two other confounding factors: 1) latent traits that caused people to
        both become friends and engage in the behavior, and 2) latent preferences
        for the behavior. This paper addresses the challenges of estimating social
        influence with three contributions. First, we formalize social influence as
        a causal effect, one which requires inferences about hypothetical interventions.
        Second, we develop Poisson Influence Factorization (PIF), a method for estimating
        social influence from observational data. PIF fits probabilistic factor models
        to networks and behavior data to infer variables that serve as substitutes
        for the confounding latent traits. Third, we develop assumptions under which
        PIF recovers estimates of social influence. We empirically study PIF with
        semi-synthetic and real data from Last.fm, and conduct a sensitivity analysis.
        We find that PIF estimates social influence most accurately compared to related
        methods and remains robust under some violations of its assumptions.", "venue":
        "CLEaR", "year": 2022, "referenceCount": 42, "citationCount": 2, "influentialCitationCount":
        0, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
        "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle"], "publicationDate": "2022-03-24", "journal":
        {"pages": "712-733"}, "authors": [{"authorId": "153485411", "name": "Dhanya
        Sridhar"}, {"authorId": "1920647", "name": "C. D. Bacco"}, {"authorId": "1796335",
        "name": "D. Blei"}]}, {"paperId": "b0f268385132c92db78e137256523904bb234166",
        "externalIds": {"CorpusId": 263736865}, "corpusId": 263736865, "publicationVenue":
        null, "url": "https://www.semanticscholar.org/paper/b0f268385132c92db78e137256523904bb234166",
        "title": "Title Serendipity based recommender system for perovskites material
        discovery: balancing exploration and exploitation across multiple models [short]
        Serendipity recommender for perovskites discovery", "abstract": "Machine learning
        is a useful tool for accelerating materials discovery, however it is a challenge
        to develop accurate methods that successfully transfer between domains while
        also broadening the scope of reaction conditions considered. In this paper,
        we consider how active-and transfer-learning methods can be used as building
        blocks for predicting reaction outcomes of metal halide perovskite synthesis.
        We then introduce a serendipity-based recommendation system that guides these
        methods to balance novelty and accuracy. The model-agnostic recommendation
        system is tested across active-and transfer-learning algorithms, using laboratory
        experiments for training and testing and a time-separated hold out that includes
        four different chemical systems. The serendipity recommendation system achieves
        high accuracy while increasing the scope of the synthesis conditions explored.
        Teaser", "venue": "", "year": 2022, "referenceCount": 62, "citationCount":
        0, "influentialCitationCount": 0, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category": "Computer Science",
        "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": null,
        "journal": null, "authors": [{"authorId": "2315741", "name": "Venkateswaran
        Shekar"}, {"authorId": "2135631419", "name": "Vincent Yu"}, {"authorId": "2256142625",
        "name": "Benjamin J. Garcia"}, {"authorId": "2255846738", "name": "David Benjamin
        Gordon"}, {"authorId": "2256036183", "name": "Gemma E. Moran"}, {"authorId":
        "1796335", "name": "D. Blei"}, {"authorId": "2256035827", "name": "Lo\u00efc
        M. Roch"}, {"authorId": "2256049528", "name": "Alberto Garc\u00eda-Dur\u00e1n"},
        {"authorId": "6672863", "name": "M. Najeeb"}, {"authorId": "2136017243", "name":
        "Margaret Zeile"}, {"authorId": "1753375274", "name": "Philip W. Nega"}, {"authorId":
        "2118214054", "name": "Zhi Li"}, {"authorId": "2255563795", "name": "Mina
        A. Kim"}, {"authorId": "2255445478", "name": "Emory M. Chan"}, {"authorId":
        "5787497", "name": "A. Norquist"}, {"authorId": "34597147", "name": "Sorelle
        A. Friedler"}, {"authorId": "49916345", "name": "Joshua Schrier"}]}, {"paperId":
        "bfbd8f1cec08f842e65354c0d6598d42b906f74d", "externalIds": {"DBLP": "journals/tacl/SridharDB22",
        "DOI": "10.1162/tacl_a_00487", "CorpusId": 248914131}, "corpusId": 248914131,
        "publicationVenue": {"id": "e0dbf116-86aa-418d-859f-a49952d7e44a", "name":
        "Transactions of the Association for Computational Linguistics", "type": "journal",
        "alternate_names": ["Trans Assoc Comput Linguistics", "TACL"], "issn": "2307-387X",
        "url": "https://www.mitpressjournals.org/loi/tacl", "alternate_urls": ["http://www.transacl.org/"]},
        "url": "https://www.semanticscholar.org/paper/bfbd8f1cec08f842e65354c0d6598d42b906f74d",
        "title": "Heterogeneous Supervised Topic Models", "abstract": "Abstract Researchers
        in the social sciences are often interested in the relationship between text
        and an outcome of interest, where the goal is to both uncover latent patterns
        in the text and predict outcomes for unseen texts. To this end, this paper
        develops the heterogeneous supervised topic model (HSTM), a probabilistic
        approach to text analysis and prediction. HSTMs posit a joint model of text
        and outcomes to find heterogeneous patterns that help with both text analysis
        and prediction. The main benefit of HSTMs is that they capture heterogeneity
        in the relationship between text and the outcome across latent topics. To
        fit HSTMs, we develop a variational inference algorithm based on the auto-encoding
        variational Bayes framework. We study the performance of HSTMs on eight datasets
        and find that they consistently outperform related methods, including fine-tuned
        black-box models. Finally, we apply HSTMs to analyze news articles labeled
        with pro- or anti-tone. We find evidence of differing language used to signal
        a pro- and anti-tone.", "venue": "Transactions of the Association for Computational
        Linguistics", "year": 2022, "referenceCount": 42, "citationCount": 4, "influentialCitationCount":
        0, "isOpenAccess": true, "openAccessPdf": {"url": "https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00487/2030688/tacl_a_00487.pdf",
        "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
        "publicationDate": "2022-06-01", "journal": {"volume": "10", "pages": "732-745",
        "name": "Transactions of the Association for Computational Linguistics"},
        "authors": [{"authorId": "153485411", "name": "Dhanya Sridhar"}, {"authorId":
        "1722360", "name": "Hal Daum\u00e9"}, {"authorId": "1796335", "name": "D.
        Blei"}]}, {"paperId": "caf9b1c5458ee90a6351f6438c3a705e4f2086df", "externalIds":
        {"ArXiv": "2206.06584", "DBLP": "conf/aistats/WangGYZB23", "DOI": "10.48550/arXiv.2206.06584",
        "CorpusId": 249642382}, "corpusId": 249642382, "publicationVenue": {"id":
        "2d136b11-c2b5-484b-b008-7f4a852fd61e", "name": "International Conference
        on Artificial Intelligence and Statistics", "type": "conference", "alternate_names":
        ["AISTATS", "Int Conf Artif Intell Stat"]}, "url": "https://www.semanticscholar.org/paper/caf9b1c5458ee90a6351f6438c3a705e4f2086df",
        "title": "Probabilistic Conformal Prediction Using Conditional Random Samples",
        "abstract": "This paper proposes probabilistic conformal prediction (PCP),
        a predictive inference algorithm that estimates a target variable by a discontinuous
        predictive set. Given inputs, PCP construct the predictive set based on random
        samples from an estimated generative model. It is efficient and compatible
        with either explicit or implicit conditional generative models. Theoretically,
        we show that PCP guarantees correct marginal coverage with finite samples.
        Empirically, we study PCP on a variety of simulated and real datasets. Compared
        to existing methods for conformal inference, PCP provides sharper predictive
        sets.", "venue": "International Conference on Artificial Intelligence and
        Statistics", "year": 2022, "referenceCount": 56, "citationCount": 6, "influentialCitationCount":
        2, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science",
        "source": "external"}, {"category": "Mathematics", "source": "external"},
        {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
        ["JournalArticle"], "publicationDate": "2022-06-14", "journal": {"pages":
        "8814-8836"}, "authors": [{"authorId": "2108160372", "name": "Zhendong Wang"},
        {"authorId": "151184349", "name": "Ruijiang Gao"}, {"authorId": "13115663",
        "name": "Mingzhang Yin"}, {"authorId": "38026572", "name": "Mingyuan Zhou"},
        {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId": "cde5bd28b06c8d89af4c372bc74311ae44e4de78",
        "externalIds": {"ArXiv": "2202.06797", "DOI": "10.1214/22-aoas1608", "CorpusId":
        246471636}, "corpusId": 246471636, "publicationVenue": {"id": "8b03733e-25e3-49b5-803c-58c0e9e11180",
        "name": "Annals of Applied Statistics", "type": "journal", "alternate_names":
        ["Ann Appl Stat", "The Annals of Applied Statistics"], "issn": "1932-6157",
        "url": "http://www.imstat.org/aoas/", "alternate_urls": ["http://www.jstor.org/action/showPublication?journalCode=annaapplstat",
        "https://www.imstat.org/journals-and-publications/annals-of-applied-statistics/",
        "https://projecteuclid.org/DPubS?handle=euclid.aoas&service=UI&verb=Display&version=1.0",
        "https://www.jstor.org/journal/annaapplstat"]}, "url": "https://www.semanticscholar.org/paper/cde5bd28b06c8d89af4c372bc74311ae44e4de78",
        "title": "Mapping interstellar dust with Gaussian processes", "abstract":
        "Interstellar dust corrupts nearly every stellar observation, and accounting
        for it is crucial to measuring physical properties of stars. We model the
        dust distribution as a spatially varying latent field with a Gaussian process
        (GP) and develop a likelihood model and inference method that scales to millions
        of astronomical observations. Modeling interstellar dust is complicated by
        two factors. The first is integrated observations. The data come from a vantage
        point on Earth and each observation is an integral of the unobserved function
        along our line of sight, resulting in a complex likelihood and a more difficult
        inference problem than in classical GP inference. The second complication
        is scale; stellar catalogs have millions of observations. To address these
        challenges we develop ziggy, a scalable approach to GP inference with integrated
        observations based on stochastic variational inference. We study ziggy on
        synthetic data and the Ananke dataset, a high-fidelity mechanistic model of
        the Milky Way with millions of stars. ziggy reliably infers the spatial dust
        map with well-calibrated posterior uncertainties.", "venue": "Annals of Applied
        Statistics", "year": 2022, "referenceCount": 53, "citationCount": 3, "influentialCitationCount":
        0, "isOpenAccess": true, "openAccessPdf": {"url": "http://arxiv.org/pdf/2202.06797",
        "status": null}, "fieldsOfStudy": ["Physics", "Mathematics"], "s2FieldsOfStudy":
        [{"category": "Physics", "source": "external"}, {"category": "Mathematics",
        "source": "external"}, {"category": "Physics", "source": "s2-fos-model"},
        {"category": "Geology", "source": "s2-fos-model"}], "publicationTypes": null,
        "publicationDate": "2022-02-14", "journal": {"name": "The Annals of Applied
        Statistics"}, "authors": [{"authorId": "144360230", "name": "Andrew C. Miller"},
        {"authorId": "41234221", "name": "L. Anderson"}, {"authorId": "2410015", "name":
        "B. Leistedt"}, {"authorId": "2575774", "name": "J. Cunningham"}, {"authorId":
        "144735014", "name": "D. Hogg"}, {"authorId": "1796335", "name": "D. Blei"}]},
        {"paperId": "e8f170e3eee1fce6c9e9d6ca7b7f56f97a40e02b", "externalIds": {"ArXiv":
        "2202.08370", "CorpusId": 252917830}, "corpusId": 252917830, "publicationVenue":
        null, "url": "https://www.semanticscholar.org/paper/e8f170e3eee1fce6c9e9d6ca7b7f56f97a40e02b",
        "title": "CAREER: Transfer Learning for Economic Prediction of Labor Sequence
        Data", "abstract": "Labor economists regularly analyze employment data by
        fitting predictive models to small, carefully constructed longitudinal survey
        datasets. Although modern machine learning methods offer promise for such
        problems, these survey datasets are too small to take advantage of them. In
        recent years large datasets of online resumes have also become available,
        providing data about the career trajectories of millions of individuals. However,
        standard econometric models cannot take advantage of their scale or incorporate
        them into the analysis of survey data. To this end we develop CAREER, a transformer-based
        model that uses transfer learning to learn representations of job sequences.
        CAREER is first fit to large, passively-collected resume data and then fine-tuned
        to smaller, better-curated datasets for economic inferences. We fit CAREER
        to a dataset of 24 million job sequences from resumes, and fine-tune its representations
        on longitudinal survey datasets. We find that CAREER forms accurate predictions
        of job sequences on three widely-used economics datasets. We further find
        that CAREER can be used to form good predictions of other downstream variables;
        incorporating CAREER into a wage model provides better predictions than the
        econometric models currently in use.", "venue": "", "year": 2022, "referenceCount":
        57, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Economics"],
        "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
        {"category": "Economics", "source": "external"}, {"category": "Economics",
        "source": "s2-fos-model"}], "publicationTypes": ["Review"], "publicationDate":
        "2022-02-16", "journal": null, "authors": [{"authorId": "70025184", "name":
        "Keyon Vafa"}, {"authorId": "2083101971", "name": "Emil Palikot"}, {"authorId":
        null, "name": "Tianyu Du"}, {"authorId": "2661277", "name": "Ayush Kanodia"},
        {"authorId": "2631417", "name": "S. Athey"}, {"authorId": "1796335", "name":
        "D. Blei"}]}, {"paperId": "f354354d32e820ce55f26f2cb6508599df8cc698", "externalIds":
        {"DBLP": "conf/uai/MenonBV22", "ArXiv": "2207.09535", "DOI": "10.48550/arXiv.2207.09535",
        "CorpusId": 249917642}, "corpusId": 249917642, "publicationVenue": {"id":
        "f9af8000-42f8-410d-a622-e8811e41660a", "name": "Conference on Uncertainty
        in Artificial Intelligence", "type": "conference", "alternate_names": ["Uncertainty
        in Artificial Intelligence", "UAI", "Conf Uncertain Artif Intell", "Uncertain
        Artif Intell"], "url": "http://www.auai.org/"}, "url": "https://www.semanticscholar.org/paper/f354354d32e820ce55f26f2cb6508599df8cc698",
        "title": "Forget-me-not! Contrastive Critics for Mitigating Posterior Collapse",
        "abstract": "Variational autoencoders (VAEs) suffer from posterior collapse,
        where the powerful neural networks used for modeling and inference optimize
        the objective without meaningfully using the latent representation. We introduce
        inference critics that detect and incentivize against posterior collapse by
        requiring correspondence between latent variables and the observations. By
        connecting the critic''s objective to the literature in self-supervised contrastive
        representation learning, we show both theoretically and empirically that optimizing
        inference critics increases the mutual information between observations and
        latents, mitigating posterior collapse. This approach is straightforward to
        implement and requires significantly less training time than prior methods,
        yet obtains competitive results on three established datasets. Overall, the
        approach lays the foundation to bridge the previously disconnected frameworks
        of contrastive learning and probabilistic modeling with variational autoencoders,
        underscoring the benefits both communities may find at their intersection.",
        "venue": "Conference on Uncertainty in Artificial Intelligence", "year": 2022,
        "referenceCount": 50, "citationCount": 1, "influentialCitationCount": 1, "isOpenAccess":
        false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
        "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
        {"category": "Mathematics", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
        "Conference"], "publicationDate": "2022-07-19", "journal": {"volume": "abs/2207.09535",
        "name": "ArXiv"}, "authors": [{"authorId": "46245898", "name": "Sachit Menon"},
        {"authorId": "1796335", "name": "D. Blei"}, {"authorId": "1856025", "name":
        "Carl Vondrick"}]}, {"paperId": "f924b383cc7e83e454f8a43e7ff60355539e2110",
        "externalIds": {"PubMedCentral": "9581469", "DOI": "10.1126/sciadv.ade6585",
        "CorpusId": 253019588, "PubMed": "36260667"}, "corpusId": 253019588, "publicationVenue":
        {"id": "cb30f0c9-2980-4b7d-bbcb-68fc5472b97c", "name": "Science Advances",
        "type": "journal", "alternate_names": ["Sci Adv"], "issn": "2375-2548", "url":
        "http://www.scienceadvances.org/", "alternate_urls": ["https://advances.sciencemag.org/"]},
        "url": "https://www.semanticscholar.org/paper/f924b383cc7e83e454f8a43e7ff60355539e2110",
        "title": "Causal inference from text: A commentary", "abstract": "Statistical
        and machine learning methods help social scientists and other researchers
        make causal inferences from texts.", "venue": "Science Advances", "year":
        2022, "referenceCount": 4, "citationCount": 0, "influentialCitationCount":
        0, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.science.org/doi/pdf/10.1126/sciadv.ade6585?download=true",
        "status": null}, "fieldsOfStudy": ["Medicine"], "s2FieldsOfStudy": [{"category":
        "Medicine", "source": "external"}, {"category": "Computer Science", "source":
        "s2-fos-model"}], "publicationTypes": ["Review", "JournalArticle"], "publicationDate":
        "2022-10-01", "journal": {"volume": "8", "name": "Science Advances"}, "authors":
        [{"authorId": "153485411", "name": "Dhanya Sridhar"}, {"authorId": "1796335",
        "name": "D. Blei"}]}, {"paperId": "06546042da3547154d7446832dd5943c0b92bcbe",
        "externalIds": {"DBLP": "journals/jbi/ZhangWSBH22", "ArXiv": "2110.12235",
        "DOI": "10.1016/j.jbi.2022.104204", "CorpusId": 248427325, "PubMed": "36108816"},
        "corpusId": 248427325, "publicationVenue": {"id": "f9827422-a381-440c-a8a4-e5e50415934e",
        "name": "Journal of Biomedical Informatics", "type": "journal", "alternate_names":
        ["J Biomed Informatics"], "issn": "1532-0464", "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/622857/description#description",
        "alternate_urls": ["https://www.journals.elsevier.com/journal-of-biomedical-informatics/",
        "http://www.sciencedirect.com/science/journal/15320464", "http://www.journals.elsevier.com/journal-of-biomedical-informatics/"]},
        "url": "https://www.semanticscholar.org/paper/06546042da3547154d7446832dd5943c0b92bcbe",
        "title": "Adjusting for indirectly measured confounding using large-scale
        propensity score", "abstract": null, "venue": "Journal of Biomedical Informatics",
        "year": 2021, "referenceCount": 71, "citationCount": 7, "influentialCitationCount":
        0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science", "Medicine", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer
        Science", "source": "external"}, {"category": "Medicine", "source": "external"},
        {"category": "Mathematics", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
        "publicationDate": "2021-10-23", "journal": {"pages": "\n          104204\n        ",
        "name": "Journal of biomedical informatics"}, "authors": [{"authorId": "3414608",
        "name": "Linying Zhang"}, {"authorId": "2108734693", "name": "Yixin Wang"},
        {"authorId": "2904133", "name": "M. Schuemie"}, {"authorId": "1796335", "name":
        "D. Blei"}, {"authorId": "1686114", "name": "G. Hripcsak"}]}, {"paperId":
        "0bf88192d02c08661b9185b2b16399306694c4a4", "externalIds": {"ArXiv": "2110.10804",
        "DBLP": "journals/tmlr/MoranSWB22", "CorpusId": 246904884}, "corpusId": 246904884,
        "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/0bf88192d02c08661b9185b2b16399306694c4a4",
        "title": "Identifiable Deep Generative Models via Sparse Decoding", "abstract":
        "We develop the sparse VAE for unsupervised representation learning on high-dimensional
        data. The sparse VAE learns a set of latent factors (representations) which
        summarize the associations in the observed data features. The underlying model
        is sparse in that each observed feature (i.e. each dimension of the data)
        depends on a small subset of the latent factors. As examples, in ratings data
        each movie is only described by a few genres; in text data each word is only
        applicable to a few topics; in genomics, each gene is active in only a few
        biological processes. We prove such sparse deep generative models are identifiable:
        with infinite data, the true model parameters can be learned. (In contrast,
        most deep generative models are not identifiable.) We empirically study the
        sparse VAE with both simulated and real data. We find that it recovers meaningful
        latent factors and has smaller heldout reconstruction error than related methods.",
        "venue": "Trans. Mach. Learn. Res.", "year": 2021, "referenceCount": 53, "citationCount":
        15, "influentialCitationCount": 2, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": ["Mathematics", "Computer Science"], "s2FieldsOfStudy":
        [{"category": "Mathematics", "source": "external"}, {"category": "Computer
        Science", "source": "external"}, {"category": "Computer Science", "source":
        "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
        "2021-10-20", "journal": {"volume": "2022", "name": "Trans. Mach. Learn. Res."},
        "authors": [{"authorId": "3433248", "name": "Gemma E. Moran"}, {"authorId":
        "153485411", "name": "Dhanya Sridhar"}, {"authorId": "2108734693", "name":
        "Yixin Wang"}, {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId": "1006312979506add9e3e1dadd012b0fb858ca40d",
        "externalIds": {"DBLP": "journals/corr/abs-2103-00393", "ArXiv": "2103.00393",
        "CorpusId": 232076304}, "corpusId": 232076304, "publicationVenue": {"id":
        "2d136b11-c2b5-484b-b008-7f4a852fd61e", "name": "International Conference
        on Artificial Intelligence and Statistics", "type": "conference", "alternate_names":
        ["AISTATS", "Int Conf Artif Intell Stat"]}, "url": "https://www.semanticscholar.org/paper/1006312979506add9e3e1dadd012b0fb858ca40d",
        "title": "Hierarchical Inducing Point Gaussian Process for Inter-domain Observations",
        "abstract": "We examine the general problem of inter-domain Gaussian Processes
        (GPs): problems where the GP realization and the noisy observations of that
        realization lie on different domains. When the mapping between those domains
        is linear, such as integration or differentiation, inference is still closed
        form. However, many of the scaling and approximation techniques that our community
        has developed do not apply to this setting. In this work, we introduce the
        hierarchical inducing point GP (HIP-GP), a scalable inter-domain GP inference
        method that enables us to improve the approximation accuracy by increasing
        the number of inducing points to the millions. HIP-GP, which relies on inducing
        points with grid structure and a stationary kernel assumption, is suitable
        for low-dimensional problems. In developing HIP-GP, we introduce (1) a fast
        whitening strategy, and (2) a novel preconditioner for conjugate gradients
        which can be helpful in general GP settings. Our code is available at https:
        //github.com/cunningham-lab/hipgp.", "venue": "International Conference on
        Artificial Intelligence and Statistics", "year": 2021, "referenceCount": 43,
        "citationCount": 9, "influentialCitationCount": 1, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
        "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
        {"category": "Mathematics", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
        "publicationDate": "2021-02-28", "journal": {"pages": "2926-2934"}, "authors":
        [{"authorId": "1410052582", "name": "Luhuan Wu"}, {"authorId": "2115894884",
        "name": "Andrew Miller"}, {"authorId": "2111938392", "name": "Lauren Anderson"},
        {"authorId": "10804137", "name": "Geoff Pleiss"}, {"authorId": "1796335",
        "name": "D. Blei"}, {"authorId": "2575774", "name": "J. Cunningham"}]}, {"paperId":
        "1c709eef701d933af1383c790c13209f06806b60", "externalIds": {"DBLP": "journals/corr/abs-2109-06387",
        "ACL": "2021.emnlp-main.807", "ArXiv": "2109.06387", "DOI": "10.18653/v1/2021.emnlp-main.807",
        "CorpusId": 237503612}, "corpusId": 237503612, "publicationVenue": {"id":
        "41bf9ed3-85b3-4c90-b015-150e31690253", "name": "Conference on Empirical Methods
        in Natural Language Processing", "type": "conference", "alternate_names":
        ["Empir Method Nat Lang Process", "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process", "EMNLP"], "url": "https://www.aclweb.org/portal/emnlp"},
        "url": "https://www.semanticscholar.org/paper/1c709eef701d933af1383c790c13209f06806b60",
        "title": "Rationales for Sequential Predictions", "abstract": "Sequence models
        are a critical component of modern NLP systems, but their predictions are
        difficult to explain. We consider model explanations though rationales, subsets
        of context that can explain individual model predictions. We find sequential
        rationales by solving a combinatorial optimization: the best rationale is
        the smallest subset of input tokens that would predict the same output as
        the full sequence. Enumerating all subsets is intractable, so we propose an
        efficient greedy algorithm to approximate this objective. The algorithm, which
        is called greedy rationalization, applies to any model. For this approach
        to be effective, the model should form compatible conditional distributions
        when making predictions on incomplete subsets of the context. This condition
        can be enforced with a short fine-tuning step. We study greedy rationalization
        on language modeling and machine translation. Compared to existing baselines,
        greedy rationalization is best at optimizing the sequential objective and
        provides the most faithful rationales. On a new dataset of annotated sequential
        rationales, greedy rationales are most similar to human rationales.", "venue":
        "Conference on Empirical Methods in Natural Language Processing", "year":
        2021, "referenceCount": 53, "citationCount": 16, "influentialCitationCount":
        3, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.emnlp-main.807.pdf",
        "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
        "Conference"], "publicationDate": "2021-09-14", "journal": {"pages": "10314-10332"},
        "authors": [{"authorId": "70025184", "name": "Keyon Vafa"}, {"authorId": "2505751",
        "name": "Yuntian Deng"}, {"authorId": "1796335", "name": "D. Blei"}, {"authorId":
        "2531268", "name": "Alexander M. Rush"}]}, {"paperId": "34ee368fc2f5de1a95d408811b809ae234c137bc",
        "externalIds": {"ArXiv": "2109.11990", "DBLP": "journals/corr/abs-2109-11990",
        "CorpusId": 237635107}, "corpusId": 237635107, "publicationVenue": {"id":
        "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names":
        ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/34ee368fc2f5de1a95d408811b809ae234c137bc",
        "title": "Optimization-based Causal Estimation from Heterogenous Environments",
        "abstract": "This paper presents a new optimization approach to causal estimation.
        Given data that contains covariates and an outcome, which covariates are causes
        of the outcome, and what is the strength of the causality? In classical machine
        learning (ML), the goal of optimization is to maximize predictive accuracy.
        However, some covariates might exhibit a non-causal association to the outcome.
        Such spurious associations provide predictive power for classical ML, but
        they prevent us from causally interpreting the result. This paper proposes
        CoCo, an optimization algorithm that bridges the gap between pure prediction
        and causal inference. CoCo leverages the recently-proposed idea of environments,
        datasets of covariates/response where the causal relationships remain invariant
        but where the distribution of the covariates changes from environment to environment.
        Given datasets from multiple environments -- and ones that exhibit sufficient
        heterogeneity -- CoCo maximizes an objective for which the only solution is
        the causal solution. We describe the theoretical foundations of this approach
        and demonstrate its effectiveness on simulated and real datasets. Compared
        to classical ML and existing methods, CoCo provides more accurate estimates
        of the causal model.", "venue": "arXiv.org", "year": 2021, "referenceCount":
        73, "citationCount": 5, "influentialCitationCount": 1, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Mathematics", "Computer Science"],
        "s2FieldsOfStudy": [{"category": "Mathematics", "source": "external"}, {"category":
        "Computer Science", "source": "external"}, {"category": "Computer Science",
        "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
        "2021-09-24", "journal": {"volume": "abs/2109.11990", "name": "ArXiv"}, "authors":
        [{"authorId": "13115663", "name": "Mingzhang Yin"}, {"authorId": "2108734693",
        "name": "Yixin Wang"}, {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId":
        "466abebd6519c16f952c4633c10c0b693e240e3e", "externalIds": {"DBLP": "conf/aistats/ShiSMB22",
        "ArXiv": "2112.05671", "CorpusId": 245117638}, "corpusId": 245117638, "publicationVenue":
        {"id": "2d136b11-c2b5-484b-b008-7f4a852fd61e", "name": "International Conference
        on Artificial Intelligence and Statistics", "type": "conference", "alternate_names":
        ["AISTATS", "Int Conf Artif Intell Stat"]}, "url": "https://www.semanticscholar.org/paper/466abebd6519c16f952c4633c10c0b693e240e3e",
        "title": "On the Assumptions of Synthetic Control Methods", "abstract": "Synthetic
        control (SC) methods have been widely applied to estimate the causal effect
        of large-scale interventions, e.g., the state-wide effect of a change in policy.
        The idea of synthetic controls is to approximate one unit''s counterfactual
        outcomes using a weighted combination of some other units'' observed outcomes.
        The motivating question of this paper is: how does the SC strategy lead to
        valid causal inferences? We address this question by re-formulating the causal
        inference problem targeted by SC with a more fine-grained model, where we
        change the unit of the analysis from\"large units\"(e.g., states) to\"small
        units\"(e.g., individuals in states). Under this re-formulation, we derive
        sufficient conditions for the non-parametric causal identification of the
        causal effect. We highlight two implications of the reformulation: (1) it
        clarifies where\"linearity\"comes from, and how it falls naturally out of
        the more fine-grained and flexible model, and (2) it suggests new ways of
        using available data with SC methods for valid causal inference, in particular,
        new ways of selecting observations from which to estimate the counterfactual.",
        "venue": "International Conference on Artificial Intelligence and Statistics",
        "year": 2021, "referenceCount": 45, "citationCount": 10, "influentialCitationCount":
        3, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Mathematics",
        "Economics", "Computer Science"], "s2FieldsOfStudy": [{"category": "Mathematics",
        "source": "external"}, {"category": "Economics", "source": "external"}, {"category":
        "Computer Science", "source": "external"}, {"category": "Economics", "source":
        "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
        "2021-12-10", "journal": {"pages": "7163-7175"}, "authors": [{"authorId":
        "133797256", "name": "Claudia Shi"}, {"authorId": "153485411", "name": "Dhanya
        Sridhar"}, {"authorId": "145502289", "name": "V. Misra"}, {"authorId": "1796335",
        "name": "D. Blei"}]}, {"paperId": "5645f1cc9c3c86091b1eeb58150a68e4f062de27",
        "externalIds": {"ArXiv": "2112.03493", "DOI": "10.1080/01621459.2022.2102503",
        "CorpusId": 244920826}, "corpusId": 244920826, "publicationVenue": {"id":
        "8c91bff0-940d-46f7-86d4-582c09ed787c", "name": "Journal of the American Statistical
        Association", "type": "journal", "alternate_names": ["J Am Stat Assoc"], "issn":
        "0162-1459", "url": "http://openurl.ingenta.com/content?genre=journal&issn=0162-1459",
        "alternate_urls": ["https://www.jstor.org/journal/jamerstatasso", "http://www.tandfonline.com/loi/uasa20#.VHzNXU10ymQ",
        "http://amstat.tandfonline.com/loi/uasa20", "http://www.jstor.org/journals/01621459.html",
        "http://www.tandfonline.com/toc/uasa20/current"]}, "url": "https://www.semanticscholar.org/paper/5645f1cc9c3c86091b1eeb58150a68e4f062de27",
        "title": "Conformal Sensitivity Analysis for Individual Treatment Effects",
        "abstract": "Estimating an individual treatment effect (ITE) is essential
        to personalized decision making. However, existing methods for estimating
        the ITE often rely on unconfoundedness, an assumption that is fundamentally
        untestable with observed data. To assess the robustness of individual-level
        causal conclusion with unconfoundedness, this paper proposes a method for
        sensitivity analysis of the ITE, a way to estimate a range of the ITE under
        unobserved confounding. The method we develop quantifies unmeasured confounding
        through a marginal sensitivity model [Ros2002, Tan2006], and adapts the framework
        of conformal inference to estimate an ITE interval at a given confounding
        strength. In particular, we formulate this sensitivity analysis problem as
        a conformal inference problem under distribution shift, and we extend existing
        methods of covariate-shifted conformal inference to this more general setting.
        The result is a predictive interval that has guaranteed nominal coverage of
        the ITE, a method that provides coverage with distribution-free and nonasymptotic
        guarantees. We evaluate the method on synthetic data and illustrate its application
        in an observational study.", "venue": "Journal of the American Statistical
        Association", "year": 2021, "referenceCount": 81, "citationCount": 21, "influentialCitationCount":
        2, "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/2112.03493",
        "status": null}, "fieldsOfStudy": ["Mathematics"], "s2FieldsOfStudy": [{"category":
        "Mathematics", "source": "external"}, {"category": "Mathematics", "source":
        "s2-fos-model"}], "publicationTypes": null, "publicationDate": "2021-12-07",
        "journal": {"name": "Journal of the American Statistical Association"}, "authors":
        [{"authorId": "13115663", "name": "Mingzhang Yin"}, {"authorId": "133797256",
        "name": "Claudia Shi"}, {"authorId": "2108734693", "name": "Yixin Wang"},
        {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId": "5cdfc4fa6cf6a297599182a80f2460027070183b",
        "externalIds": {"DBLP": "conf/icml/ParkLKB21", "ArXiv": "2112.04014", "CorpusId":
        235825504}, "corpusId": 235825504, "publicationVenue": {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
        "name": "International Conference on Machine Learning", "type": "conference",
        "alternate_names": ["ICML", "Int Conf Mach Learn"], "url": "https://icml.cc/"},
        "url": "https://www.semanticscholar.org/paper/5cdfc4fa6cf6a297599182a80f2460027070183b",
        "title": "Unsupervised Representation Learning via Neural Activation Coding",
        "abstract": "We present neural activation coding (NAC) as a novel approach
        for learning deep representations from unlabeled data for downstream applications.
        We argue that the deep encoder should maximize its nonlinear expressivity
        on the data for downstream predictors to take full advantage of its representation
        power. To this end, NAC maximizes the mutual information between activation
        patterns of the encoder and the data over a noisy communication channel. We
        show that learning for a noise-robust activation code increases the number
        of distinct linear regions of ReLU encoders, hence the maximum nonlinear expressivity.
        More interestingly, NAC learns both continuous and discrete representations
        of data, which we respectively evaluate on two downstream tasks: (i) linear
        classification on CIFAR-10 and ImageNet-1K and (ii) nearest neighbor retrieval
        on CIFAR-10 and FLICKR-25K. Empirical results show that NAC attains better
        or comparable performance on both tasks over recent baselines including SimCLR
        and DistillHash. In addition, NAC pretraining provides significant benefits
        to the training of deep generative models. Our code is available at https://github.com/yookoon/nac.",
        "venue": "International Conference on Machine Learning", "year": 2021, "referenceCount":
        44, "citationCount": 6, "influentialCitationCount": 0, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
        "Conference"], "publicationDate": "2021-12-07", "journal": {"volume": "abs/2112.04014",
        "name": "ArXiv"}, "authors": [{"authorId": "134893586", "name": "Yookoon Park"},
        {"authorId": "2144567767", "name": "Sangho Lee"}, {"authorId": "70308241",
        "name": "Gunhee Kim"}, {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId":
        "6fb0a646bc4d62fb103a2702e4e5ca3c484e3c3f", "externalIds": {"CorpusId": 244920558},
        "corpusId": 244920558, "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/6fb0a646bc4d62fb103a2702e4e5ca3c484e3c3f",
        "title": "Posterior Predictive Null Checks", "abstract": "Bayesian model criticism
        is an important part of the practice of Bayesian statistics. Traditionally,
        model criticism methods have been based on the predictive check, an adaptation
        of goodness-of-fit testing to Bayesian modeling and an effective method to
        understand how well a model captures the distribution of the data. In modern
        practice, however, researchers iteratively build and develop many models,
        exploring a space of models to help solve the problem at hand. While classical
        predictive checks can help assess each one, they cannot help the researcher
        understand how the models relate to each other. This paper introduces the
        posterior predictive null check (PPN), a method for Bayesian model criticism
        that helps characterize the relationships between models. The idea behind
        the PPN is to check whether data from one model\u2019s predictive distribution
        can pass a predictive check designed for another model. This form of criticism
        complements the classical predictive check by providing a comparative tool.
        A collection of PPNs, which we call a PPN study, can help us understand which
        models are equivalent and which models provide different perspectives on the
        data. With mixture models, we demonstrate how a PPN study, along with traditional
        predictive checks, can help select the number of components by the principle
        of parsimony. With probabilistic factor models, we demonstrate how a PPN study
        can help understand relationships between different classes of models, such
        as linear models and models based on neural networks. Finally, we analyze
        data from the literature on predictive checks to show how a PPN study can
        improve the practice of Bayesian model criticism. Code to replicate the results
        in this paper is available at https://github.com/gemoran/ppn-code.", "venue":
        "", "year": 2021, "referenceCount": 33, "citationCount": 1, "influentialCitationCount":
        0, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
        null, "publicationDate": null, "journal": null, "authors": [{"authorId": "3433248",
        "name": "Gemma E. Moran"}, {"authorId": "2575774", "name": "J. Cunningham"},
        {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId": "87f8173265cfe3b077e53d09fd3598f14f2667e4",
        "externalIds": {"DBLP": "conf/icml/WangB21", "CorpusId": 235825545}, "corpusId":
        235825545, "publicationVenue": {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
        "name": "International Conference on Machine Learning", "type": "conference",
        "alternate_names": ["ICML", "Int Conf Mach Learn"], "url": "https://icml.cc/"},
        "url": "https://www.semanticscholar.org/paper/87f8173265cfe3b077e53d09fd3598f14f2667e4",
        "title": "A Proxy Variable View of Shared Confounding", "abstract": "Causal
        inference from observational data can be biased by unobserved confounders.
        Confounders\u2014the variables that affect both the treatments and the outcome\u2014induce
        spurious non-causal correlations between the two. Without additional conditions,
        unobserved confounders generally make causal quantities hard to identify.
        In this paper, we focus on the setting where there are many treatments with
        shared confounding, and we study under what conditions is causal identification
        possible. The key observation is that we can view subsets of treatments as
        proxies of the unobserved confounder and identify the intervention distributions
        of the rest. Moreover, while existing identification formulas for proxy variables
        involve solving integral equations, we show that one can circumvent the need
        for such solutions by directly modeling the data. Finally, we extend these
        results to an expanded class of causal graphs, those with other confounders
        and selection variables.", "venue": "International Conference on Machine Learning",
        "year": 2021, "referenceCount": 36, "citationCount": 8, "influentialCitationCount":
        1, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
        "external"}, {"category": "Economics", "source": "s2-fos-model"}, {"category":
        "Mathematics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
        "Conference"], "publicationDate": null, "journal": {"pages": "10697-10707"},
        "authors": [{"authorId": "2108734693", "name": "Yixin Wang"}, {"authorId":
        "1796335", "name": "D. Blei"}]}, {"paperId": "89a587b76b2ad28417ec209c205848362df1ef8e",
        "externalIds": {"ArXiv": "2106.00075", "DBLP": "conf/uai/MorettiZNVBP21",
        "CorpusId": 235265794}, "corpusId": 235265794, "publicationVenue": {"id":
        "f9af8000-42f8-410d-a622-e8811e41660a", "name": "Conference on Uncertainty
        in Artificial Intelligence", "type": "conference", "alternate_names": ["Uncertainty
        in Artificial Intelligence", "UAI", "Conf Uncertain Artif Intell", "Uncertain
        Artif Intell"], "url": "http://www.auai.org/"}, "url": "https://www.semanticscholar.org/paper/89a587b76b2ad28417ec209c205848362df1ef8e",
        "title": "Variational Combinatorial Sequential Monte Carlo Methods for Bayesian
        Phylogenetic Inference", "abstract": "Bayesian phylogenetic inference is often
        conducted via local or sequential search over topologies and branch lengths
        using algorithms such as random-walk Markov chain Monte Carlo (MCMC) or Combinatorial
        Sequential Monte Carlo (CSMC). However, when MCMC is used for evolutionary
        parameter learning, convergence requires long runs with inefficient exploration
        of the state space. We introduce Variational Combinatorial Sequential Monte
        Carlo (VCSMC), a powerful framework that establishes variational sequential
        search to learn distributions over intricate combinatorial structures. We
        then develop nested CSMC, an efficient proposal distribution for CSMC and
        prove that nested CSMC is an exact approximation to the (intractable) locally
        optimal proposal. We use nested CSMC to define a second objective, VNCSMC
        which yields tighter lower bounds than VCSMC. We show that VCSMC and VNCSMC
        are computationally efficient and explore higher probability spaces than existing
        methods on a range of tasks.", "venue": "Conference on Uncertainty in Artificial
        Intelligence", "year": 2021, "referenceCount": 55, "citationCount": 8, "influentialCitationCount":
        1, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science",
        "source": "external"}, {"category": "Mathematics", "source": "external"},
        {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
        ["JournalArticle", "Conference"], "publicationDate": "2021-05-31", "journal":
        {"pages": "971-981"}, "authors": [{"authorId": "32032560", "name": "A. Moretti"},
        {"authorId": "2108871879", "name": "Liyi Zhang"}, {"authorId": "2328322",
        "name": "C. A. Naesseth"}, {"authorId": "2101317642", "name": "Hadiah K Venner"},
        {"authorId": "1796335", "name": "D. Blei"}, {"authorId": "101161161", "name":
        "I. Pe\u2019er"}]}, {"paperId": "933393df9068059dd49f555eaa67f5f9ce58aeec",
        "externalIds": {"DBLP": "conf/www/ScheinVSVQMBG21", "DOI": "10.1145/3442381.3449800",
        "CorpusId": 235324871}, "corpusId": 235324871, "publicationVenue": {"id":
        "e07422f9-c065-40c3-a37b-75e98dce79fe", "name": "The Web Conference", "type":
        "conference", "alternate_names": ["Web Conf", "WWW"], "url": "http://www.iw3c2.org/"},
        "url": "https://www.semanticscholar.org/paper/933393df9068059dd49f555eaa67f5f9ce58aeec",
        "title": "Assessing the Effects of Friend-to-Friend Texting onTurnout in the
        2018 US Midterm Elections", "abstract": "Recent mobile app technology lets
        people systematize the process of messaging their friends to urge them to
        vote. Prior to the most recent US midterm elections in 2018, the mobile app
        Outvote randomized an aspect of their system, hoping to unobtrusively assess
        the causal effect of their users\u2019 messages on voter turnout. However,
        properly assessing this causal effect is hindered by multiple statistical
        challenges, including attenuation bias due to mismeasurement of subjects\u2019
        outcomes and low precision due to two-sided non-compliance with subjects\u2019
        assignments. We address these challenges, which are likely to impinge upon
        any study that seeks to randomize authentic friend-to-friend interactions,
        by tailoring the statistical analysis to make use of additional data about
        both users and subjects. Using meta-data of users\u2019 in-app behavior, we
        reconstruct subjects\u2019 positions in users\u2019 queues. We use this information
        to refine the study population to more compliant subjects who were higher
        in the queues, and we do so in a systematic way which optimizes a proxy for
        the study\u2019s power. To mitigate attenuation bias, we then use ancillary
        data of subjects\u2019 matches to the voter rolls that lets us refine the
        study population to one with low rates of outcome mismeasurement. Our analysis
        reveals statistically significant treatment effects from friend-to-friend
        mobilization efforts ( 8.3, CI = (1.2, 15.3)) that are among the largest reported
        in the get-out-the-vote (GOTV) literature. While social pressure from friends
        has long been conjectured to play a role in effective GOTV treatments, the
        present study is among the first to assess these effects experimentally.",
        "venue": "The Web Conference", "year": 2021, "referenceCount": 26, "citationCount":
        1, "influentialCitationCount": 0, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
        "Computer Science", "source": "external"}, {"category": "Psychology", "source":
        "s2-fos-model"}], "publicationTypes": ["Book", "JournalArticle", "Conference"],
        "publicationDate": "2021-04-19", "journal": {"name": "Proceedings of the Web
        Conference 2021"}, "authors": [{"authorId": "50545056", "name": "Aaron Schein"},
        {"authorId": "70025184", "name": "Keyon Vafa"}, {"authorId": "153485411",
        "name": "Dhanya Sridhar"}, {"authorId": "2974320", "name": "Victor Veitch"},
        {"authorId": "2050597", "name": "Jeffrey M. Quinn"}, {"authorId": "2037183204",
        "name": "James Moffet"}, {"authorId": "1796335", "name": "D. Blei"}, {"authorId":
        "145931139", "name": "D. Green"}]}, {"paperId": "ab830a6e9fd3319e75e3b871a6e2f0f0a9168751",
        "externalIds": {"ArXiv": "2112.03333", "DOI": "10.1214/22-BA1313", "CorpusId":
        248646417}, "corpusId": 248646417, "publicationVenue": {"id": "e60aa3db-ffab-43ed-97b5-de22436b3ea4",
        "name": "Bayesian Analysis", "type": "journal", "alternate_names": ["Bayesian
        Anal"], "issn": "1931-6690", "alternate_issns": ["1936-0975"], "url": "http://bayesian.org/BA",
        "alternate_urls": ["http://projecteuclid.org/ba"]}, "url": "https://www.semanticscholar.org/paper/ab830a6e9fd3319e75e3b871a6e2f0f0a9168751",
        "title": "The Posterior Predictive Null", "abstract": ". Bayesian model criticism
        is an important part of the practice of Bayesian statistics. Traditionally,
        model criticism methods have been based on the predictive check, an adaptation
        of goodness-of-\ufb01t testing to Bayesian modeling and an e\ufb00ective method
        to understand how well a model captures the distribution of the data. In modern
        practice, however, researchers iteratively build and develop many models,
        exploring a space of models to help solve the problem at hand. While classical
        predictive checks can help assess each one, they cannot help the researcher
        understand how the models relate to each other. This paper introduces the
        posterior predictive null check (PPN), a method for Bayesian model criticism
        that helps characterize the relationships between models. The idea behind
        the PPN is to check whether data from one model\u2019s predictive distribution
        can pass a predictive check designed for another model. This form of criticism
        complements the classical predictive check by providing a comparative tool.
        A collection of PPNs, which we call a PPN study, can help us understand which
        models are equivalent and which models provide di\ufb00erent perspectives
        on the data. With mixture models, we demonstrate how a PPN study, along with
        traditional predictive checks, can help select the number of components by
        the principle of parsimony. With probabilistic factor models, we demonstrate
        how a PPN study can help understand relationships between di\ufb00erent classes
        of models, such as linear models and models based on neural networks. Finally,
        we analyze data from the literature on predictive checks to show how a PPN
        study can improve the practice of Bayesian model criticism. Code to replicate
        the results in this paper is available at https://github.com/gemoran/ppn-code
        .", "venue": "Bayesian Analysis", "year": 2021, "referenceCount": 35, "citationCount":
        0, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url":
        "https://projecteuclid.org/journals/bayesian-analysis/advance-publication/The-Posterior-Predictive-Null/10.1214/22-BA1313.pdf",
        "status": null}, "fieldsOfStudy": ["Mathematics"], "s2FieldsOfStudy": [{"category":
        "Mathematics", "source": "external"}, {"category": "Computer Science", "source":
        "s2-fos-model"}], "publicationTypes": null, "publicationDate": "2021-12-06",
        "journal": {"name": "Bayesian Analysis"}, "authors": [{"authorId": "3433248",
        "name": "Gemma E. Moran"}, {"authorId": "2575774", "name": "J. Cunningham"},
        {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId": "ba4d377d62e614d8a243b8113ce507501cab2e80",
        "externalIds": {"CorpusId": 245476111}, "corpusId": 245476111, "publicationVenue":
        null, "url": "https://www.semanticscholar.org/paper/ba4d377d62e614d8a243b8113ce507501cab2e80",
        "title": "Assessing the Effects of Friend-to-Friend Texting on Turnout in
        the 2020 U.S. Presidential Election", "abstract": "Political campaigns in
        recent elections have started to embrace friend-to-friend organizing, in which
        volunteers organize and encourage their own close contacts to cast a ballot
        on Election Day. Unlike traditional \u201cget out the vote\u201d (GOTV) campaigns,
        which often rely on texts, calls, or visits from strangers, friend-to-friend
        organizing is premised on the notion that GOTV encouragements are especially
        effective when delivered by trusted messengers, like friends or family members.",
        "venue": "", "year": 2021, "referenceCount": 10, "citationCount": 1, "influentialCitationCount":
        0, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy":
        [{"category": "Political Science", "source": "s2-fos-model"}], "publicationTypes":
        null, "publicationDate": null, "journal": null, "authors": [{"authorId": "50545056",
        "name": "Aaron Schein"}, {"authorId": "1796335", "name": "D. Blei"}, {"authorId":
        "145931139", "name": "D. Green"}]}, {"paperId": "ba4ea42d99b6ac8d2f76ce62dc9fa5b28795ac1e",
        "externalIds": {"CorpusId": 239769301}, "corpusId": 239769301, "publicationVenue":
        null, "url": "https://www.semanticscholar.org/paper/ba4ea42d99b6ac8d2f76ce62dc9fa5b28795ac1e",
        "title": "Adjusting for Unobserved Confounding Using Large-Scale Propensity
        Scores", "abstract": "Even though observational data contain an enormous number
        of covariates, the existence of unobserved confounders still cannot be excluded
        and remains a major barrier to drawing causal inference from observational
        data. A large-scale propensity score (LSPS) approach may adjust for unobserved
        confounders by including tens of thousands of available covariates that may
        be correlated with them. In this paper, we present conditions under which
        LSPS can remove bias due to unobserved confounders. In addition, we show that
        LSPS may avoid bias that can be induced when adjusting for various unwanted
        variables (e.g., M-structure colliders). We demonstrate the performance of
        LSPS on bias reduction using both simulations and real medical data.", "venue":
        "", "year": 2021, "referenceCount": 68, "citationCount": 0, "influentialCitationCount":
        0, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy":
        [{"category": "Economics", "source": "s2-fos-model"}, {"category": "Mathematics",
        "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": null,
        "journal": null, "authors": [{"authorId": "3414608", "name": "Linying Zhang"},
        {"authorId": "2108734693", "name": "Yixin Wang"}, {"authorId": "2904133",
        "name": "M. Schuemie"}, {"authorId": "1796335", "name": "D. Blei"}, {"authorId":
        "1686114", "name": "G. Hripcsak"}]}, {"paperId": "bede99a33904742db2abe4f3b93cae70f4fdbe91",
        "externalIds": {"DBLP": "journals/jmlr/LoperBCP21", "CorpusId": 238997308},
        "corpusId": 238997308, "publicationVenue": {"id": "c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
        "name": "Journal of machine learning research", "type": "journal", "alternate_names":
        ["Journal of Machine Learning Research", "J mach learn res", "J Mach Learn
        Res"], "issn": "1532-4435", "alternate_issns": ["1533-7928"], "url": "http://www.ai.mit.edu/projects/jmlr/",
        "alternate_urls": ["http://jmlr.csail.mit.edu/", "http://www.jmlr.org/", "http://portal.acm.org/affiliated/jmlr"]},
        "url": "https://www.semanticscholar.org/paper/bede99a33904742db2abe4f3b93cae70f4fdbe91",
        "title": "A general linear-time inference method for Gaussian Processes on
        one dimension", "abstract": "Gaussian Processes (GPs) provide powerful probabilistic
        frameworks for interpolation, forecasting, and smoothing, but have been hampered
        by computational scaling issues. Here we investigate data sampled on one dimension
        (e.g., a scalar or vector time series sampled at arbitrarily-spaced intervals),
        for which state-space models are popular due to their linearly-scaling computational
        costs. It has long been conjectured that state-space models are general, able
        to approximate any one-dimensional GP. We provide the first general proof
        of this conjecture, showing that any stationary GP on one dimension with vector-valued
        observations governed by a Lebesgue-integrable continuous kernel can be approximated
        to any desired precision using a specifically-chosen state-space model: the
        Latent Exponentially Generated (LEG) family. This new family offers several
        advantages compared to the general state-space model: it is always stable
        (no unbounded growth), the covariance can be computed in closed form, and
        its parameter space is unconstrained (allowing straightforward estimation
        via gradient descent). The theorem\u2019s proof also draws connections to
        Spectral Mixture Kernels, providing insight about this popular family of kernels.
        We develop parallelized algorithms for performing inference and learning in
        the c \u00a92021 Jackson Loper, David Blei, John P. Cunningham, Liam Paninski.
        License: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution
        requirements are provided at http://jmlr.org/papers/v22/21-0072.html. Loper,
        Blei, Cunningham, and Paninski LEG model, test the algorithm on real and synthetic
        data, and demonstrate scaling to datasets with billions of samples.", "venue":
        "Journal of machine learning research", "year": 2021, "referenceCount": 59,
        "citationCount": 4, "influentialCitationCount": 0, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
        "publicationDate": null, "journal": {"volume": "22", "pages": "234:1-234:36",
        "name": "J. Mach. Learn. Res."}, "authors": [{"authorId": "145640248", "name":
        "Jackson Loper"}, {"authorId": "1796335", "name": "D. Blei"}, {"authorId":
        "2575774", "name": "J. Cunningham"}, {"authorId": "1763504", "name": "L. Paninski"}]},
        {"paperId": "e597faf0a561de858600ccead4e44ff782a37c3d", "externalIds": {"DOI":
        "10.1007/s11129-021-09245-y", "CorpusId": 260681527}, "corpusId": 260681527,
        "publicationVenue": {"id": "076f5568-3eb0-4d12-9089-c4391607fded", "name":
        "Quantitative Marketing and Economics", "alternate_names": ["Quant Mark Econ"],
        "issn": "1570-7156", "url": "http://www.springer.com/west/home/statistics?SGWID=4-10128-70-35615696-0",
        "alternate_urls": ["https://link.springer.com/journal/11129"]}, "url": "https://www.semanticscholar.org/paper/e597faf0a561de858600ccead4e44ff782a37c3d",
        "title": "Correction to: Counterfactual inference for consumer choice across
        many product categories", "abstract": null, "venue": "Quantitative Marketing
        and Economics", "year": 2021, "referenceCount": 0, "citationCount": 0, "influentialCitationCount":
        0, "isOpenAccess": true, "openAccessPdf": {"url": "https://link.springer.com/content/pdf/10.1007/s11129-021-09245-y.pdf",
        "status": null}, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category": "Business",
        "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": "2021-12-01",
        "journal": {"volume": "19", "pages": "409 - 409", "name": "Quantitative Marketing
        and Economics"}, "authors": [{"authorId": "145171749", "name": "Rob Donnelly"},
        {"authorId": "2055969088", "name": "Francisco J. R. Ruiz"}, {"authorId": "1796335",
        "name": "D. Blei"}, {"authorId": "2631417", "name": "S. Athey"}]}, {"paperId":
        "fdb74161249e7d787c9b641afcacd248af71c48d", "externalIds": {"DBLP": "journals/corr/abs-2110-10804",
        "CorpusId": 239050066}, "corpusId": 239050066, "publicationVenue": {"id":
        "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names":
        ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/fdb74161249e7d787c9b641afcacd248af71c48d",
        "title": "Identifiable Variational Autoencoders via Sparse Decoding", "abstract":
        "We develop the Sparse VAE, a deep generative model for unsupervised representation
        learning on high-dimensional data. Given a dataset of observations, the Sparse
        VAE learns a set of latent factors that captures its distribution. The model
        is sparse in the sense that each feature of the dataset (i.e., each dimension)
        depends on a small subset of the latent factors. As examples, in ratings data
        each movie is only described by a few genres; in text data each word is only
        applicable to a few topics; in genomics, each gene is active in only a few
        biological processes. We first show that the Sparse VAE is identifiable: given
        data drawn from the model, there exists a uniquely optimal set of factors.
        (In contrast, most VAE-based models are not identifiable.) The key assumption
        behind Sparse-VAE identifiability is the existence of \u201canchor features\u201d,
        where for each factor there exists a feature that depends only on that factor.
        Importantly, the anchor features do not need to be known in advance. We then
        show how to fit the Sparse VAE with variational EM. Finally, we empirically
        study the Sparse VAE with both simulated and real data. We find that it recovers
        meaningful latent factors and has smaller heldout reconstruction error than
        related methods.", "venue": "arXiv.org", "year": 2021, "referenceCount": 39,
        "citationCount": 16, "influentialCitationCount": 2, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
        "publicationDate": null, "journal": {"volume": "abs/2110.10804", "name": "ArXiv"},
        "authors": [{"authorId": "3433248", "name": "Gemma E. Moran"}, {"authorId":
        "153485411", "name": "Dhanya Sridhar"}, {"authorId": "2108734693", "name":
        "Yixin Wang"}, {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId": "33b7237bd3e9477e2432ce7210e45cfb5866e9f5",
        "externalIds": {"DBLP": "conf/amia/HripcsakBBSZ20", "CorpusId": 233195372},
        "corpusId": 233195372, "publicationVenue": {"id": "c9e16b3c-2fdf-4b4c-82bf-5cdf3d3435b7",
        "name": "American Medical Informatics Association Annual Symposium", "type":
        "conference", "alternate_names": ["Conference of American Medical Informatics
        Association", "Am Med Informatics Assoc Annu Symp", "Conf Am Med Informatics
        Assoc", "AMIA"], "url": "https://knowledge.amia.org/"}, "url": "https://www.semanticscholar.org/paper/33b7237bd3e9477e2432ce7210e45cfb5866e9f5",
        "title": "Causal Inference from Observational Healthcare Data: Implications,
        Impacts and Innovations", "abstract": null, "venue": "American Medical Informatics
        Association Annual Symposium", "year": 2020, "referenceCount": 0, "citationCount":
        0, "influentialCitationCount": 0, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
        "Computer Science", "source": "external"}], "publicationTypes": ["JournalArticle"],
        "publicationDate": null, "journal": null, "authors": [{"authorId": "1686114",
        "name": "G. Hripcsak"}, {"authorId": "1796335", "name": "D. Blei"}, {"authorId":
        "2778721", "name": "E. Bareinboim"}, {"authorId": "2904133", "name": "M. Schuemie"},
        {"authorId": "3414608", "name": "Linying Zhang"}]}, {"paperId": "44071a76a00a37caea959068800f5be1b0192f1c",
        "externalIds": {"DBLP": "journals/corr/abs-2011-12379", "ArXiv": "2011.12379",
        "MAG": "3109339436", "CorpusId": 227162635}, "corpusId": 227162635, "publicationVenue":
        {"id": "f9af8000-42f8-410d-a622-e8811e41660a", "name": "Conference on Uncertainty
        in Artificial Intelligence", "type": "conference", "alternate_names": ["Uncertainty
        in Artificial Intelligence", "UAI", "Conf Uncertain Artif Intell", "Uncertain
        Artif Intell"], "url": "http://www.auai.org/"}, "url": "https://www.semanticscholar.org/paper/44071a76a00a37caea959068800f5be1b0192f1c",
        "title": "Invariant Representation Learning for Treatment Effect Estimation",
        "abstract": "The defining challenge for causal inference from observational
        data is the presence of `confounders'', covariates that affect both treatment
        assignment and the outcome. To address this challenge, practitioners collect
        and adjust for the covariates, hoping that they adequately correct for confounding.
        However, including every observed covariate in the adjustment runs the risk
        of including `bad controls'', variables that \\emph{induce} bias when they
        are conditioned on. The problem is that we do not always know which variables
        in the covariate set are safe to adjust for and which are not. To address
        this problem, we develop Nearly Invariant Causal Estimation (NICE). NICE uses
        invariant risk minimization (IRM) [Arj19] to learn a representation of the
        covariates that, under some assumptions, strips out bad controls but preserves
        sufficient information to adjust for confounding. Adjusting for the learned
        representation, rather than the covariates themselves, avoids the induced
        bias and provides valid causal inferences. NICE is appropriate in the following
        setting. i) We observe data from multiple environments that share a common
        causal mechanism for the outcome, but that differ in other ways. ii) In each
        environment, the collected covariates are a superset of the causal parents
        of the outcome, and contain sufficient information for causal identification.
        iii) But the covariates also may contain bad controls, and it is unknown which
        covariates are safe to adjust for and which ones induce bias. We evaluate
        NICE on both synthetic and semi-synthetic data. When the covariates contain
        unknown collider variables and other bad controls, NICE performs better than
        existing methods that adjust for all the covariates.", "venue": "Conference
        on Uncertainty in Artificial Intelligence", "year": 2020, "referenceCount":
        50, "citationCount": 20, "influentialCitationCount": 2, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
        "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
        {"category": "Mathematics", "source": "external"}, {"category": "Economics",
        "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"],
        "publicationDate": "2020-11-24", "journal": {"volume": "abs/2011.12379", "name":
        "ArXiv"}, "authors": [{"authorId": "133797256", "name": "Claudia Shi"}, {"authorId":
        "2974320", "name": "Victor Veitch"}, {"authorId": "1796335", "name": "D. Blei"}]},
        {"paperId": "4438483026e4f34b3b6349554543447991f4eef1", "externalIds": {"DBLP":
        "conf/amia/ZhangWOCBH20", "CorpusId": 233198913}, "corpusId": 233198913, "publicationVenue":
        {"id": "c9e16b3c-2fdf-4b4c-82bf-5cdf3d3435b7", "name": "American Medical Informatics
        Association Annual Symposium", "type": "conference", "alternate_names": ["Conference
        of American Medical Informatics Association", "Am Med Informatics Assoc Annu
        Symp", "Conf Am Med Informatics Assoc", "AMIA"], "url": "https://knowledge.amia.org/"},
        "url": "https://www.semanticscholar.org/paper/4438483026e4f34b3b6349554543447991f4eef1",
        "title": "The Multi-Outcome Medical Deconfounder: Assessing Treatment Effect
        on Multiple Renal Measures", "abstract": null, "venue": "American Medical
        Informatics Association Annual Symposium", "year": 2020, "referenceCount":
        0, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Medicine",
        "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
        null, "journal": null, "authors": [{"authorId": "3414608", "name": "Linying
        Zhang"}, {"authorId": "2108734693", "name": "Yixin Wang"}, {"authorId": "82276238",
        "name": "A. Ostropolets"}, {"authorId": "5434619", "name": "Ruijun Chen"},
        {"authorId": "1796335", "name": "D. Blei"}, {"authorId": "1686114", "name":
        "G. Hripcsak"}]}, {"paperId": "464c366ed20aa4c3fa57d71332dceb460586ed40",
        "externalIds": {"DBLP": "conf/nips/NaessethLB20", "MAG": "3099082121", "ArXiv":
        "2003.10374", "CorpusId": 214612147}, "corpusId": 214612147, "publicationVenue":
        {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd", "name": "Neural Information
        Processing Systems", "type": "conference", "alternate_names": ["Neural Inf
        Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"}, "url": "https://www.semanticscholar.org/paper/464c366ed20aa4c3fa57d71332dceb460586ed40",
        "title": "Markovian Score Climbing: Variational Inference with KL(p||q)",
        "abstract": "Modern variational inference (VI) uses stochastic gradients to
        avoid intractable expectations, enabling large-scale probabilistic inference
        in complex models. VI posits a family of approximating distributions $q$ and
        then finds the member of that family that is closest to the exact posterior
        $p$. Traditionally, VI algorithms minimize the \"exclusive KL\" KL$(q\\|p)$,
        often for computational convenience. Recent research, however, has also focused
        on the \"inclusive KL\" KL$(p\\|q)$, which has good statistical properties
        that makes it more appropriate for certain inference problems. This paper
        develops a simple algorithm for reliably minimizing the inclusive KL. Consider
        a valid MCMC method, a Markov chain whose stationary distribution is $p$.
        The algorithm we develop iteratively samples the chain $z[k]$, and then uses
        those samples to follow the score function of the variational approximation,
        $\\nabla \\log q(z[k])$ with a Robbins-Monro step-size schedule. This method,
        which we call Markovian score climbing (MSC), converges to a local optimum
        of the inclusive KL. It does not suffer from the systematic errors inherent
        in existing methods, such as Reweighted Wake-Sleep and Neural Adaptive Sequential
        Monte Carlo, which lead to bias in their final estimates. In a variant that
        ties the variational approximation directly to the Markov chain, MSC further
        provides a new algorithm that melds VI and MCMC. We illustrate convergence
        on a toy model and demonstrate the utility of MSC on Bayesian probit regression
        for classification as well as a stochastic volatility model for financial
        data.", "venue": "Neural Information Processing Systems", "year": 2020, "referenceCount":
        66, "citationCount": 30, "influentialCitationCount": 4, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Mathematics", "Computer Science"],
        "s2FieldsOfStudy": [{"category": "Mathematics", "source": "external"}, {"category":
        "Computer Science", "source": "external"}, {"category": "Computer Science",
        "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
        "2020-03-23", "journal": {"volume": "abs/2003.10374", "name": "ArXiv"}, "authors":
        [{"authorId": "2328322", "name": "C. A. Naesseth"}, {"authorId": "1759780",
        "name": "F. Lindsten"}, {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId":
        "49f9d8505e8d27041cc57cd3d06c2741f21120c1", "externalIds": {"ArXiv": "2005.04232",
        "DBLP": "conf/acl/VafaNB20", "MAG": "3021355601", "ACL": "2020.acl-main.475",
        "DOI": "10.18653/v1/2020.acl-main.475", "CorpusId": 218581891}, "corpusId":
        218581891, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
        "name": "Annual Meeting of the Association for Computational Linguistics",
        "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc
        Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"},
        "url": "https://www.semanticscholar.org/paper/49f9d8505e8d27041cc57cd3d06c2741f21120c1",
        "title": "Text-Based Ideal Points", "abstract": "Ideal point models analyze
        lawmakers\u2019 votes to quantify their political positions, or ideal points.
        But votes are not the only way to express a political position. Lawmakers
        also give speeches, release press statements, and post tweets. In this paper,
        we introduce the text-based ideal point model (TBIP), an unsupervised probabilistic
        topic model that analyzes texts to quantify the political positions of its
        authors. We demonstrate the TBIP with two types of politicized text data:
        U.S. Senate speeches and senator tweets. Though the model does not analyze
        their votes or political affiliations, the TBIP separates lawmakers by party,
        learns interpretable politicized topics, and infers ideal points close to
        the classical vote-based ideal points. One benefit of analyzing texts, as
        opposed to votes, is that the TBIP can estimate ideal points of anyone who
        authors political texts, including non-voting actors. To this end, we use
        it to study tweets from the 2020 Democratic presidential candidates. Using
        only the texts of their tweets, it identifies them along an interpretable
        progressive-to-moderate spectrum.", "venue": "Annual Meeting of the Association
        for Computational Linguistics", "year": 2020, "referenceCount": 63, "citationCount":
        16, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf":
        {"url": "https://www.aclweb.org/anthology/2020.acl-main.475.pdf", "status":
        null}, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics",
        "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-08",
        "journal": {"volume": "abs/2005.04232", "name": "ArXiv"}, "authors": [{"authorId":
        "70025184", "name": "Keyon Vafa"}, {"authorId": "47824203", "name": "S. Naidu"},
        {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId": "552facf726bd0ee7c4a54632d2ce36c2cf67efe1",
        "externalIds": {"MAG": "3106984542", "DOI": "10.1111/insr.12430", "CorpusId":
        229412748, "PubMed": "35356801"}, "corpusId": 229412748, "publicationVenue":
        null, "url": "https://www.semanticscholar.org/paper/552facf726bd0ee7c4a54632d2ce36c2cf67efe1",
        "title": "Double Empirical Bayes Testing", "abstract": "Analysing data from
        large\u2010scale, multiexperiment studies requires scientists to both analyse
        each experiment and to assess the results as a whole. In this article, we
        develop double empirical Bayes testing (DEBT), an empirical Bayes method for
        analysing multiexperiment studies when many covariates are gathered per experiment.
        DEBT is a two\u2010stage method: in the first stage, it reports which experiments
        yielded significant outcomes and in the second stage, it hypothesises which
        covariates drive the experimental significance. In both of its stages, DEBT
        builds on the work of Efron, who laid out an elegant empirical Bayes approach
        to testing. DEBT enhances this framework by learning a series of black box
        predictive models to boost power and control the false discovery rate. In
        Stage 1, it uses a deep neural network prior to report which experiments yielded
        significant outcomes. In Stage 2, it uses an empirical Bayes version of the
        knockoff filter to select covariates that have significant predictive power
        of Stage 1 significance. In both simulated and real data, DEBT increases the
        proportion of discovered significant outcomes and selects more features when
        signals are weak. In a real study of cancer cell lines, DEBT selects a robust
        set of biologically plausible genomic drivers of drug sensitivity and resistance
        in cancer.", "venue": "International statistical review = Revue internationale
        de statistique", "year": 2020, "referenceCount": 39, "citationCount": 2, "influentialCitationCount":
        0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Medicine",
        "Computer Science"], "s2FieldsOfStudy": [{"category": "Medicine", "source":
        "external"}, {"category": "Computer Science", "source": "external"}, {"category":
        "Business", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
        "publicationDate": "2020-11-25", "journal": {"volume": "88", "pages": "S113
        - S91", "name": "International Statistical Review"}, "authors": [{"authorId":
        "145305182", "name": "Wesley Tansey"}, {"authorId": "2108734693", "name":
        "Yixin Wang"}, {"authorId": "40620158", "name": "R. Rabad\u00e1n"}, {"authorId":
        "1796335", "name": "D. Blei"}]}, {"paperId": "6cdddd82ace8c0970bca736565c9607c45a13874",
        "externalIds": {"CorpusId": 237258551}, "corpusId": 237258551, "publicationVenue":
        null, "url": "https://www.semanticscholar.org/paper/6cdddd82ace8c0970bca736565c9607c45a13874",
        "title": "Posterio Collapse and Latent Variable Non-identi\ufb01ability",
        "abstract": "Variational autoencoders model high-dimensional data by positing
        low-dimensional latent variables that are mapped through a \ufb02exible distribution
        parametrized by a neural network. Unfortunately, variational autoencoders
        often su\ufb00er from posterior collapse: the posterior of the latent variables
        is equal to its prior, rendering the variational autoencoder useless as a
        means to produce meaningful representations. Existing approaches to posterior
        collapse often attribute it to the use of neural networks or optimization
        issues due to variational approximation. In this paper, we show that posterior
        collapse is a problem of latent variable non-identi\ufb01ability. We prove
        that the posterior collapses if and only if the latent variables are non-identi\ufb01able
        in the generative model. This fact implies that posterior collapse is not
        a phenomenon speci\ufb01c to the use of \ufb02exible distributions or approximate
        inference. Rather, it can occur in classical probabilistic models even with
        exact inference, which we also demon-strate. Based on these results, we propose
        a class of identi\ufb01able variational autoencoders, deep generative models
        which enforce identi\ufb01ability without sacri\ufb01cing \ufb02exibility.
        This model class resolves the problem of latent variable non-identi\ufb01ability
        by leveraging bijective Bre-nier maps and parameterizing them with input convex
        neural networks, without special variational inference objectives or optimization
        tricks. Across synthetic and real datasets, identi\ufb01able variational autoencoders
        outperform existing methods in mitigating posterior collapse and providing
        meaningful representations of the data.", "venue": "", "year": 2020, "referenceCount":
        65, "citationCount": 11, "influentialCitationCount": 1, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category":
        "Computer Science", "source": "s2-fos-model"}], "publicationTypes": null,
        "publicationDate": null, "journal": null, "authors": [{"authorId": "2108734693",
        "name": "Yixin Wang"}, {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId":
        "85a988216a1c6590fc762050eb24ff69cf732a6e", "externalIds": {"ArXiv": "2003.04948",
        "MAG": "3011069949", "DBLP": "journals/corr/abs-2003-04948", "CorpusId": 212657752},
        "corpusId": 212657752, "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
        "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url":
        "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/85a988216a1c6590fc762050eb24ff69cf732a6e",
        "title": "Towards Clarifying the Theory of the Deconfounder", "abstract":
        "Wang and Blei (2019) studies multiple causal inference and proposes the deconfounder
        algorithm. The paper discusses theoretical requirements and presents empirical
        studies. Several refinements have been suggested around the theory of the
        deconfounder. Among these, Imai and Jiang clarified the assumption of \"no
        unobserved single-cause confounders.\" Using their assumption, this paper
        clarifies the theory. Furthermore, Ogburn et al. (2020) proposes counterexamples
        to the theory. But the proposed counterexamples do not satisfy the required
        assumptions.", "venue": "arXiv.org", "year": 2020, "referenceCount": 19, "citationCount":
        19, "influentialCitationCount": 2, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics",
        "source": "external"}, {"category": "Economics", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle"], "publicationDate": "2020-03-10", "journal":
        {"volume": "abs/2003.04948", "name": "ArXiv"}, "authors": [{"authorId": "2108734693",
        "name": "Yixin Wang"}, {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId":
        "a75c9e46d5d5d84fb5b089186af123a20e22d8e6", "externalIds": {"MAG": "3096255238",
        "DOI": "10.2139/ssrn.3696179", "CorpusId": 229013161}, "corpusId": 229013161,
        "publicationVenue": {"id": "75d7a8c1-d871-42db-a8e4-7cf5146fdb62", "name":
        "Social Science Research Network", "type": "journal", "alternate_names": ["SSRN,
        Social Science Research Network (SSRN) home page", "SSRN Electronic Journal",
        "Soc Sci Res Netw", "SSRN", "SSRN Home Page", "SSRN Electron J", "Social Science
        Electronic Publishing presents Social Science Research Network"], "issn":
        "1556-5068", "url": "http://www.ssrn.com/", "alternate_urls": ["www.ssrn.com/",
        "https://fatcat.wiki/container/tol7woxlqjeg5bmzadeg6qrg3e", "https://www.wikidata.org/wiki/Q53949192",
        "www.ssrn.com/en", "http://www.ssrn.com/en/", "http://umlib.nl/ssrn", "umlib.nl/ssrn"]},
        "url": "https://www.semanticscholar.org/paper/a75c9e46d5d5d84fb5b089186af123a20e22d8e6",
        "title": "A Digital Field Experiment Reveals Large Effects of Friend-to-Friend
        Texting on Voter Turnout", "abstract": "Two decades of field experiments on
        get-out-the-vote tactics suggest that impersonal tactics, like mass emails,
        have only a modest or negligible effect on voter turnout, while more personal
        tactics, like door-to-door canvassing, are more effective. However, the COVID-19
        pandemic threatens to upend the vast face-to-face voter mobilization efforts
        that have figured prominently in recent presidential election campaigns. If
        campaigns can no longer send canvassers to voters'' doors, what tactics can
        they turn to in order to mobilize their supporters? This paper evaluates a
        promising alternative to face-to-face get-out-the-vote tactics: mobile app
        technology that enables millions of people to message their friends to urge
        them to vote. Prior to the most recent US midterm elections in 2018, the mobile
        app Outvote randomized an aspect of their system, hoping to unobtrusively
        assess the causal effect of their users'' messages on voter turnout. We develop
        a statistical methodology to address the challenges of such data, and then
        analyze the Outvote study. Our analysis reveals evidence of very large and
        statistically significant treatment effects from friend-to-friend mobilization
        efforts ($\\widehat{\\textrm{CACE}}$= 8.3, $\\textrm{CI}$ = (1.2, 15.3)).
        Further, the statistical methodology can be used to study other friend-to-friend
        messaging efforts. These results suggest that friend-to-friend texting, which
        is a personal voter mobilization effort that does not require face-to-face
        contact, is an effective alternative to conventional voter mobilization tactics.",
        "venue": "Social Science Research Network", "year": 2020, "referenceCount":
        0, "citationCount": 4, "influentialCitationCount": 0, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Political Science"], "s2FieldsOfStudy":
        [{"category": "Political Science", "source": "external"}, {"category": "Economics",
        "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": "2020-09-21",
        "journal": {"name": "SSRN Electronic Journal"}, "authors": [{"authorId": "50545056",
        "name": "Aaron Schein"}, {"authorId": "70025184", "name": "Keyon Vafa"}, {"authorId":
        "153485411", "name": "Dhanya Sridhar"}, {"authorId": "2974320", "name": "Victor
        Veitch"}, {"authorId": "2050597", "name": "Jeffrey M. Quinn"}, {"authorId":
        "2037183204", "name": "James Moffet"}, {"authorId": "1796335", "name": "D.
        Blei"}, {"authorId": "145931139", "name": "D. Green"}]}, {"paperId": "ca94b305307b8df8997fc14ffaf90fa96623cc1e",
        "externalIds": {"DBLP": "conf/recsys/WangLCB20", "MAG": "3089238887", "DOI":
        "10.1145/3383313.3412225", "CorpusId": 221785145}, "corpusId": 221785145,
        "publicationVenue": {"id": "61275a16-1e0d-479f-ac4e-f295310761f0", "name":
        "ACM Conference on Recommender Systems", "type": "conference", "alternate_names":
        ["Conf Recomm Syst", "RecSys", "ACM Conf Recomm Syst", "Conference on Recommender
        Systems"], "url": "http://recsys.acm.org/"}, "url": "https://www.semanticscholar.org/paper/ca94b305307b8df8997fc14ffaf90fa96623cc1e",
        "title": "Causal Inference for Recommender Systems", "abstract": "The task
        of recommender systems is classically framed as a prediction of users\u2019
        preferences and users\u2019 ratings. However, its spirit is to answer a counterfactual
        question: \u201cWhat would the rating be if we \u2018forced\u2019 the user
        to watch the movie?\u201d This is a question about an intervention, that is
        a causal inference question. The key challenge of this causal inference is
        unobserved confounders, variables that affect both which items the users decide
        to interact with and how they rate them. To this end, we develop an algorithm
        that leverages classical recommendation models for causal recommendation.
        Across simulated and real datasets, we demonstrate that the proposed algorithm
        is more robust to unobserved confounders and improves recommendation.", "venue":
        "ACM Conference on Recommender Systems", "year": 2020, "referenceCount": 27,
        "citationCount": 88, "influentialCitationCount": 13, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["Book", "JournalArticle"],
        "publicationDate": "2020-09-22", "journal": {"name": "Proceedings of the 14th
        ACM Conference on Recommender Systems"}, "authors": [{"authorId": "2108734693",
        "name": "Yixin Wang"}, {"authorId": "1702877", "name": "Dawen Liang"}, {"authorId":
        "1778839", "name": "Laurent Charlin"}, {"authorId": "1796335", "name": "D.
        Blei"}]}, {"paperId": "e9ca8923abf66839bbf212262ab371f34b809380", "externalIds":
        {"DBLP": "journals/jmlr/RicatteGT20", "MAG": "3021965653", "CorpusId": 216619630},
        "corpusId": 216619630, "publicationVenue": {"id": "c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
        "name": "Journal of machine learning research", "type": "journal", "alternate_names":
        ["Journal of Machine Learning Research", "J mach learn res", "J Mach Learn
        Res"], "issn": "1532-4435", "alternate_issns": ["1533-7928"], "url": "http://www.ai.mit.edu/projects/jmlr/",
        "alternate_urls": ["http://jmlr.csail.mit.edu/", "http://www.jmlr.org/", "http://portal.acm.org/affiliated/jmlr"]},
        "url": "https://www.semanticscholar.org/paper/e9ca8923abf66839bbf212262ab371f34b809380",
        "title": "Skill Rating for Multiplayer Games. Introducing Hypernode Graphs
        and their Spectral Theory", "abstract": "We consider the skill rating problem
        for multiplayer games, that is how to infer player skills from game outcomes
        in multiplayer games. We formulate the problem as a minimization problem arg
        min s s T \u2206s where \u2206 is a positive semidefinite matrix and s a real-valued
        function, of which some entries are the skill values to be inferred and other
        entries are constrained by the game outcomes. We leverage graph-based semi-supervised
        learning (SSL) algorithms for this problem. We apply our algorithms on several
        data sets of multiplayer games and obtain very promising results compared
        to Elo Duelling (see Elo, 1978) and TrueSkill (see Herbrich et al., 2006).
        As we leverage graph-based SSL algorithms and because games can be seen as
        relations between sets of players, we then generalize the approach. For this
        aim, we introduce a new finite model, called hypernode graph, defined to be
        a set of weighted binary relations between sets of nodes. We define Laplacians
        of hy-pernode graphs. Then, we show that the skill rating problem for multiplayer
        games can be formulated as arg min s s T \u2206s where \u2206 is the Laplacian
        of a hypernode graph constructed from a set of games. From a fundamental perspective,
        we show that hypernode graph Laplacians are symmetric positive semidefinite
        matrices with constant functions in their null space. We show that problems
        on hypernode graphs can not be solved with graph constructions and graph kernels.
        We relate hypernode graphs to signed graphs showing that positive relations
        between groups can lead to negative relations between individuals.", "venue":
        "Journal of machine learning research", "year": 2020, "referenceCount": 17,
        "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
        "publicationDate": null, "journal": {"volume": "21", "pages": "48:1-48:18",
        "name": "J. Mach. Learn. Res."}, "authors": [{"authorId": "3150998", "name":
        "Thomas Ricatte"}, {"authorId": "1741371", "name": "R\u00e9mi Gilleron"},
        {"authorId": "144640325", "name": "M. Tommasi"}, {"authorId": "1796335", "name":
        "D. Blei"}]}, {"paperId": "ee56f900dcfea827590a4fa9d50df4db03c7c551", "externalIds":
        {"MAG": "3010682719", "DBLP": "journals/corr/abs-2003-05554", "ArXiv": "2003.05554",
        "CorpusId": 212675438}, "corpusId": 212675438, "publicationVenue": {"id":
        "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names":
        ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/ee56f900dcfea827590a4fa9d50df4db03c7c551",
        "title": "General linear-time inference for Gaussian Processes on one dimension",
        "abstract": "Gaussian Processes (GPs) provide a powerful probabilistic framework
        for interpolation, forecasting, and smoothing, but have been hampered by computational
        scaling issues. Here we prove that for data sampled on one dimension (e.g.,
        a time series sampled at arbitrarily-spaced intervals), approximate GP inference
        at any desired level of accuracy requires computational effort that scales
        linearly with the number of observations; this new theorem enables inference
        on much larger datasets than was previously feasible. To achieve this improved
        scaling we propose a new family of stationary covariance kernels: the Latent
        Exponentially Generated (LEG) family, which admits a convenient stable state-space
        representation that allows linear-time inference. We prove that any continuous
        integrable stationary kernel can be approximated arbitrarily well by some
        member of the LEG family. The proof draws connections to Spectral Mixture
        Kernels, providing new insight about the flexibility of this popular family
        of kernels. We propose parallelized algorithms for performing inference and
        learning in the LEG model, test the algorithm on real and synthetic data,
        and demonstrate scaling to datasets with billions of samples.", "venue": "arXiv.org",
        "year": 2020, "referenceCount": 70, "citationCount": 14, "influentialCitationCount":
        0, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science",
        "source": "external"}, {"category": "Mathematics", "source": "external"},
        {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
        ["JournalArticle"], "publicationDate": "2020-03-11", "journal": {"volume":
        "abs/2003.05554", "name": "ArXiv"}, "authors": [{"authorId": "145640248",
        "name": "Jackson Loper"}, {"authorId": "1796335", "name": "D. Blei"}, {"authorId":
        "2575774", "name": "J. Cunningham"}, {"authorId": "1763504", "name": "L. Paninski"}]},
        {"paperId": "09428af106c378616d7767a37c4f4070a2664e5a", "externalIds": {"MAG":
        "3089984751", "DBLP": "conf/uai/VeitchSB20", "CorpusId": 220793347}, "corpusId":
        220793347, "publicationVenue": {"id": "f9af8000-42f8-410d-a622-e8811e41660a",
        "name": "Conference on Uncertainty in Artificial Intelligence", "type": "conference",
        "alternate_names": ["Uncertainty in Artificial Intelligence", "UAI", "Conf
        Uncertain Artif Intell", "Uncertain Artif Intell"], "url": "http://www.auai.org/"},
        "url": "https://www.semanticscholar.org/paper/09428af106c378616d7767a37c4f4070a2664e5a",
        "title": "Adapting Text Embeddings for Causal Inference", "abstract": "Does
        adding a theorem to a paper affect its chance of acceptance? Does labeling
        a post with the author''s gender affect the post popularity? This paper develops
        a method to estimate such causal effects from observational text data, adjusting
        for confounding features of the text such as the subject or writing quality.
        We assume that the text suffices for causal adjustment but that, in practice,
        it is prohibitively high-dimensional. To address this challenge, we develop
        causally sufficient embeddings, low-dimensional document representations that
        preserve sufficient information for causal identification and allow for efficient
        estimation of causal effects. Causally sufficient embeddings combine two ideas.
        The first is supervised dimensionality reduction: causal adjustment requires
        only the aspects of text that are predictive of both the treatment and outcome.
        The second is efficient language modeling: representations of text are designed
        to dispose of linguistically irrelevant information, and this information
        is also causally irrelevant. Our method adapts language models (specifically,
        word embeddings and topic models) to learn document embeddings that are able
        to predict both treatment and outcome. We study causally sufficient embeddings
        with semi-synthetic datasets and find that they improve causal estimation
        over related embedding methods. We illustrate the methods by answering the
        two motivating questions---the effect of a theorem on paper acceptance and
        the effect of a gender label on post popularity. Code and data available at
        this https URL}{this http URL", "venue": "Conference on Uncertainty in Artificial
        Intelligence", "year": 2019, "referenceCount": 29, "citationCount": 56, "influentialCitationCount":
        10, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
        "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-05-29",
        "journal": {"pages": "919-928"}, "authors": [{"authorId": "2974320", "name":
        "Victor Veitch"}, {"authorId": "153485411", "name": "Dhanya Sridhar"}, {"authorId":
        "1796335", "name": "D. Blei"}]}, {"paperId": "0a538c7e1f943a30f048eec0a3b0dfff25062b1a",
        "externalIds": {"DBLP": "journals/corr/abs-1905-10859", "ArXiv": "1905.10859",
        "MAG": "2970209593", "CorpusId": 166228083}, "corpusId": 166228083, "publicationVenue":
        {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd", "name": "Neural Information
        Processing Systems", "type": "conference", "alternate_names": ["Neural Inf
        Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"}, "url": "https://www.semanticscholar.org/paper/0a538c7e1f943a30f048eec0a3b0dfff25062b1a",
        "title": "Variational Bayes under Model Misspecification", "abstract": "Variational
        Bayes (VB) is a scalable alternative to Markov chain Monte Carlo (MCMC) for
        Bayesian posterior inference. Though popular, VB comes with few theoretical
        guarantees, most of which focus on well-specified models. However, models
        are rarely well-specified in practice. In this work, we study VB under model
        misspecification. We prove the VB posterior is asymptotically normal and centers
        at the value that minimizes the Kullback-Leibler (KL) divergence to the true
        data-generating distribution. Moreover, the VB posterior mean centers at the
        same value and is also asymptotically normal. These results generalize the
        variational Bernstein--von Mises theorem [29] to misspecified models. As a
        consequence of these results, we find that the model misspecification error
        dominates the variational approximation error in VB posterior predictive distributions.
        It explains the widely observed phenomenon that VB achieves comparable predictive
        accuracy with MCMC even though VB uses an approximating family. As illustrations,
        we study VB under three forms of model misspecification, ranging from model
        over-/under-dispersion to latent dimensionality misspecification. We conduct
        two simulation studies that demonstrate the theoretical results.", "venue":
        "Neural Information Processing Systems", "year": 2019, "referenceCount": 45,
        "citationCount": 36, "influentialCitationCount": 2, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
        "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
        {"category": "Mathematics", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
        "publicationDate": "2019-05-26", "journal": {"volume": "abs/1905.10859", "name":
        "ArXiv"}, "authors": [{"authorId": "2108734693", "name": "Yixin Wang"}, {"authorId":
        "1796335", "name": "D. Blei"}]}, {"paperId": "0ee3535732c7fe44ae43e1a6d77ce1ff770bf208",
        "externalIds": {"DBLP": "journals/corr/abs-1910-12991", "ArXiv": "1910.12991",
        "MAG": "2982658424", "CorpusId": 202782141}, "corpusId": 202782141, "publicationVenue":
        {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd", "name": "Neural Information
        Processing Systems", "type": "conference", "alternate_names": ["Neural Inf
        Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"}, "url": "https://www.semanticscholar.org/paper/0ee3535732c7fe44ae43e1a6d77ce1ff770bf208",
        "title": "Poisson-Randomized Gamma Dynamical Systems", "abstract": "This paper
        presents the Poisson-randomized gamma dynamical system (PRGDS), a model for
        sequentially observed count tensors that encodes a strong inductive bias toward
        sparsity and burstiness. The PRGDS is based on a new motif in Bayesian latent
        variable modeling, an alternating chain of discrete Poisson and continuous
        gamma latent states that is analytically convenient and computationally tractable.
        This motif yields closed-form complete conditionals for all variables by way
        of the Bessel distribution and a novel discrete distribution that we call
        the shifted confluent hypergeometric distribution. We draw connections to
        closely related models and compare the PRGDS to these models in studies of
        real-world count data sets of text, international events, and neural spike
        trains. We find that a sparse variant of the PRGDS, which allows the continuous
        gamma latent states to take values of exactly zero, often obtains better predictive
        performance than other models and is uniquely capable of inferring latent
        structures that are highly localized in time.", "venue": "Neural Information
        Processing Systems", "year": 2019, "referenceCount": 65, "citationCount":
        15, "influentialCitationCount": 1, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": ["Mathematics", "Computer Science"], "s2FieldsOfStudy":
        [{"category": "Mathematics", "source": "external"}, {"category": "Computer
        Science", "source": "external"}, {"category": "Computer Science", "source":
        "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
        "2019-10-28", "journal": {"volume": "abs/1910.12991", "name": "ArXiv"}, "authors":
        [{"authorId": "50545056", "name": "Aaron Schein"}, {"authorId": "2342841",
        "name": "Scott W. Linderman"}, {"authorId": "38026572", "name": "Mingyuan
        Zhou"}, {"authorId": "1796335", "name": "D. Blei"}, {"authorId": "1831395",
        "name": "Hanna M. Wallach"}]}, {"paperId": "10633897910bec410e04406004f14aad90cecbb8",
        "externalIds": {"DBLP": "conf/mlhc/ZhangWOMBH19", "MAG": "2990197068", "CorpusId":
        201070827}, "corpusId": 201070827, "publicationVenue": {"id": "6171bcff-8306-41c7-af12-fa1d87117cf1",
        "name": "Machine Learning in Health Care", "type": "conference", "alternate_names":
        ["MLHC", "Mach Learn Health Care"], "url": "http://mucmd.org"}, "url": "https://www.semanticscholar.org/paper/10633897910bec410e04406004f14aad90cecbb8",
        "title": "The Medical Deconfounder: Assessing Treatment Effects with Electronic
        Health Records", "abstract": "The treatment effects of medications play a
        key role in guiding medical prescriptions. They are usually assessed with
        randomized controlled trials (RCTs), which are expensive. Recently, large-scale
        electronic health records (EHRs) have become available, opening up new opportunities
        for more cost-effective assessments. However, assessing a treatment effect
        from EHRs is challenging: it is biased by unobserved confounders, unmeasured
        variables that affect both patients'' medical prescription and their outcome,
        e.g. the patients'' social economic status. To adjust for unobserved confounders,
        we develop the medical deconfounder, a machine learning algorithm that unbiasedly
        estimates treatment effects from EHRs. The medical deconfounder first constructs
        a substitute confounder by modeling which medications were prescribed to each
        patient; this substitute confounder is guaranteed to capture all multi-medication
        confounders, observed or unobserved (arXiv:1805.06826). It then uses this
        substitute confounder to adjust for the confounding bias in the analysis.
        We validate the medical deconfounder on two simulated and two real medical
        data sets. Compared to classical approaches, the medical deconfounder produces
        closer-to-truth treatment effect estimates; it also identifies effective medications
        that are more consistent with the findings in the medical literature.", "venue":
        "Machine Learning in Health Care", "year": 2019, "referenceCount": 69, "citationCount":
        17, "influentialCitationCount": 3, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": ["Medicine", "Computer Science"], "s2FieldsOfStudy":
        [{"category": "Medicine", "source": "external"}, {"category": "Computer Science",
        "source": "external"}, {"category": "Medicine", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle"], "publicationDate": "2019-10-28", "journal":
        {"pages": "490-512"}, "authors": [{"authorId": "3414608", "name": "Linying
        Zhang"}, {"authorId": "2108734693", "name": "Yixin Wang"}, {"authorId": "82276238",
        "name": "A. Ostropolets"}, {"authorId": "89075246", "name": "J. J. Mulgrave"},
        {"authorId": "1796335", "name": "D. Blei"}, {"authorId": "1686114", "name":
        "G. Hripcsak"}]}, {"paperId": "298b72096b8a770b0cdb263dd53cf2463b8a1a1d",
        "externalIds": {"MAG": "2947442676", "DBLP": "journals/corr/abs-1905-12741",
        "ArXiv": "1905.12741", "CorpusId": 170079051}, "corpusId": 170079051, "publicationVenue":
        {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names":
        ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/298b72096b8a770b0cdb263dd53cf2463b8a1a1d",
        "title": "Using Text Embeddings for Causal Inference", "abstract": "We address
        causal inference with text documents. For example, does adding a theorem to
        a paper affect its chance of acceptance? Does reporting the gender of a forum
        post author affect the popularity of the post? We estimate these effects from
        observational data, where they may be confounded by features of the text such
        as the subject or writing quality. Although the text suffices for causal adjustment,
        it is prohibitively high-dimensional. The challenge is to find a low-dimensional
        text representation that can be used in causal inference. A key insight is
        that causal adjustment requires only the aspects of text that are predictive
        of both the treatment and outcome. Our proposed method adapts deep language
        models to learn low-dimensional embeddings from text that predict these values
        well; these embeddings suffice for causal adjustment. We establish theoretical
        properties of this method. We study it empirically on semi-simulated and real
        data on paper acceptance and forum post popularity. Code is available at this
        https URL.", "venue": "arXiv.org", "year": 2019, "referenceCount": 26, "citationCount":
        20, "influentialCitationCount": 2, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics",
        "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle"], "publicationDate": "2019-05-29", "journal":
        {"volume": "abs/1905.12741", "name": "ArXiv"}, "authors": [{"authorId": "2974320",
        "name": "Victor Veitch"}, {"authorId": "153485411", "name": "Dhanya Sridhar"},
        {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId": "47f247ef2fe0308087bc63dc8011e7b63f3d3aa6",
        "externalIds": {"ArXiv": "1906.04072", "MAG": "2949025153", "DBLP": "journals/corr/abs-1906-04072",
        "CorpusId": 182952891}, "corpusId": 182952891, "publicationVenue": {"id":
        "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names":
        ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/47f247ef2fe0308087bc63dc8011e7b63f3d3aa6",
        "title": "Bayesian Tensor Filtering: Smooth, Locally-Adaptive Factorization
        of Functional Matrices", "abstract": "We consider the problem of functional
        matrix factorization, finding low-dimensional structure in a matrix where
        every entry is a noisy function evaluated at a set of discrete points. Such
        problems arise frequently in drug discovery, where biological samples form
        the rows, candidate drugs form the columns, and entries contain the dose-response
        curve of a sample treated at different concentrations of a drug. We propose
        Bayesian Tensor Filtering (BTF), a hierarchical Bayesian model of matrices
        of functions. BTF captures the smoothness in each individual function while
        also being locally adaptive to sharp discontinuities. The BTF model is agnostic
        to the likelihood of the underlying observations, making it flexible enough
        to handle many different kinds of data. We derive efficient Gibbs samplers
        for three classes of likelihoods: (i) Gaussian, for which updates are fully
        conjugate; (ii) Binomial and related likelihoods, for which updates are conditionally
        conjugate through P{\\''o}lya--Gamma augmentation; and (iii) Black-box likelihoods,
        for which updates are non-conjugate but admit an analytic truncated elliptical
        slice sampling routine. We compare BTF against a state-of-the-art method for
        dynamic Poisson matrix factorization, showing BTF better reconstructs held
        out data in synthetic experiments. Finally, we build a dose-response model
        around BTF and show on real data from a multi-sample, multi-drug cancer study
        that BTF outperforms the current standard approach in biology. Code for BTF
        is available at https://github.com/tansey/functionalmf.", "venue": "arXiv.org",
        "year": 2019, "referenceCount": 28, "citationCount": 0, "influentialCitationCount":
        0, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Mathematics",
        "Computer Science"], "s2FieldsOfStudy": [{"category": "Mathematics", "source":
        "external"}, {"category": "Computer Science", "source": "external"}, {"category":
        "Computer Science", "source": "s2-fos-model"}, {"category": "Mathematics",
        "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
        "2019-06-10", "journal": {"volume": "abs/1906.04072", "name": "ArXiv"}, "authors":
        [{"authorId": "145305182", "name": "Wesley Tansey"}, {"authorId": "2603895",
        "name": "Christopher Tosh"}, {"authorId": "1796335", "name": "D. Blei"}]},
        {"paperId": "6a079726a8377b76b5b85a4695492035a77cd8d2", "externalIds": {"MAG":
        "2964259006", "DOI": "10.1214/19-STS710", "CorpusId": 199671168}, "corpusId":
        199671168, "publicationVenue": {"id": "e21fda92-cfa3-422e-9629-41609b92122a",
        "name": "Statistical Science", "type": "journal", "alternate_names": ["Stat
        Sci"], "issn": "0883-4237", "url": "https://www.jstor.org/journal/statscie",
        "alternate_urls": ["http://www.jstor.org/journals/08834237.html", "http://projecteuclid.org/Dienst/UI/1.0/Journal?authority=euclid.ss&issue=1015346315",
        "http://www.imstat.org/sts/", "https://projecteuclid.org/DPubS?handle=euclid.ss&service=UI&verb=Display&version=1.0"]},
        "url": "https://www.semanticscholar.org/paper/6a079726a8377b76b5b85a4695492035a77cd8d2",
        "title": "Comment: Variational Autoencoders as Empirical Bayes", "abstract":
        "We thank Professor Efron for his informative and unifying review of empirical
        Bayes. In this comment, we discuss the connection between empirical Bayes
        and the variational autoencoder (VAE), a popular statistical inference framework
        in the machine learning community. We hope this connection motivates new algorithmic
        approaches for empirical Bayesians and gives new perspectives on VAEs for
        machine learners.", "venue": "Statistical Science", "year": 2019, "referenceCount":
        13, "citationCount": 3, "influentialCitationCount": 0, "isOpenAccess": true,
        "openAccessPdf": {"url": "https://projecteuclid.org/journals/statistical-science/volume-34/issue-2/Comment-Variational-Autoencoders-as-Empirical-Bayes/10.1214/19-STS710.pdf",
        "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["Review"], "publicationDate":
        "2019-05-01", "journal": {"name": "Statistical Science"}, "authors": [{"authorId":
        "2108734693", "name": "Yixin Wang"}, {"authorId": "144360230", "name": "Andrew
        C. Miller"}, {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId": "8160f14cb60f8decfaf18c1e0f1597f872f7779d",
        "externalIds": {"MAG": "2942538153", "DOI": "10.1101/621540", "CorpusId":
        155402644}, "corpusId": 155402644, "publicationVenue": {"id": "027ffd21-ebb0-4af8-baf5-911124292fd0",
        "name": "bioRxiv", "type": "journal", "url": "http://biorxiv.org/"}, "url":
        "https://www.semanticscholar.org/paper/8160f14cb60f8decfaf18c1e0f1597f872f7779d",
        "title": "Hierarchical recurrent state space models reveal discrete and continuous
        dynamics of neural activity in C. elegans", "abstract": "Modern recording
        techniques enable large-scale measurements of neural activity in a variety
        of model organisms. The dynamics of neural activity shed light on how organisms
        process sensory information and generate motor behavior. Here, we study these
        dynamics using optical recordings of neural activity in the nematode C. elegans.
        To understand these data, we develop state space models that decompose neural
        time-series into segments with simple, linear dynamics. We incorporate these
        models into a hierarchical framework that combines partial recordings from
        many worms to learn shared structure, while still allowing for individual
        variability. This framework reveals latent states of population neural activity,
        along with the discrete behavioral states that govern dynamics in this state
        space. We find stochastic transition patterns between discrete states and
        see that transition probabilities are determined by both current brain activity
        and sensory cues. Our methods automatically recover transition times that
        closely match manual labels of different behaviors, such as forward crawling,
        reversals, and turns. Finally, the resulting model can simulate neural data,
        faithfully capturing salient patterns of whole brain dynamics seen in real
        data.", "venue": "bioRxiv", "year": 2019, "referenceCount": 103, "citationCount":
        69, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf":
        {"url": "https://www.biorxiv.org/content/biorxiv/early/2019/04/29/621540.full.pdf",
        "status": null}, "fieldsOfStudy": ["Biology", "Computer Science"], "s2FieldsOfStudy":
        [{"category": "Biology", "source": "external"}, {"category": "Computer Science",
        "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": null, "publicationDate": "2019-04-29", "journal": {"name":
        "bioRxiv"}, "authors": [{"authorId": "2342841", "name": "Scott W. Linderman"},
        {"authorId": "33369809", "name": "Annika L. A. Nichols"}, {"authorId": "1796335",
        "name": "D. Blei"}, {"authorId": "15271521", "name": "M. Zimmer"}, {"authorId":
        "1763504", "name": "L. Paninski"}]}, {"paperId": "818d138bb6614f226a6cf73f69404ca917aebfdf",
        "externalIds": {"ArXiv": "1906.02635", "DBLP": "journals/corr/abs-1906-02635",
        "MAG": "2948288249", "DOI": "10.1007/s11129-021-09241-2", "CorpusId": 174802360},
        "corpusId": 174802360, "publicationVenue": {"id": "076f5568-3eb0-4d12-9089-c4391607fded",
        "name": "Quantitative Marketing and Economics", "alternate_names": ["Quant
        Mark Econ"], "issn": "1570-7156", "url": "http://www.springer.com/west/home/statistics?SGWID=4-10128-70-35615696-0",
        "alternate_urls": ["https://link.springer.com/journal/11129"]}, "url": "https://www.semanticscholar.org/paper/818d138bb6614f226a6cf73f69404ca917aebfdf",
        "title": "Counterfactual inference for consumer choice across many product
        categories", "abstract": null, "venue": "Quantitative Marketing and Economics",
        "year": 2019, "referenceCount": 59, "citationCount": 25, "influentialCitationCount":
        1, "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/1906.02635",
        "status": null}, "fieldsOfStudy": ["Computer Science", "Economics", "Mathematics"],
        "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
        {"category": "Economics", "source": "external"}, {"category": "Mathematics",
        "source": "external"}, {"category": "Business", "source": "s2-fos-model"},
        {"category": "Economics", "source": "s2-fos-model"}], "publicationTypes":
        ["JournalArticle"], "publicationDate": "2019-06-06", "journal": {"volume":
        "19", "pages": "369 - 407", "name": "Quantitative Marketing and Economics"},
        "authors": [{"authorId": "145171749", "name": "Rob Donnelly"}, {"authorId":
        "2055969088", "name": "Francisco J. R. Ruiz"}, {"authorId": "1796335", "name":
        "D. Blei"}, {"authorId": "2631417", "name": "S. Athey"}]}, {"paperId": "853be905f533fb347d58c463a61bc365e133c2ca",
        "externalIds": {"ArXiv": "1907.05545", "MAG": "2961010487", "DBLP": "journals/corr/abs-1907-05545",
        "CorpusId": 196470782}, "corpusId": 196470782, "publicationVenue": {"id":
        "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names":
        ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/853be905f533fb347d58c463a61bc365e133c2ca",
        "title": "The Dynamic Embedded Topic Model", "abstract": "Topic modeling analyzes
        documents to learn meaningful patterns of words. For documents collected in
        sequence, dynamic topic models capture how these patterns vary over time.
        We develop the dynamic embedded topic model (D-ETM), a generative model of
        documents that combines dynamic latent Dirichlet allocation (D-LDA) and word
        embeddings. The D-ETM models each word with a categorical distribution parameterized
        by the inner product between the word embedding and a per-time-step embedding
        representation of its assigned topic. The D-ETM learns smooth topic trajectories
        by defining a random walk prior over the embedding representations of the
        topics. We fit the D-ETM using structured amortized variational inference
        with a recurrent neural network. On three different corpora---a collection
        of United Nations debates, a set of ACL abstracts, and a dataset of Science
        Magazine articles---we found that the D-ETM outperforms D-LDA on a document
        completion task. We further found that the D-ETM learns more diverse and coherent
        topics than D-LDA while requiring significantly less time to fit.", "venue":
        "arXiv.org", "year": 2019, "referenceCount": 58, "citationCount": 64, "influentialCitationCount":
        15, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science",
        "source": "external"}, {"category": "Mathematics", "source": "external"},
        {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
        ["JournalArticle"], "publicationDate": "2019-07-12", "journal": {"volume":
        "abs/1907.05545", "name": "ArXiv"}, "authors": [{"authorId": "34205035", "name":
        "Adji B. Dieng"}, {"authorId": "144211059", "name": "Francisco J. R. Ruiz"},
        {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId": "8a514faeb037eeffbe52b20abb4e4fceb523a67e",
        "externalIds": {"DBLP": "journals/corr/abs-1910-07320", "ArXiv": "1910.07320",
        "MAG": "2981034429", "CorpusId": 204734416}, "corpusId": 204734416, "publicationVenue":
        {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names":
        ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/8a514faeb037eeffbe52b20abb4e4fceb523a67e",
        "title": "The Blessings of Multiple Causes: A Reply to Ogburn et al. (2019)",
        "abstract": "Ogburn et al. (2019, arXiv:1910.05438) discuss \"The Blessings
        of Multiple Causes\" (Wang and Blei, 2018, arXiv:1805.06826). Many of their
        remarks are interesting. But they also claim that the paper has \"foundational
        errors\" and that its \"premise is...incorrect.\" These claims are not substantiated.
        We correct the record here.", "venue": "arXiv.org", "year": 2019, "referenceCount":
        6, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Mathematics", "Computer Science"],
        "s2FieldsOfStudy": [{"category": "Mathematics", "source": "external"}, {"category":
        "Computer Science", "source": "external"}, {"category": "Philosophy", "source":
        "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
        "2019-10-15", "journal": {"volume": "abs/1910.07320", "name": "ArXiv"}, "authors":
        [{"authorId": "2108734693", "name": "Yixin Wang"}, {"authorId": "1796335",
        "name": "D. Blei"}]}, {"paperId": "9c3ab9aa3de816e37778f9514b89355b02e09fad",
        "externalIds": {"ArXiv": "1902.04114", "DBLP": "journals/corr/abs-1902-04114",
        "MAG": "2931255693", "CorpusId": 60441433}, "corpusId": 60441433, "publicationVenue":
        {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names":
        ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/9c3ab9aa3de816e37778f9514b89355b02e09fad",
        "title": "Using Embeddings to Correct for Unobserved Confounding", "abstract":
        "We consider causal inference in the presence of unobserved confounding. In
        particular, we study the case where a proxy is available for the confounder
        but the proxy has non-iid structure. As one example, the link structure of
        a social network carries information about its members. As another, the text
        of a document collection carries information about their meanings. In both
        these settings, we show how to effectively use the proxy to do causal inference.
        The main idea is to reduce the causal estimation problem to a semi-supervised
        prediction of both the treatments and outcomes. Networks and text both admit
        high-quality embedding models that can be used for this semi-supervised prediction.
        Our method yields valid inferences under suitable (weak) conditions on the
        quality of the predictive model. We validate the method with experiments on
        a semi-synthetic social network dataset. We demonstrate the method by estimating
        the causal effect of properties of computer science submissions on whether
        they are accepted at a conference.", "venue": "arXiv.org", "year": 2019, "referenceCount":
        39, "citationCount": 12, "influentialCitationCount": 1, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics",
        "Psychology"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
        "external"}, {"category": "Mathematics", "source": "external"}, {"category":
        "Psychology", "source": "external"}, {"category": "Computer Science", "source":
        "s2-fos-model"}, {"category": "Economics", "source": "s2-fos-model"}], "publicationTypes":
        ["JournalArticle"], "publicationDate": "2019-02-11", "journal": {"volume":
        "abs/1902.04114", "name": "ArXiv"}, "authors": [{"authorId": "2974320", "name":
        "Victor Veitch"}, {"authorId": "2108734693", "name": "Yixin Wang"}, {"authorId":
        "1796335", "name": "D. Blei"}]}, {"paperId": "a278c07c8bd2921e59dd862cd91a0540dd340030",
        "externalIds": {"MAG": "2970278855", "DBLP": "journals/corr/abs-1906-02120",
        "ArXiv": "1906.02120", "CorpusId": 174799552}, "corpusId": 174799552, "publicationVenue":
        {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd", "name": "Neural Information
        Processing Systems", "type": "conference", "alternate_names": ["Neural Inf
        Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"}, "url": "https://www.semanticscholar.org/paper/a278c07c8bd2921e59dd862cd91a0540dd340030",
        "title": "Adapting Neural Networks for the Estimation of Treatment Effects",
        "abstract": "This paper addresses the use of neural networks for the estimation
        of treatment effects from observational data. Generally, estimation proceeds
        in two stages. First, we fit models for the expected outcome and the probability
        of treatment (propensity score) for each unit. Second, we plug these fitted
        models into a downstream estimator of the effect. Neural networks are a natural
        choice for the models in the first step. The question we address is: how can
        we adapt the design and training of the neural networks used in the first
        step in order to improve the quality of the final estimate of the treatment
        effect? We propose two adaptations based on insights from the statistical
        literature on the estimation of treatment effects. The first is a new architecture,
        the Dragonnet, that exploits the sufficiency of the propensity score for estimation
        adjustment. The second is a regularization procedure, targeted regularization,
        that induces a bias towards models that have non-parametrically optimal asymptotic
        properties `out-of-the-box`. Studies on benchmark datasets for causal inference
        show these adaptations outperform existing methods. Code is available at this
        http URL.", "venue": "Neural Information Processing Systems", "year": 2019,
        "referenceCount": 29, "citationCount": 216, "influentialCitationCount": 59,
        "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science",
        "source": "external"}, {"category": "Mathematics", "source": "external"},
        {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Economics",
        "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
        "2019-06-05", "journal": {"pages": "2503-2513"}, "authors": [{"authorId":
        "133797256", "name": "Claudia Shi"}, {"authorId": "1796335", "name": "D. Blei"},
        {"authorId": "2974320", "name": "Victor Veitch"}]}, {"paperId": "b352c244259c17eecea8dc50a53ca1d694357e4c",
        "externalIds": {"ArXiv": "1904.02098", "MAG": "2925690300", "DBLP": "journals/corr/abs-1904-02098",
        "CorpusId": 102486928}, "corpusId": 102486928, "publicationVenue": {"id":
        "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names":
        ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/b352c244259c17eecea8dc50a53ca1d694357e4c",
        "title": "The Medical Deconfounder: Assessing Treatment Effect with Electronic
        Health Records (EHRs)", "abstract": "Causal estimation of treatment effect
        has an important role in guiding physicians'' decision process for drug prescription.
        While treatment effect is classically assessed with randomized controlled
        trials (RCTs), the availability of electronic health records (EHRs) bring
        an unprecedented opportunity for more efficient estimation. However, the presence
        of unobserved confounders makes treatment effect assessment from EHRs a challenging
        task. Confounders are the variables that affect both drug prescription and
        the patient''s outcome; examples include a patient''s gender, race, social
        economic status and comorbidities. When these confounders are unobserved,
        they bias the estimation. To adjust for unobserved confounders, we develop
        the medical deconfounder, a machine learning algorithm that unbiasedly estimates
        treatment effect from EHRs. The medical deconfounder first constructs a substitute
        confounder by modeling which drugs were prescribed to each patient; this substitute
        confounder is guaranteed to capture all multi-drug confounders, observed or
        unobserved (Wang and Blei, 2018). It then uses this substitute confounder
        to adjust for the confounding bias in the analysis. We validate the medical
        deconfounder on simulations and two medical data sets. The medical deconfounder
        produces closer-to-truth estimates in simulations and identifies effective
        medications that are more consistent with the findings reported in the medical
        literature compared to classical approaches.", "venue": "arXiv.org", "year":
        2019, "referenceCount": 73, "citationCount": 1, "influentialCitationCount":
        0, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Mathematics",
        "Computer Science"], "s2FieldsOfStudy": [{"category": "Mathematics", "source":
        "external"}, {"category": "Computer Science", "source": "external"}, {"category":
        "Medicine", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
        "publicationDate": "2019-04-03", "journal": {"volume": "abs/1904.02098", "name":
        "ArXiv"}, "authors": [{"authorId": "3414608", "name": "Linying Zhang"}, {"authorId":
        "2108734693", "name": "Yixin Wang"}, {"authorId": "82276238", "name": "A.
        Ostropolets"}, {"authorId": "89075246", "name": "J. J. Mulgrave"}, {"authorId":
        "1796335", "name": "D. Blei"}, {"authorId": "1686114", "name": "G. Hripcsak"}]},
        {"paperId": "be3d91b978c5691910270ed0de132cf8dc1be62b", "externalIds": {"DBLP":
        "journals/corr/abs-1905-10870", "MAG": "2945415297", "ArXiv": "1905.10870",
        "CorpusId": 166227908}, "corpusId": 166227908, "publicationVenue": {"id":
        "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names":
        ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/be3d91b978c5691910270ed0de132cf8dc1be62b",
        "title": "Equal Opportunity and Affirmative Action via Counterfactual Predictions",
        "abstract": "Machine learning (ML) can automate decision-making by learning
        to predict decisions from historical data. However, these predictors may inherit
        discriminatory policies from past decisions and reproduce unfair decisions.
        In this paper, we propose two algorithms that adjust fitted ML predictors
        to make them fair. We focus on two legal notions of fairness: (a) providing
        equal opportunity (EO) to individuals regardless of sensitive attributes and
        (b) repairing historical disadvantages through affirmative action (AA). More
        technically, we produce fair EO and AA predictors by positing a causal model
        and considering counterfactual decisions. We prove that the resulting predictors
        are theoretically optimal in predictive performance while satisfying fairness.
        We evaluate the algorithms, and the trade-offs between accuracy and fairness,
        on datasets about admissions, income, credit and recidivism.", "venue": "arXiv.org",
        "year": 2019, "referenceCount": 4, "citationCount": 17, "influentialCitationCount":
        1, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Mathematics",
        "Computer Science"], "s2FieldsOfStudy": [{"category": "Mathematics", "source":
        "external"}, {"category": "Computer Science", "source": "external"}, {"category":
        "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
        "publicationDate": "2019-05-26", "journal": {"volume": "abs/1905.10870", "name":
        "ArXiv"}, "authors": [{"authorId": "2108734693", "name": "Yixin Wang"}, {"authorId":
        "153485411", "name": "Dhanya Sridhar"}, {"authorId": "1796335", "name": "D.
        Blei"}]}, {"paperId": "c4b38500c9cc0a09c3265e34d8ca652017d201df", "externalIds":
        {"MAG": "2980180779", "CorpusId": 209969739}, "corpusId": 209969739, "publicationVenue":
        null, "url": "https://www.semanticscholar.org/paper/c4b38500c9cc0a09c3265e34d8ca652017d201df",
        "title": "Relational Dose-Response Modeling for Cancer Drug Studies.", "abstract":
        "Exploratory cancer drug studies test multiple tumor cell lines against multiple
        candidate drugs. The goal in each paired (cell line, drug) experiment is to
        map out the dose-response curve of the cell line as the dose level of the
        drug increases. The level of natural variation and technical noise in these
        experiments is high, even when multiple replicates are run. Further, running
        all possible combinations of cell lines and drugs may be prohibitively expensive,
        leading to missing data. Thus, estimating the dose-response curve is a denoising
        and imputation task. We cast this task as a functional matrix factorization
        problem: finding low-dimensional structure in a matrix where every entry is
        a noisy function evaluated at a set of discrete points. We propose Bayesian
        Tensor Filtering (BTF), a hierarchical Bayesian model of matrices of functions.
        BTF captures the smoothness in each individual function while also being locally
        adaptive to sharp discontinuities. The BTF model can incorporate many types
        of likelihoods, making it flexible enough to handle a wide variety of data.
        We derive efficient Gibbs samplers for three classes of likelihoods: (i) Gaussian,
        for which updates are fully conjugate; (ii) binomial and related likelihoods,
        for which updates are conditionally conjugate through Polya-Gamma augmentation;
        and (iii) non-conjugate likelihoods, for which we develop an analytic truncated
        elliptical slice sampling routine. We compare BTF against a state-of-the-art
        method for dynamic Poisson matrix factorization, showing BTF better reconstructs
        held out data in synthetic experiments. Finally, we build a dose-response
        model around BTF and apply it to real data from two multi-sample, multi-drug
        cancer studies. We show that the BTF-based dose-response model outperforms
        the current standard approach in biology. Code is available at this https
        URL.", "venue": "", "year": 2019, "referenceCount": 25, "citationCount": 2,
        "influentialCitationCount": 1, "isOpenAccess": false, "openAccessPdf": null,
        "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category": "Computer Science",
        "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": "2019-06-10",
        "journal": {"volume": "", "name": "arXiv: Machine Learning"}, "authors": [{"authorId":
        "145305182", "name": "Wesley Tansey"}, {"authorId": "2603895", "name": "Christopher
        Tosh"}, {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId": "d5f47453a6d00ede2881dfb65fc7ea141a50deeb",
        "externalIds": {"DBLP": "journals/tacl/DiengRB20", "ArXiv": "1907.04907",
        "MAG": "3045464143", "DOI": "10.1162/tacl_a_00325", "CorpusId": 195886143},
        "corpusId": 195886143, "publicationVenue": {"id": "e0dbf116-86aa-418d-859f-a49952d7e44a",
        "name": "Transactions of the Association for Computational Linguistics", "type":
        "journal", "alternate_names": ["Trans Assoc Comput Linguistics", "TACL"],
        "issn": "2307-387X", "url": "https://www.mitpressjournals.org/loi/tacl", "alternate_urls":
        ["http://www.transacl.org/"]}, "url": "https://www.semanticscholar.org/paper/d5f47453a6d00ede2881dfb65fc7ea141a50deeb",
        "title": "Topic Modeling in Embedding Spaces", "abstract": "Abstract Topic
        modeling analyzes documents to learn meaningful patterns of words. However,
        existing topic models fail to learn interpretable topics when working with
        large and heavy-tailed vocabularies. To this end, we develop the embedded
        topic model (etm), a generative model of documents that marries traditional
        topic models with word embeddings. More specifically, the etm models each
        word with a categorical distribution whose natural parameter is the inner
        product between the word\u2019s embedding and an embedding of its assigned
        topic. To fit the etm, we develop an efficient amortized variational inference
        algorithm. The etm discovers interpretable topics even with large vocabularies
        that include rare words and stop words. It outperforms existing document models,
        such as latent Dirichlet allocation, in terms of both topic quality and predictive
        performance.", "venue": "Transactions of the Association for Computational
        Linguistics", "year": 2019, "referenceCount": 58, "citationCount": 345, "influentialCitationCount":
        71, "isOpenAccess": true, "openAccessPdf": {"url": "https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00325/1923074/tacl_a_00325.pdf",
        "status": null}, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics",
        "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle"], "publicationDate": "2019-07-08", "journal":
        {"volume": "8", "pages": "439-453", "name": "Transactions of the Association
        for Computational Linguistics"}, "authors": [{"authorId": "34205035", "name":
        "Adji B. Dieng"}, {"authorId": "144211059", "name": "Francisco J. R. Ruiz"},
        {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId": "e1a868d2782fa8756433661e21771517f7960e5c",
        "externalIds": {"DOI": "10.1017/9781108644181.005", "CorpusId": 243702296},
        "corpusId": 243702296, "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/e1a868d2782fa8756433661e21771517f7960e5c",
        "title": "Model-based Classification", "abstract": "\u2022 A probability model
        is a joint distribution of a set of observations. \u2022 Often, a model is
        indexed by a parameter. Each value of the parameter gives a different distribution
        of the data. \u2013 The parameter of a Bernoulli is the probability of heads.
        \u2013 The parameters of a Gaussian are its mean and variance. \u2022 Many
        models (but not all) assume the data are independent and identically distributed.
        N n=1 p (x i | \u03c0). (1) Each term is a Bernoulli, p (x n | \u03c0) = \u03c0
        1(x n =h) (1 \u2212 \u03c0) 1(x n =t) (2) \u2022 Suppose we flip a coin N
        times and record the outcomes. \u2022 Further suppose that we think that the
        probability of heads is \u03c0. (This is distinct from whatever the probability
        of heads \" really \" is.)", "venue": "Model-Based Clustering and Classification
        for Data Science", "year": 2019, "referenceCount": 395, "citationCount": 2,
        "influentialCitationCount": 0, "isOpenAccess": false, "openAccessPdf": null,
        "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category": "Mathematics", "source":
        "s2-fos-model"}], "publicationTypes": null, "publicationDate": "2019-07-31",
        "journal": {"name": "Model-Based Clustering and Classification for Data Science"},
        "authors": [{"authorId": "1796335", "name": "D. Blei"}]}, {"paperId": "eb1b2363fa6cf44dbef9dc3342b3bc0ffac78f46",
        "externalIds": {"MAG": "2965581376", "DBLP": "journals/corr/abs-1908-00882",
        "ArXiv": "1908.00882", "CorpusId": 199405354}, "corpusId": 199405354, "publicationVenue":
        {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names":
        ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/eb1b2363fa6cf44dbef9dc3342b3bc0ffac78f46",
        "title": "Population Predictive Checks", "abstract": "Bayesian modeling has
        become a staple for researchers analyzing data. Thanks to recent developments
        in approximate posterior inference, modern researchers can easily build, use,
        and revise complicated Bayesian models for large and rich data. These new
        abilities, however, bring into focus the problem of model assessment. Researchers
        need tools to diagnose the fitness of their models, to understand where a
        model falls short, and to guide its revision. In this paper we develop a new
        method for Bayesian model checking, the population predictive check (Pop-PC).
        Pop-PCs are built on posterior predictive checks (PPC), a seminal method that
        checks a model by assessing the posterior predictive distribution on the observed
        data. Though powerful, PPCs use the data twice---both to calculate the posterior
        predictive and to evaluate it---which can lead to overconfident assessments.
        Pop-PCs, in contrast, compare the posterior predictive distribution to the
        population distribution of the data. This strategy blends Bayesian modeling
        with frequentist assessment, leading to a robust check that validates the
        model on its generalization. Of course the population distribution is not
        usually available; thus we use tools like the bootstrap and cross validation
        to estimate the Pop-PC. Further, we extend Pop-PCs to hierarchical models.
        We study Pop-PCs on classical regression and a hierarchical model of text.
        We show that Pop-PCs are robust to overfitting and can be easily deployed
        on a broad family of models.", "venue": "arXiv.org", "year": 2019, "referenceCount":
        65, "citationCount": 8, "influentialCitationCount": 1, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Mathematics", "Computer Science"],
        "s2FieldsOfStudy": [{"category": "Mathematics", "source": "external"}, {"category":
        "Computer Science", "source": "external"}, {"category": "Computer Science",
        "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
        "2019-08-02", "journal": {"volume": "abs/1908.00882", "name": "ArXiv"}, "authors":
        [{"authorId": "2615814", "name": "R. Ranganath"}, {"authorId": "1796335",
        "name": "D. Blei"}]}, {"paperId": "f432e55bfb342435da59e5c3f896778f436117f2",
        "externalIds": {"DBLP": "conf/nips/VeitchWB19", "MAG": "2970738418", "CorpusId":
        173187820}, "corpusId": 173187820, "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
        "name": "Neural Information Processing Systems", "type": "conference", "alternate_names":
        ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
        "url": "https://www.semanticscholar.org/paper/f432e55bfb342435da59e5c3f896778f436117f2",
        "title": "Using Embeddings to Correct for Unobserved Confounding in Networks",
        "abstract": "We consider causal inference in the presence of unobserved confounding.
        We study the case where a proxy is available for the unobserved confounding
        in the form of a network connecting the units. For example, the link structure
        of a social network carries information about its members. We show how to
        effectively use the proxy to do causal inference. The main idea is to reduce
        the causal estimation problem to a semi-supervised prediction of both the
        treatments and outcomes. Networks admit high-quality embedding models that
        can be used for this semi-supervised prediction. We show that the method yields
        valid inferences under suitable (weak) conditions on the quality of the predictive
        model. We validate the method with experiments on a semi-synthetic social
        network dataset. Code is available at this http URL.", "venue": "Neural Information
        Processing Systems", "year": 2019, "referenceCount": 52, "citationCount":
        40, "influentialCitationCount": 6, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
        "Computer Science", "source": "external"}, {"category": "Computer Science",
        "source": "s2-fos-model"}, {"category": "Economics", "source": "s2-fos-model"},
        {"category": "Mathematics", "source": "s2-fos-model"}], "publicationTypes":
        ["JournalArticle"], "publicationDate": "2019-02-11", "journal": {"pages":
        "13769-13779"}, "authors": [{"authorId": "2974320", "name": "Victor Veitch"},
        {"authorId": "2108734693", "name": "Yixin Wang"}, {"authorId": "1796335",
        "name": "D. Blei"}]}, {"paperId": "f84ca302bd7ff93fd706c9937676535887a4aef6",
        "externalIds": {"ArXiv": "1905.12793", "DBLP": "journals/corr/abs-1905-12793",
        "MAG": "2947614616", "CorpusId": 170079248}, "corpusId": 170079248, "publicationVenue":
        {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names":
        ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/f84ca302bd7ff93fd706c9937676535887a4aef6",
        "title": "Multiple Causes: A Causal Graphical View", "abstract": "Unobserved
        confounding is a major hurdle for causal inference from observational data.
        Confounders---the variables that affect both the causes and the outcome---induce
        spurious non-causal correlations between the two. Wang & Blei (2018) lower
        this hurdle with \"the blessings of multiple causes,\" where the correlation
        structure of multiple causes provides indirect evidence for unobserved confounding.
        They leverage these blessings with an algorithm, called the deconfounder,
        that uses probabilistic factor models to correct for the confounders. In this
        paper, we take a causal graphical view of the deconfounder. In a graph that
        encodes shared confounding, we show how the multiplicity of causes can help
        identify intervention distributions. We then justify the deconfounder, showing
        that it makes valid inferences of the intervention. Finally, we expand the
        class of graphs, and its theory, to those that include other confounders and
        selection variables. Our results expand the theory in Wang & Blei (2018),
        justify the deconfounder for causal graphs, and extend the settings where
        it can be used.", "venue": "arXiv.org", "year": 2019, "referenceCount": 24,
        "citationCount": 10, "influentialCitationCount": 1, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
        "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
        {"category": "Mathematics", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
        "publicationDate": "2019-05-30", "journal": {"volume": "abs/1905.12793", "name":
        "ArXiv"}, "authors": [{"authorId": "2108734693", "name": "Yixin Wang"}, {"authorId":
        "1796335", "name": "D. Blei"}]}, {"paperId": "fae3d474c4d7745be06458df0c20bf837a6055ef",
        "externalIds": {"MAG": "2979776030", "DBLP": "journals/corr/abs-1910-04302",
        "ArXiv": "1910.04302", "CorpusId": 204008419}, "corpusId": 204008419, "publicationVenue":
        {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names":
        ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/fae3d474c4d7745be06458df0c20bf837a6055ef",
        "title": "Prescribed Generative Adversarial Networks", "abstract": "Generative
        adversarial networks (GANs) are a powerful approach to unsupervised learning.
        They have achieved state-of-the-art performance in the image domain. However,
        GANs are limited in two ways. They often learn distributions with low support---a
        phenomenon known as mode collapse---and they do not guarantee the existence
        of a probability density, which makes evaluating generalization using predictive
        log-likelihood impossible. In this paper, we develop the prescribed GAN (PresGAN)
        to address these shortcomings. PresGANs add noise to the output of a density
        network and optimize an entropy-regularized adversarial loss. The added noise
        renders tractable approximations of the predictive log-likelihood and stabilizes
        the training procedure. The entropy regularizer encourages PresGANs to capture
        all the modes of the data distribution. Fitting PresGANs involves computing
        the intractable gradients of the entropy regularization term; PresGANs sidestep
        this intractability using unbiased stochastic estimates. We evaluate PresGANs
        on several datasets and found they mitigate mode collapse and generate samples
        with high perceptual quality. We further found that PresGANs reduce the gap
        in performance in terms of predictive log-likelihood between traditional GANs
        and variational autoencoders (VAEs).", "venue": "arXiv.org", "year": 2019,
        "referenceCount": 70, "citationCount": 57, "influentialCitationCount": 5,
        "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science",
        "source": "external"}, {"category": "Mathematics", "source": "external"},
        {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
        ["JournalArticle"], "publicationDate": "2019-10-09", "journal": {"volume":
        "abs/1910.04302", "name": "ArXiv"}, "authors": [{"authorId": "34205035", "name":
        "Adji B. Dieng"}, {"authorId": "144211059", "name": "Francisco J. R. Ruiz"},
        {"authorId": "1796335", "name": "D. Blei"}, {"authorId": "1722732", "name":
        "Michalis K. Titsias"}]}, {"paperId": "faf9108b8698f34fe80abb5c996b4394611bbc53",
        "externalIds": {"MAG": "2996859260", "DOI": "10.1080/01621459.2019.1690841",
        "CorpusId": 213292128}, "corpusId": 213292128, "publicationVenue": {"id":
        "8c91bff0-940d-46f7-86d4-582c09ed787c", "name": "Journal of the American Statistical
        Association", "type": "journal", "alternate_names": ["J Am Stat Assoc"], "issn":
        "0162-1459", "url": "http://openurl.ingenta.com/content?genre=journal&issn=0162-1459",
        "alternate_urls": ["https://www.jstor.org/journal/jamerstatasso", "http://www.tandfonline.com/loi/uasa20#.VHzNXU10ymQ",
        "http://amstat.tandfonline.com/loi/uasa20", "http://www.jstor.org/journals/01621459.html",
        "http://www.tandfonline.com/toc/uasa20/current"]}, "url": "https://www.semanticscholar.org/paper/faf9108b8698f34fe80abb5c996b4394611bbc53",
        "title": "The Blessings of Multiple Causes: Rejoinder", "abstract": "We thank
        all the discussants for taking the time and energy to build on this work;
        and we thank the editors for putting together an engaging and thought-provoking
        collection of discussions. After reading these contributions, we were struck
        that these are not mere discussions\u2014indeed, each is an article in itself.
        This collection pushes forward \u201cThe Blessings of Multiple Causes\u201d
        in significant ways, offering new theory, new criticism, and new application.
        After highlighting some of the themes of these articles, we will turn to each
        individually. \u201cThe Blessings of Multiple Causes\u201d provide assumptions,
        theory, and algorithms for multiple causal inference. The deconfounder algorithm
        involves modeling the causes, using the model to infer a substitute confounder,
        and then using the substitute confounder in a downstream causal inference.
        The deconfounder is not a black-box solution to causal inference. Rather,
        it is a way to use careful domain-specific modeling in the service of causal
        inference. Causal inference with the deconfounder involves a number of assumptions
        and trade-offs, and many of the discussants highlighted these. Among them
        are the following. (1) There can be no unobserved single-cause confounders.
        (2) When we apply the deconfounder, we trade an increase in estimation variance
        for a reduction in confounding bias; there is no free lunch. (3) We do not
        recommend using the deconfounder with causally dependent causes, such as a
        time series; finding a substitute confounder may be too difficult in these
        scenarios. There are many directions for further research, and the discussants
        have pointed out several of the most important ones. We need a more complete
        picture of identification; D\u2019Amour (2019) and the discussions here make
        good progress (see Table1). We need to understand the finite-sample properties
        of the deconfounder, and how to estimate uncertainty about causal inferences
        when using a substitute multi-cause confounder. We need rigorous methods of
        model criticism for assessing the validity of the substitute confounder. Deconfounder-like
        methods have already been used for genome-wide association studies (e.g.,
        Pritchard et al. 2000) and estimating peer effects in networks (Shalizi and
        McFowland III 2016). More broadly, the deconfounder strategy points to many
        applications, including in genetics, psychology, education, and marketing,
        where factor models are routinely fit to largescale data. We hope that statisticians
        and machine learners will", "venue": "Journal of the American Statistical
        Association", "year": 2019, "referenceCount": 5, "citationCount": 9, "influentialCitationCount":
        2, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Psychology"],
        "s2FieldsOfStudy": [{"category": "Psychology", "source": "external"}, {"category":
        "Economics", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate":
        "2019-10-02", "journal": {"volume": "114", "pages": "1616 - 1619", "name":
        "Journal of the American Statistical Association"}, "authors": [{"authorId":
        "2108734693", "name": "Yixin Wang"}, {"authorId": "1796335", "name": "D. Blei"}]},
        {"paperId": "077cacdc9754c7dc2c7fa072ee57417cb7d61218", "externalIds": {"DOI":
        "10.1090/gsm/194/06", "CorpusId": 239808751}, "corpusId": 239808751, "publicationVenue":
        {"id": "ddb90fc2-9dde-4443-9bd0-3bd533691800", "name": "Graduate Studies in
        Mathematics", "alternate_names": ["Grad Stud Math"], "issn": "1065-7339"},
        "url": "https://www.semanticscholar.org/paper/077cacdc9754c7dc2c7fa072ee57417cb7d61218",
        "title": "Exponential Families", "abstract": "Surprisingly many of the distributions
        we use in statistics for random variables X taking value in some space X (often
        R or N0 but sometimes R n, Z, or some other space), indexed by a parameter
        \u03b8 from some parameter set \u0398, can be written in exponential family
        form, with pdf or pmf f(x | \u03b8) = exp [\u03b7(\u03b8)t(x) \u2212B(\u03b8)]
        h(x) for some statistic t : X \u2192 R, natural parameter \u03b7 : \u0398
        \u2192 R, and functions B : \u0398 \u2192 R and h : X \u2192 R+. The likelihood
        function for a random sample of size n from the exponential family is", "venue":
        "Graduate Studies in Mathematics", "year": 2018, "referenceCount": 0, "citationCount":
        0, "influentialCitationCount": 0, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category": "Mathematics",
        "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": "2018-11-19",
        "journal": {"name": "Graduate Studies in Mathematics"}, "authors": [{"authorId":
        "1796335", "name": "D. Blei"}]}, {"paperId": "0e2cce2843780cd03a4108829728833dbfa23632",
        "externalIds": {"ArXiv": "1808.06581", "MAG": "2886453691", "DBLP": "journals/corr/abs-1808-06581",
        "CorpusId": 52051246}, "corpusId": 52051246, "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
        "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url":
        "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/0e2cce2843780cd03a4108829728833dbfa23632",
        "title": "The Deconfounded Recommender: A Causal Inference Approach to Recommendation",
        "abstract": "The goal of a recommender system is to show its users items that
        they will like. In forming its prediction, the recommender system tries to
        answer: \"what would the rating be if we ''forced'' the user to watch the
        movie?\" This is a question about an intervention in the world, a causal question,
        and so traditional recommender systems are doing causal inference from observational
        data. This paper develops a causal inference approach to recommendation. Traditional
        recommenders are likely biased by unobserved confounders, variables that affect
        both the \"treatment assignments\" (which movies the users watch) and the
        \"outcomes\" (how they rate them). We develop the deconfounded recommender,
        a strategy to leverage classical recommendation models for causal predictions.
        The deconfounded recommender uses Poisson factorization on which movies users
        watched to infer latent confounders in the data; it then augments common recommendation
        models to correct for potential confounding bias. The deconfounded recommender
        improves recommendation and it enjoys stable performance against interventions
        on test sets.", "venue": "arXiv.org", "year": 2018, "referenceCount": 25,
        "citationCount": 65, "influentialCitationCount": 8, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
        "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
        {"category": "Mathematics", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
        "publicationDate": "2018-08-20", "journal": {"volume": "abs/1808.06581", "name":
        "ArXiv"}, "authors": [{"authorId": "2108734693", "name": "Yixin Wang"}, {"authorId":
        "1702877", "name": "Dawen Liang"}, {"authorId": "1778839", "name": "Laurent
        Charlin"}, {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId": "15397bbfbf84385a55d7f116cb9fbb68472cd66c",
        "externalIds": {"CorpusId": 55437418}, "corpusId": 55437418, "publicationVenue":
        null, "url": "https://www.semanticscholar.org/paper/15397bbfbf84385a55d7f116cb9fbb68472cd66c",
        "title": "Interpreting Mixed Membership Models : Implications of Erosheva
        \u2019 s Representation Theorem", "abstract": null, "venue": "", "year": 2018,
        "referenceCount": 29, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess":
        false, "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy": [],
        "publicationTypes": null, "publicationDate": null, "journal": null, "authors":
        [{"authorId": "2247552", "name": "E. Airoldi"}, {"authorId": "1796335", "name":
        "D. Blei"}, {"authorId": "1868490", "name": "E. Erosheva"}, {"authorId": "1684961",
        "name": "S. Fienberg"}]}, {"paperId": "188933428ec7e2776c6e83407a5a4768c4a0ef62",
        "externalIds": {"DBLP": "reference/snam/X18tr", "DOI": "10.1007/978-1-4939-7131-2_100640",
        "CorpusId": 27832720}, "corpusId": 27832720, "publicationVenue": null, "url":
        "https://www.semanticscholar.org/paper/188933428ec7e2776c6e83407a5a4768c4a0ef62",
        "title": "Matrix Factorization", "abstract": null, "venue": "Encyclopedia
        of Social Network Analysis and Mining. 2nd Ed.", "year": 2018, "referenceCount":
        0, "citationCount": 31, "influentialCitationCount": 0, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate":
        null, "journal": null, "authors": [{"authorId": "1796335", "name": "D. Blei"},
        {"authorId": "34699434", "name": "A. Ng"}, {"authorId": "1694621", "name":
        "Michael I. Jordan"}]}, {"paperId": "36399406816e1cbd6c9ebed30714f140bbe59d1d",
        "externalIds": {"MAG": "2787322705", "CorpusId": 46390283}, "corpusId": 46390283,
        "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/36399406816e1cbd6c9ebed30714f140bbe59d1d",
        "title": "Word2net: Deep Representations of Language", "abstract": "Word embeddings
        extract semantic features of words from large datasets of text. Most embedding
        methods rely on a log-bilinear model to predict the occurrence of a word in
        a context of other words. Here we propose word2net, a method that replaces
        their linear parametrization with neural networks. For each term in the vocabulary,
        word2net posits a neural network that takes the context as input and outputs
        a probability of occurrence. Further, word2net can use the hierarchical organization
        of its word networks to incorporate additional meta-data, such as syntactic
        features, into the embedding model. For example, we show how to share parameters
        across word networks to develop an embedding model that includes part-of-speech
        information. We study word2net with two datasets, a collection of Wikipedia
        articles and a corpus of U.S. Senate speeches. Quantitatively, we found that
        word2net outperforms popular embedding methods on predicting held- out words
        and that sharing parameters based on part of speech further boosts performance.
        Qualitatively, word2net learns interpretable semantic representations and,
        compared to vector-based methods, better incorporates syntactic information.",
        "venue": "", "year": 2018, "referenceCount": 27, "citationCount": 0, "influentialCitationCount":
        0, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
        "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": null, "publicationDate": "2018-02-15", "journal": {"volume":
        "", "name": ""}, "authors": [{"authorId": "144016056", "name": "Maja R. Rudolph"},
        {"authorId": "144211059", "name": "Francisco J. R. Ruiz"}, {"authorId": "1796335",
        "name": "D. Blei"}]}, {"paperId": "394929d51af693e42063f6e033add1db35096553",
        "externalIds": {"MAG": "2963963616", "DBLP": "journals/corr/abs-1801-07826",
        "ArXiv": "1801.07826", "DOI": "10.1257/PANDP.20181031", "CorpusId": 37672756},
        "corpusId": 37672756, "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
        "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url":
        "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/394929d51af693e42063f6e033add1db35096553",
        "title": "Estimating Heterogeneous Consumer Preferences for Restaurants and
        Travel Time Using Mobile Location Data", "abstract": "This paper analyzes
        consumer choices over lunchtime restaurants using data from a sample of several
        thousand anonymous mobile phone users in the San Francisco Bay Area. The data
        is used to identify users'' approximate typical morning location, as well
        as their choices of lunchtime restaurants. We build a model where restaurants
        have latent characteristics (whose distribution may depend on restaurant observables,
        such as star ratings, food category, and price range), each user has preferences
        for these latent characteristics, and these preferences are heterogeneous
        across users. Similarly, each item has latent characteristics that describe
        users'' willingness to travel to the restaurant, and each user has individual-specific
        preferences for those latent characteristics. Thus, both users'' willingness
        to travel and their base utility for each restaurant vary across user-restaurant
        pairs. We use a Bayesian approach to estimation. To make the estimation computationally
        feasible, we rely on variational inference to approximate the posterior distribution,
        as well as stochastic gradient descent as a computational approach. Our model
        performs better than more standard competing models such as multinomial logit
        and nested logit models, in part due to the personalization of the estimates.
        We analyze how consumers re-allocate their demand after a restaurant closes
        to nearby restaurants versus more distant restaurants with similar characteristics,
        and we compare our predictions to actual outcomes. Finally, we show how the
        model can be used to analyze counterfactual questions such as what type of
        restaurant would attract the most consumers in a given location.", "venue":
        "arXiv.org", "year": 2018, "referenceCount": 18, "citationCount": 57, "influentialCitationCount":
        2, "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/1801.07826",
        "status": null}, "fieldsOfStudy": ["Computer Science", "Economics", "Mathematics"],
        "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
        {"category": "Economics", "source": "external"}, {"category": "Mathematics",
        "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"},
        {"category": "Economics", "source": "s2-fos-model"}], "publicationTypes":
        ["JournalArticle"], "publicationDate": "2018-01-22", "journal": {"volume":
        "abs/1801.07826", "name": "ArXiv"}, "authors": [{"authorId": "2631417", "name":
        "S. Athey"}, {"authorId": "1796335", "name": "D. Blei"}, {"authorId": "145171749",
        "name": "Rob Donnelly"}, {"authorId": "144211059", "name": "Francisco J. R.
        Ruiz"}, {"authorId": "2070679879", "name": "Tobias Schmidt"}]}, {"paperId":
        "3c94e61000c01763e785242a199e358f7a99446a", "externalIds": {"DBLP": "journals/cacm/Blei18",
        "MAG": "2794699521", "DOI": "10.1145/3186260", "CorpusId": 4897912}, "corpusId":
        4897912, "publicationVenue": {"id": "4d9ce1c4-dc84-46b9-903e-e3751c00c7dd",
        "name": "Communications of the ACM", "type": "journal", "alternate_names":
        ["Commun ACM", "Communications of The ACM"], "issn": "0001-0782", "url": "http://www.acm.org/pubs/cacm/",
        "alternate_urls": ["http://portal.acm.org/cacm", "http://www.acm.org/pubs/contents/journals/cacm/",
        "https://cacm.acm.org/"]}, "url": "https://www.semanticscholar.org/paper/3c94e61000c01763e785242a199e358f7a99446a",
        "title": "Technical perspective: Expressive probabilistic models and scalable
        method of moments", "abstract": null, "venue": "Communications of the ACM",
        "year": 2018, "referenceCount": 0, "citationCount": 5, "influentialCitationCount":
        0, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
        "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle"], "publicationDate": "2018-03-26", "journal":
        {"volume": "61", "pages": "84 - 84", "name": "Communications of the ACM"},
        "authors": [{"authorId": "1796335", "name": "D. Blei"}]}, {"paperId": "40da199f9a377b8f041574d2ded87e085975a75b",
        "externalIds": {"MAG": "2902149380", "DBLP": "journals/corr/abs-1812-00209",
        "ArXiv": "1812.00209", "CorpusId": 54438943}, "corpusId": 54438943, "publicationVenue":
        {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names":
        ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/40da199f9a377b8f041574d2ded87e085975a75b",
        "title": "A Probabilistic Model of Cardiac Physiology and Electrocardiograms",
        "abstract": "An electrocardiogram (EKG) is a common, non-invasive test that
        measures the electrical activity of a patient''s heart. EKGs contain useful
        diagnostic information about patient health that may be absent from other
        electronic health record (EHR) data. As multi-dimensional waveforms, they
        could be modeled using generic machine learning tools, such as a linear factor
        model or a variational autoencoder. We take a different approach:~we specify
        a model that directly represents the underlying electrophysiology of the heart
        and the EKG measurement process. We apply our model to two datasets, including
        a sample of emergency department EKG reports with missing data. We show that
        our model can more accurately reconstruct missing data (measured by test reconstruction
        error) than a standard baseline when there is significant missing data. More
        broadly, this physiological representation of heart function may be useful
        in a variety of settings, including prediction, causal analysis, and discovery.",
        "venue": "arXiv.org", "year": 2018, "referenceCount": 16, "citationCount":
        0, "influentialCitationCount": 0, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": ["Mathematics", "Computer Science", "Biology"], "s2FieldsOfStudy":
        [{"category": "Mathematics", "source": "external"}, {"category": "Computer
        Science", "source": "external"}, {"category": "Biology", "source": "external"},
        {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
        ["JournalArticle"], "publicationDate": "2018-12-01", "journal": {"volume":
        "abs/1812.00209", "name": "ArXiv"}, "authors": [{"authorId": "144360230",
        "name": "Andrew C. Miller"}, {"authorId": "3797258", "name": "Z. Obermeyer"},
        {"authorId": "1796335", "name": "D. Blei"}, {"authorId": "2575774", "name":
        "J. Cunningham"}, {"authorId": "2062143", "name": "S. Mullainathan"}]}, {"paperId":
        "417acb6b197d8f81368e67b63a9e280288f30599", "externalIds": {"MAG": "2788036042",
        "DBLP": "conf/www/RudolphB18", "DOI": "10.1145/3178876.3185999", "CorpusId":
        4892663}, "corpusId": 4892663, "publicationVenue": {"id": "e07422f9-c065-40c3-a37b-75e98dce79fe",
        "name": "The Web Conference", "type": "conference", "alternate_names": ["Web
        Conf", "WWW"], "url": "http://www.iw3c2.org/"}, "url": "https://www.semanticscholar.org/paper/417acb6b197d8f81368e67b63a9e280288f30599",
        "title": "Dynamic Embeddings for Language Evolution", "abstract": "Word embeddings
        are a powerful approach for unsupervised analysis of language. Recently, Rudolph
        et al. developed exponential family embeddings, which cast word embeddings
        in a probabilistic framework. Here, we develop dynamic embeddings, building
        on exponential family embeddings to capture how the meanings of words change
        over time. We use dynamic embeddings to analyze three large collections of
        historical texts: the U.S. Senate speeches from 1858 to 2009, the history
        of computer science ACM abstracts from 1951 to 2014, and machine learning
        papers on the ArXiv from 2007 to 2015. We find dynamic embeddings provide
        better fits than classical embeddings and capture interesting patterns about
        how language changes.", "venue": "The Web Conference", "year": 2018, "referenceCount":
        48, "citationCount": 128, "influentialCitationCount": 12, "isOpenAccess":
        true, "openAccessPdf": {"url": "http://dl.acm.org/ft_gateway.cfm?id=3185999&type=pdf",
        "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["Book", "JournalArticle",
        "Conference"], "publicationDate": "2018-04-10", "journal": {"name": "Proceedings
        of the 2018 World Wide Web Conference"}, "authors": [{"authorId": "144016056",
        "name": "Maja R. Rudolph"}, {"authorId": "1796335", "name": "D. Blei"}]},
        {"paperId": "4ae9c97b65ef9c816f2f88e73ecf5416863a17ad", "externalIds": {"PubMedCentral":
        "5879694", "MAG": "2791572729", "DOI": "10.1073/pnas.1719792115", "CorpusId":
        3846754, "PubMed": "29531061"}, "corpusId": 3846754, "publicationVenue": {"id":
        "bb95bf2e-8383-4748-bf9d-d6906d091085", "name": "Proceedings of the National
        Academy of Sciences of the United States of America", "type": "journal", "alternate_names":
        ["PNAS", "PNAS online", "Proceedings of the National Academy of Sciences of
        the United States of America.", "Proc National Acad Sci", "Proceedings of
        the National Academy of Sciences", "Proc National Acad Sci u s Am"], "issn":
        "0027-8424", "alternate_issns": ["1091-6490"], "url": "https://www.jstor.org/journal/procnatiacadscie",
        "alternate_urls": ["http://www.pnas.org/", "https://www.pnas.org/", "http://www.jstor.org/journals/00278424.html",
        "www.pnas.org/"]}, "url": "https://www.semanticscholar.org/paper/4ae9c97b65ef9c816f2f88e73ecf5416863a17ad",
        "title": "Measuring discursive influence across scholarship", "abstract":
        "Significance Scientific and scholarly influence is multifaceted, shifts over
        time, and varies across disciplines. We present a dynamic topic model to credit
        documents with influence that shapes future discourse based on their content
        and contextual features. We trace discursive innovation in scholarship and
        identify the influence of particular articles along with their authors, affiliations,
        and journals. In collections of science, social science, and humanities research
        spanning over a century, our measure helps predict citations and reveals signals
        that recognize authors who make diverse contributions and whose contributions
        take longer to be appreciated, allowing us to compensate for bias in citation
        behavior. Assessing scholarly influence is critical for understanding the
        collective system of scholarship and the history of academic inquiry. Influence
        is multifaceted, and citations reveal only part of it. Citation counts exhibit
        preferential attachment and follow a rigid \u201cnews cycle\u201d that can
        miss sustained and indirect forms of influence. Building on dynamic topic
        models that track distributional shifts in discourse over time, we introduce
        a variant that incorporates features, such as authorship, affiliation, and
        publication venue, to assess how these contexts interact with content to shape
        future scholarship. We perform in-depth analyses on collections of physics
        research (500,000 abstracts; 102 years) and scholarship generally (JSTOR repository:
        2 million full-text articles; 130 years). Our measure of document influence
        helps predict citations and shows how outcomes, such as winning a Nobel Prize
        or affiliation with a highly ranked institution, boost influence. Analysis
        of citations alongside discursive influence reveals that citations tend to
        credit authors who persist in their fields over time and discount credit for
        works that are influential over many topics or are \u201cahead of their time.\u201d
        In this way, our measures provide a way to acknowledge diverse contributions
        that take longer and travel farther to achieve scholarly appreciation, enabling
        us to correct citation biases and enhance sensitivity to the full spectrum
        of scholarly impact.", "venue": "Proceedings of the National Academy of Sciences
        of the United States of America", "year": 2018, "referenceCount": 59, "citationCount":
        37, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf":
        {"url": "https://www.pnas.org/content/pnas/115/13/3308.full.pdf", "status":
        null}, "fieldsOfStudy": ["Medicine"], "s2FieldsOfStudy": [{"category": "Medicine",
        "source": "external"}, {"category": "Sociology", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle"], "publicationDate": "2018-03-12", "journal":
        {"volume": "115", "pages": "3308 - 3313", "name": "Proceedings of the National
        Academy of Sciences of the United States of America"}, "authors": [{"authorId":
        "2146802", "name": "Aaron Gerow"}, {"authorId": "40390550", "name": "Yuening
        Hu"}, {"authorId": "1389036863", "name": "Jordan L. Boyd-Graber"}, {"authorId":
        "1796335", "name": "D. Blei"}, {"authorId": "144002439", "name": "James A.
        Evans"}]}, {"paperId": "4e6b198578895bbfe968c92d64dd1e3ef2e977be", "externalIds":
        {"MAG": "2804985110", "DBLP": "journals/corr/abs-1805-06826", "ArXiv": "1805.06826",
        "DOI": "10.1080/01621459.2019.1686987", "CorpusId": 21694910}, "corpusId":
        21694910, "publicationVenue": {"id": "8c91bff0-940d-46f7-86d4-582c09ed787c",
        "name": "Journal of the American Statistical Association", "type": "journal",
        "alternate_names": ["J Am Stat Assoc"], "issn": "0162-1459", "url": "http://openurl.ingenta.com/content?genre=journal&issn=0162-1459",
        "alternate_urls": ["https://www.jstor.org/journal/jamerstatasso", "http://www.tandfonline.com/loi/uasa20#.VHzNXU10ymQ",
        "http://amstat.tandfonline.com/loi/uasa20", "http://www.jstor.org/journals/01621459.html",
        "http://www.tandfonline.com/toc/uasa20/current"]}, "url": "https://www.semanticscholar.org/paper/4e6b198578895bbfe968c92d64dd1e3ef2e977be",
        "title": "The Blessings of Multiple Causes", "abstract": "Abstract Causal
        inference from observational data is a vital problem, but it comes with strong
        assumptions. Most methods assume that we observe all confounders, variables
        that affect both the causal variables and the outcome variables. This assumption
        is standard but it is also untestable. In this article, we develop the deconfounder,
        a way to do causal inference with weaker assumptions than the traditional
        methods require. The deconfounder is designed for problems of multiple causal
        inference: scientific studies that involve multiple causes whose effects are
        simultaneously of interest. Specifically, the deconfounder combines unsupervised
        machine learning and predictive model checking to use the dependencies among
        multiple causes as indirect evidence for some of the unobserved confounders.
        We develop the deconfounder algorithm, prove that it is unbiased, and show
        that it requires weaker assumptions than traditional causal inference. We
        analyze its performance in three types of studies: semi-simulated data around
        smoking and lung cancer, semi-simulated data around genome-wide association
        studies, and a real dataset about actors and movie revenue. The deconfounder
        is an effective approach to estimating causal effects in problems of multiple
        causal inference. Supplementary materials for this article are available online.",
        "venue": "Journal of the American Statistical Association", "year": 2018,
        "referenceCount": 130, "citationCount": 234, "influentialCitationCount": 33,
        "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/1805.06826",
        "status": null}, "fieldsOfStudy": ["Mathematics", "Computer Science", "Psychology"],
        "s2FieldsOfStudy": [{"category": "Mathematics", "source": "external"}, {"category":
        "Computer Science", "source": "external"}, {"category": "Psychology", "source":
        "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle"], "publicationDate": "2018-05-17", "journal":
        {"volume": "114", "pages": "1574 - 1596", "name": "Journal of the American
        Statistical Association"}, "authors": [{"authorId": "2108734693", "name":
        "Yixin Wang"}, {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId": "5f4bee489f595bd3d3dda7fd88de8d79b006aa52",
        "externalIds": {"MAG": "2963987720", "ArXiv": "1807.04863", "DBLP": "journals/corr/abs-1807-04863",
        "CorpusId": 49742003}, "corpusId": 49742003, "publicationVenue": {"id": "2d136b11-c2b5-484b-b008-7f4a852fd61e",
        "name": "International Conference on Artificial Intelligence and Statistics",
        "type": "conference", "alternate_names": ["AISTATS", "Int Conf Artif Intell
        Stat"]}, "url": "https://www.semanticscholar.org/paper/5f4bee489f595bd3d3dda7fd88de8d79b006aa52",
        "title": "Avoiding Latent Variable Collapse With Generative Skip Models",
        "abstract": "Variational autoencoders learn distributions of high-dimensional
        data. They model data with a deep latent-variable model and then fit the model
        by maximizing a lower bound of the log marginal likelihood. VAEs can capture
        complex distributions, but they can also suffer from an issue known as \"latent
        variable collapse,\" especially if the likelihood model is powerful. Specifically,
        the lower bound involves an approximate posterior of the latent variables;
        this posterior \"collapses\" when it is set equal to the prior, i.e., when
        the approximate posterior is independent of the data. While VAEs learn good
        generative models, latent variable collapse prevents them from learning useful
        representations. In this paper, we propose a simple new way to avoid latent
        variable collapse by including skip connections in our generative model; these
        connections enforce strong links between the latent variables and the likelihood
        function. We study generative skip models both theoretically and empirically.
        Theoretically, we prove that skip models increase the mutual information between
        the observations and the inferred latent variables. Empirically, we study
        images (MNIST and Omniglot) and text (Yahoo). Compared to existing VAE architectures,
        we show that generative skip models maintain similar predictive performance
        but lead to less collapse and provide more meaningful representations of the
        data.", "venue": "International Conference on Artificial Intelligence and
        Statistics", "year": 2018, "referenceCount": 36, "citationCount": 154, "influentialCitationCount":
        21, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Mathematics",
        "Computer Science"], "s2FieldsOfStudy": [{"category": "Mathematics", "source":
        "external"}, {"category": "Computer Science", "source": "external"}, {"category":
        "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
        "publicationDate": "2018-07-12", "journal": {"volume": "abs/1807.04863", "name":
        "ArXiv"}, "authors": [{"authorId": "34205035", "name": "Adji B. Dieng"}, {"authorId":
        "38367242", "name": "Yoon Kim"}, {"authorId": "2531268", "name": "Alexander
        M. Rush"}, {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId": "672f28e2772b7d7895c5ce08ccd07eac3e60219e",
        "externalIds": {"DBLP": "conf/icml/DiengRAB18", "MAG": "2952556607", "ArXiv":
        "1805.01500", "CorpusId": 3276568}, "corpusId": 3276568, "publicationVenue":
        {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29", "name": "International Conference
        on Machine Learning", "type": "conference", "alternate_names": ["ICML", "Int
        Conf Mach Learn"], "url": "https://icml.cc/"}, "url": "https://www.semanticscholar.org/paper/672f28e2772b7d7895c5ce08ccd07eac3e60219e",
        "title": "Noisin: Unbiased Regularization for Recurrent Neural Networks",
        "abstract": "Recurrent neural networks (RNNs) are powerful models of sequential
        data. They have been successfully used in domains such as text and speech.
        However, RNNs are susceptible to overfitting; regularization is important.
        In this paper we develop Noisin, a new method for regularizing RNNs. Noisin
        injects random noise into the hidden states of the RNN and then maximizes
        the corresponding marginal likelihood of the data. We show how Noisin applies
        to any RNN and we study many different types of noise. Noisin is unbiased--it
        preserves the underlying RNN on average. We characterize how Noisin regularizes
        its RNN both theoretically and empirically. On language modeling benchmarks,
        Noisin improves over dropout by as much as 12.2% on the Penn Treebank and
        9.4% on the Wikitext-2 dataset. We also compared the state-of-the-art language
        model of Yang et al. 2017, both with and without Noisin. On the Penn Treebank,
        the method with Noisin more quickly reaches state-of-the-art performance.",
        "venue": "International Conference on Machine Learning", "year": 2018, "referenceCount":
        49, "citationCount": 18, "influentialCitationCount": 4, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
        "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
        {"category": "Mathematics", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
        "Conference"], "publicationDate": "2018-05-03", "journal": {"volume": "abs/1805.01500",
        "name": "ArXiv"}, "authors": [{"authorId": "34205035", "name": "Adji B. Dieng"},
        {"authorId": "2615814", "name": "R. Ranganath"}, {"authorId": "2124555", "name":
        "Jaan Altosaar"}, {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId":
        "7e99908dff899e1c744cdf928a1f1ab3ec58cdd4", "externalIds": {"MAG": "2905457872",
        "ArXiv": "1812.05691", "CorpusId": 88516674}, "corpusId": 88516674, "publicationVenue":
        null, "url": "https://www.semanticscholar.org/paper/7e99908dff899e1c744cdf928a1f1ab3ec58cdd4",
        "title": "Dose-response modeling in high-throughput cancer drug screenings:
        A case study with recommendations for practitioners", "abstract": "Personalized
        cancer treatments based on the molecular profile of a patient''s tumor are
        becoming a standard of care in oncology. Experimentalists and pharmacologists
        rely on high-throughput, \\textit{in vitro} screenings of many compounds against
        many different cell lines to build models of drug response. These models help
        them discover new potential therapeutics that may apply to broad classes of
        tumors matching some molecular pattern. We propose a hierarchical Bayesian
        model of how cancer cell lines respond to drugs in these experiments and develop
        a method for fitting the model to real-world data. Through a case study, the
        model is shown both quantitatively and qualitatively to capture nontrivial
        associations between molecular features and drug response. Finally, we draw
        five conclusions and recommendations that may benefit experimentalists, analysts,
        and clinicians working in the field of personalized medicine for cancer therapeutics.",
        "venue": "", "year": 2018, "referenceCount": 32, "citationCount": 2, "influentialCitationCount":
        0, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Mathematics"],
        "s2FieldsOfStudy": [{"category": "Mathematics", "source": "external"}, {"category":
        "Biology", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate":
        "2018-12-13", "journal": {"volume": "", "name": "arXiv: Applications"}, "authors":
        [{"authorId": "145305182", "name": "Wesley Tansey"}, {"authorId": "2158259764",
        "name": "Kathy Li"}, {"authorId": "46702624", "name": "H. Zhang"}, {"authorId":
        "2342841", "name": "Scott W. Linderman"}, {"authorId": "40620158", "name":
        "R. Rabad\u00e1n"}, {"authorId": "1796335", "name": "D. Blei"}, {"authorId":
        "50107501", "name": "C. Wiggins"}]}, {"paperId": "850e3676a1288939ad299d69a5f02ac8d596cb91",
        "externalIds": {"MAG": "2898721507", "ArXiv": "1811.00645", "CorpusId": 88523801},
        "corpusId": 88523801, "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/850e3676a1288939ad299d69a5f02ac8d596cb91",
        "title": "The Holdout Randomization Test: Principled and Easy Black Box Feature
        Selection", "abstract": "We consider the problem of feature selection using
        black box predictive models. For example, high-throughput devices in science
        are routinely used to gather thousands of features for each sample in an experiment.
        The scientist must then sift through the many candidate features to find explanatory
        signals in the data, such as which genes are associated with sensitivity to
        a prospective therapy. Often, predictive models are used for this task: the
        model is fit, error on held out data is measured, and strong performing models
        are assumed to have discovered some fundamental properties of the system.
        A model-specific heuristic is then used to inspect the model parameters and
        rank important features, with top features reported as \"discoveries.\" However,
        such heuristics provide no statistical guarantees and can produce unreliable
        results. We propose the holdout randomization test (HRT) as a principled approach
        to feature selection using black box predictive models. The HRT is model agnostic
        and produces a valid p-value for each feature, enabling control over the false
        discovery rate (or Type I error) for any predictive model. Further, the HRT
        is computationally efficient and, in simulations, has greater power than a
        competing knockoffs-based approach. Code is available at this https URL.",
        "venue": "", "year": 2018, "referenceCount": 31, "citationCount": 30, "influentialCitationCount":
        7, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science",
        "source": "external"}, {"category": "Mathematics", "source": "external"},
        {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
        null, "publicationDate": "2018-11-01", "journal": {"volume": "", "name": "arXiv:
        Methodology"}, "authors": [{"authorId": "145305182", "name": "Wesley Tansey"},
        {"authorId": "2974320", "name": "Victor Veitch"}, {"authorId": "46702624",
        "name": "H. Zhang"}, {"authorId": "40620158", "name": "R. Rabad\u00e1n"},
        {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId": "852bcc3cc41df93e079582a2ef3c40689ff8411a",
        "externalIds": {"DBLP": "journals/corr/abs-1806-03143", "MAG": "2952153566",
        "ArXiv": "1806.03143", "CorpusId": 47016186}, "corpusId": 47016186, "publicationVenue":
        {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29", "name": "International Conference
        on Machine Learning", "type": "conference", "alternate_names": ["ICML", "Int
        Conf Mach Learn"], "url": "https://icml.cc/"}, "url": "https://www.semanticscholar.org/paper/852bcc3cc41df93e079582a2ef3c40689ff8411a",
        "title": "Black Box FDR", "abstract": "Analyzing large-scale, multi-experiment
        studies requires scientists to test each experimental outcome for statistical
        significance and then assess the results as a whole. We present Black Box
        FDR (BB-FDR), an empirical-Bayes method for analyzing multi-experiment studies
        when many covariates are gathered per experiment. BB-FDR learns a series of
        black box predictive models to boost power and control the false discovery
        rate (FDR) at two stages of study analysis. In Stage 1, it uses a deep neural
        network prior to report which experiments yielded significant outcomes. In
        Stage 2, a separate black box model of each covariate is used to select features
        that have significant predictive power across all experiments. In benchmarks,
        BB-FDR outperforms competing state-of-the-art methods in both stages of analysis.
        We apply BB-FDR to two real studies on cancer drug efficacy. For both studies,
        BB-FDR increases the proportion of significant outcomes discovered and selects
        variables that reveal key genomic drivers of drug sensitivity and resistance
        in cancer.", "venue": "International Conference on Machine Learning", "year":
        2018, "referenceCount": 31, "citationCount": 15, "influentialCitationCount":
        2, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science",
        "source": "external"}, {"category": "Mathematics", "source": "external"},
        {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
        ["JournalArticle", "Conference"], "publicationDate": "2018-06-08", "journal":
        {"pages": "4874-4883"}, "authors": [{"authorId": "145305182", "name": "Wesley
        Tansey"}, {"authorId": "2108734693", "name": "Yixin Wang"}, {"authorId": "1796335",
        "name": "D. Blei"}, {"authorId": "40620158", "name": "R. Rabad\u00e1n"}]},
        {"paperId": "888ba07b575eda30a26edc69ef846c3a387f8394", "externalIds": {"MAG":
        "2796547658", "PubMedCentral": "5890980", "DOI": "10.1371/journal.pone.0195024",
        "CorpusId": 4980866, "PubMed": "29630604"}, "corpusId": 4980866, "publicationVenue":
        {"id": "0aed7a40-85f3-4c66-9e1b-c1556c57001b", "name": "PLoS ONE", "type":
        "journal", "alternate_names": ["Plo ONE", "PLOS ONE", "PLO ONE"], "issn":
        "1932-6203", "url": "https://journals.plos.org/plosone/", "alternate_urls":
        ["http://www.plosone.org/"]}, "url": "https://www.semanticscholar.org/paper/888ba07b575eda30a26edc69ef846c3a387f8394",
        "title": "Readmission prediction via deep contextual embedding of clinical
        concepts", "abstract": "Objective Hospital readmission costs a lot of money
        every year. Many hospital readmissions are avoidable, and excessive hospital
        readmissions could also be harmful to the patients. Accurate prediction of
        hospital readmission can effectively help reduce the readmission risk. However,
        the complex relationship between readmission and potential risk factors makes
        readmission prediction a difficult task. The main goal of this paper is to
        explore deep learning models to distill such complex relationships and make
        accurate predictions. Materials and methods We propose CONTENT, a deep model
        that predicts hospital readmissions via learning interpretable patient representations
        by capturing both local and global contexts from patient Electronic Health
        Records (EHR) through a hybrid Topic Recurrent Neural Network (TopicRNN) model.
        The experiment was conducted using the EHR of a real world Congestive Heart
        Failure (CHF) cohort of 5,393 patients. Results The proposed model outperforms
        state-of-the-art methods in readmission prediction (e.g. 0.6103 \u00b1 0.0130
        vs. second best 0.5998 \u00b1 0.0124 in terms of ROC-AUC). The derived patient
        representations were further utilized for patient phenotyping. The learned
        phenotypes provide more precise understanding of readmission risks. Discussion
        Embedding both local and global context in patient representation not only
        improves prediction performance, but also brings interpretable insights of
        understanding readmission risks for heterogeneous chronic clinical conditions.
        Conclusion This is the first of its kind model that integrates the power of
        both conventional deep neural network and the probabilistic generative models
        for highly interpretable deep patient representation learning. Experimental
        results and case studies demonstrate the improved performance and interpretability
        of the model.", "venue": "PLoS ONE", "year": 2018, "referenceCount": 44, "citationCount":
        103, "influentialCitationCount": 8, "isOpenAccess": true, "openAccessPdf":
        {"url": "https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0195024&type=printable",
        "status": null}, "fieldsOfStudy": ["Medicine", "Computer Science"], "s2FieldsOfStudy":
        [{"category": "Medicine", "source": "external"}, {"category": "Computer Science",
        "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"},
        {"category": "Medicine", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
        "publicationDate": "2018-04-09", "journal": {"volume": "13", "name": "PLoS
        ONE"}, "authors": [{"authorId": "145781464", "name": "Cao Xiao"}, {"authorId":
        "40411766", "name": "Tengfei Ma"}, {"authorId": "34205035", "name": "Adji
        B. Dieng"}, {"authorId": "1796335", "name": "D. Blei"}, {"authorId": "1682816",
        "name": "Fei Wang"}]}, {"paperId": "8fb7a8657e801396fd61ea2a8af21b86eeed8705",
        "externalIds": {"CorpusId": 251567788}, "corpusId": 251567788, "publicationVenue":
        null, "url": "https://www.semanticscholar.org/paper/8fb7a8657e801396fd61ea2a8af21b86eeed8705",
        "title": "Measuring discursive in\ufb02uence across scholarship", "abstract":
        "Assessing scholarly in\ufb02uence is critical for understanding the col-
        lective system of scholarship and the history of academic inquiry. In\ufb02uence
        is multifaceted, and citations reveal only part of it. Cita- tion counts exhibit
        preferential attachment and follow a rigid \u201cnews cycle\u201d that can
        miss sustained and indirect forms of in\ufb02u- ence. Building on dynamic
        topic models that track distributional shifts in discourse over time, we introduce
        a variant that incor- porates features, such as authorship, af\ufb01liation,
        and publication venue,toassesshowthesecontextsinteractwithcontenttoshapefuturescholarship.Weperformin-depthanalysesoncollectionsof",
        "venue": "", "year": 2018, "referenceCount": 38, "citationCount": 3, "influentialCitationCount":
        0, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy":
        [{"category": "Sociology", "source": "s2-fos-model"}], "publicationTypes":
        null, "publicationDate": null, "journal": null, "authors": [{"authorId": "2146802",
        "name": "Aaron Gerow"}, {"authorId": "40390550", "name": "Yuening Hu"}, {"authorId":
        "1389036863", "name": "Jordan L. Boyd-Graber"}, {"authorId": "1796335", "name":
        "D. Blei"}, {"authorId": "144002439", "name": "James A. Evans"}]}, {"paperId":
        "9db5b92337a361a68207e3560704b541c23622cf", "externalIds": {"PubMedCentral":
        "6386217", "MAG": "2950640959", "DOI": "10.15252/msb.20188557", "CorpusId":
        67858701, "PubMed": "30796088"}, "corpusId": 67858701, "publicationVenue":
        {"id": "027ffd21-ebb0-4af8-baf5-911124292fd0", "name": "bioRxiv", "type":
        "journal", "url": "http://biorxiv.org/"}, "url": "https://www.semanticscholar.org/paper/9db5b92337a361a68207e3560704b541c23622cf",
        "title": "De novo gene signature identification from single\u2010cell RNA\u2010seq
        with hierarchical Poisson factorization", "abstract": "Common approaches to
        gene signature discovery in single cell RNA-sequencing (scRNA-seq) depend
        upon predefined structures like clusters or pseudo-temporal order, require
        prior normalization, or do not account for the sparsity of single cell data.
        We present single cell Hierarchical Poisson Factorization (scHPF), a Bayesian
        factorization method that adapts Hierarchical Poisson Factorization [1] for
        de novo discovery of both continuous and discrete expression patterns from
        scRNA-seq. scHPF does not require prior normalization and captures statistical
        properties of single cell data better than other methods in benchmark datasets.
        Applied to scRNA-seq of the core and margin of a high-grade glioma, scHPF
        uncovers marked differences in the abundance of glioma subpopulations across
        tumor regions and subtle, regionally-associated expression biases within glioma
        subpopulations. scHFP revealed an expression signature that was spatially
        biased towards the glioma-infiltrated margins and associated with inferior
        survival in glioblastoma.", "venue": "bioRxiv", "year": 2018, "referenceCount":
        66, "citationCount": 71, "influentialCitationCount": 3, "isOpenAccess": true,
        "openAccessPdf": null, "fieldsOfStudy": ["Medicine", "Biology", "Computer
        Science"], "s2FieldsOfStudy": [{"category": "Medicine", "source": "external"},
        {"category": "Biology", "source": "external"}, {"category": "Computer Science",
        "source": "external"}, {"category": "Biology", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle"], "publicationDate": "2018-07-11", "journal":
        {"volume": "15", "name": "Molecular Systems Biology"}, "authors": [{"authorId":
        "40843172", "name": "H. M. Levitin"}, {"authorId": "6163384", "name": "Jinzhou
        Yuan"}, {"authorId": "40101801", "name": "Y. Cheng"}, {"authorId": "144211059",
        "name": "Francisco J. R. Ruiz"}, {"authorId": "40507077", "name": "Erin Bush"},
        {"authorId": "2300323", "name": "J. Bruce"}, {"authorId": "3271503", "name":
        "P. Canoll"}, {"authorId": "2700430", "name": "A. Iavarone"}, {"authorId":
        "2053649", "name": "A. Lasorella"}, {"authorId": "1796335", "name": "D. Blei"},
        {"authorId": "16713182", "name": "P. Sims"}]}, {"paperId": "a6b62f412e74edb5daccfbb1c46db887ad9b5ea4",
        "externalIds": {"MAG": "2811308998", "DBLP": "conf/aistats/VeitchAZBO19",
        "ArXiv": "1806.10701", "CorpusId": 49524201}, "corpusId": 49524201, "publicationVenue":
        {"id": "2d136b11-c2b5-484b-b008-7f4a852fd61e", "name": "International Conference
        on Artificial Intelligence and Statistics", "type": "conference", "alternate_names":
        ["AISTATS", "Int Conf Artif Intell Stat"]}, "url": "https://www.semanticscholar.org/paper/a6b62f412e74edb5daccfbb1c46db887ad9b5ea4",
        "title": "Empirical Risk Minimization and Stochastic Gradient Descent for
        Relational Data", "abstract": "Empirical risk minimization is the main tool
        for prediction problems, but its extension to relational data remains unsolved.
        We solve this problem using recent ideas from graph sampling theory to (i)
        define an empirical risk for relational data and (ii) obtain stochastic gradients
        for this empirical risk that are automatically unbiased. This is achieved
        by considering the method by which data is sampled from a graph as an explicit
        component of model design. By integrating fast implementations of graph sampling
        schemes with standard automatic differentiation tools, we provide an efficient
        turnkey solver for the risk minimization problem. We establish basic theoretical
        properties of the procedure. Finally, we demonstrate relational ERM with application
        to two non-standard problems: one-stage training for semi-supervised node
        classification, and learning embedding vectors for vertex attributes. Experiments
        confirm that the turnkey inference procedure is effective in practice, and
        that the sampling scheme used for model specification has a strong effect
        on model performance. Code is available at this https URL.", "venue": "International
        Conference on Artificial Intelligence and Statistics", "year": 2018, "referenceCount":
        32, "citationCount": 8, "influentialCitationCount": 0, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
        "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
        {"category": "Mathematics", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
        "publicationDate": "2018-06-27", "journal": {"volume": "abs/1806.10701", "name":
        "ArXiv"}, "authors": [{"authorId": "2974320", "name": "Victor Veitch"}, {"authorId":
        "9548350", "name": "Morgane Austern"}, {"authorId": "47862579", "name": "Wenda
        Zhou"}, {"authorId": "1796335", "name": "D. Blei"}, {"authorId": "2497523",
        "name": "Peter Orbanz"}]}, {"paperId": "ad81013e8afc5e2fd5d279468dd6c9bb504179e7",
        "externalIds": {"MAG": "2909379646", "CorpusId": 188652742}, "corpusId": 188652742,
        "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/ad81013e8afc5e2fd5d279468dd6c9bb504179e7",
        "title": "Learning with Reflective Likelihoods", "abstract": null, "venue":
        "", "year": 2018, "referenceCount": 0, "citationCount": 4, "influentialCitationCount":
        0, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
        "external"}], "publicationTypes": null, "publicationDate": "2018-09-27", "journal":
        {"volume": "", "name": ""}, "authors": [{"authorId": "34205035", "name": "Adji
        B. Dieng"}, {"authorId": "1979489", "name": "Kyunghyun Cho"}, {"authorId":
        "1796335", "name": "D. Blei"}, {"authorId": "1688882", "name": "Yann LeCun"}]},
        {"paperId": "b495ea6d5bcabdb757371a7cb4ce6bfda4df93c6", "externalIds": {"MAG":
        "2795454886", "DBLP": "journals/corr/abs-1803-09123", "ArXiv": "1803.09123",
        "CorpusId": 4303760}, "corpusId": 4303760, "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
        "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url":
        "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/b495ea6d5bcabdb757371a7cb4ce6bfda4df93c6",
        "title": "Equation Embeddings", "abstract": "We present an unsupervised approach
        for discovering semantic representations of mathematical equations. Equations
        are challenging to analyze because each is unique, or nearly unique. Our method,
        which we call equation embeddings, finds good representations of equations
        by using the representations of their surrounding words. We used equation
        embeddings to analyze four collections of scientific articles from the arXiv,
        covering four computer science domains (NLP, IR, AI, and ML) and $\\sim$98.5k
        equations. Quantitatively, we found that equation embeddings provide better
        models when compared to existing word embedding approaches. Qualitatively,
        we found that equation embeddings provide coherent semantic representations
        of equations and can capture semantic similarity to other equations and to
        words.", "venue": "arXiv.org", "year": 2018, "referenceCount": 16, "citationCount":
        23, "influentialCitationCount": 1, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": ["Mathematics", "Computer Science"], "s2FieldsOfStudy":
        [{"category": "Mathematics", "source": "external"}, {"category": "Computer
        Science", "source": "external"}, {"category": "Computer Science", "source":
        "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
        "2018-03-24", "journal": {"volume": "abs/1803.09123", "name": "ArXiv"}, "authors":
        [{"authorId": "2872916", "name": "K. Krstovski"}, {"authorId": "1796335",
        "name": "D. Blei"}]}, {"paperId": "ba84afa9279d794cada027bed70e9a343b9add6e",
        "externalIds": {"MAG": "3026075704", "DOI": "10.1093/biostatistics/kxaa047",
        "CorpusId": 218870030, "PubMed": "33417699"}, "corpusId": 218870030, "publicationVenue":
        {"id": "f04c159d-2aba-498c-aad9-72bc0ffb9b4b", "name": "Biostatistics", "type":
        "journal", "issn": "1465-4644", "url": "http://biostatistics.oxfordjournals.org/"},
        "url": "https://www.semanticscholar.org/paper/ba84afa9279d794cada027bed70e9a343b9add6e",
        "title": "Dose-response modeling in high-throughput cancer drug screenings:
        an end-to-end approach.", "abstract": "Personalized cancer treatments based
        on the molecular profile of a patient''s tumor are an emerging and exciting
        class of treatments in oncology. As genomic tumor profiling is becoming more
        common, targeted treatments for specific molecular alterations are gaining
        traction. To discover new potential therapeutics that may apply to broad classes
        of tumors matching some molecular pattern, experimentalists and pharmacologists
        rely on high-throughput, in vitro screens of many compounds against many different
        cell lines. We propose a hierarchical Bayesian model of how cancer cell lines
        respond to drugs in these experiments and develop a method for fitting the
        model to real-world high-throughput screening data. Through a case study,
        the model is shown to capture nontrivial associations between molecular features
        and drug response, such as requiring both wild type TP53 and overexpression
        of MDM2 to be sensitive to Nutlin-3(a). In quantitative benchmarks, the model
        outperforms a standard approach in biology, with $\\approx20\\%$ lower predictive
        error on held out data. When combined with a conditional randomization testing
        procedure, the model discovers markers of therapeutic response that recapitulate
        known biology and suggest new avenues for investigation. All code for the
        article is publicly available at https://github.com/tansey/deep-dose-response.",
        "venue": "Biostatistics", "year": 2018, "referenceCount": 46, "citationCount":
        8, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url":
        "https://academic.oup.com/biostatistics/advance-article-pdf/doi/10.1093/biostatistics/kxaa047/35548107/kxaa047.pdf",
        "status": null}, "fieldsOfStudy": ["Medicine", "Computer Science"], "s2FieldsOfStudy":
        [{"category": "Medicine", "source": "external"}, {"category": "Computer Science",
        "source": "external"}, {"category": "Biology", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle"], "publicationDate": "2018-12-13", "journal":
        {"name": "Biostatistics"}, "authors": [{"authorId": "145305182", "name": "Wesley
        Tansey"}, {"authorId": "2158259764", "name": "Kathy Li"}, {"authorId": "46702624",
        "name": "H. Zhang"}, {"authorId": "2342841", "name": "Scott W. Linderman"},
        {"authorId": "40620158", "name": "R. Rabad\u00e1n"}, {"authorId": "1796335",
        "name": "D. Blei"}, {"authorId": "50107501", "name": "C. Wiggins"}]}, {"paperId":
        "c4e72c764266d85ce0ade71fa812bda43bdd6437", "externalIds": {"CorpusId": 3767493},
        "corpusId": 3767493, "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/c4e72c764266d85ce0ade71fa812bda43bdd6437",
        "title": "use a Gaussian random walk to capture drift in the underlying language
        model ; for example", "abstract": "Word embeddings are a powerful approach
        for unsupervised analysis of language. Recently, Rudolph et al. [35] developed
        exponential family embeddings, which cast word embeddings in a probabilistic
        framework. Here, we develop dynamic embeddings, building on exponential family
        embeddings to capture how the meanings of words change over time. We use dynamic
        embeddings to analyze three large collections of historical texts: the U.S.
        Senate speeches from 1858 to 2009, the history of computer science ACM abstracts
        from 1951 to 2014, and machine learning papers on the ArXiv from 2007 to 2015.
        We find dynamic embeddings provide better fits than classical embeddings and
        capture interesting patterns about how language changes.", "venue": "", "year":
        2018, "referenceCount": 48, "citationCount": 0, "influentialCitationCount":
        0, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
        null, "publicationDate": null, "journal": null, "authors": [{"authorId": "144016056",
        "name": "Maja R. Rudolph"}, {"authorId": "1796335", "name": "D. Blei"}]},
        {"paperId": "d640dffe89d91367577e4e017d69a9c2bb3c8339", "externalIds": {"ArXiv":
        "1802.04220", "DBLP": "conf/icml/RuizTDB18", "MAG": "2964330911", "CorpusId":
        3603516}, "corpusId": 3603516, "publicationVenue": {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
        "name": "International Conference on Machine Learning", "type": "conference",
        "alternate_names": ["ICML", "Int Conf Mach Learn"], "url": "https://icml.cc/"},
        "url": "https://www.semanticscholar.org/paper/d640dffe89d91367577e4e017d69a9c2bb3c8339",
        "title": "Augment and Reduce: Stochastic Inference for Large Categorical Distributions",
        "abstract": "Categorical distributions are ubiquitous in machine learning,
        e.g., in classification, language models, and recommendation systems. They
        are also at the core of discrete choice models. However, when the number of
        possible outcomes is very large, using categorical distributions becomes computationally
        expensive, as the complexity scales linearly with the number of outcomes.
        To address this problem, we propose augment and reduce (A&R), a method to
        alleviate the computational complexity. A&R uses two ideas: latent variable
        augmentation and stochastic variational inference. It maximizes a lower bound
        on the marginal likelihood of the data. Unlike existing methods which are
        specific to softmax, A&R is more general and is amenable to other categorical
        models, such as multinomial probit. On several large-scale classification
        problems, we show that A&R provides a tighter bound on the marginal likelihood
        and has better predictive performance than existing approaches.", "venue":
        "International Conference on Machine Learning", "year": 2018, "referenceCount":
        57, "citationCount": 21, "influentialCitationCount": 2, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
        "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
        {"category": "Mathematics", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
        "Conference"], "publicationDate": "2018-02-12", "journal": {"volume": "abs/1802.04220",
        "name": "ArXiv"}, "authors": [{"authorId": "144211059", "name": "Francisco
        J. R. Ruiz"}, {"authorId": "1722732", "name": "Michalis K. Titsias"}, {"authorId":
        "34205035", "name": "Adji B. Dieng"}, {"authorId": "1796335", "name": "D.
        Blei"}]}, {"paperId": "ee9c54cee156bc1a71a7c2e118b3f7b7703231d5", "externalIds":
        {"CorpusId": 202691724}, "corpusId": 202691724, "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/ee9c54cee156bc1a71a7c2e118b3f7b7703231d5",
        "title": "Correlated RandomMeasures Rajesh Ranganatha", "abstract": null,
        "venue": "", "year": 2018, "referenceCount": 62, "citationCount": 0, "influentialCitationCount":
        0, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy":
        [], "publicationTypes": null, "publicationDate": null, "journal": null, "authors":
        [{"authorId": "2615814", "name": "R. Ranganath"}, {"authorId": "1796335",
        "name": "D. Blei"}]}, {"paperId": "f3ec477307e0ea7840debfc8826af5fe22990ca4",
        "externalIds": {"MAG": "2899228325", "DOI": "10.1101/457283", "CorpusId":
        53613284}, "corpusId": 53613284, "publicationVenue": {"id": "027ffd21-ebb0-4af8-baf5-911124292fd0",
        "name": "bioRxiv", "type": "journal", "url": "http://biorxiv.org/"}, "url":
        "https://www.semanticscholar.org/paper/f3ec477307e0ea7840debfc8826af5fe22990ca4",
        "title": "The Markov link method: a nonparametric approach to combine observations
        from multiple experiments", "abstract": "This paper studies measurement linkage.
        An example from cell biology helps explain the problem: imagine for a given
        cell we can either sequence the cell\u2019s RNA or we can examine its morphology,
        but not both. Given a cell\u2019s morphology, what do we expect to see in
        its RNA? Given a cell\u2019s RNA, what do we expect in its morphology? More
        broadly, given a measurement of one type, can we predict measurements of the
        other type? This measurement linkage problem arises in many scientific and
        technological fields. To solve this problem, we develop a nonparametric approach
        we dub the \u201cMarkov link method\u201d (MLM). The MLM makes a conditional
        independence assumption that holds in many multi-measurement contexts and
        provides a a way to estimate the link, the conditional probability of one
        type of measurement given the other. We derive conditions under which the
        MLM estimator is consistent and we use simulated data to show that it provides
        accurate measures of uncertainty. We evaluate the MLM on real data generated
        by a pair of single-cell RNA sequencing techniques. The MLM characterizes
        the link between them and helps connect the two notions of cell type derived
        from each technique. Further, the MLM reveals that some aspects of the link
        cannot be determined from the available data, and suggests new experiments
        that would allow for better estimates.", "venue": "bioRxiv", "year": 2018,
        "referenceCount": 30, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess":
        true, "openAccessPdf": {"url": "https://www.biorxiv.org/content/biorxiv/early/2019/01/09/457283.full.pdf",
        "status": null}, "fieldsOfStudy": ["Biology", "Computer Science"], "s2FieldsOfStudy":
        [{"category": "Biology", "source": "external"}, {"category": "Computer Science",
        "source": "external"}, {"category": "Biology", "source": "s2-fos-model"}],
        "publicationTypes": null, "publicationDate": "2018-10-30", "journal": {"name":
        "bioRxiv"}, "authors": [{"authorId": "145640248", "name": "Jackson Loper"},
        {"authorId": "39371225", "name": "Trygve E Bakken"}, {"authorId": "1581719589",
        "name": "U. Sumbul"}, {"authorId": "6070839", "name": "G. Murphy"}, {"authorId":
        "2194809", "name": "Hongkui Zeng"}, {"authorId": "1796335", "name": "D. Blei"},
        {"authorId": "1763504", "name": "L. Paninski"}]}, {"paperId": "f7725abd17be6bf40f286de54be6aeeee61e0bcf",
        "externalIds": {"MAG": "3135927585", "DBLP": "journals/jcgs/TanseyVZRB22",
        "DOI": "10.1080/10618600.2021.1923520", "CorpusId": 237678812}, "corpusId":
        237678812, "publicationVenue": {"id": "4d1db669-f64b-4a44-bd66-992fdc502ddd",
        "name": "Journal of Computational And Graphical Statistics", "type": "journal",
        "alternate_names": ["Journal of Computational and Graphical Statistics", "J
        Comput Graph Stat"], "issn": "1061-8600", "url": "http://openurl.ingenta.com/content?genre=journal&issn=1061-8600",
        "alternate_urls": ["http://www.jstor.org/journals/10618600.html", "https://www.jstor.org/journal/jcompgrapstat",
        "http://www.tandfonline.com/loi/ucgs20", "http://www.amstat.org/publications/jcgs.cfm"]},
        "url": "https://www.semanticscholar.org/paper/f7725abd17be6bf40f286de54be6aeeee61e0bcf",
        "title": "The Holdout Randomization Test for Feature Selection in Black Box
        Models", "abstract": "Abstract We propose the holdout randomization test (HRT),
        an approach to feature selection using black box predictive models. The HRT
        is a specialized version of the conditional randomization test (CRT) that
        uses data splitting for feasible computation. The HRT works with any predictive
        model and produces a valid p-value for each feature. To make the HRT more
        practical, we propose a set of extensions to maximize power and speed up computation.
        In simulations, these extensions lead to greater power than a competing knockoffs-based
        approach, without sacrificing control of the error rate. We apply the HRT
        to two case studies from the scientific literature where heuristics were originally
        used to select important features for predictive models. The results illustrate
        how such heuristics can be misleading relative to principled methods like
        the HRT. Code is available at https://github.com/tansey/hrt. Supplementary
        materials for this article are available online.", "venue": "Journal of Computational
        And Graphical Statistics", "year": 2018, "referenceCount": 42, "citationCount":
        23, "influentialCitationCount": 6, "isOpenAccess": true, "openAccessPdf":
        {"url": "https://arxiv.org/pdf/1811.00645", "status": null}, "fieldsOfStudy":
        ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science",
        "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle"], "publicationDate": "2018-11-01", "journal":
        {"volume": "31", "pages": "151 - 162", "name": "Journal of Computational and
        Graphical Statistics"}, "authors": [{"authorId": "145305182", "name": "Wesley
        Tansey"}, {"authorId": "2974320", "name": "Victor Veitch"}, {"authorId": "46702624",
        "name": "H. Zhang"}, {"authorId": "40620158", "name": "R. Rabad\u00e1n"},
        {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId": "fa5ed34d2576b4ad3ada354d6ad571f61d08482f",
        "externalIds": {"MAG": "2786043755", "CorpusId": 64524170}, "corpusId": 64524170,
        "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/fa5ed34d2576b4ad3ada354d6ad571f61d08482f",
        "title": "Noise-Based Regularizers for Recurrent Neural Networks", "abstract":
        "Recurrent neural networks (RNNs) are powerful models for sequential data.
        They can approximate arbitrary computations, and have been used successfully
        in domains such as text and speech. However, the flexibility of RNNs makes
        them susceptible to overfitting and regularization is important. We develop
        a noise-based regularization method for RNNs. The idea is simple and easy
        to implement: we inject noise in the hidden units of the RNN and then maximize
        the original RNN''s likelihood averaged over the injected noise. On a language
        modeling benchmark, our method achieves better performance than the deterministic
        RNN and the variational dropout.", "venue": "", "year": 2018, "referenceCount":
        0, "citationCount": 2, "influentialCitationCount": 0, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate":
        "2018-02-15", "journal": {"volume": "", "name": ""}, "authors": [{"authorId":
        "34205035", "name": "Adji B. Dieng"}, {"authorId": "2124555", "name": "Jaan
        Altosaar"}, {"authorId": "2615814", "name": "R. Ranganath"}, {"authorId":
        "1796335", "name": "D. Blei"}]}, {"paperId": "07e16f31d9b4c1fd4dd5f2d65d9c31a04654292f",
        "externalIds": {"CorpusId": 32219576}, "corpusId": 32219576, "publicationVenue":
        null, "url": "https://www.semanticscholar.org/paper/07e16f31d9b4c1fd4dd5f2d65d9c31a04654292f",
        "title": "Bayesian Inference for Latent Hawkes Processes", "abstract": "Hawkes
        processes are multivariate point processes that model excitatory interactions
        among vertices in a network. Each vertex emits a sequence of discrete events:
        points in time with associated content, or marks. Unlike Poisson processes,
        Hawkes processes allow events on one vertex to influence the subsequent rate
        of events on downstream vertices. With this property, they are ideally suited
        to model a variety of phenomena, like social network interactions, the spread
        of earthquake aftershocks, and spiking activity in neural circuits. This paper
        addresses a natural question: what if some vertices, marks, or time intervals
        are hidden from view? Such scenarios often arise in practice, as when neuroscientists
        work with recordings of subsets of neurons, or when social exchanges are not
        fully observed. These latent Hawkes processes pose a serious inferential challenge:
        we must perform inference in a model whose latent variables are marked point
        processes. We derive Bayesian inference and learning algorithms for latent
        Hawkes processes and demonstrate their efficacy on a variety of synthetic
        data and neural data.", "venue": "", "year": 2017, "referenceCount": 27, "citationCount":
        16, "influentialCitationCount": 1, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category": "Computer Science",
        "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": null,
        "journal": null, "authors": [{"authorId": "2342841", "name": "Scott W. Linderman"},
        {"authorId": "2108734693", "name": "Yixin Wang"}, {"authorId": "1796335",
        "name": "D. Blei"}]}, {"paperId": "160d86a1e8804507444daa74ede04a0821df16b2",
        "externalIds": {"CorpusId": 260843881}, "corpusId": 260843881, "publicationVenue":
        null, "url": "https://www.semanticscholar.org/paper/160d86a1e8804507444daa74ede04a0821df16b2",
        "title": "Tea Leaves : How Humans Interpret Topic Models", "abstract": "Probabilistic
        topic models are a popular tool for the unsupervised analysis of text, providing
        both a predictive model of future text and a latent topic representation of
        the corpus. Practitioners typically assume that the latent space is semantically
        meaningful. It is used to check models, summarize the corpus, and guide exploration
        of its contents. However, whether the latent space is interpretable is in
        need of quantitative evaluation. In this paper, we present new quantitative
        methods for measuring semantic meaning in inferred topics. We back these measures
        with large-scale user studies, showing that they capture aspects of the model
        that are undetected by previous measures of model quality based on held-out
        likelihood. Surprisingly, topic models which perform better on held-out likelihood
        may infer less semantically meaningful topics.", "venue": "", "year": 2017,
        "referenceCount": 25, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess":
        false, "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category":
        "Computer Science", "source": "s2-fos-model"}], "publicationTypes": null,
        "publicationDate": null, "journal": null, "authors": [{"authorId": "80936017",
        "name": "Jonathan D. Chang"}, {"authorId": "1389036863", "name": "Jordan L.
        Boyd-Graber"}, {"authorId": "2108881999", "name": "Chong Wang"}, {"authorId":
        "21007048", "name": "S. Gerrish"}, {"authorId": "1796335", "name": "D. Blei"}]},
        {"paperId": "163c48055cc2d26a1a1251c257ab55070794c07b", "externalIds": {"MAG":
        "2787729803", "DOI": "10.1080/01621459.2017.1388244", "CorpusId": 126380197},
        "corpusId": 126380197, "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/163c48055cc2d26a1a1251c257ab55070794c07b",
        "title": "Comment: A Discussion of \u201cNonparametric Bayes Modeling of Populations
        of Networks\u201d", "abstract": "Kucukelbir, A., Tran, D., Ranganath, R.,
        Gelman, A., and Blei, D.M. (2017), \u201cAutomatic Differentiation Variational
        Inference,\u201d Journal of Machine Learning Research, 18, 430\u2013474. [1542]
        Ma, Y., Chen, T., and Fox, E. B. (2016), \u201cA Complete Recipe for Stochastic
        Gradient MCMC,\u201d in Advances in Neural Information Processing Systems
        (Vol. 28), pp. 2899\u20132907, Cambridge, MA: MIT Press. [1542] Ma, Y., Foti,
        N. J., and Fox, E. B. (2017), \u201cStochastic Gradient MCMC Methods for Hidden
        Markov Models,\u201d in International Conference on Machine Learning, Proceedings
        of Machine Learning Research (Vol. 70), pp. 2265\u20132274. [1542] Neal, R.
        M. (2010), \u201cMCMC Using Hamiltonian Dynamics,\u201d Handbook of Markov
        Chain Monte Carlo, 54, 113\u2013162. [1542] Rezende, D. J., Mohamed, S., and
        Wierstra, D. (2014), \u201cStochastic Backpropagation and Approximate Inference
        in Deep Generative Models,\u201d in International Conference on Machine Learning.
        [1540] Tran, D., Kucukelbir, A., Dieng, A. B., Rudolph, M., Liang, D., and
        Blei, D. M. (2016), \u201cEdward: A Library for Probabilistic Modeling, Inference,
        and Criticism,\u201d arXiv preprint arXiv:1610.09787. [1540]", "venue": "",
        "year": 2017, "referenceCount": 23, "citationCount": 0, "influentialCitationCount":
        0, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
        "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": null, "publicationDate": "2017-10-02", "journal": {"volume":
        "112", "pages": "1543 - 1547", "name": "Journal of the American Statistical
        Association"}, "authors": [{"authorId": "2342841", "name": "Scott W. Linderman"},
        {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId": "1e4e25d69d599f7b8ac010ddf57692b51950a132",
        "externalIds": {"CorpusId": 250983064}, "corpusId": 250983064, "publicationVenue":
        null, "url": "https://www.semanticscholar.org/paper/1e4e25d69d599f7b8ac010ddf57692b51950a132",
        "title": "Appendix to Variational Inference: A Review for Statisticians",
        "abstract": "One of the core problems of modern statistics is to approximate
        di\ufb03cult-to-compute probability densities. This problem is especially
        important in Bayesian statistics, which frames all inference about unknown
        quantities as a calculation involving the posterior density. In this paper,
        we review variational inference ( vi ), a method from machine learning that
        ap-proximates probability densities through optimization. vi has been used
        in many applications and tends to be faster than classical methods, such as
        Markov chain Monte Carlo sampling. The idea behind vi is to \ufb01rst posit
        a family of densities and then to \ufb01nd the member of that family which
        is close to the target. Closeness is measured by Kullback-Leibler divergence.
        We review the ideas behind mean-\ufb01eld variational inference, discuss the
        special case of vi applied to exponential family models, present a full example
        with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic
        optimization to scale up to massive data. We discuss modern research in vi
        and highlight important open problems. vi is powerful, but it is not yet well
        understood. Our hope in writing this paper is to catalyze statistical research
        on this class of algorithms.", "venue": "", "year": 2017, "referenceCount":
        9, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category":
        "Mathematics", "source": "s2-fos-model"}], "publicationTypes": ["Review"],
        "publicationDate": null, "journal": null, "authors": [{"authorId": "1796335",
        "name": "D. Blei"}, {"authorId": "3081817", "name": "A. Kucukelbir"}, {"authorId":
        "40411909", "name": "Jon D. McAuliffe"}]}, {"paperId": "22f3c0a727bc7f07e7d5073b6d6bb4f73102ba74",
        "externalIds": {"MAG": "2953292692", "DBLP": "journals/corr/abs-1709-10367",
        "ArXiv": "1709.10367", "CorpusId": 20151713}, "corpusId": 20151713, "publicationVenue":
        {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd", "name": "Neural Information
        Processing Systems", "type": "conference", "alternate_names": ["Neural Inf
        Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"}, "url": "https://www.semanticscholar.org/paper/22f3c0a727bc7f07e7d5073b6d6bb4f73102ba74",
        "title": "Structured Embedding Models for Grouped Data", "abstract": "Word
        embeddings are a powerful approach for analyzing language, and exponential
        family embeddings (EFE) extend them to other types of data. Here we develop
        structured exponential family embeddings (S-EFE), a method for discovering
        embeddings that vary across related groups of data. We study how the word
        usage of U.S. Congressional speeches varies across states and party affiliation,
        how words are used differently across sections of the ArXiv, and how the co-purchase
        patterns of groceries can vary across seasons. Key to the success of our method
        is that the groups share statistical information. We develop two sharing strategies:
        hierarchical modeling and amortization. We demonstrate the benefits of this
        approach in empirical studies of speeches, abstracts, and shopping baskets.
        We show how S-EFE enables group-specific interpretation of word usage, and
        outperforms EFE in predicting held-out data.", "venue": "Neural Information
        Processing Systems", "year": 2017, "referenceCount": 40, "citationCount":
        35, "influentialCitationCount": 5, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics",
        "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2017-09-28",
        "journal": {"pages": "251-261"}, "authors": [{"authorId": "144016056", "name":
        "Maja R. Rudolph"}, {"authorId": "144211059", "name": "Francisco J. R. Ruiz"},
        {"authorId": "2631417", "name": "S. Athey"}, {"authorId": "1796335", "name":
        "D. Blei"}]}, {"paperId": "24865152648081fa29bf5e3c6c77027e1d2b0205", "externalIds":
        {"CorpusId": 27008703}, "corpusId": 27008703, "publicationVenue": null, "url":
        "https://www.semanticscholar.org/paper/24865152648081fa29bf5e3c6c77027e1d2b0205",
        "title": "Variational Inference via \u03c7 Upper Bound Minimization", "abstract":
        "Variational inference (VI) is widely used as an efficient alternative to
        Markov chain Monte Carlo. It posits a family of approximating distributions
        q and finds the closest member to the exact posterior p. Closeness is usually
        measured via a divergence D(q||p) from q to p. While successful, this approach
        also has problems. Notably, it typically leads to underestimation of the posterior
        variance. In this paper we propose CHIVI, a black-box variational inference
        algorithm that minimizes D\u03c7(p||q), the \u03c7-divergence from p to q.
        CHIVI minimizes an upper bound of the model evidence, which we term the \u03c7
        upper bound (CUBO). Minimizing the CUBO leads to improved posterior uncertainty,
        and it can also be used with the classical VI lower bound (ELBO) to provide
        a sandwich estimate of the model evidence. We study CHIVI on three models:
        probit regression, Gaussian process classification, and a Cox process model
        of basketball plays. When compared to expectation propagation and classical
        VI, CHIVI produces better error rates and more accurate estimates of posterior
        variance.", "venue": "", "year": 2017, "referenceCount": 34, "citationCount":
        104, "influentialCitationCount": 18, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category": "Computer Science",
        "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": null,
        "journal": null, "authors": [{"authorId": "38638095", "name": "Adji Bousso
        Dieng"}, {"authorId": "1933666", "name": "Dustin Tran"}, {"authorId": "2615814",
        "name": "R. Ranganath"}, {"authorId": "143855009", "name": "J. Paisley"},
        {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId": "375aa8e5ed444c6f83e2abeff8475f76833a0a2e",
        "externalIds": {"MAG": "2963476931", "DBLP": "conf/nips/TranRB17", "CorpusId":
        13803015}, "corpusId": 13803015, "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
        "name": "Neural Information Processing Systems", "type": "conference", "alternate_names":
        ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
        "url": "https://www.semanticscholar.org/paper/375aa8e5ed444c6f83e2abeff8475f76833a0a2e",
        "title": "Hierarchical Implicit Models and Likelihood-Free Variational Inference",
        "abstract": "Implicit probabilistic models are a flexible class of models
        defined by a simulation process for data. They form the basis for theories
        which encompass our understanding of the physical world. Despite this fundamental
        nature, the use of implicit models remains limited due to challenges in specifying
        complex latent structure in them, and in performing inferences in such models
        with large data sets. In this paper, we first introduce hierarchical implicit
        models (HIMs). HIMs combine the idea of implicit densities with hierarchical
        Bayesian modeling, thereby defining models via simulators of data with rich
        hidden structure. Next, we develop likelihood-free variational inference (LFVI),
        a scalable variational inference algorithm for HIMs. Key to LFVI is specifying
        a variational family that is also implicit. This matches the model''s flexibility
        and allows for accurate approximation of the posterior. We demonstrate diverse
        applications: a large-scale physical simulator for predator-prey populations
        in ecology; a Bayesian generative adversarial network for discrete data; and
        a deep implicit model for text generation.", "venue": "Neural Information
        Processing Systems", "year": 2017, "referenceCount": 65, "citationCount":
        220, "influentialCitationCount": 18, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics",
        "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2017-02-28",
        "journal": {"pages": "5523-5533"}, "authors": [{"authorId": "47497262", "name":
        "Dustin Tran"}, {"authorId": "2615814", "name": "R. Ranganath"}, {"authorId":
        "1796335", "name": "D. Blei"}]}, {"paperId": "3a4bea2033c86ac1d26f76d1d9c07216c9a1a541",
        "externalIds": {"MAG": "3016709501", "DBLP": "journals/corr/abs-1711-03560",
        "ArXiv": "1711.03560", "DOI": "10.1214/19-aoas1265", "CorpusId": 32251006},
        "corpusId": 32251006, "publicationVenue": {"id": "8b03733e-25e3-49b5-803c-58c0e9e11180",
        "name": "Annals of Applied Statistics", "type": "journal", "alternate_names":
        ["Ann Appl Stat", "The Annals of Applied Statistics"], "issn": "1932-6157",
        "url": "http://www.imstat.org/aoas/", "alternate_urls": ["http://www.jstor.org/action/showPublication?journalCode=annaapplstat",
        "https://www.imstat.org/journals-and-publications/annals-of-applied-statistics/",
        "https://projecteuclid.org/DPubS?handle=euclid.aoas&service=UI&verb=Display&version=1.0",
        "https://www.jstor.org/journal/annaapplstat"]}, "url": "https://www.semanticscholar.org/paper/3a4bea2033c86ac1d26f76d1d9c07216c9a1a541",
        "title": "SHOPPER: A Probabilistic Model of Consumer Choice with Substitutes
        and Complements", "abstract": "We develop SHOPPER, a sequential probabilistic
        model of market baskets. SHOPPER uses interpretable components to model the
        forces that drive how a customer chooses products; in particular, we designed
        SHOPPER to capture how items interact with other items. We develop an efficient
        posterior inference algorithm to estimate these forces from large-scale data,
        and we analyze a large dataset from a major chain grocery store. We are interested
        in answering counterfactual queries about changes in prices. We found that
        SHOPPER provides accurate predictions even under price interventions, and
        that it helps identify complementary and substitutable pairs of products.",
        "venue": "Annals of Applied Statistics", "year": 2017, "referenceCount": 70,
        "citationCount": 67, "influentialCitationCount": 10, "isOpenAccess": true,
        "openAccessPdf": {"url": "https://projecteuclid.org/journals/annals-of-applied-statistics/volume-14/issue-1/SHOPPER--A-probabilistic-model-of-consumer-choice-with-substitutes/10.1214/19-AOAS1265.pdf",
        "status": null}, "fieldsOfStudy": ["Mathematics", "Computer Science", "Economics"],
        "s2FieldsOfStudy": [{"category": "Mathematics", "source": "external"}, {"category":
        "Computer Science", "source": "external"}, {"category": "Economics", "source":
        "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle"], "publicationDate": "2017-11-09", "journal":
        {"volume": "abs/1711.03560", "name": "ArXiv"}, "authors": [{"authorId": "144211059",
        "name": "Francisco J. R. Ruiz"}, {"authorId": "2631417", "name": "S. Athey"},
        {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId": "3f9bfa1c5cce02b86667c53ddfd34902d8bd0eee",
        "externalIds": {"CorpusId": 263158993}, "corpusId": 263158993, "publicationVenue":
        null, "url": "https://www.semanticscholar.org/paper/3f9bfa1c5cce02b86667c53ddfd34902d8bd0eee",
        "title": "Multilingual Topic Models for Unaligned Text", "abstract": "We develop
        the multilingual topic model for unaligned text (MuTo), a probabilistic model
        of text that is designed to analyze corpora composed of documents in two languages.
        From these documents, MuTo uses stochastic EM to simultaneously discover both
        a matching between the languages and multilingual latent topics. We demonstrate
        that MuTo is able to find shared topics on real-world multilingual corpora,
        successfully pairing related documents across languages. MuTo provides a new
        framework for creating multilingual topic models without needing carefully
        curated parallel corpora and allows applications built using the topic model
        formalism to be applied to a much wider class of corpora. Topic models are
        a powerful formalism for unsupervised analysis of corpora [1, 8]. They are
        an important tool in information retrieval [27], sentiment analysis [25],
        and collaborative filtering [18]. When interpreted as a mixed membership model,
        similar assumptions have been successfully applied to vision [6], population
        survey analysis [4], and genetics [5]. In this work, we build on latent Dirichlet
        allocation (LDA) [2], a generative, probabilistic topic model of text. LDA
        assumes that documents have a distribution over topics and that these topics
        are distributions over the vocabulary. Posterior inference discovers the topics
        that best explain a corpus; the uncovered topics tend to reflect thematically
        consistent patterns of words [8]. The goal of this paper is to find topics
        that express thematic coherence across multiple languages. LDA can capture
        coherence in a single language because semantically similar words tend to
        be used in similar contexts. This is not the case in multilingual corpora.
        For example, even though \u201cHund\u201d and \u201chound\u201d are orthographically
        similar and have nearly identical meanings in German and English (i.e., \u201cdog\u201d),
        they will likely not appear in similar contexts because almost all documents
        are written in a single language. Consequently, a topic model fit on a bilingual
        corpus reveals coherent topics but bifurcates the topic space between the
        two languages (Table 1). In order to build coherent topics across languages,
        there must be some connection to tie the languages together. Previous multilingual
        topic models connect the languages by assuming parallelism at either the sentence
        level [28] or document level [13, 23, 19]. Many parallel corpora are available,
        but they represent a small fraction of corpora. They also tend to be relatively
        well annotated and understood, making them less suited for unsupervised methods
        like LDA. A topic model on unaligned text in multiple languages would allow
        the exciting applications developed for monolingual topics models to be applied
        to a broader class of corpora and would help monolingual users to explore
        and understand multilingual corpora. We propose the MUltilingual TOpic model
        for unaligned text (MUTO). MUTO does not assume that it is given any explicit
        parallelism but instead discovers a parallelism at the vocabulary level. To
        find this parallelism, the model assumes that similar themes and ideas appear
        in both languages. For example, if the word \u201cHund\u201d appears in the
        German side of the corpus, \u201chound\u201d or \u201cdog\u201d should appear
        somewhere on the English side. The assumption that similar terms will appear
        in similar contexts has also been used to build lexicons from nonparallel
        but comparable corpora. What makes contexts similar can be evaluated through
        such measures as cooccurrence [20, 24] or tf-idf [7]. Although the emphasis
        of our work is on building consistent topic spaces and not the task of building
        dictionaries per se, good translations are required to find consistent topics.
        However, we can build on successful techniques at building lexicons across
        languages. This paper is organized as follows. We detail the model and its
        assumptions in Section 1, develop a stochastic expectation maximization (EM)
        inference procedure in Section 2, discuss the corpora and other linguistic
        resources necessary to evaluate the model in Section 3, and evaluate the performance
        of the model in Section 4.", "venue": "", "year": 2017, "referenceCount":
        28, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category":
        "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["Review"],
        "publicationDate": null, "journal": null, "authors": [{"authorId": "2240779865",
        "name": "Jordan L. Boyd-Graber"}, {"authorId": "1796335", "name": "D. Blei"}]},
        {"paperId": "534403a65fa011cc29a09e29864a88cd6216a0dd", "externalIds": {"MAG":
        "2612481336", "ArXiv": "1705.03439", "DBLP": "journals/corr/WangB17a", "DOI":
        "10.1080/01621459.2018.1473776", "CorpusId": 26011675}, "corpusId": 26011675,
        "publicationVenue": {"id": "8c91bff0-940d-46f7-86d4-582c09ed787c", "name":
        "Journal of the American Statistical Association", "type": "journal", "alternate_names":
        ["J Am Stat Assoc"], "issn": "0162-1459", "url": "http://openurl.ingenta.com/content?genre=journal&issn=0162-1459",
        "alternate_urls": ["https://www.jstor.org/journal/jamerstatasso", "http://www.tandfonline.com/loi/uasa20#.VHzNXU10ymQ",
        "http://amstat.tandfonline.com/loi/uasa20", "http://www.jstor.org/journals/01621459.html",
        "http://www.tandfonline.com/toc/uasa20/current"]}, "url": "https://www.semanticscholar.org/paper/534403a65fa011cc29a09e29864a88cd6216a0dd",
        "title": "Frequentist Consistency of Variational Bayes", "abstract": "ABSTRACT
        A key challenge for modern Bayesian statistics is how to perform scalable
        inference of posterior distributions. To address this challenge, variational
        Bayes (VB) methods have emerged as a popular alternative to the classical
        Markov chain Monte Carlo (MCMC) methods. VB methods tend to be faster while
        achieving comparable predictive performance. However, there are few theoretical
        results around VB. In this article, we establish frequentist consistency and
        asymptotic normality of VB methods. Specifically, we connect VB methods to
        point estimates based on variational approximations, called frequentist variational
        approximations, and we use the connection to prove a variational Bernstein\u2013von
        Mises theorem. The theorem leverages the theoretical characterizations of
        frequentist variational approximations to understand asymptotic properties
        of VB. In summary, we prove that (1) the VB posterior converges to the Kullback\u2013Leibler
        (KL) minimizer of a normal distribution, centered at the truth and (2) the
        corresponding variational expectation of the parameter is consistent and asymptotically
        normal. As applications of the theorem, we derive asymptotic properties of
        VB posteriors in Bayesian mixture models, Bayesian generalized linear mixed
        models, and Bayesian stochastic block models. We conduct a simulation study
        to illustrate these theoretical results. Supplementary materials for this
        article are available online.", "venue": "Journal of the American Statistical
        Association", "year": 2017, "referenceCount": 108, "citationCount": 163, "influentialCitationCount":
        15, "isOpenAccess": true, "openAccessPdf": {"url": "http://arxiv.org/pdf/1705.03439",
        "status": null}, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics",
        "source": "external"}, {"category": "Mathematics", "source": "s2-fos-model"},
        {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
        ["JournalArticle"], "publicationDate": "2017-05-09", "journal": {"volume":
        "114", "pages": "1147 - 1161", "name": "Journal of the American Statistical
        Association"}, "authors": [{"authorId": "2108734693", "name": "Yixin Wang"},
        {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId": "67c04e949cc98c7dcc48dea0c94043d4ebf542db",
        "externalIds": {"ArXiv": "1703.08052", "DBLP": "journals/corr/RudolphB17",
        "MAG": "2600534099", "CorpusId": 8600487}, "corpusId": 8600487, "publicationVenue":
        {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names":
        ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/67c04e949cc98c7dcc48dea0c94043d4ebf542db",
        "title": "Dynamic Bernoulli Embeddings for Language Evolution", "abstract":
        "Word embeddings are a powerful approach for unsupervised analysis of language.
        Recently, Rudolph et al. (2016) developed exponential family embeddings, which
        cast word embeddings in a probabilistic framework. Here, we develop dynamic
        embeddings, building on exponential family embeddings to capture how the meanings
        of words change over time. We use dynamic embeddings to analyze three large
        collections of historical texts: the U.S. Senate speeches from 1858 to 2009,
        the history of computer science ACM abstracts from 1951 to 2014, and machine
        learning papers on the Arxiv from 2007 to 2015. We find dynamic embeddings
        provide better fits than classical embeddings and capture interesting patterns
        about how language changes.", "venue": "arXiv.org", "year": 2017, "referenceCount":
        48, "citationCount": 31, "influentialCitationCount": 1, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Mathematics", "Computer Science"],
        "s2FieldsOfStudy": [{"category": "Mathematics", "source": "external"}, {"category":
        "Computer Science", "source": "external"}, {"category": "Computer Science",
        "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
        "2017-03-23", "journal": {"volume": "abs/1703.08052", "name": "ArXiv"}, "authors":
        [{"authorId": "144016056", "name": "Maja R. Rudolph"}, {"authorId": "1796335",
        "name": "D. Blei"}]}, {"paperId": "7d520c022397773ab25feef5c5c6492bf1900999",
        "externalIds": {"DBLP": "conf/icml/KucukelbirWB17", "MAG": "2740945067", "CorpusId":
        7432732}, "corpusId": 7432732, "publicationVenue": {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
        "name": "International Conference on Machine Learning", "type": "conference",
        "alternate_names": ["ICML", "Int Conf Mach Learn"], "url": "https://icml.cc/"},
        "url": "https://www.semanticscholar.org/paper/7d520c022397773ab25feef5c5c6492bf1900999",
        "title": "Evaluating Bayesian Models with Posterior Dispersion Indices", "abstract":
        "Probabilistic modeling is cyclical: we specify a model, infer its posterior,
        and evaluate its performance. Evaluation drives the cycle, as we revise our
        model based on how it performs. This requires a metric. Traditionally, predictive
        accuracy prevails. Yet, predictive accuracy does not tell the whole story.
        We propose to evaluate a model through posterior dispersion. The idea is to
        analyze how each datapoint fares in relation to posterior uncertainty around
        the hidden structure. This highlights datapoints the model struggles to explain
        and provides complimentary insight to datapoints with low predictive accuracy.
        We present a family of posterior dispersion indices (PDI) that capture this
        idea. We show how a PDI identifies patterns of model mismatch in three real
        data examples: voting preferences, supermarket shopping, and population genetics.",
        "venue": "International Conference on Machine Learning", "year": 2017, "referenceCount":
        39, "citationCount": 3, "influentialCitationCount": 0, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
        "Conference"], "publicationDate": "2017-07-17", "journal": {"pages": "1925-1934"},
        "authors": [{"authorId": "3081817", "name": "A. Kucukelbir"}, {"authorId":
        "2108734693", "name": "Yixin Wang"}, {"authorId": "1796335", "name": "D. Blei"}]},
        {"paperId": "836b6b502c1891efece1046c36a6142ef677c197", "externalIds": {"DOI":
        "10.1080/01621459.2016.1270044", "CorpusId": 219598043}, "corpusId": 219598043,
        "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/836b6b502c1891efece1046c36a6142ef677c197",
        "title": "Comment", "abstract": "Stan Development Team (2016), \u201cStan:
        A C++ Library for Probability and Sampling, (version 2.9.0),\u201d available
        at http://mc-stan.org. [138,149,151,153] Tan, L. S. L., and Nott, D. J. (2013),
        \u201cVariational Inference for Generalized Linear Mixed Models Using Partially
        Noncentered Parametrizations,\u201d Statistical Science, 28, 168\u2013188.
        [151] Uhler, C., Lenkoski, A., and Richards, D. (2014), \u201cExact Formulas
        for the Normalizing Constants of Wishart Distributions for Graphical Models,\u201d
        unpublished manuscript (arXiv:1406.490). [147] Verbyla, A. P., Cullis, B.
        R., Kenward, M. G., andWelham, S. J. (1999), \u201cThe Analysis of Designed
        Experiments and Longitudinal Data by Using Smoothing Splines\u201d (with discussion),
        Applied Statistics, 48, 269\u2013312. [149] Wainwright, M. J., and Jordan,
        M. I. (2008), \u201cGraphical Models, Exponential Families and Variational
        Inference,\u201d Foundations and Trends in Machine Learning, 1, 1\u2013305.
        [141] Wand, M. P. (2009), \u201cSemiparametric Regression and Graphical Models,\u201d
        Australian and New Zealand Journal of Statistics, 51, 9\u201341. [137,140]
        Wand, M. P. (2014), \u201cFully Simplified multivariate normal updates in
        NonConjugate Variational Message Passing,\u201d Journal of Machine Learning
        Research, 15, 1351\u20131369. [151] Wand, M. P., and Ormerod, J. T. (2008),
        \u201cOn Semiparametric Regression with O\u2019sullivan Penalized Splines,\u201dAustralian
        and New Zealand Journal of Statistics, 50, 179\u2013198. [140] Wand, M. P.,
        and Ormerod, J. T. (2011), \u201cPenalized Wavelets: Embedding Wavelets into
        Semiparametric Regression,\u201d Electronic Journal of Statistics, 5, 1654\u20131717.
        [137,140] Wang, S. S. J., andWand,M. P. (2011), \u201cUsing Infer.NET for
        Statistical Analyses,\u201d The American Statistician, 65, 115\u2013126. [149]
        Winn, J., and Bishop, C. M. (2005), \u201cVariational Message Passing,\u201d
        Journal of Machine Learning Research, 6, 661\u2013694. [137,138,140] Wood,
        S. N. (2006), Generalized Additive Models: An Introduction with R, Boca Raton,
        FL: CRC Press. [149] Wood, S. N., Scheipl, F., and Faraway, F. F. (2013),
        \u201cStraightforward Intermediate Rank Tensor Product Smoothing in Mixed
        Models,\u201d Statistics and Computing, 23, 341\u20133601. [149]", "venue":
        "", "year": 2017, "referenceCount": 8, "citationCount": 1, "influentialCitationCount":
        0, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy":
        [{"category": "Mathematics", "source": "s2-fos-model"}], "publicationTypes":
        null, "publicationDate": "2017-01-02", "journal": {"volume": "112", "pages":
        "156 - 158", "name": "Journal of the American Statistical Association"}, "authors":
        [{"authorId": "47497262", "name": "Dustin Tran"}, {"authorId": "1796335",
        "name": "D. Blei"}]}, {"paperId": "83cffda7d9b47d0927d03fdc574a019487a3d5d8",
        "externalIds": {"ArXiv": "1710.10742", "DBLP": "conf/iclr/TranB18", "MAG":
        "2962771559", "CorpusId": 27046713}, "corpusId": 27046713, "publicationVenue":
        {"id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40", "name": "International Conference
        on Learning Representations", "type": "conference", "alternate_names": ["Int
        Conf Learn Represent", "ICLR"], "url": "https://iclr.cc/"}, "url": "https://www.semanticscholar.org/paper/83cffda7d9b47d0927d03fdc574a019487a3d5d8",
        "title": "Implicit Causal Models for Genome-wide Association Studies", "abstract":
        "Progress in probabilistic generative models has accelerated, developing richer
        models with neural architectures, implicit densities, and with scalable algorithms
        for their Bayesian inference. However, there has been limited progress in
        models that capture causal relationships, for example, how individual genetic
        factors cause major human diseases. In this work, we focus on two challenges
        in particular: How do we build richer causal models, which can capture highly
        nonlinear relationships and interactions between multiple causes? How do we
        adjust for latent confounders, which are variables influencing both cause
        and effect and which prevent learning of causal relationships? To address
        these challenges, we synthesize ideas from causality and modern probabilistic
        modeling. For the first, we describe implicit causal models, a class of causal
        models that leverages neural architectures with an implicit density. For the
        second, we describe an implicit causal model that adjusts for confounders
        by sharing strength across examples. In experiments, we scale Bayesian inference
        on up to a billion genetic measurements. We achieve state of the art accuracy
        for identifying causal factors: we significantly outperform existing genetics
        methods by an absolute difference of 15-45.3%.", "venue": "International Conference
        on Learning Representations", "year": 2017, "referenceCount": 45, "citationCount":
        41, "influentialCitationCount": 2, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": ["Mathematics", "Computer Science", "Biology"], "s2FieldsOfStudy":
        [{"category": "Mathematics", "source": "external"}, {"category": "Computer
        Science", "source": "external"}, {"category": "Biology", "source": "external"},
        {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
        ["JournalArticle"], "publicationDate": "2017-10-01", "journal": {"volume":
        "abs/1710.10742", "name": "ArXiv"}, "authors": [{"authorId": "47497262", "name":
        "Dustin Tran"}, {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId":
        "8e9532262bb862c62d2d011a93da014da2d97ee9", "externalIds": {"DBLP": "conf/iclr/TranHSB0B17",
        "MAG": "2949164981", "ArXiv": "1701.03757", "CorpusId": 851777}, "corpusId":
        851777, "publicationVenue": {"id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
        "name": "International Conference on Learning Representations", "type": "conference",
        "alternate_names": ["Int Conf Learn Represent", "ICLR"], "url": "https://iclr.cc/"},
        "url": "https://www.semanticscholar.org/paper/8e9532262bb862c62d2d011a93da014da2d97ee9",
        "title": "Deep Probabilistic Programming", "abstract": "We propose Edward,
        a Turing-complete probabilistic programming language. Edward defines two compositional
        representations---random variables and inference. By treating inference as
        a first class citizen, on a par with modeling, we show that probabilistic
        programming can be as flexible and computationally efficient as traditional
        deep learning. For flexibility, Edward makes it easy to fit the same model
        using a variety of composable inference methods, ranging from point estimation
        to variational inference to MCMC. In addition, Edward can reuse the modeling
        representation as part of inference, facilitating the design of rich variational
        models and generative adversarial networks. For efficiency, Edward is integrated
        into TensorFlow, providing significant speedups over existing probabilistic
        systems. For example, we show on a benchmark logistic regression task that
        Edward is at least 35x faster than Stan and 6x faster than PyMC3. Further,
        Edward incurs no runtime overhead: it is as fast as handwritten TensorFlow.",
        "venue": "International Conference on Learning Representations", "year": 2017,
        "referenceCount": 80, "citationCount": 185, "influentialCitationCount": 23,
        "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Mathematics",
        "Computer Science"], "s2FieldsOfStudy": [{"category": "Mathematics", "source":
        "external"}, {"category": "Computer Science", "source": "external"}, {"category":
        "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
        "publicationDate": "2017-01-13", "journal": {"volume": "abs/1701.03757", "name":
        "ArXiv"}, "authors": [{"authorId": "47497262", "name": "Dustin Tran"}, {"authorId":
        "28552618", "name": "M. Hoffman"}, {"authorId": "2278009", "name": "R. Saurous"},
        {"authorId": "2445241", "name": "E. Brevdo"}, {"authorId": "1702318", "name":
        "K. Murphy"}, {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId": "9a69c7ef2940f1770acba9033f8d4220d69f41d3",
        "externalIds": {"MAG": "2623647148", "CorpusId": 64839443}, "corpusId": 64839443,
        "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/9a69c7ef2940f1770acba9033f8d4220d69f41d3",
        "title": "Scalable Topic Modeling: Online Learning, Diagnostics, and Recommendation",
        "abstract": "Abstract : The main activity of my research group is to build
        and develop the probabilistic pipeline. When solving problems with data, we
        take the following steps. 1. We make assumptions about our data, embedding
        it in a probability model containing hidden and observed random variables.
        2. Given observations, we use inference algorithms to estimate the conditional
        distribution of the hidden variables. This is the central statistical and
        computational problem. 3. With the results of inference, we use our model
        to form predictions about the future, explore the data, or otherwise apply
        what we learned to solve a problem. 4. We criticize our model, understand
        where it went right and wrong, and repeat the process to revise it.", "venue":
        "", "year": 2017, "referenceCount": 15, "citationCount": 0, "influentialCitationCount":
        0, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
        "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": null, "publicationDate": "2017-03-01", "journal": {"volume":
        "", "name": ""}, "authors": [{"authorId": "1796335", "name": "D. Blei"}]},
        {"paperId": "b0f7e146bff2f5be42c4d84eac2f7309310c54b7", "externalIds": {"MAG":
        "2741977875", "DBLP": "conf/icml/LiuB17", "CorpusId": 28313803}, "corpusId":
        28313803, "publicationVenue": {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
        "name": "International Conference on Machine Learning", "type": "conference",
        "alternate_names": ["ICML", "Int Conf Mach Learn"], "url": "https://icml.cc/"},
        "url": "https://www.semanticscholar.org/paper/b0f7e146bff2f5be42c4d84eac2f7309310c54b7",
        "title": "Zero-Inflated Exponential Family Embeddings", "abstract": "Word
        embeddings are a widely-used tool to analyze language, and exponential family
        embeddings (Rudolph et al., 2016) generalize the technique to other types
        of data. One challenge to fitting embedding methods is sparse data, such as
        a document/term matrix that contains many zeros. To address this issue, practitioners
        typically downweight or subsample the zeros, thus focusing learning on the
        non-zero entries. In this paper, we develop zero-inflated embeddings, a new
        embedding method that is designed to learn from sparse observations. In a
        zero-inflated embedding (ZIE), a zero in the data can come from an interaction
        to other data (i.e., an embedding) or from a separate process by which many
        observations are equal to zero (i.e. a probability mass at zero). Fitting
        a ZIE naturally downweights the zeros and dampens their influence on the model.
        Across many types of data\u2014 language, movie ratings, shopping histories,
        and bird watching logs\u2014we found that zero-inflated embeddings provide
        improved predictive performance over standard approaches and find better vector
        representation of items.", "venue": "International Conference on Machine Learning",
        "year": 2017, "referenceCount": 22, "citationCount": 6, "influentialCitationCount":
        1, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Mathematics",
        "Computer Science"], "s2FieldsOfStudy": [{"category": "Mathematics", "source":
        "external"}, {"category": "Computer Science", "source": "external"}, {"category":
        "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
        "Conference"], "publicationDate": "2017-07-17", "journal": {"pages": "2140-2148"},
        "authors": [{"authorId": "46458178", "name": "Li-Ping Liu"}, {"authorId":
        "1796335", "name": "D. Blei"}]}, {"paperId": "b5c504b1383170939a1b122e400a906332fe0423",
        "externalIds": {"MAG": "2962724589", "ArXiv": "1705.08931", "DBLP": "conf/aistats/AltosaarRB18",
        "CorpusId": 3817714}, "corpusId": 3817714, "publicationVenue": {"id": "2d136b11-c2b5-484b-b008-7f4a852fd61e",
        "name": "International Conference on Artificial Intelligence and Statistics",
        "type": "conference", "alternate_names": ["AISTATS", "Int Conf Artif Intell
        Stat"]}, "url": "https://www.semanticscholar.org/paper/b5c504b1383170939a1b122e400a906332fe0423",
        "title": "Proximity Variational Inference", "abstract": "Variational inference
        is a powerful approach for approximate posterior inference. However, it is
        sensitive to initialization and can be subject to poor local optima. In this
        paper, we develop proximity variational inference (PVI). PVI is a new method
        for optimizing the variational objective that constrains subsequent iterates
        of the variational parameters to robustify the optimization path. Consequently,
        PVI is less sensitive to initialization and optimization quirks and finds
        better local optima. We demonstrate our method on three proximity statistics.
        We study PVI on a Bernoulli factor model and sigmoid belief network with both
        real and synthetic data and compare to deterministic annealing (Katahira et
        al., 2008). We highlight the flexibility of PVI by designing a proximity statistic
        for Bayesian deep learning models such as the variational autoencoder (Kingma
        and Welling, 2014; Rezende et al., 2014). Empirically, we show that PVI consistently
        finds better local optima and gives better predictive performance.", "venue":
        "International Conference on Artificial Intelligence and Statistics", "year":
        2017, "referenceCount": 31, "citationCount": 14, "influentialCitationCount":
        0, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science",
        "source": "external"}, {"category": "Mathematics", "source": "external"},
        {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
        ["JournalArticle"], "publicationDate": "2017-05-24", "journal": {"volume":
        "abs/1705.08931", "name": "ArXiv"}, "authors": [{"authorId": "2124555", "name":
        "Jaan Altosaar"}, {"authorId": "2615814", "name": "R. Ranganath"}, {"authorId":
        "1796335", "name": "D. Blei"}]}, {"paperId": "c0225f99c9b1619c3be74b63241faffe02d275d7",
        "externalIds": {"DBLP": "journals/pnas/BleiS17", "MAG": "2744726626", "DOI":
        "10.1073/pnas.1702076114", "CorpusId": 3315957, "PubMed": "28784795"}, "corpusId":
        3315957, "publicationVenue": {"id": "bb95bf2e-8383-4748-bf9d-d6906d091085",
        "name": "Proceedings of the National Academy of Sciences of the United States
        of America", "type": "journal", "alternate_names": ["PNAS", "PNAS online",
        "Proceedings of the National Academy of Sciences of the United States of America.",
        "Proc National Acad Sci", "Proceedings of the National Academy of Sciences",
        "Proc National Acad Sci u s Am"], "issn": "0027-8424", "alternate_issns":
        ["1091-6490"], "url": "https://www.jstor.org/journal/procnatiacadscie", "alternate_urls":
        ["http://www.pnas.org/", "https://www.pnas.org/", "http://www.jstor.org/journals/00278424.html",
        "www.pnas.org/"]}, "url": "https://www.semanticscholar.org/paper/c0225f99c9b1619c3be74b63241faffe02d275d7",
        "title": "Science and data science", "abstract": "Data science has attracted
        a lot of attention, promising to turn vast amounts of data into useful predictions
        and insights. In this article, we ask why scientists should care about data
        science. To answer, we discuss data science from three perspectives: statistical,
        computational, and human. Although each of the three is a critical component
        of data science, we argue that the effective combination of all three components
        is the essence of what data science is about.", "venue": "Proceedings of the
        National Academy of Sciences of the United States of America", "year": 2017,
        "referenceCount": 25, "citationCount": 127, "influentialCitationCount": 8,
        "isOpenAccess": true, "openAccessPdf": {"url": "https://www.pnas.org/content/pnas/114/33/8689.full.pdf",
        "status": null}, "fieldsOfStudy": ["Medicine", "Computer Science"], "s2FieldsOfStudy":
        [{"category": "Medicine", "source": "external"}, {"category": "Computer Science",
        "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"},
        {"category": "Education", "source": "s2-fos-model"}], "publicationTypes":
        ["JournalArticle"], "publicationDate": "2017-08-07", "journal": {"volume":
        "114", "pages": "8689 - 8692", "name": "Proceedings of the National Academy
        of Sciences"}, "authors": [{"authorId": "1796335", "name": "D. Blei"}, {"authorId":
        "50860274", "name": "Padhraic Smyth"}]}, {"paperId": "e57c09d50d67060b18806f9597674fdc28320dd2",
        "externalIds": {"ArXiv": "1702.08896", "DBLP": "journals/corr/TranRB17", "MAG":
        "2592796789", "CorpusId": 10287476}, "corpusId": 10287476, "publicationVenue":
        {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names":
        ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/e57c09d50d67060b18806f9597674fdc28320dd2",
        "title": "Deep and Hierarchical Implicit Models", "abstract": "Implicit probabilistic
        models are a flexible class for modeling data. They define a process to simulate
        observations, and unlike traditional models, they do not require a tractable
        likelihood function. In this paper, we develop two families of models: hierarchical
        implicit models and deep implicit models. They combine the idea of implicit
        densities with hierarchical Bayesian modeling and deep neural networks. The
        use of implicit models with Bayesian analysis has been limited by our ability
        to perform accurate and scalable inference. We develop likelihood-free variational
        inference (LFVI). Key to LFVI is specifying a variational family that is also
        implicit. This matches the model''s flexibility and allows for accurate approximation
        of the posterior. Our work scales up implicit models to sizes previously not
        possible and advances their modeling design. We demonstrate diverse applications:
        a large-scale physical simulator for predator-prey populations in ecology;
        a Bayesian generative adversarial network for discrete data; and a deep implicit
        model for text generation.", "venue": "arXiv.org", "year": 2017, "referenceCount":
        66, "citationCount": 90, "influentialCitationCount": 7, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
        "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
        {"category": "Mathematics", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
        "publicationDate": "2017-02-28", "journal": {"volume": "abs/1702.08896", "name":
        "ArXiv"}, "authors": [{"authorId": "47497262", "name": "Dustin Tran"}, {"authorId":
        "2615814", "name": "R. Ranganath"}, {"authorId": "1796335", "name": "D. Blei"}]},
        {"paperId": "ea68a5c75e0e228e54efd91db972f71c1a917e51", "externalIds": {"ArXiv":
        "1704.04289", "DBLP": "journals/jmlr/MandtHB17", "MAG": "2962915600", "CorpusId":
        9469223}, "corpusId": 9469223, "publicationVenue": {"id": "c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
        "name": "Journal of machine learning research", "type": "journal", "alternate_names":
        ["Journal of Machine Learning Research", "J mach learn res", "J Mach Learn
        Res"], "issn": "1532-4435", "alternate_issns": ["1533-7928"], "url": "http://www.ai.mit.edu/projects/jmlr/",
        "alternate_urls": ["http://jmlr.csail.mit.edu/", "http://www.jmlr.org/", "http://portal.acm.org/affiliated/jmlr"]},
        "url": "https://www.semanticscholar.org/paper/ea68a5c75e0e228e54efd91db972f71c1a917e51",
        "title": "Stochastic Gradient Descent as Approximate Bayesian Inference",
        "abstract": "Stochastic Gradient Descent with a constant learning rate (constant
        SGD) simulates a Markov chain with a stationary distribution. With this perspective,
        we derive several new results. (1) We show that constant SGD can be used as
        an approximate Bayesian posterior inference algorithm. Specifically, we show
        how to adjust the tuning parameters of constant SGD to best match the stationary
        distribution to a posterior, minimizing the Kullback-Leibler divergence between
        these two distributions. (2) We demonstrate that constant SGD gives rise to
        a new variational EM algorithm that optimizes hyperparameters in complex probabilistic
        models. (3) We also propose SGD with momentum for sampling and show how to
        adjust the damping coefficient accordingly. (4) We analyze MCMC algorithms.
        For Langevin Dynamics and Stochastic Gradient Fisher Scoring, we quantify
        the approximation errors due to finite learning rates. Finally (5), we use
        the stochastic process perspective to give a short proof of why Polyak averaging
        is optimal. Based on this idea, we propose a scalable approximate MCMC algorithm,
        the Averaged Stochastic Gradient Sampler.", "venue": "Journal of machine learning
        research", "year": 2017, "referenceCount": 53, "citationCount": 516, "influentialCitationCount":
        82, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science",
        "source": "external"}, {"category": "Mathematics", "source": "external"},
        {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
        ["JournalArticle"], "publicationDate": "2017-04-13", "journal": {"volume":
        "abs/1704.04289", "name": "ArXiv"}, "authors": [{"authorId": "1783468", "name":
        "S. Mandt"}, {"authorId": "28552618", "name": "M. Hoffman"}, {"authorId":
        "1796335", "name": "D. Blei"}]}, {"paperId": "fbb8aaac1b6bc7326fad06c4bb8ef10c61c8f1d2",
        "externalIds": {"DBLP": "conf/aistats/NaessethLRB18", "MAG": "2616889532",
        "ArXiv": "1705.11140", "CorpusId": 3796515}, "corpusId": 3796515, "publicationVenue":
        {"id": "2d136b11-c2b5-484b-b008-7f4a852fd61e", "name": "International Conference
        on Artificial Intelligence and Statistics", "type": "conference", "alternate_names":
        ["AISTATS", "Int Conf Artif Intell Stat"]}, "url": "https://www.semanticscholar.org/paper/fbb8aaac1b6bc7326fad06c4bb8ef10c61c8f1d2",
        "title": "Variational Sequential Monte Carlo", "abstract": "Many recent advances
        in large scale probabilistic inference rely on variational methods. The success
        of variational approaches depends on (i) formulating a flexible parametric
        family of distributions, and (ii) optimizing the parameters to find the member
        of this family that most closely approximates the exact posterior. In this
        paper we present a new approximating family of distributions, the variational
        sequential Monte Carlo (VSMC) family, and show how to optimize it in variational
        inference. VSMC melds variational inference (VI) and sequential Monte Carlo
        (SMC), providing practitioners with flexible, accurate, and powerful Bayesian
        inference. The VSMC family is a variational family that can approximate the
        posterior arbitrarily well, while still allowing for efficient optimization
        of its parameters. We demonstrate its utility on state space models, stochastic
        volatility models for financial data, and deep Markov models of brain neural
        circuits.", "venue": "International Conference on Artificial Intelligence
        and Statistics", "year": 2017, "referenceCount": 58, "citationCount": 181,
        "influentialCitationCount": 37, "isOpenAccess": false, "openAccessPdf": null,
        "fieldsOfStudy": ["Mathematics", "Computer Science"], "s2FieldsOfStudy": [{"category":
        "Mathematics", "source": "external"}, {"category": "Computer Science", "source":
        "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle"], "publicationDate": "2017-05-31", "journal":
        {"pages": "968-977"}, "authors": [{"authorId": "2328322", "name": "C. A. Naesseth"},
        {"authorId": "2342841", "name": "Scott W. Linderman"}, {"authorId": "2615814",
        "name": "R. Ranganath"}, {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId":
        "fdb0543229042854dee3cfbe67e3742518bda4be", "externalIds": {"MAG": "2718421337",
        "DBLP": "conf/aistats/LindermanJMABP17", "CorpusId": 16317824}, "corpusId":
        16317824, "publicationVenue": {"id": "2d136b11-c2b5-484b-b008-7f4a852fd61e",
        "name": "International Conference on Artificial Intelligence and Statistics",
        "type": "conference", "alternate_names": ["AISTATS", "Int Conf Artif Intell
        Stat"]}, "url": "https://www.semanticscholar.org/paper/fdb0543229042854dee3cfbe67e3742518bda4be",
        "title": "Bayesian Learning and Inference in Recurrent Switching Linear Dynamical
        Systems", "abstract": "Many natural systems, such as neurons firing in the
        brain or basketball teams traversing a court, give rise to time series data
        with complex, nonlinear dynamics. We can gain insight into these systems by
        decomposing the data into segments that are each explained by simpler dynamic
        units. Building on switching linear dynamical systems (SLDS), we develop a
        model class and Bayesian inference algorithms that not only discover these
        dynamical units but also, by learning how transition probabilities depend
        on observations or continuous latent states, explain their switching behavior.
        Our key innovation is to design these recurrent SLDS models to enable recent
        P\u00f3lya-gamma auxiliary variable techniques and thus make approximate Bayesian
        learning and inference in these models easy, fast, and scalable.", "venue":
        "International Conference on Artificial Intelligence and Statistics", "year":
        2017, "referenceCount": 21, "citationCount": 166, "influentialCitationCount":
        13, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
        "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal":
        {"pages": "914-922"}, "authors": [{"authorId": "2342841", "name": "Scott W.
        Linderman"}, {"authorId": "143945326", "name": "Matthew J. Johnson"}, {"authorId":
        "144360230", "name": "Andrew C. Miller"}, {"authorId": "1722180", "name":
        "Ryan P. Adams"}, {"authorId": "1796335", "name": "D. Blei"}, {"authorId":
        "1763504", "name": "L. Paninski"}]}, {"paperId": "fea7064e129db2e577f8c3b84a6d11a9f835a8c3",
        "externalIds": {"MAG": "2751875615", "DBLP": "conf/nips/LiuRAB17", "CorpusId":
        38279642}, "corpusId": 38279642, "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
        "name": "Neural Information Processing Systems", "type": "conference", "alternate_names":
        ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
        "url": "https://www.semanticscholar.org/paper/fea7064e129db2e577f8c3b84a6d11a9f835a8c3",
        "title": "Context Selection for Embedding Models", "abstract": "Word embeddings
        are an effective tool to analyze language. They have been recently extended
        to model other types of data beyond text, such as items in recommendation
        systems. Embedding models consider the probability of a target observation
        (a word or an item) conditioned on the elements in the context (other words
        or items). In this paper, we show that conditioning on all the elements in
        the context is not optimal. Instead, we model the probability of the target
        conditioned on a learned subset of the elements in the context. We use amortized
        variational inference to automatically choose this subset. Compared to standard
        embedding models, this method improves predictions and the quality of the
        embeddings.", "venue": "Neural Information Processing Systems", "year": 2017,
        "referenceCount": 33, "citationCount": 13, "influentialCitationCount": 0,
        "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science",
        "source": "external"}, {"category": "Mathematics", "source": "external"},
        {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
        ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages":
        "4816-4825"}, "authors": [{"authorId": "46458178", "name": "Li-Ping Liu"},
        {"authorId": "144211059", "name": "Francisco J. R. Ruiz"}, {"authorId": "2631417",
        "name": "S. Athey"}, {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId":
        "ff543c45096d817b748e29874317b31ec766dd33", "externalIds": {"DBLP": "journals/neuroimage/ManningZWRSHBN18",
        "MAG": "2586197533", "DOI": "10.1016/j.neuroimage.2018.01.071", "CorpusId":
        3330561, "PubMed": "29448074"}, "corpusId": 3330561, "publicationVenue": {"id":
        "fd4c7628-c16e-4b50-8555-3ac3ad6da2d7", "name": "NeuroImage", "type": "journal",
        "issn": "1053-8119", "url": "http://www.elsevier.com/locate/ynimg", "alternate_urls":
        ["http://www.elsevier.com/wps/find/journaldescription.cws_home/622925/description#description",
        "https://www.journals.elsevier.com/neuroimage", "http://www.sciencedirect.com/science/journal/10538119",
        "http://www.idealibrary.com/"]}, "url": "https://www.semanticscholar.org/paper/ff543c45096d817b748e29874317b31ec766dd33",
        "title": "A probabilistic approach to discovering dynamic full-brain functional
        connectivity patterns", "abstract": null, "venue": "NeuroImage", "year": 2017,
        "referenceCount": 49, "citationCount": 24, "influentialCitationCount": 3,
        "isOpenAccess": true, "openAccessPdf": {"url": "http://manuscript.elsevier.com/S1053811918300715/pdf/S1053811918300715.pdf",
        "status": null}, "fieldsOfStudy": ["Computer Science", "Biology", "Medicine"],
        "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
        {"category": "Biology", "source": "external"}, {"category": "Medicine", "source":
        "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category":
        "Psychology", "source": "s2-fos-model"}], "publicationTypes": ["Review", "JournalArticle"],
        "publicationDate": "2017-02-07", "journal": {"volume": "180", "pages": "243-252",
        "name": "NeuroImage"}, "authors": [{"authorId": "2586738", "name": "Jeremy
        R. Manning"}, {"authorId": "1409939578", "name": "Xia Zhu"}, {"authorId":
        "2999876", "name": "Theodore L. Willke"}, {"authorId": "2615814", "name":
        "R. Ranganath"}, {"authorId": "2238306108", "name": "Kimberly Stachenfeld"},
        {"authorId": "1787630", "name": "U. Hasson"}, {"authorId": "1796335", "name":
        "D. Blei"}, {"authorId": "1780319", "name": "K. Norman"}]}, {"paperId": "057496a0154947d112ecb8b6f4d8272c6409bd69",
        "externalIds": {"ArXiv": "1609.05615", "MAG": "2523161278", "CorpusId": 88522034},
        "corpusId": 88522034, "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/057496a0154947d112ecb8b6f4d8272c6409bd69",
        "title": "Discussion of \"Fast Approximate Inference for Arbitrarily Large
        Semiparametric Regression Models via Message Passing\"", "abstract": "Discussion
        paper on \"Fast Approximate Inference for Arbitrarily Large Semiparametric
        Regression Models via Message Passing\" by Wand [arXiv:1602.07412].", "venue":
        "", "year": 2016, "referenceCount": 10, "citationCount": 33, "influentialCitationCount":
        6, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Mathematics"],
        "s2FieldsOfStudy": [{"category": "Mathematics", "source": "external"}, {"category":
        "Computer Science", "source": "s2-fos-model"}, {"category": "Mathematics",
        "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": "2016-09-19",
        "journal": {"volume": "", "name": "arXiv: Computation"}, "authors": [{"authorId":
        "47497262", "name": "Dustin Tran"}, {"authorId": "1796335", "name": "D. Blei"}]},
        {"paperId": "0998c939e00af09b49ae04fc78aaca7625a0c895", "externalIds": {"MAG":
        "2964260519", "ArXiv": "1610.05683", "DBLP": "conf/aistats/NaessethRLB17",
        "CorpusId": 10756562}, "corpusId": 10756562, "publicationVenue": {"id": "2d136b11-c2b5-484b-b008-7f4a852fd61e",
        "name": "International Conference on Artificial Intelligence and Statistics",
        "type": "conference", "alternate_names": ["AISTATS", "Int Conf Artif Intell
        Stat"]}, "url": "https://www.semanticscholar.org/paper/0998c939e00af09b49ae04fc78aaca7625a0c895",
        "title": "Reparameterization Gradients through Acceptance-Rejection Sampling
        Algorithms", "abstract": "Variational inference using the reparameterization
        trick has enabled large-scale approximate Bayesian inference in complex probabilistic
        models, leveraging stochastic optimization to sidestep intractable expectations.
        The reparameterization trick is applicable when we can simulate a random variable
        by applying a differentiable deterministic function on an auxiliary random
        variable whose distribution is fixed. For many distributions of interest (such
        as the gamma or Dirichlet), simulation of random variables relies on acceptance-rejection
        sampling. The discontinuity introduced by the accept-reject step means that
        standard reparameterization tricks are not applicable. We propose a new method
        that lets us leverage reparameterization gradients even when variables are
        outputs of a acceptance-rejection sampling algorithm. Our approach enables
        reparameterization on a larger class of variational distributions. In several
        studies of real and synthetic data, we show that the variance of the estimator
        of the gradient is significantly lower than other state-of-the-art methods.
        This leads to faster convergence of stochastic gradient variational inference.",
        "venue": "International Conference on Artificial Intelligence and Statistics",
        "year": 2016, "referenceCount": 43, "citationCount": 99, "influentialCitationCount":
        12, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Mathematics",
        "Computer Science"], "s2FieldsOfStudy": [{"category": "Mathematics", "source":
        "external"}, {"category": "Computer Science", "source": "external"}, {"category":
        "Computer Science", "source": "s2-fos-model"}, {"category": "Mathematics",
        "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
        "2016-10-18", "journal": {"pages": "489-498"}, "authors": [{"authorId": "2328322",
        "name": "C. A. Naesseth"}, {"authorId": "144211059", "name": "Francisco J.
        R. Ruiz"}, {"authorId": "2342841", "name": "Scott W. Linderman"}, {"authorId":
        "1796335", "name": "D. Blei"}]}, {"paperId": "0f95aa631f88512667da9b06e95deedfe410a8b8",
        "externalIds": {"CorpusId": 7067369}, "corpusId": 7067369, "publicationVenue":
        null, "url": "https://www.semanticscholar.org/paper/0f95aa631f88512667da9b06e95deedfe410a8b8",
        "title": "Causal Inference for Recommendation", "abstract": "We develop a
        causal inference approach to recommender systems. Observational recommendation
        data contains two sources of information: which items each user decided to
        look at and which of those items each user liked. We assume these two types
        of information come from differentmodels\u2014the exposure data comes from
        a model by which users discover items to consider; the click data comes from
        a model by which users decide which items they like. Traditionally, recommender
        systems use the click data alone (or ratings data) to infer the user preferences.
        But this inference is biased by the exposure data, i.e., that users do not
        consider each item independently at random. We use causal inference to correct
        for this bias. On real-world data, we demonstrate that causal inference for
        recommender systems leads to improved generalization to new data.", "venue":
        "", "year": 2016, "referenceCount": 17, "citationCount": 89, "influentialCitationCount":
        12, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
        null, "publicationDate": null, "journal": null, "authors": [{"authorId": "1778839",
        "name": "Laurent Charlin"}, {"authorId": "1796335", "name": "D. Blei"}]},
        {"paperId": "26a125c7453d56d8ff52ee3f0902631174b99fd1", "externalIds": {"MAG":
        "2535941552", "CorpusId": 45904529}, "corpusId": 45904529, "publicationVenue":
        null, "url": "https://www.semanticscholar.org/paper/26a125c7453d56d8ff52ee3f0902631174b99fd1",
        "title": "Rejection Sampling Variational Inference", "abstract": "Variational
        inference using the reparameterization trick has enabled large-scale approximate
        Bayesian inference in complex probabilistic models, leveraging stochastic
        optimization to sidestep intractable expectations. The reparameterization
        trick is applicable when we can simulate a random variable by applying a (differentiable)
        deterministic function on an auxiliary random variable whose distribution
        is fixed. For many distributions of interest (such as the gamma or Dirichlet),
        simulation of random variables relies on rejection sampling. The discontinuity
        introduced by the accept\u2013reject step means that standard reparameterization
        tricks are not applicable. We propose a new method that lets us leverage reparameterization
        gradients even when variables are outputs of a rejection sampling algorithm.
        Our approach enables reparameterization on a larger class of variational distributions.
        In several studies of real and synthetic data, we show that the variance of
        the estimator of the gradient is significantly lower than other state-of-the-art
        methods. This leads to faster convergence of stochastic optimization variational
        inference. Let p(x, z) be a probabilistic model, i.e., a joint probability
        distribution of data x and latent (unobserved) variables z. In Bayesian inference,
        we are interested in the posterior distribution p(z|x) = p(x,z) p(x) . For
        most models, the posterior distribution is analytically intractable and we
        have to use an approximation, such as Monte Carlo methods or variational inference.
        In this paper, we focus on variational inference. In variational inference,
        we approximate the posterior with a variational family of distributions q(z
        ; \u03b8), parameterized by \u03b8. Typically, we choose the variational parameters
        \u03b8 that minimize the Kullback-Leibler (KL) divergence between q(z ; \u03b8)
        and p(z|x). This minimization is equivalent to maximizing the evidence lower
        bound (ELBO) [Jordan et al., 1999], L(\u03b8) = Eq(z ;\u03b8) [f(z)] +H[q(z
        ; \u03b8)], f(z) := log p(x, z), H[q(z ; \u03b8)] := Eq(z ;\u03b8)[\u2212
        log q(z ; \u03b8)]. (1) When the model and variational family satisfy conjugacy
        requirements, we can use coordinate ascent to find a local optimum of the
        ELBO [Ghahramani and Beal, 2001, Blei et al., 2016]. If the conjugacy requirements
        are not satisfied, a common approach is to build a Monte Carlo estimator of
        the gradient of the ELBO [Paisley et al., 2012, Ranganath et al., 2014, Salimans
        and Knowles, 2013]. Empirically, the reparameterization trick has been shown
        to be beneficial over direct Monte Carlo estimation of the gradient using
        the score fuction estimator [Rezende et al., 2014, Kingma and Welling, 2014,
        Titsias and L\u00e1zaro-Gredilla, 2014, Fan et al., 2015]. However, it is
        not generally applicable, it requires that: (i) the latent variables z are
        continuous; and (ii) we can simulate from q(z ; \u03b8) as follows, z = h(\u03b5,
        \u03b8), with \u03b5 \u223c s(\u03b5). (2) Here, s(\u03b5) is a distribution
        that does not depend on the variational parameters; it is typically a standard
        normal or a standard uniform. Further, h(\u03b5, \u03b8) is differentiable
        with respect to \u03b8. Using (2), we can move the derivative inside the expectation
        and rewrite the gradient of the ELBO as \u2207\u03b8L(\u03b8) = Es(\u03b5)
        [\u2207zf(h(\u03b5, \u03b8))\u2207\u03b8h(\u03b5, \u03b8)] +\u2207\u03b8H[q(z
        ; \u03b8)]. \u2217Corresponding author: christian.a.naesseth@liu.se Advances
        in Approximate Bayesian Inference (NIPS 2016 Workshop), Barcelona, Spain.
        Algorithm 1 Reparameterized Rejection Sampling Input: target q(z ; \u03b8),
        proposal r(z ; \u03b8), and constant M\u03b8, with q(z ; \u03b8) \u2264M\u03b8r(z
        ; \u03b8) Output: \u03b5 such that h(\u03b5, \u03b8) \u223c q(z ; \u03b8)
        1: i\u2190 0 2: repeat 3: i\u2190 i+ 1 4: Propose \u03b5i \u223c s(\u03b5)
        5: Simulate ui \u223c U [0, 1] 6: until ui < q(h(\u03b5i,\u03b8) ;\u03b8)
        M\u03b8r(h(\u03b5i,\u03b8) ;\u03b8) 7: return \u03b5i 6 4 2 0 2 4 6 \u03b5
        10 10 10 10 10 10 10 10 10", "venue": "", "year": 2016, "referenceCount":
        33, "citationCount": 7, "influentialCitationCount": 1, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Mathematics"], "s2FieldsOfStudy":
        [{"category": "Mathematics", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate":
        "2016-10-18", "journal": {"volume": "", "name": ""}, "authors": [{"authorId":
        "2328322", "name": "C. A. Naesseth"}, {"authorId": "144211059", "name": "Francisco
        J. R. Ruiz"}, {"authorId": "2342841", "name": "Scott W. Linderman"}, {"authorId":
        "1796335", "name": "D. Blei"}]}, {"paperId": "2a91c778288f67c856041447f350a1a022fc6554",
        "externalIds": {"DBLP": "conf/icml/MandtHB16", "ArXiv": "1602.02666", "MAG":
        "2259695933", "CorpusId": 354863}, "corpusId": 354863, "publicationVenue":
        {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29", "name": "International Conference
        on Machine Learning", "type": "conference", "alternate_names": ["ICML", "Int
        Conf Mach Learn"], "url": "https://icml.cc/"}, "url": "https://www.semanticscholar.org/paper/2a91c778288f67c856041447f350a1a022fc6554",
        "title": "A Variational Analysis of Stochastic Gradient Algorithms", "abstract":
        "Stochastic Gradient Descent (SGD) is an important algorithm in machine learning.
        With constant learning rates, it is a stochastic process that, after an initial
        phase of convergence, generates samples from a stationary distribution. We
        show that SGD with constant rates can be effectively used as an approximate
        posterior inference algorithm for probabilistic modeling. Specifically, we
        show how to adjust the tuning parameters of SGD such as to match the resulting
        stationary distribution to the posterior. This analysis rests on interpreting
        SGD as a continuous-time stochastic process and then minimizing the Kullback-Leibler
        divergence between its stationary distribution and the target posterior. (This
        is in the spirit of variational inference.) In more detail, we model SGD as
        a multivariate Ornstein-Uhlenbeck process and then use properties of this
        process to derive the optimal parameters. This theoretical framework also
        connects SGD to modern scalable inference algorithms; we analyze the recently
        proposed stochastic gradient Fisher scoring under this perspective. We demonstrate
        that SGD with properly chosen constant rates gives a new way to optimize hyperparameters
        in probabilistic models.", "venue": "International Conference on Machine Learning",
        "year": 2016, "referenceCount": 33, "citationCount": 126, "influentialCitationCount":
        11, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Mathematics",
        "Computer Science"], "s2FieldsOfStudy": [{"category": "Mathematics", "source":
        "external"}, {"category": "Computer Science", "source": "external"}, {"category":
        "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
        "Conference"], "publicationDate": "2016-02-08", "journal": {"pages": "354-363"},
        "authors": [{"authorId": "1783468", "name": "S. Mandt"}, {"authorId": "28552618",
        "name": "M. Hoffman"}, {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId":
        "30691d2a4eb1a6e88116c357e95b49f9573bcdae", "externalIds": {"MAG": "2962994101",
        "ArXiv": "1603.00788", "DBLP": "journals/corr/KucukelbirTRGB16", "CorpusId":
        9593520}, "corpusId": 9593520, "publicationVenue": {"id": "c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
        "name": "Journal of machine learning research", "type": "journal", "alternate_names":
        ["Journal of Machine Learning Research", "J mach learn res", "J Mach Learn
        Res"], "issn": "1532-4435", "alternate_issns": ["1533-7928"], "url": "http://www.ai.mit.edu/projects/jmlr/",
        "alternate_urls": ["http://jmlr.csail.mit.edu/", "http://www.jmlr.org/", "http://portal.acm.org/affiliated/jmlr"]},
        "url": "https://www.semanticscholar.org/paper/30691d2a4eb1a6e88116c357e95b49f9573bcdae",
        "title": "Automatic Differentiation Variational Inference", "abstract": "Probabilistic
        modeling is iterative. A scientist posits a simple model, fits it to her data,
        refines it according to her analysis, and repeats. However, fitting complex
        models to large data is a bottleneck in this process. Deriving algorithms
        for new models can be both mathematically and computationally challenging,
        which makes it difficult to efficiently cycle through the steps. To this end,
        we develop automatic differentiation variational inference (ADVI). Using our
        method, the scientist only provides a probabilistic model and a dataset, nothing
        else. ADVI automatically derives an efficient variational inference algorithm,
        freeing the scientist to refine and explore many models. ADVI supports a broad
        class of models-no conjugacy assumptions are required. We study ADVI across
        ten different models and apply it to a dataset with millions of observations.
        ADVI is integrated into Stan, a probabilistic programming system; it is available
        for immediate use.", "venue": "Journal of machine learning research", "year":
        2016, "referenceCount": 85, "citationCount": 607, "influentialCitationCount":
        99, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science",
        "source": "external"}, {"category": "Mathematics", "source": "external"},
        {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
        ["JournalArticle"], "publicationDate": "2016-03-02", "journal": {"volume":
        "18", "pages": "14:1-14:45", "name": "J. Mach. Learn. Res."}, "authors": [{"authorId":
        "3081817", "name": "A. Kucukelbir"}, {"authorId": "47497262", "name": "Dustin
        Tran"}, {"authorId": "2615814", "name": "R. Ranganath"}, {"authorId": "144389145",
        "name": "A. Gelman"}, {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId":
        "3b41bb3a470fbfba54283331f31256cc09f0f37e", "externalIds": {"MAG": "2963264509",
        "DBLP": "journals/corr/RanganathPEB16", "ArXiv": "1608.02158", "CorpusId":
        2103507}, "corpusId": 2103507, "publicationVenue": {"id": "6171bcff-8306-41c7-af12-fa1d87117cf1",
        "name": "Machine Learning in Health Care", "type": "conference", "alternate_names":
        ["MLHC", "Mach Learn Health Care"], "url": "http://mucmd.org"}, "url": "https://www.semanticscholar.org/paper/3b41bb3a470fbfba54283331f31256cc09f0f37e",
        "title": "Deep Survival Analysis", "abstract": "The electronic health record
        (EHR) provides an unprecedented opportunity to build actionable tools to support
        physicians at the point of care. In this paper, we investigate survival analysis
        in the context of EHR data. We introduce deep survival analysis, a hierarchical
        generative approach to survival analysis. It departs from previous approaches
        in two primary ways: (1) all observations, including covariates, are modeled
        jointly conditioned on a rich latent structure; and (2) the observations are
        aligned by their failure time, rather than by an arbitrary time zero as in
        traditional survival analysis. Further, it (3) scalably handles heterogeneous
        (continuous and discrete) data types that occur in the EHR. We validate deep
        survival analysis model by stratifying patients according to risk of developing
        coronary heart disease (CHD). Specifically, we study a dataset of 313,000
        patients corresponding to 5.5 million months of observations. When compared
        to the clinically validated Framingham CHD risk score, deep survival analysis
        is significantly superior in stratifying patients according to their risk.",
        "venue": "Machine Learning in Health Care", "year": 2016, "referenceCount":
        33, "citationCount": 156, "influentialCitationCount": 7, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
        "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
        {"category": "Mathematics", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
        "publicationDate": "2016-08-06", "journal": {"pages": "101-114"}, "authors":
        [{"authorId": "2615814", "name": "R. Ranganath"}, {"authorId": "3237761",
        "name": "A. Perotte"}, {"authorId": "2763493", "name": "No\u00e9mie Elhadad"},
        {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId": "434954d40776c87d8b354677ac393cb121f5c80b",
        "externalIds": {"DBLP": "conf/icml/WangKB17", "MAG": "2739530031", "CorpusId":
        6335393}, "corpusId": 6335393, "publicationVenue": {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
        "name": "International Conference on Machine Learning", "type": "conference",
        "alternate_names": ["ICML", "Int Conf Mach Learn"], "url": "https://icml.cc/"},
        "url": "https://www.semanticscholar.org/paper/434954d40776c87d8b354677ac393cb121f5c80b",
        "title": "Robust Probabilistic Modeling with Bayesian Data Reweighting", "abstract":
        "Probabilistic models analyze data by relying on a set of assumptions. Data
        that exhibit deviations from these assumptions can undermine inference and
        prediction quality. Robust models offer protection against mismatch between
        a model''s assumptions and reality. We propose a way to systematically detect
        and mitigate mismatch of a large class of probabilistic models. The idea is
        to raise the likelihood of each observation to a weight and then to infer
        both the latent variables and the weights from data. Inferring the weights
        allows a model to identify observations that match its assumptions and down-weight
        others. This enables robust inference and improves predictive accuracy. We
        study four different forms of mismatch with reality, ranging from missing
        latent groups to structure misspecification. A Poisson factorization analysis
        of the Movielens 1M dataset shows the benefits of this approach in a practical
        scenario.", "venue": "International Conference on Machine Learning", "year":
        2016, "referenceCount": 37, "citationCount": 68, "influentialCitationCount":
        7, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science",
        "source": "external"}, {"category": "Mathematics", "source": "external"},
        {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
        ["JournalArticle", "Conference"], "publicationDate": "2016-06-13", "journal":
        {"pages": "3646-3655"}, "authors": [{"authorId": "2108734693", "name": "Yixin
        Wang"}, {"authorId": "3081817", "name": "A. Kucukelbir"}, {"authorId": "1796335",
        "name": "D. Blei"}]}, {"paperId": "4e88a9e28c4a4a4804700809df13ab822ac6a6ad",
        "externalIds": {"DOI": "10.5860/choice.48-2727", "CorpusId": 18108416}, "corpusId":
        18108416, "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/4e88a9e28c4a4a4804700809df13ab822ac6a6ad",
        "title": "Bayesian Nonparametrics I", "abstract": "We begin by discussing
        the central problem of model selection, and quickly illustrate how Bayesian
        nonparametrics can help us with that problem and a lot more. We briefly introduce
        the notion of random measures, before reviewing the Chinese restaurant process
        (CRP) and infinite mixture models. We then formally define the Dirichlet process,
        demonstrate its properties as a random measure, and then derive the CRP from
        the definition of a Dirichlet process. We conclude with the stick-breaking
        process, another construction of the Dirichlet process, but did not have time
        to derive it.", "venue": "", "year": 2016, "referenceCount": 0, "citationCount":
        434, "influentialCitationCount": 54, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category": "Computer Science",
        "source": "s2-fos-model"}], "publicationTypes": ["Review"], "publicationDate":
        null, "journal": null, "authors": [{"authorId": "1796335", "name": "D. Blei"}]},
        {"paperId": "56b874006900ade34feb39f1317ebbe6c0b0e1d2", "externalIds": {"MAG":
        "2563531440", "DBLP": "conf/emnlp/ChaneyWCB16", "ACL": "D16-1122", "DOI":
        "10.18653/v1/D16-1122", "CorpusId": 14928092}, "corpusId": 14928092, "publicationVenue":
        {"id": "41bf9ed3-85b3-4c90-b015-150e31690253", "name": "Conference on Empirical
        Methods in Natural Language Processing", "type": "conference", "alternate_names":
        ["Empir Method Nat Lang Process", "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process", "EMNLP"], "url": "https://www.aclweb.org/portal/emnlp"},
        "url": "https://www.semanticscholar.org/paper/56b874006900ade34feb39f1317ebbe6c0b0e1d2",
        "title": "Detecting and Characterizing Events", "abstract": "Significant events
        are characterized by interactions between entities (such as countries, organizations,
        or individuals) that deviate from typical interaction patterns. Analysts,
        including historians, political scientists, and journalists, commonly read
        large quantities of text to construct an accurate picture of when and where
        an event happened, who was involved, and in what ways. In this paper, we present
        the Capsule model for analyzing documents to detect and characterize events
        of potential significance. Specifically, we develop a model based on topic
        modeling that distinguishes between topics that describe \u201cbusiness as
        usual\u201d and topics that deviate from these patterns. To demonstrate this
        model, we analyze a corpus of over two million U.S. State Department cables
        from the 1970s. We provide an open-source implementation of an inference algorithm
        for the model and a pipeline for exploring its results.", "venue": "Conference
        on Empirical Methods in Natural Language Processing", "year": 2016, "referenceCount":
        40, "citationCount": 19, "influentialCitationCount": 0, "isOpenAccess": true,
        "openAccessPdf": {"url": "https://doi.org/10.18653/v1/d16-1122", "status":
        null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
        "Computer Science", "source": "external"}, {"category": "Computer Science",
        "source": "s2-fos-model"}, {"category": "Sociology", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2016-11-01",
        "journal": {"pages": "1142-1152"}, "authors": [{"authorId": "2771308", "name":
        "A. Chaney"}, {"authorId": "1831395", "name": "Hanna M. Wallach"}, {"authorId":
        "144131078", "name": "M. Connelly"}, {"authorId": "1796335", "name": "D. Blei"}]},
        {"paperId": "6476e3738935f89c0e5d7b300ea77b6cd203966c", "externalIds": {"CorpusId":
        250453123}, "corpusId": 250453123, "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/6476e3738935f89c0e5d7b300ea77b6cd203966c",
        "title": "Correlated Random Measures: Appendix", "abstract": null, "venue":
        "", "year": 2016, "referenceCount": 11, "citationCount": 0, "influentialCitationCount":
        0, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy":
        [{"category": "Mathematics", "source": "s2-fos-model"}], "publicationTypes":
        null, "publicationDate": null, "journal": null, "authors": [{"authorId": "2615814",
        "name": "R. Ranganath"}, {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId":
        "6f24d7a6e1c88828e18d16c6db20f5329f6a6827", "externalIds": {"MAG": "3101380508",
        "DBLP": "journals/corr/BleiKM16", "ArXiv": "1601.00670", "DOI": "10.1080/01621459.2017.1285773",
        "CorpusId": 3554631}, "corpusId": 3554631, "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
        "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url":
        "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/6f24d7a6e1c88828e18d16c6db20f5329f6a6827",
        "title": "Variational Inference: A Review for Statisticians", "abstract":
        "ABSTRACTOne of the core problems of modern statistics is to approximate difficult-to-compute
        probability densities. This problem is especially important in Bayesian statistics,
        which frames all inference about unknown quantities as a calculation involving
        the posterior density. In this article, we review variational inference (VI),
        a method from machine learning that approximates probability densities through
        optimization. VI has been used in many applications and tends to be faster
        than classical methods, such as Markov chain Monte Carlo sampling. The idea
        behind VI is to first posit a family of densities and then to find a member
        of that family which is close to the target density. Closeness is measured
        by Kullback\u2013Leibler divergence. We review the ideas behind mean-field
        variational inference, discuss the special case of VI applied to exponential
        family models, present a full example with a Bayesian mixture of Gaussians,
        and derive a variant that uses stochastic optimization to scale up to massive
        data...", "venue": "arXiv.org", "year": 2016, "referenceCount": 178, "citationCount":
        3576, "influentialCitationCount": 448, "isOpenAccess": true, "openAccessPdf":
        {"url": "http://arxiv.org/pdf/1601.00670", "status": null}, "fieldsOfStudy":
        ["Mathematics", "Computer Science"], "s2FieldsOfStudy": [{"category": "Mathematics",
        "source": "external"}, {"category": "Computer Science", "source": "external"},
        {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
        ["JournalArticle", "Review"], "publicationDate": "2016-01-04", "journal":
        {"volume": "abs/1601.00670", "name": "ArXiv"}, "authors": [{"authorId": "1796335",
        "name": "D. Blei"}, {"authorId": "3081817", "name": "A. Kucukelbir"}, {"authorId":
        "40411909", "name": "Jon D. McAuliffe"}]}, {"paperId": "70454f5b45f77cd8ad34cd88b300f8ae92c8e978",
        "externalIds": {"CorpusId": 263158991}, "corpusId": 263158991, "publicationVenue":
        null, "url": "https://www.semanticscholar.org/paper/70454f5b45f77cd8ad34cd88b300f8ae92c8e978",
        "title": "Multilingual Topic Models for Unaligned Text", "abstract": "We develop
        the multilingual topic model for unaligned text (MuTo), a probabilistic model
        of text that is designed to analyze corpora composed of documents in two languages.
        From these documents, MuTo uses stochastic EM to simultaneously discover both
        a matching between the languages and multilingual latent topics. We demonstrate
        that MuTo is able to find shared topics on real-world multilingual corpora,
        successfully pairing related documents across languages. MuTo provides a new
        framework for creating multilingual topic models without needing carefully
        curated parallel corpora and allows applications built using the topic model
        formalism to be applied to a much wider class of corpora. Topic models are
        a powerful formalism for unsupervised analysis of corpora [1, 8]. They are
        an important tool in information retrieval [27], sentiment analysis [25],
        and collaborative filtering [18]. When interpreted as a mixed membership model,
        similar assumptions have been successfully applied to vision [6], population
        survey analysis [4], and genetics [5]. In this work, we build on latent Dirichlet
        allocation (LDA) [2], a generative, probabilistic topic model of text. LDA
        assumes that documents have a distribution over topics and that these topics
        are distributions over the vocabulary. Posterior inference discovers the topics
        that best explain a corpus; the uncovered topics tend to reflect thematically
        consistent patterns of words [8]. The goal of this paper is to find topics
        that express thematic coherence across multiple languages. LDA can capture
        coherence in a single language because semantically similar words tend to
        be used in similar contexts. This is not the case in multilingual corpora.
        For example, even though \u201cHund\u201d and \u201chound\u201d are orthographically
        similar and have nearly identical meanings in German and English (i.e., \u201cdog\u201d),
        they will likely not appear in similar contexts because almost all documents
        are written in a single language. Consequently, a topic model fit on a bilingual
        corpus reveals coherent topics but bifurcates the topic space between the
        two languages (Table 1). In order to build coherent topics across languages,
        there must be some connection to tie the languages together. Previous multilingual
        topic models connect the languages by assuming parallelism at either the sentence
        level [28] or document level [13, 23, 19]. Many parallel corpora are available,
        but they represent a small fraction of corpora. They also tend to be relatively
        well annotated and understood, making them less suited for unsupervised methods
        like LDA. A topic model on unaligned text in multiple languages would allow
        the exciting applications developed for monolingual topics models to be applied
        to a broader class of corpora and would help monolingual users to explore
        and understand multilingual corpora. We propose the MUltilingual TOpic model
        for unaligned text (MUTO). MUTO does not assume that it is given any explicit
        parallelism but instead discovers a parallelism at the vocabulary level. To
        find this parallelism, the model assumes that similar themes and ideas appear
        in both languages. For example, if the word \u201cHund\u201d appears in the
        German side of the corpus, \u201chound\u201d or \u201cdog\u201d should appear
        somewhere on the English side. The assumption that similar terms will appear
        in similar contexts has also been used to build lexicons from nonparallel
        but comparable corpora. What makes contexts similar can be evaluated through
        such measures as cooccurrence [20, 24] or tf-idf [7]. Although the emphasis
        of our work is on building consistent topic spaces and not the task of building
        dictionaries per se, good translations are required to find consistent topics.
        However, we can build on successful techniques at building lexicons across
        languages. This paper is organized as follows. We detail the model and its
        assumptions in Section 1, develop a stochastic expectation maximization (EM)
        inference procedure in Section 2, discuss the corpora and other linguistic
        resources necessary to evaluate the model in Section 3, and evaluate the performance
        of the model in Section 4.", "venue": "", "year": 2016, "referenceCount":
        28, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category":
        "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["Review"],
        "publicationDate": null, "journal": null, "authors": [{"authorId": "2240779865",
        "name": "Jordan L. Boyd-Graber"}, {"authorId": "1796335", "name": "D. Blei"}]},
        {"paperId": "7346ef92076885eb6b59e55027c32cb1fa142a06", "externalIds": {"DOI":
        "10.1080/01621459.2016.1245071", "CorpusId": 219596752}, "corpusId": 219596752,
        "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/7346ef92076885eb6b59e55027c32cb1fa142a06",
        "title": "Comment", "abstract": "Blei, D. M., Griffiths, T. L., and Jordan,
        M. I. (2010), \u201cThe Nested Chinese Restaurant Process and Bayesian Nonparametric
        Inference of Topic Hierarchies,\u201d Journal of the ACM (JACM), 57, 7. [1407]
        Pauca, V. P., Piper, J., and Plemmons, R. J. (2006), \u201cNonnegativeMatrix
        Factorization for Spectral Data Analysis,\u201d Linear Algebra and Its Applications,
        416, 29\u201347. [1407] Pauca, V. P., Shahnaz, F., Berry, M. W., and Plemmons,
        R. J. (2004), \u201cText Mining Using Non-Negative Matrix Factorizations,\u201d
        in SDM (Vol. 4), SIAM, pp. 452\u2013456. [1407] Price, B. S., Geyer, C. J.,
        and Rothman, A. J. (2015), \u201cRidge Fusion in Statistical Learning,\u201d
        Journal of Computational and Graphical Statistics, 24, 439\u2013454. [1407]",
        "venue": "", "year": 2016, "referenceCount": 23, "citationCount": 0, "influentialCitationCount":
        0, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
        null, "publicationDate": "2016-10-01", "journal": {"volume": "111", "pages":
        "1408 - 1410", "name": "Journal of the American Statistical Association"},
        "authors": [{"authorId": "1796335", "name": "D. Blei"}]}, {"paperId": "787ffb182e0555691d1c90047e16b8f3ff49bf0b",
        "externalIds": {"DBLP": "conf/nips/RuizTB16", "ArXiv": "1610.02287", "MAG":
        "2531823172", "CorpusId": 1145971}, "corpusId": 1145971, "publicationVenue":
        {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd", "name": "Neural Information
        Processing Systems", "type": "conference", "alternate_names": ["Neural Inf
        Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"}, "url": "https://www.semanticscholar.org/paper/787ffb182e0555691d1c90047e16b8f3ff49bf0b",
        "title": "The Generalized Reparameterization Gradient", "abstract": "The reparameterization
        gradient has become a widely used method to obtain Monte Carlo gradients to
        optimize the variational objective. However, this technique does not easily
        apply to commonly used distributions such as beta or gamma without further
        approximations, and most practical applications of the reparameterization
        gradient fit Gaussian distributions. In this paper, we introduce the generalized
        reparameterization gradient, a method that extends the reparameterization
        gradient to a wider class of variational distributions. Generalized reparameterizations
        use invertible transformations of the latent variables which lead to transformed
        distributions that weakly depend on the variational parameters. This results
        in new Monte Carlo gradients that combine reparameterization gradients and
        score function gradients. We demonstrate our approach on variational inference
        for two complex probabilistic models. The generalized reparameterization is
        effective: even a single sample from the variational distribution is enough
        to obtain a low-variance gradient.", "venue": "Neural Information Processing
        Systems", "year": 2016, "referenceCount": 34, "citationCount": 152, "influentialCitationCount":
        14, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Mathematics",
        "Computer Science"], "s2FieldsOfStudy": [{"category": "Mathematics", "source":
        "external"}, {"category": "Computer Science", "source": "external"}, {"category":
        "Computer Science", "source": "s2-fos-model"}, {"category": "Mathematics",
        "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"],
        "publicationDate": "2016-10-07", "journal": {"pages": "460-468"}, "authors":
        [{"authorId": "144211059", "name": "Francisco J. R. Ruiz"}, {"authorId": "1722732",
        "name": "Michalis K. Titsias"}, {"authorId": "1796335", "name": "D. Blei"}]},
        {"paperId": "79a970ad49d35173f3b789995de8237775b675ff", "externalIds": {"ArXiv":
        "1610.08466", "MAG": "2545581354", "CorpusId": 88522159}, "corpusId": 88522159,
        "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/79a970ad49d35173f3b789995de8237775b675ff",
        "title": "Recurrent switching linear dynamical systems", "abstract": "Many
        natural systems, such as neurons firing in the brain or basketball teams traversing
        a court, give rise to time series data with complex, nonlinear dynamics. We
        can gain insight into these systems by decomposing the data into segments
        that are each explained by simpler dynamic units. Building on switching linear
        dynamical systems (SLDS), we present a new model class that not only discovers
        these dynamical units, but also explains how their switching behavior depends
        on observations or continuous latent states. These \"recurrent\" switching
        linear dynamical systems provide further insight by discovering the conditions
        under which each unit is deployed, something that traditional SLDS models
        fail to do. We leverage recent algorithmic advances in approximate inference
        to make Bayesian inference in these models easy, fast, and scalable.", "venue":
        "", "year": 2016, "referenceCount": 22, "citationCount": 48, "influentialCitationCount":
        8, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science",
        "source": "external"}, {"category": "Mathematics", "source": "external"},
        {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
        null, "publicationDate": "2016-10-26", "journal": {"volume": "", "name": "arXiv:
        Machine Learning"}, "authors": [{"authorId": "2342841", "name": "Scott W.
        Linderman"}, {"authorId": "144360230", "name": "Andrew C. Miller"}, {"authorId":
        "1722180", "name": "Ryan P. Adams"}, {"authorId": "1796335", "name": "D. Blei"},
        {"authorId": "1763504", "name": "L. Paninski"}, {"authorId": "143945326",
        "name": "Matthew J. Johnson"}]}, {"paperId": "89a16eb847e5039fe5d9c6372ab45145400c9aa1",
        "externalIds": {"DBLP": "conf/recsys/LiangACB16", "MAG": "2508504774", "DOI":
        "10.1145/2959100.2959182", "CorpusId": 1196797}, "corpusId": 1196797, "publicationVenue":
        {"id": "61275a16-1e0d-479f-ac4e-f295310761f0", "name": "ACM Conference on
        Recommender Systems", "type": "conference", "alternate_names": ["Conf Recomm
        Syst", "RecSys", "ACM Conf Recomm Syst", "Conference on Recommender Systems"],
        "url": "http://recsys.acm.org/"}, "url": "https://www.semanticscholar.org/paper/89a16eb847e5039fe5d9c6372ab45145400c9aa1",
        "title": "Factorization Meets the Item Embedding: Regularizing Matrix Factorization
        with Item Co-occurrence", "abstract": "Matrix factorization (MF) models and
        their extensions are standard in modern recommender systems. MF models decompose
        the observed user-item interaction matrix into user and item latent factors.
        In this paper, we propose a co-factorization model, CoFactor, which jointly
        decomposes the user-item interaction matrix and the item-item co-occurrence
        matrix with shared item latent factors. For each pair of items, the co-occurrence
        matrix encodes the number of users that have consumed both items. CoFactor
        is inspired by the recent success of word embedding models (e.g., word2vec)
        which can be interpreted as factorizing the word co-occurrence matrix. We
        show that this model significantly improves the performance over MF models
        on several datasets with little additional computational overhead. We provide
        qualitative results that explain how CoFactor improves the quality of the
        inferred factors and characterize the circumstances where it provides the
        most significant improvements.", "venue": "ACM Conference on Recommender Systems",
        "year": 2016, "referenceCount": 26, "citationCount": 245, "influentialCitationCount":
        27, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
        "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["Book", "JournalArticle"], "publicationDate": "2016-09-07",
        "journal": {"name": "Proceedings of the 10th ACM Conference on Recommender
        Systems"}, "authors": [{"authorId": "1702877", "name": "Dawen Liang"}, {"authorId":
        "2124555", "name": "Jaan Altosaar"}, {"authorId": "1778839", "name": "Laurent
        Charlin"}, {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId": "9574b281c8d4879aceec428e3cc3d6714efc989a",
        "externalIds": {"ArXiv": "1606.03860", "DBLP": "journals/corr/WangKB16", "MAG":
        "2951322470", "CorpusId": 16672419}, "corpusId": 16672419, "publicationVenue":
        {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names":
        ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/9574b281c8d4879aceec428e3cc3d6714efc989a",
        "title": "Reweighted Data for Robust Probabilistic Models", "abstract": "Probabilistic
        models analyze data by relying on a set of assumptions. When a model performs
        poorly, we challenge its assumptions. This approach has led to myriad hand-crafted
        robust models; they offer protection against small deviations from their assumptions.
        We propose a simple way to systematically mitigate mismatch of a large class
        of probabilistic models. The idea is to raise the likelihood of each observation
        to a weight. Inferring these weights allows a model to identify observations
        that match its assumptions; down-weighting others enables robust inference
        and improved predictive accuracy. We study four different forms of model mismatch,
        ranging from missing latent groups to structure misspecification. A Poisson
        factorization analysis of the Movielens dataset shows the benefits of reweighting
        in a real data scenario.", "venue": "arXiv.org", "year": 2016, "referenceCount":
        30, "citationCount": 11, "influentialCitationCount": 0, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
        "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
        {"category": "Mathematics", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}, {"category": "Economics", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle"], "publicationDate": "2016-06-13", "journal":
        {"volume": "abs/1606.03860", "name": "ArXiv"}, "authors": [{"authorId": "2108734693",
        "name": "Yixin Wang"}, {"authorId": "3081817", "name": "A. Kucukelbir"}, {"authorId":
        "1796335", "name": "D. Blei"}]}, {"paperId": "97fcee01d9777372a3d8966ab7cd4c17e3a6a5ca",
        "externalIds": {"MAG": "2963477172", "DBLP": "journals/corr/RanganathATB16",
        "ArXiv": "1610.09033", "CorpusId": 736758}, "corpusId": 736758, "publicationVenue":
        {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd", "name": "Neural Information
        Processing Systems", "type": "conference", "alternate_names": ["Neural Inf
        Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"}, "url": "https://www.semanticscholar.org/paper/97fcee01d9777372a3d8966ab7cd4c17e3a6a5ca",
        "title": "Operator Variational Inference", "abstract": "Variational inference
        is an umbrella term for algorithms which cast Bayesian inference as optimization.
        Classically, variational inference uses the Kullback-Leibler divergence to
        define the optimization. Though this divergence has been widely used, the
        resultant posterior approximation can suffer from undesirable statistical
        properties. To address this, we reexamine variational inference from its roots
        as an optimization problem. We use operators, or functions of functions, to
        design variational objectives. As one example, we design a variational objective
        with a Langevin-Stein operator. We develop a black box algorithm, operator
        variational inference (OPVI), for optimizing any operator objective. Importantly,
        operators enable us to make explicit the statistical and computational tradeoffs
        for variational inference. We can characterize different properties of variational
        objectives, such as objectives that admit data subsampling---allowing inference
        to scale to massive data---as well as objectives that admit variational programs---a
        rich class of posterior approximations that does not require a tractable density.
        We illustrate the benefits of OPVI on a mixture model and a generative model
        of images.", "venue": "Neural Information Processing Systems", "year": 2016,
        "referenceCount": 33, "citationCount": 106, "influentialCitationCount": 16,
        "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science",
        "source": "external"}, {"category": "Mathematics", "source": "external"},
        {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
        ["JournalArticle", "Conference"], "publicationDate": "2016-10-27", "journal":
        {"volume": "abs/1610.09033", "name": "ArXiv"}, "authors": [{"authorId": "2615814",
        "name": "R. Ranganath"}, {"authorId": "47497262", "name": "Dustin Tran"},
        {"authorId": "2124555", "name": "Jaan Altosaar"}, {"authorId": "1796335",
        "name": "D. Blei"}]}, {"paperId": "ad4ddc21a0cf5cc2a124a289d0cbb821f5aaeaab",
        "externalIds": {"MAG": "2413827825", "CorpusId": 64121017}, "corpusId": 64121017,
        "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/ad4ddc21a0cf5cc2a124a289d0cbb821f5aaeaab",
        "title": "32nd International Conference on Machine Learning : (ICML 2015)
        : Lile, France, 6-11 July 2015", "abstract": null, "venue": "", "year": 2016,
        "referenceCount": 0, "citationCount": 5, "influentialCitationCount": 0, "isOpenAccess":
        false, "openAccessPdf": null, "fieldsOfStudy": ["Engineering"], "s2FieldsOfStudy":
        [{"category": "Engineering", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate":
        null, "journal": {"volume": "", "name": ""}, "authors": [{"authorId": "144570279",
        "name": "F. Bach"}, {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId":
        "b4c8cc15005058146d0b66a8f4490a323d5bbe7f", "externalIds": {"ArXiv": "1611.00328",
        "CorpusId": 204897808}, "corpusId": 204897808, "publicationVenue": null, "url":
        "https://www.semanticscholar.org/paper/b4c8cc15005058146d0b66a8f4490a323d5bbe7f",
        "title": "The $\\chi$-Divergence for Approximate Inference", "abstract": "Variational
        inference enables Bayesian analysis for complex probabilistic models with
        massive data sets. It posits a family of approximating distributions and finds
        the member closest to the posterior. While successful, variational inference
        methods can run into pathologies; for example, they typically underestimate
        posterior uncertainty. In this paper we propose CHIVI, a complementary algorithm
        to traditional variational inference. CHIVI is a black box algorithm that
        minimizes the \u03c7-divergence from the posterior to the family of approximating
        distributions and provides an upper bound of the model evidence. We studied
        CHIVI in several scenarios. On Bayesian probit regression and Gaussian process
        classification it yielded better classification error rates than expectation
        propagation (EP) and classical variational inference (VI). When modeling basketball
        data with a Cox process, it gave better estimates of posterior uncertainty.
        Finally, we show how to use the CHIVI upper bound and classical VI lower bound
        to sandwich estimate the model evidence.", "venue": "", "year": 2016, "referenceCount":
        44, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Mathematics", "Computer Science"],
        "s2FieldsOfStudy": [{"category": "Mathematics", "source": "external"}, {"category":
        "Computer Science", "source": "external"}, {"category": "Computer Science",
        "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": "2016-11-01",
        "journal": null, "authors": [{"authorId": "38638095", "name": "Adji Bousso
        Dieng"}, {"authorId": "47497262", "name": "Dustin Tran"}, {"authorId": "2615814",
        "name": "R. Ranganath"}, {"authorId": "143855009", "name": "J. Paisley"},
        {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId": "cec8412a03aa42778ef0f4daf61c5c8c81ac9cb1",
        "externalIds": {"ArXiv": "1608.00778", "DBLP": "conf/nips/RudolphRMB16", "MAG":
        "2475611006", "DOI": "10.7916/D8NZ9RHT", "CorpusId": 2232639}, "corpusId":
        2232639, "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
        "name": "Neural Information Processing Systems", "type": "conference", "alternate_names":
        ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
        "url": "https://www.semanticscholar.org/paper/cec8412a03aa42778ef0f4daf61c5c8c81ac9cb1",
        "title": "Exponential Family Embeddings", "abstract": "Word embeddings are
        a powerful approach for capturing semantic similarity among terms in a vocabulary.
        In this paper, we develop exponential family embeddings, a class of methods
        that extends the idea of word embeddings to other types of high-dimensional
        data. As examples, we studied neural data with real-valued observations, count
        data from a market basket analysis, and ratings data from a movie recommendation
        system. The main idea is to model each observation conditioned on a set of
        other observations. This set is called the context, and the way the context
        is defined is a modeling choice that depends on the problem. In language the
        context is the surrounding words; in neuroscience the context is close-by
        neurons; in market basket data the context is other items in the shopping
        cart. Each type of embedding model defines the context, the exponential family
        of conditional distributions, and how the latent embedding vectors are shared
        across data. We infer the embeddings with a scalable algorithm based on stochastic
        gradient descent. On all three applications\u2014neural activity of zebrafish,
        users'' shopping behavior, and movie ratings\u2014we found exponential family
        embedding models to be more effective than other types of dimension reduction.
        They better reconstruct held-out data and find interesting qualitative structure.",
        "venue": "Neural Information Processing Systems", "year": 2016, "referenceCount":
        29, "citationCount": 102, "influentialCitationCount": 11, "isOpenAccess":
        false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
        "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
        {"category": "Mathematics", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
        "Conference"], "publicationDate": "2016-08-02", "journal": {"pages": "478-486"},
        "authors": [{"authorId": "144016056", "name": "Maja R. Rudolph"}, {"authorId":
        "144211059", "name": "Francisco J. R. Ruiz"}, {"authorId": "1783468", "name":
        "S. Mandt"}, {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId": "d51fefa58ceafe9bebddf03f2379842068dae3bc",
        "externalIds": {"ArXiv": "1610.09787", "DBLP": "journals/corr/TranKDRLB16",
        "MAG": "2539792571", "CorpusId": 1849801}, "corpusId": 1849801, "publicationVenue":
        {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names":
        ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/d51fefa58ceafe9bebddf03f2379842068dae3bc",
        "title": "Edward: A library for probabilistic modeling, inference, and criticism",
        "abstract": "Probabilistic modeling is a powerful approach for analyzing empirical
        information. We describe Edward, a library for probabilistic modeling. Edward''s
        design reflects an iterative process pioneered by George Box: build a model
        of a phenomenon, make inferences about the model given data, and criticize
        the model''s fit to the data. Edward supports a broad class of probabilistic
        models, efficient algorithms for inference, and many techniques for model
        criticism. The library builds on top of TensorFlow to support distributed
        training and hardware such as GPUs. Edward enables the development of complex
        probabilistic models and their algorithms at a massive scale.", "venue": "arXiv.org",
        "year": 2016, "referenceCount": 83, "citationCount": 279, "influentialCitationCount":
        30, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Mathematics",
        "Computer Science"], "s2FieldsOfStudy": [{"category": "Mathematics", "source":
        "external"}, {"category": "Computer Science", "source": "external"}, {"category":
        "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
        "publicationDate": "2016-10-31", "journal": {"volume": "abs/1610.09787", "name":
        "ArXiv"}, "authors": [{"authorId": "47497262", "name": "Dustin Tran"}, {"authorId":
        "3081817", "name": "A. Kucukelbir"}, {"authorId": "34205035", "name": "Adji
        B. Dieng"}, {"authorId": "144016056", "name": "Maja R. Rudolph"}, {"authorId":
        "1702877", "name": "Dawen Liang"}, {"authorId": "1796335", "name": "D. Blei"}]},
        {"paperId": "d56117c84a559670f5c4e6e2993a2f117dc34be9", "externalIds": {"DBLP":
        "conf/nips/DiengTRPB17", "MAG": "2951160318", "CorpusId": 123960}, "corpusId":
        123960, "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
        "name": "Neural Information Processing Systems", "type": "conference", "alternate_names":
        ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
        "url": "https://www.semanticscholar.org/paper/d56117c84a559670f5c4e6e2993a2f117dc34be9",
        "title": "Variational Inference via \\chi Upper Bound Minimization", "abstract":
        "Variational inference (VI) is widely used as an efficient alternative to
        Markov chain Monte Carlo. It posits a family of approximating distributions
        $q$ and finds the closest member to the exact posterior $p$. Closeness is
        usually measured via a divergence $D(q || p)$ from $q$ to $p$. While successful,
        this approach also has problems. Notably, it typically leads to underestimation
        of the posterior variance. In this paper we propose CHIVI, a black-box variational
        inference algorithm that minimizes $D_{\\chi}(p || q)$, the $\\chi$-divergence
        from $p$ to $q$. CHIVI minimizes an upper bound of the model evidence, which
        we term the $\\chi$ upper bound (CUBO). Minimizing the CUBO leads to improved
        posterior uncertainty, and it can also be used with the classical VI lower
        bound (ELBO) to provide a sandwich estimate of the model evidence. We study
        CHIVI on three models: probit regression, Gaussian process classification,
        and a Cox process model of basketball plays. When compared to expectation
        propagation and classical VI, CHIVI produces better error rates and more accurate
        estimates of posterior variance.", "venue": "Neural Information Processing
        Systems", "year": 2016, "referenceCount": 45, "citationCount": 72, "influentialCitationCount":
        11, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science",
        "source": "external"}, {"category": "Mathematics", "source": "external"},
        {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
        ["JournalArticle", "Conference"], "publicationDate": "2016-11-01", "journal":
        {"pages": "2732-2741"}, "authors": [{"authorId": "38638095", "name": "Adji
        Bousso Dieng"}, {"authorId": "47497262", "name": "Dustin Tran"}, {"authorId":
        "2615814", "name": "R. Ranganath"}, {"authorId": "143855009", "name": "J.
        Paisley"}, {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId": "d952453fc406a13ddac04c8d408a556c37f7baa1",
        "externalIds": {"MAG": "2394568797", "ArXiv": "1605.07604", "DBLP": "journals/corr/KucukelbirB16",
        "CorpusId": 9178692}, "corpusId": 9178692, "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
        "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url":
        "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/d952453fc406a13ddac04c8d408a556c37f7baa1",
        "title": "Posterior Dispersion Indices", "abstract": "Probabilistic modeling
        is cyclical: we specify a model, infer its posterior, and evaluate its performance.
        Evaluation drives the cycle, as we revise our model based on how it performs.
        This requires a metric. Traditionally, predictive accuracy prevails. Yet,
        predictive accuracy does not tell the whole story. We propose to evaluate
        a model through posterior dispersion. The idea is to analyze how each datapoint
        fares in relation to posterior uncertainty around the hidden structure. We
        propose a family of posterior dispersion indices (PDI) that capture this idea.
        A PDI identifies rich patterns of model mismatch in three real data examples:
        voting preferences, supermarket shopping, and population genetics.", "venue":
        "arXiv.org", "year": 2016, "referenceCount": 34, "citationCount": 0, "influentialCitationCount":
        0, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Mathematics",
        "Computer Science"], "s2FieldsOfStudy": [{"category": "Mathematics", "source":
        "external"}, {"category": "Computer Science", "source": "external"}, {"category":
        "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
        "publicationDate": "2016-05-24", "journal": {"volume": "abs/1605.07604", "name":
        "ArXiv"}, "authors": [{"authorId": "3081817", "name": "A. Kucukelbir"}, {"authorId":
        "1796335", "name": "D. Blei"}]}, {"paperId": "da92a6855f3b7a57b2f799d3c106bc3371504974",
        "externalIds": {"CorpusId": 263158989}, "corpusId": 263158989, "publicationVenue":
        null, "url": "https://www.semanticscholar.org/paper/da92a6855f3b7a57b2f799d3c106bc3371504974",
        "title": "Multilingual Topic Models for Unaligned Text", "abstract": "We develop
        the multilingual topic model for unaligned text (MuTo), a probabilistic model
        of text that is designed to analyze corpora composed of documents in two languages.
        From these documents, MuTo uses stochastic EM to simultaneously discover both
        a matching between the languages and multilingual latent topics. We demonstrate
        that MuTo is able to find shared topics on real-world multilingual corpora,
        successfully pairing related documents across languages. MuTo provides a new
        framework for creating multilingual topic models without needing carefully
        curated parallel corpora and allows applications built using the topic model
        formalism to be applied to a much wider class of corpora. Topic models are
        a powerful formalism for unsupervised analysis of corpora [1, 8]. They are
        an important tool in information retrieval [27], sentiment analysis [25],
        and collaborative filtering [18]. When interpreted as a mixed membership model,
        similar assumptions have been successfully applied to vision [6], population
        survey analysis [4], and genetics [5]. In this work, we build on latent Dirichlet
        allocation (LDA) [2], a generative, probabilistic topic model of text. LDA
        assumes that documents have a distribution over topics and that these topics
        are distributions over the vocabulary. Posterior inference discovers the topics
        that best explain a corpus; the uncovered topics tend to reflect thematically
        consistent patterns of words [8]. The goal of this paper is to find topics
        that express thematic coherence across multiple languages. LDA can capture
        coherence in a single language because semantically similar words tend to
        be used in similar contexts. This is not the case in multilingual corpora.
        For example, even though \u201cHund\u201d and \u201chound\u201d are orthographically
        similar and have nearly identical meanings in German and English (i.e., \u201cdog\u201d),
        they will likely not appear in similar contexts because almost all documents
        are written in a single language. Consequently, a topic model fit on a bilingual
        corpus reveals coherent topics but bifurcates the topic space between the
        two languages (Table 1). In order to build coherent topics across languages,
        there must be some connection to tie the languages together. Previous multilingual
        topic models connect the languages by assuming parallelism at either the sentence
        level [28] or document level [13, 23, 19]. Many parallel corpora are available,
        but they represent a small fraction of corpora. They also tend to be relatively
        well annotated and understood, making them less suited for unsupervised methods
        like LDA. A topic model on unaligned text in multiple languages would allow
        the exciting applications developed for monolingual topics models to be applied
        to a broader class of corpora and would help monolingual users to explore
        and understand multilingual corpora. We propose the MUltilingual TOpic model
        for unaligned text (MUTO). MUTO does not assume that it is given any explicit
        parallelism but instead discovers a parallelism at the vocabulary level. To
        find this parallelism, the model assumes that similar themes and ideas appear
        in both languages. For example, if the word \u201cHund\u201d appears in the
        German side of the corpus, \u201chound\u201d or \u201cdog\u201d should appear
        somewhere on the English side. The assumption that similar terms will appear
        in similar contexts has also been used to build lexicons from nonparallel
        but comparable corpora. What makes contexts similar can be evaluated through
        such measures as cooccurrence [20, 24] or tf-idf [7]. Although the emphasis
        of our work is on building consistent topic spaces and not the task of building
        dictionaries per se, good translations are required to find consistent topics.
        However, we can build on successful techniques at building lexicons across
        languages. This paper is organized as follows. We detail the model and its
        assumptions in Section 1, develop a stochastic expectation maximization (EM)
        inference procedure in Section 2, discuss the corpora and other linguistic
        resources necessary to evaluate the model in Section 3, and evaluate the performance
        of the model in Section 4.", "venue": "", "year": 2016, "referenceCount":
        28, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category":
        "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["Review"],
        "publicationDate": null, "journal": null, "authors": [{"authorId": "1796335",
        "name": "D. Blei"}]}, {"paperId": "f2485ee93ef70bc307a418ec904271ebe54af76d",
        "externalIds": {"MAG": "2952927655", "ArXiv": "1606.01855", "DBLP": "journals/corr/ScheinZBW16",
        "CorpusId": 8500579}, "corpusId": 8500579, "publicationVenue": {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
        "name": "International Conference on Machine Learning", "type": "conference",
        "alternate_names": ["ICML", "Int Conf Mach Learn"], "url": "https://icml.cc/"},
        "url": "https://www.semanticscholar.org/paper/f2485ee93ef70bc307a418ec904271ebe54af76d",
        "title": "Bayesian Poisson Tucker Decomposition for Learning the Structure
        of International Relations", "abstract": "We introduce Bayesian Poisson Tucker
        decomposition (BPTD) for modeling country--country interaction event data.
        These data consist of interaction events of the form \"country $i$ took action
        $a$ toward country $j$ at time $t$.\" BPTD discovers overlapping country--community
        memberships, including the number of latent communities. In addition, it discovers
        directed community--community interaction networks that are specific to \"topics\"
        of action types and temporal \"regimes.\" We show that BPTD yields an efficient
        MCMC inference algorithm and achieves better predictive performance than related
        models. We also demonstrate that it discovers interpretable latent structure
        that agrees with our knowledge of international relations.", "venue": "International
        Conference on Machine Learning", "year": 2016, "referenceCount": 40, "citationCount":
        75, "influentialCitationCount": 3, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics",
        "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2016-06-06",
        "journal": {"pages": "2810-2819"}, "authors": [{"authorId": "50545056", "name":
        "Aaron Schein"}, {"authorId": "38026572", "name": "Mingyuan Zhou"}, {"authorId":
        "1796335", "name": "D. Blei"}, {"authorId": "1831395", "name": "Hanna M. Wallach"}]},
        {"paperId": "fccf3f8fea450a32a5c1f85e113127d70ed9ad7c", "externalIds": {"MAG":
        "2953136213", "DBLP": "conf/uai/RuizTB16", "ArXiv": "1603.01140", "CorpusId":
        12111680}, "corpusId": 12111680, "publicationVenue": {"id": "f9af8000-42f8-410d-a622-e8811e41660a",
        "name": "Conference on Uncertainty in Artificial Intelligence", "type": "conference",
        "alternate_names": ["Uncertainty in Artificial Intelligence", "UAI", "Conf
        Uncertain Artif Intell", "Uncertain Artif Intell"], "url": "http://www.auai.org/"},
        "url": "https://www.semanticscholar.org/paper/fccf3f8fea450a32a5c1f85e113127d70ed9ad7c",
        "title": "Overdispersed Black-Box Variational Inference", "abstract": "We
        introduce overdispersed black-box variational inference, a method to reduce
        the variance of the Monte Carlo estimator of the gradient in black-box variational
        inference. Instead of taking samples from the variational distribution, we
        use importance sampling to take samples from an overdispersed distribution
        in the same exponential family as the variational approximation. Our approach
        is general since it can be readily applied to any exponential family distribution,
        which is the typical choice for the variational approximation. We run experiments
        on two non-conjugate probabilistic models to show that our method effectively
        reduces the variance, and the overhead introduced by the computation of the
        proposal parameters and the importance weights is negligible. We find that
        our overdispersed importance sampling scheme provides lower variance than
        black-box variational inference, even when the latter uses twice the number
        of samples. This results in faster convergence of the black-box inference
        procedure.", "venue": "Conference on Uncertainty in Artificial Intelligence",
        "year": 2016, "referenceCount": 42, "citationCount": 46, "influentialCitationCount":
        10, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science",
        "source": "external"}, {"category": "Mathematics", "source": "external"},
        {"category": "Mathematics", "source": "s2-fos-model"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
        "Conference"], "publicationDate": "2016-03-03", "journal": {"volume": "",
        "pages": "647-656", "name": ""}, "authors": [{"authorId": "144211059", "name":
        "Francisco J. R. Ruiz"}, {"authorId": "1722732", "name": "Michalis K. Titsias"},
        {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId": "feb8736dbeb1b6d4049511bd461b2fef37b64a69",
        "externalIds": {"DBLP": "journals/corr/DiengTRPB16", "MAG": "2548555805",
        "CorpusId": 16806427}, "corpusId": 16806427, "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
        "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url":
        "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/feb8736dbeb1b6d4049511bd461b2fef37b64a69",
        "title": "The $\u03c7$-Divergence for Approximate Inference", "abstract":
        "Variational inference enables Bayesian analysis for complex probabilistic
        models with massive data sets. It works by positing a family of distributions
        and finding the member in the family that is closest to the posterior. While
        successful, variational methods can run into pathologies; for example, they
        typically underestimate posterior uncertainty. We propose CHI-VI, a complementary
        algorithm to traditional variational inference with KL($q$ || $p$) and an
        alternative algorithm to EP. CHI-VI is a black box algorithm that minimizes
        the $\\chi$-divergence from the posterior to the family of approximating distributions.
        In EP, only local minimization of the KL($p$ || $q$) objective is possible.
        In contrast, CHI-VI optimizes a well-defined global objective. It directly
        minimizes an upper bound to the model evidence that equivalently minimizes
        the $\\chi$-divergence. In experiments, we illustrate the utility of the upper
        bound for sandwich estimating the model evidence. We also compare several
        probabilistic models and a Cox process for basketball data. We find CHI-VI
        often yields better classification error rates and better posterior uncertainty.",
        "venue": "arXiv.org", "year": 2016, "referenceCount": 40, "citationCount":
        8, "influentialCitationCount": 0, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": ["Mathematics", "Computer Science"], "s2FieldsOfStudy":
        [{"category": "Mathematics", "source": "external"}, {"category": "Computer
        Science", "source": "external"}, {"category": "Computer Science", "source":
        "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
        "2016-11-01", "journal": {"volume": "abs/1611.00328", "name": "ArXiv"}, "authors":
        [{"authorId": "34205035", "name": "Adji B. Dieng"}, {"authorId": "47497262",
        "name": "Dustin Tran"}, {"authorId": "2615814", "name": "R. Ranganath"}, {"authorId":
        "143855009", "name": "J. Paisley"}, {"authorId": "1796335", "name": "D. Blei"}]},
        {"paperId": "19de6b41be976f7035fa8927932a0a89bf359a42", "externalIds": {"MAG":
        "2194761908", "ArXiv": "1510.05078", "DOI": "10.1214/17-BA1090", "CorpusId":
        55815812}, "corpusId": 55815812, "publicationVenue": {"id": "e60aa3db-ffab-43ed-97b5-de22436b3ea4",
        "name": "Bayesian Analysis", "type": "journal", "alternate_names": ["Bayesian
        Anal"], "issn": "1931-6690", "alternate_issns": ["1936-0975"], "url": "http://bayesian.org/BA",
        "alternate_urls": ["http://projecteuclid.org/ba"]}, "url": "https://www.semanticscholar.org/paper/19de6b41be976f7035fa8927932a0a89bf359a42",
        "title": "A General Method for Robust Bayesian Modeling", "abstract": "Robust
        Bayesian models are appealing alternatives to standard models, providing protection
        from data that contains outliers or other departures from the model assumptions.
        Historically, robust models were mostly developed on a case-by-case basis;
        examples include robust linear regression, robust mixture models, and bursty
        topic models. In this paper we develop a general approach to robust Bayesian
        modeling. We show how to turn an existing Bayesian model into a robust model,
        and then develop a generic strategy for computing with it. We use our method
        to study robust variants of several models, including linear regression, Poisson
        regression, logistic regression, and probabilistic topic models. We discuss
        the connections between our methods and existing approaches, especially empirical
        Bayes and James-Stein estimation.", "venue": "Bayesian Analysis", "year":
        2015, "referenceCount": 82, "citationCount": 44, "influentialCitationCount":
        1, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Mathematics",
        "Computer Science"], "s2FieldsOfStudy": [{"category": "Mathematics", "source":
        "external"}, {"category": "Computer Science", "source": "external"}, {"category":
        "Computer Science", "source": "s2-fos-model"}], "publicationTypes": null,
        "publicationDate": "2015-10-17", "journal": {"name": "Bayesian Analysis"},
        "authors": [{"authorId": "2108881999", "name": "Chong Wang"}, {"authorId":
        "1796335", "name": "D. Blei"}]}, {"paperId": "344882b08c412076b22ba9e98885e8dd397ec38f",
        "externalIds": {"CorpusId": 15744802}, "corpusId": 15744802, "publicationVenue":
        null, "url": "https://www.semanticscholar.org/paper/344882b08c412076b22ba9e98885e8dd397ec38f",
        "title": "Continuous-Time Limit of Stochastic Gradient Descent Revisited",
        "abstract": "Stochastic Gradient Descent (SGD) is an important algorithm in
        machine learning. With constant learning rates, it is a stochastic process
        that reaches a stationary distribution. We revisit an analysis of SGD in terms
        of stochastic differential equations in the limit of small constant gradient
        steps. This limit, which we feel is not appreciated in the machine learning
        community, allows us to approximate SGD in terms of a multivariate Ornstein-Uhlenbeck
        process, and hence to compute stationary distributions in closed form. This
        formalism has interesting new implications for machine learning. We consider
        the case where the objective has the interpretation of a log-posterior. Traditional
        theory suggests choosing the learning rate such that the stationary distribution
        approximates a point mass at the optimum, but this can lead to wasted effort
        and overfitting. When the goal is instead to approximate the posterior as
        well as possible, we can derive criteria for optimal minibatch sizes, learning
        rates, and preconditioning matrices.", "venue": "", "year": 2015, "referenceCount":
        20, "citationCount": 19, "influentialCitationCount": 2, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category":
        "Computer Science", "source": "s2-fos-model"}], "publicationTypes": null,
        "publicationDate": null, "journal": null, "authors": [{"authorId": "1783468",
        "name": "S. Mandt"}, {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId":
        "39f249d4095fcb875f0b23655d1e734bd8c71b71", "externalIds": {"DBLP": "journals/corr/CharlinRMB15",
        "ArXiv": "1509.04640", "MAG": "3101752741", "DOI": "10.1145/2792838.2800174",
        "CorpusId": 40922}, "corpusId": 40922, "publicationVenue": {"id": "61275a16-1e0d-479f-ac4e-f295310761f0",
        "name": "ACM Conference on Recommender Systems", "type": "conference", "alternate_names":
        ["Conf Recomm Syst", "RecSys", "ACM Conf Recomm Syst", "Conference on Recommender
        Systems"], "url": "http://recsys.acm.org/"}, "url": "https://www.semanticscholar.org/paper/39f249d4095fcb875f0b23655d1e734bd8c71b71",
        "title": "Dynamic Poisson Factorization", "abstract": "Models for recommender
        systems use latent factors to explain the preferences and behaviors of users
        with respect to a set of items (e.g., movies, books, academic papers). Typically,
        the latent factors are assumed to be static and, given these factors, the
        observed pref- erences and behaviors of users are assumed to be generated
        without order. These assumptions limit the explorative and predictive capabilities
        of such models, since users'' interests and item popularity may evolve over
        time. To address this, we propose dPF, a dynamic matrix factorization model
        based on the recent Poisson factorization model for recommendations. dPF models
        the time evolving latent factors with a Kalman filter and the actions with
        Poisson distributions. We derive a scalable variational inference algorithm
        to infer the latent factors. Finally, we demonstrate dPF on 10 years of user
        click data from arXiv.org, one of the largest repository of scientific papers
        and a formidable source of information about the behavior of scientists. Empirically
        we show performance improvement over both static and, more recently proposed,
        dynamic recommendation models. We also provide a thorough exploration of the
        inferred posteriors over the latent variables.", "venue": "ACM Conference
        on Recommender Systems", "year": 2015, "referenceCount": 22, "citationCount":
        90, "influentialCitationCount": 13, "isOpenAccess": true, "openAccessPdf":
        {"url": "https://arxiv.org/pdf/1509.04640", "status": null}, "fieldsOfStudy":
        ["Computer Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer
        Science", "source": "external"}, {"category": "Mathematics", "source": "external"},
        {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
        ["JournalArticle", "Book"], "publicationDate": "2015-09-15", "journal": {"name":
        "Proceedings of the 9th ACM Conference on Recommender Systems"}, "authors":
        [{"authorId": "1778839", "name": "Laurent Charlin"}, {"authorId": "2615814",
        "name": "R. Ranganath"}, {"authorId": "2059894216", "name": "James McInerney"},
        {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId": "3b93f37e5af2f6f66af33720dc8d0e4de7dc4e65",
        "externalIds": {"MAG": "2950152545", "ArXiv": "1510.07025", "DBLP": "conf/www/LiangCMB16",
        "DOI": "10.1145/2872427.2883090", "CorpusId": 2354755}, "corpusId": 2354755,
        "publicationVenue": {"id": "e07422f9-c065-40c3-a37b-75e98dce79fe", "name":
        "The Web Conference", "type": "conference", "alternate_names": ["Web Conf",
        "WWW"], "url": "http://www.iw3c2.org/"}, "url": "https://www.semanticscholar.org/paper/3b93f37e5af2f6f66af33720dc8d0e4de7dc4e65",
        "title": "Modeling User Exposure in Recommendation", "abstract": "Collaborative
        filtering analyzes user preferences for items (e.g., books, movies, restaurants,
        academic papers) by exploiting the similarity patterns across users. In implicit
        feedback settings, all the items, including the ones that a user did not consume,
        are taken into consideration. But this assumption does not accord with the
        common sense understanding that users have a limited scope and awareness of
        items. For example, a user might not have heard of a certain paper, or might
        live too far away from a restaurant to experience it. In the language of causal
        analysis (Imbens & Rubin, 2015), the assignment mechanism (i.e., the items
        that a user is exposed to) is a latent variable that may change for various
        user/item combinations. In this paper, we propose a new probabilistic approach
        that directly incorporates user exposure to items into collaborative filtering.
        The exposure is modeled as a latent variable and the model infers its value
        from data. In doing so, we recover one of the most successful state-of-the-art
        approaches as a special case of our model (Hu et al. 2008), and provide a
        plug-in method for conditioning exposure on various forms of exposure covariates
        (e.g., topics in text, venue locations). We show that our scalable inference
        algorithm outperforms existing benchmarks in four different domains both with
        and without exposure covariates.", "venue": "The Web Conference", "year":
        2015, "referenceCount": 33, "citationCount": 312, "influentialCitationCount":
        57, "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/1510.07025",
        "status": null}, "fieldsOfStudy": ["Mathematics", "Computer Science"], "s2FieldsOfStudy":
        [{"category": "Mathematics", "source": "external"}, {"category": "Computer
        Science", "source": "external"}, {"category": "Computer Science", "source":
        "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Book", "Conference"],
        "publicationDate": "2015-10-23", "journal": {"name": "Proceedings of the 25th
        International Conference on World Wide Web"}, "authors": [{"authorId": "1702877",
        "name": "Dawen Liang"}, {"authorId": "1778839", "name": "Laurent Charlin"},
        {"authorId": "2059894216", "name": "James McInerney"}, {"authorId": "1796335",
        "name": "D. Blei"}]}, {"paperId": "3bb64c846759e52ce8c9a417fbae32c3b80bd31e",
        "externalIds": {"MAG": "2508111799", "CorpusId": 8105350}, "corpusId": 8105350,
        "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/3bb64c846759e52ce8c9a417fbae32c3b80bd31e",
        "title": "Probabilistic Topic Models and User Behavior", "abstract": "David
        Blei is a Professor of Statistics and Computer Science at Columbia University.
        His research is in statistical machine learning, involving probabilistic topic
        models, Bayesian nonparametric methods, and approximate posterior inference.
        He works on a variety of applications, including text, images, music, social
        networks, user behavior, and scientific data. \n \nDavid earned his Bachelor''s
        degree in Computer Science and Mathematics from Brown University (1997) and
        his PhD in Computer Science from the University of California, Berkeley (2004).
        Before arriving to Columbia, he was an Associate Professor of Computer Science
        at Princeton University. He has received several awards for his research,
        including a Sloan Fellowship (2010), Office of Naval Research Young Investigator
        Award (2011), Presidential Early Career Award for Scientists and Engineers
        (2011), Blavatnik Faculty Award (2013), and ACM-Infosys Foundation Award (2013).",
        "venue": "", "year": 2015, "referenceCount": 0, "citationCount": 2, "influentialCitationCount":
        0, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
        "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": null, "publicationDate": "2015-10-16", "journal": {"volume":
        "", "name": ""}, "authors": [{"authorId": "1796335", "name": "D. Blei"}]},
        {"paperId": "4718f38e709c8203201cf4e226519f985b3d0183", "externalIds": {"MAG":
        "2189421284", "DBLP": "conf/nips/McInerneyRB15", "CorpusId": 967235}, "corpusId":
        967235, "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
        "name": "Neural Information Processing Systems", "type": "conference", "alternate_names":
        ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
        "url": "https://www.semanticscholar.org/paper/4718f38e709c8203201cf4e226519f985b3d0183",
        "title": "The Population Posterior and Bayesian Modeling on Streams", "abstract":
        "Many modern data analysis problems involve inferences from streaming data.
        However, streaming data is not easily amenable to the standard probabilistic
        modeling approaches, which require conditioning on finite data. We develop
        population variational Bayes, a new approach for using Bayesian modeling to
        analyze streams of data. It approximates a new type of distribution, the population
        posterior, which combines the notion of a population distribution of the data
        with Bayesian inference in a probabilistic model. We develop the population
        posterior for latent Dirichlet allocation and Dirichlet process mixtures.
        We study our method with several large-scale data sets.", "venue": "Neural
        Information Processing Systems", "year": 2015, "referenceCount": 28, "citationCount":
        33, "influentialCitationCount": 9, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
        "Computer Science", "source": "external"}, {"category": "Computer Science",
        "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"],
        "publicationDate": "2015-12-07", "journal": {"pages": "1153-1161"}, "authors":
        [{"authorId": "2059894216", "name": "James McInerney"}, {"authorId": "2615814",
        "name": "R. Ranganath"}, {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId":
        "47658bb25ed3ed76a44b6c4e8ff2e20ded61809e", "externalIds": {"MAG": "2971500464",
        "DOI": "10.1007/springerreference_178778", "CorpusId": 16716863}, "corpusId":
        16716863, "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/47658bb25ed3ed76a44b6c4e8ff2e20ded61809e",
        "title": "Bayesian Nonparametric Models", "abstract": null, "venue": "", "year":
        2015, "referenceCount": 0, "citationCount": 0, "influentialCitationCount":
        0, "isOpenAccess": true, "openAccessPdf": {"url": "http://www.gatsby.ucl.ac.uk/~ywteh/research/npbayes/OrbTeh2010a.pdf",
        "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate":
        null, "journal": {"volume": "", "name": ""}, "authors": [{"authorId": "1796335",
        "name": "D. Blei"}]}, {"paperId": "49885ebfd0135dd5399137b0216b629f0ee85ffe",
        "externalIds": {"MAG": "2527592395", "CorpusId": 125088217}, "corpusId": 125088217,
        "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/49885ebfd0135dd5399137b0216b629f0ee85ffe",
        "title": "HVMs extend the applicability of normalizing flows to discrete variables.
        We can also place a distribution over transformations to build an HVM without
        Jacobians (2).", "abstract": null, "venue": "", "year": 2015, "referenceCount":
        3, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Mathematics"], "s2FieldsOfStudy":
        [{"category": "Mathematics", "source": "external"}, {"category": "Mathematics",
        "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": null,
        "journal": {"volume": "", "name": ""}, "authors": [{"authorId": "2615814",
        "name": "R. Ranganath"}, {"authorId": "47497262", "name": "Dustin Tran"},
        {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId": "4ddb48f5e50ecf3595a0ef2517d36d077f968e25",
        "externalIds": {"DBLP": "conf/kdd/ScheinPBW15", "MAG": "1995368504", "ArXiv":
        "1506.03493", "DOI": "10.1145/2783258.2783414", "CorpusId": 14048856}, "corpusId":
        14048856, "publicationVenue": {"id": "a0edb93b-1e95-4128-a295-6b1659149cef",
        "name": "Knowledge Discovery and Data Mining", "type": "conference", "alternate_names":
        ["KDD", "Knowl Discov Data Min"], "url": "http://www.acm.org/sigkdd/"}, "url":
        "https://www.semanticscholar.org/paper/4ddb48f5e50ecf3595a0ef2517d36d077f968e25",
        "title": "Bayesian Poisson Tensor Factorization for Inferring Multilateral
        Relations from Sparse Dyadic Event Counts", "abstract": "We present a Bayesian
        tensor factorization model for inferring latent group structures from dynamic
        pairwise interaction patterns. For decades, political scientists have collected
        and analyzed records of the form \"country i took action a toward country
        j at time t\" - known as dyadic events - in order to form and test theories
        of international relations. We represent these event data as a tensor of counts
        and develop Bayesian Poisson tensor factorization to infer a low-dimensional,
        interpretable representation of their salient patterns. We demonstrate that
        our model''s predictive performance is better than that of standard non-negative
        tensor factorization methods. We also provide a comparison of our variational
        updates to their maximum likelihood counterparts. In doing so, we identify
        a better way to form point estimates of the latent factors than that typically
        used in Bayesian Poisson matrix factorization. Finally, we showcase our model
        as an exploratory analysis tool for political scientists. We show that the
        inferred latent factor matrices capture interpretable multilateral relations
        that both conform to and inform our knowledge of international a airs.", "venue":
        "Knowledge Discovery and Data Mining", "year": 2015, "referenceCount": 40,
        "citationCount": 92, "influentialCitationCount": 12, "isOpenAccess": true,
        "openAccessPdf": {"url": "https://arxiv.org/pdf/1506.03493", "status": null},
        "fieldsOfStudy": ["Mathematics", "Computer Science"], "s2FieldsOfStudy": [{"category":
        "Mathematics", "source": "external"}, {"category": "Computer Science", "source":
        "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle", "Book", "Conference"], "publicationDate":
        "2015-06-10", "journal": {"name": "Proceedings of the 21th ACM SIGKDD International
        Conference on Knowledge Discovery and Data Mining"}, "authors": [{"authorId":
        "50545056", "name": "Aaron Schein"}, {"authorId": "143855009", "name": "J.
        Paisley"}, {"authorId": "1796335", "name": "D. Blei"}, {"authorId": "1831395",
        "name": "Hanna M. Wallach"}]}, {"paperId": "59c8ca723801e441840fdde577f8e33b995d45a4",
        "externalIds": {"MAG": "2951560214", "DBLP": "journals/corr/RudolphEB15",
        "ArXiv": "1506.07504", "DOI": "10.1145/2872427.2883051", "CorpusId": 10159241},
        "corpusId": 10159241, "publicationVenue": {"id": "e07422f9-c065-40c3-a37b-75e98dce79fe",
        "name": "The Web Conference", "type": "conference", "alternate_names": ["Web
        Conf", "WWW"], "url": "http://www.iw3c2.org/"}, "url": "https://www.semanticscholar.org/paper/59c8ca723801e441840fdde577f8e33b995d45a4",
        "title": "Objective Variables for Probabilistic Revenue Maximization in Second-Price
        Auctions with Reserve", "abstract": "Many online companies sell advertisement
        space in second-price auctions with reserve. In this paper, we develop a probabilistic
        method to learn a profitable strategy to set the reserve price. We use historical
        auction data with features to fit a predictor of the best reserve price. This
        problem is delicate - the structure of the auction is such that a reserve
        price set too high is much worse than a reserve price set too low. To address
        this we develop objective variables, an approach for combining probabilistic
        modeling with optimal decision-making. Objective variables are \"hallucinated
        observations\" that transform the revenue maximization task into a regularized
        maximum likelihood estimation problem, which we solve with the EM algorithm.
        This framework enables a variety of prediction mechanisms to set the reserve
        price. As examples, we study objective variable methods with regression, kernelized
        regression, and neural networks on simulated and real data. Our methods outperform
        previous approaches both in terms of scalability and profit.", "venue": "The
        Web Conference", "year": 2015, "referenceCount": 23, "citationCount": 20,
        "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url":
        "https://arxiv.org/pdf/1506.07504", "status": null}, "fieldsOfStudy": ["Mathematics",
        "Computer Science"], "s2FieldsOfStudy": [{"category": "Mathematics", "source":
        "external"}, {"category": "Computer Science", "source": "external"}, {"category":
        "Computer Science", "source": "s2-fos-model"}, {"category": "Economics", "source":
        "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Book", "Conference"],
        "publicationDate": "2015-06-24", "journal": {"name": "Proceedings of the 25th
        International Conference on World Wide Web"}, "authors": [{"authorId": "144016056",
        "name": "Maja R. Rudolph"}, {"authorId": "2136860", "name": "Joseph G. Ellis"},
        {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId": "6fc46455b6294aa2dd5e25d206a259aecdf7065b",
        "externalIds": {"MAG": "2119033610", "PubMedCentral": "4482276", "DBLP": "journals/jamia/PerotteRHBE15",
        "DOI": "10.1093/jamia/ocv024", "CorpusId": 13514714, "PubMed": "25896647"},
        "corpusId": 13514714, "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/6fc46455b6294aa2dd5e25d206a259aecdf7065b",
        "title": "Risk prediction for chronic kidney disease progression using heterogeneous
        electronic health record data and time series analysis", "abstract": "Abstract
        Background As adoption of electronic health records continues to increase,
        there is an opportunity to incorporate clinical documentation as well as laboratory
        values and demographics into risk prediction modeling. Objective The authors
        develop a risk prediction model for chronic kidney disease (CKD) progression
        from stage III to stage IV that includes longitudinal data and features drawn
        from clinical documentation. Methods The study cohort consisted of 2908 primary-care
        clinic patients who had at least three visits prior to January 1, 2013 and
        developed CKD stage III during their documented history. Development and validation
        cohorts were randomly selected from this cohort and the study datasets included
        longitudinal inpatient and outpatient data from these populations. Time series
        analysis (Kalman filter) and survival analysis (Cox proportional hazards)
        were combined to produce a range of risk models. These models were evaluated
        using concordance, a discriminatory statistic. Results A risk model incorporating
        longitudinal data on clinical documentation and laboratory test results (concordance
        0.849) predicts progression from state III CKD to stage IV CKD more accurately
        when compared to a similar model without laboratory test results (concordance
        0.733, P<.001), a model that only considers the most recent laboratory test
        results (concordance 0.819, P < .031) and a model based on estimated glomerular
        filtration rate (concordance 0.779, P < .001). Conclusions A risk prediction
        model that takes longitudinal laboratory test results and clinical documentation
        into consideration can predict CKD progression from stage III to stage IV
        more accurately than three models that do not take all of these variables
        into consideration.", "venue": "J. Am. Medical Informatics Assoc.", "year":
        2015, "referenceCount": 55, "citationCount": 94, "influentialCitationCount":
        4, "isOpenAccess": true, "openAccessPdf": {"url": "https://academic.oup.com/jamia/article-pdf/22/4/872/34145809/ocv024.pdf",
        "status": null}, "fieldsOfStudy": ["Computer Science", "Medicine"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Medicine",
        "source": "external"}, {"category": "Medicine", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle"], "publicationDate": "2015-04-20", "journal":
        {"volume": "22", "pages": "872 - 880", "name": "Journal of the American Medical
        Informatics Association : JAMIA"}, "authors": [{"authorId": "3237761", "name":
        "A. Perotte"}, {"authorId": "2615814", "name": "R. Ranganath"}, {"authorId":
        "20124917", "name": "J. Hirsch"}, {"authorId": "1796335", "name": "D. Blei"},
        {"authorId": "2763493", "name": "No\u00e9mie Elhadad"}]}, {"paperId": "7f50ccd131867f65f3044d20db0a90d6374f7a12",
        "externalIds": {"MAG": "2296242069", "DBLP": "journals/corr/TranBA15", "ArXiv":
        "1506.03159", "CorpusId": 2541061}, "corpusId": 2541061, "publicationVenue":
        {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names":
        ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/7f50ccd131867f65f3044d20db0a90d6374f7a12",
        "title": "Variational inference with copula augmentation", "abstract": "We
        develop a general methodology for variational inference which preserves dependency
        among the latent variables. This is done by augmenting the families of distributions
        used in mean-field and structured approximation with copulas. Copulas allow
        one to separately model the dependency given a factorization of the variational
        distribution, and can guarantee us better approximations to the posterior
        as measured by KL divergence. We show that inference on the augmented distribution
        is highly scalable using stochastic optimization. Furthermore, the addition
        of a copula is generic and can be applied straightforwardly to any inference
        procedure using the original meanfield or structured approach. This reduces
        bias, sensitivity to local optima, sensitivity to hyperparameters, and significantly
        helps characterize and interpret the dependency among the latent variables.",
        "venue": "arXiv.org", "year": 2015, "referenceCount": 30, "citationCount":
        4, "influentialCitationCount": 1, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": ["Mathematics", "Computer Science"], "s2FieldsOfStudy":
        [{"category": "Mathematics", "source": "external"}, {"category": "Computer
        Science", "source": "external"}, {"category": "Computer Science", "source":
        "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
        "2015-06-10", "journal": {"volume": "abs/1506.03159", "name": "ArXiv"}, "authors":
        [{"authorId": "47497262", "name": "Dustin Tran"}, {"authorId": "1796335",
        "name": "D. Blei"}, {"authorId": "2247552", "name": "E. Airoldi"}]}, {"paperId":
        "889bc6066784fa1e2f05a1480f397a77c9c31468", "externalIds": {"MAG": "2952187341",
        "ArXiv": "1507.00720", "DOI": "10.1080/01621459.2016.1260468", "CorpusId":
        88515802}, "corpusId": 88515802, "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/889bc6066784fa1e2f05a1480f397a77c9c31468",
        "title": "Correlated Random Measures", "abstract": "ABSTRACT We develop correlated
        random measures, random measures where the atom weights can exhibit a flexible
        pattern of dependence, and use them to develop powerful hierarchical Bayesian
        nonparametric models. Hierarchical Bayesian nonparametric models are usually
        built from completely random measures, a Poisson-process-based construction
        in which the atom weights are independent. Completely random measures imply
        strong independence assumptions in the corresponding hierarchical model, and
        these assumptions are often misplaced in real-world settings. Correlated random
        measures address this limitation. They model correlation within the measure
        by using a Gaussian process in concert with the Poisson process. With correlated
        random measures, for example, we can develop a latent feature model for which
        we can infer both the properties of the latent features and their dependency
        pattern. We develop several other examples as well. We study a correlated
        random measure model of pairwise count data. We derive an efficient variational
        inference algorithm and show improved predictive performance on large datasets
        of documents, web clicks, and electronic health records. Supplementary materials
        for this article are available online.", "venue": "", "year": 2015, "referenceCount":
        76, "citationCount": 20, "influentialCitationCount": 1, "isOpenAccess": true,
        "openAccessPdf": {"url": "http://arxiv.org/pdf/1507.00720", "status": null},
        "fieldsOfStudy": ["Mathematics", "Computer Science"], "s2FieldsOfStudy": [{"category":
        "Mathematics", "source": "external"}, {"category": "Computer Science", "source":
        "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category":
        "Mathematics", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate":
        "2015-07-02", "journal": {"volume": "113", "pages": "417 - 430", "name": "Journal
        of the American Statistical Association"}, "authors": [{"authorId": "2615814",
        "name": "R. Ranganath"}, {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId":
        "9028e2b0d3ce51d310458c922f747b76f6c46590", "externalIds": {"CorpusId": 260525493},
        "corpusId": 260525493, "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/9028e2b0d3ce51d310458c922f747b76f6c46590",
        "title": "Automatic Differentiation Variational Inference", "abstract": "Modern
        data analysis requires an iterative cycle: in the probabilistic modeling framework,
        a sim1 ple model is fit to the data, and it is refined as we gather more knowledge
        about the data\u2019s hidden 2 structure. However, fitting complex models
        to large datasets is mathematically and computationally 3 challenging. We
        develop an automated tool called automatic differentiation variational inference
        4 (advi). The scientist only provides a probabilistic model and a dataset;
        nothing else. advi auto5 matically derives an efficient algorithm that handles
        both complex models and large datasets. No 6 conjugacy assumptions are required,
        and a broad class ofmodels is supported. We studyadvi across 7 ten different
        models and apply it to a dataset with millions of observations. advi is integrated
        into 8 Stan, a probabilistic programming system; this makes advi available
        for immediate use. 9", "venue": "", "year": 2015, "referenceCount": 52, "citationCount":
        0, "influentialCitationCount": 0, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category": "Computer Science",
        "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": null,
        "journal": null, "authors": [{"authorId": "3081817", "name": "A. Kucukelbir"},
        {"authorId": "1933666", "name": "Dustin Tran"}, {"authorId": "2615814", "name":
        "R. Ranganath"}, {"authorId": "144389145", "name": "A. Gelman"}, {"authorId":
        "1796335", "name": "D. Blei"}]}, {"paperId": "9195ed23cc16b9bfd3528ac94c8271b01320b6d3",
        "externalIds": {"MAG": "1043648093", "ArXiv": "1507.05253", "CorpusId": 88513223},
        "corpusId": 88513223, "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/9195ed23cc16b9bfd3528ac94c8271b01320b6d3",
        "title": "The Population Posterior and Bayesian Inference on Streams", "abstract":
        "Many modern data analysis problems involve inferences from streaming data.
        However, streaming data is not easily amenable to the standard probabilistic
        modeling approaches, which assume that we condition on finite data. We develop
        population variational Bayes, a new approach for using Bayesian modeling to
        analyze streams of data. It approximates a new type of distribution, the population
        posterior, which combines the notion of a population distribution of the data
        with Bayesian inference in a probabilistic model. We study our method with
        latent Dirichlet allocation and Dirichlet process mixtures on several large-scale
        data sets.", "venue": "", "year": 2015, "referenceCount": 30, "citationCount":
        6, "influentialCitationCount": 0, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": ["Mathematics"], "s2FieldsOfStudy": [{"category": "Mathematics",
        "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": null, "publicationDate": "2015-07-19", "journal": {"volume":
        "", "name": "arXiv: Machine Learning"}, "authors": [{"authorId": "2059894216",
        "name": "James McInerney"}, {"authorId": "2615814", "name": "R. Ranganath"},
        {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId": "9f08a5e4360a7823e596ed4567a5f671ec3759a3",
        "externalIds": {"MAG": "2402632249", "DBLP": "conf/uai/RanganathPEB15", "CorpusId":
        1162247}, "corpusId": 1162247, "publicationVenue": {"id": "f9af8000-42f8-410d-a622-e8811e41660a",
        "name": "Conference on Uncertainty in Artificial Intelligence", "type": "conference",
        "alternate_names": ["Uncertainty in Artificial Intelligence", "UAI", "Conf
        Uncertain Artif Intell", "Uncertain Artif Intell"], "url": "http://www.auai.org/"},
        "url": "https://www.semanticscholar.org/paper/9f08a5e4360a7823e596ed4567a5f671ec3759a3",
        "title": "The Survival Filter: Joint Survival Analysis with a Latent Time
        Series", "abstract": "Survival analysis is a core task in applied statistics,
        which models time-to-failure or time-to-event data. In the clinical domain,
        for example, meaningful events are defined as the onset of different diseases
        for a given patient. Survival analysis is limited, however, for analyzing
        modern electronic health records. Patients often have a wide range of diseases,
        and there are complex interactions among the relative risks of different events.
        To this end, we develop the survival filter model, a time-series model for
        joint survival analysis that models multiple patients and multiple diseases.
        We develop a scalable variational inference algorithm and apply our method
        to a large data set of longitudinal patient records. The survival filter gives
        good predictive performance when compared to two baselines and identifies
        clinically meaningful patterns of disease interaction.", "venue": "Conference
        on Uncertainty in Artificial Intelligence", "year": 2015, "referenceCount":
        23, "citationCount": 26, "influentialCitationCount": 0, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
        "Conference"], "publicationDate": "2015-07-12", "journal": {"pages": "742-751"},
        "authors": [{"authorId": "2615814", "name": "R. Ranganath"}, {"authorId":
        "3237761", "name": "A. Perotte"}, {"authorId": "2763493", "name": "No\u00e9mie
        Elhadad"}, {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId": "b2a35b6693c74abe7d42ba13e8b90c1545b569f7",
        "externalIds": {"DBLP": "conf/uai/GopalanHB15", "MAG": "2403286959", "CorpusId":
        1723648}, "corpusId": 1723648, "publicationVenue": {"id": "f9af8000-42f8-410d-a622-e8811e41660a",
        "name": "Conference on Uncertainty in Artificial Intelligence", "type": "conference",
        "alternate_names": ["Uncertainty in Artificial Intelligence", "UAI", "Conf
        Uncertain Artif Intell", "Uncertain Artif Intell"], "url": "http://www.auai.org/"},
        "url": "https://www.semanticscholar.org/paper/b2a35b6693c74abe7d42ba13e8b90c1545b569f7",
        "title": "Scalable Recommendation with Hierarchical Poisson Factorization",
        "abstract": "We develop hierarchical Poisson matrix factorization (HPF), a
        novel method for providing users with high quality recommendations based on
        implicit feedback, such as views, clicks, or purchases. In contrast to existing
        recommendation models, HPF has a number of desirable properties. First, we
        show that HPF more accurately captures the long-tailed user activity found
        in most consumption data by explicitly considering the fact that users have
        finite attention budgets. This leads to better estimates of users'' latent
        preferences, and therefore superior recommendations, compared to competing
        methods. Second, HPF learns these latent factors by only explicitly considering
        positive examples, eliminating the often costly step of generating artificial
        negative examples when fitting to implicit data. Third, HPF is more than just
        one method\u2014 it is the simplest in a class of probabilistic models with
        these properties, and can easily be extended to include more complex structure
        and assumptions. We develop a variational algorithm for approximate posterior
        inference for HPF that scales up to large data sets, and we demonstrate its
        performance on a wide variety of real-world recommendation problems\u2014users
        rating movies, listening to songs, reading scientific papers, and reading
        news articles.", "venue": "Conference on Uncertainty in Artificial Intelligence",
        "year": 2015, "referenceCount": 37, "citationCount": 236, "influentialCitationCount":
        43, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
        "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2015-07-12",
        "journal": {"pages": "326-335"}, "authors": [{"authorId": "2141648", "name":
        "Prem Gopalan"}, {"authorId": "40368603", "name": "J. Hofman"}, {"authorId":
        "1796335", "name": "D. Blei"}]}, {"paperId": "c02717b88beba6eb84e6a407f58f8705de649c5e",
        "externalIds": {"ArXiv": "1506.03431", "MAG": "2950204717", "DBLP": "conf/nips/KucukelbirRGB15",
        "CorpusId": 6072417}, "corpusId": 6072417, "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
        "name": "Neural Information Processing Systems", "type": "conference", "alternate_names":
        ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
        "url": "https://www.semanticscholar.org/paper/c02717b88beba6eb84e6a407f58f8705de649c5e",
        "title": "Automatic Variational Inference in Stan", "abstract": "Variational
        inference is a scalable technique for approximate Bayesian inference. Deriving
        variational inference algorithms requires tedious model-specific calculations;
        this makes it difficult for non-experts to use. We propose an automatic variational
        inference algorithm, automatic differentiation variational inference (ADVI);
        we implement it in Stan (code available), a probabilistic programming system.
        In ADVI the user provides a Bayesian model and a dataset, nothing else. We
        make no conjugacy assumptions and support a broad class of models. The algorithm
        automatically determines an appropriate variational family and optimizes the
        variational objective. We compare ADVI to MCMC sampling across hierarchical
        generalized linear models, nonconjugate matrix factorization, and a mixture
        model. We train the mixture model on a quarter million images. With ADVI we
        can use variational inference on any model we write in Stan.", "venue": "Neural
        Information Processing Systems", "year": 2015, "referenceCount": 31, "citationCount":
        209, "influentialCitationCount": 24, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": ["Mathematics", "Computer Science"], "s2FieldsOfStudy":
        [{"category": "Mathematics", "source": "external"}, {"category": "Computer
        Science", "source": "external"}, {"category": "Computer Science", "source":
        "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate":
        "2015-06-10", "journal": {"pages": "568-576"}, "authors": [{"authorId": "3081817",
        "name": "A. Kucukelbir"}, {"authorId": "2615814", "name": "R. Ranganath"},
        {"authorId": "144389145", "name": "A. Gelman"}, {"authorId": "1796335", "name":
        "D. Blei"}]}, {"paperId": "c8d44c88691ec27e37038db9cbea15d96b06d3af", "externalIds":
        {"MAG": "2963910604", "DBLP": "conf/nips/TranBA15", "CorpusId": 1636406},
        "corpusId": 1636406, "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
        "name": "Neural Information Processing Systems", "type": "conference", "alternate_names":
        ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
        "url": "https://www.semanticscholar.org/paper/c8d44c88691ec27e37038db9cbea15d96b06d3af",
        "title": "Copula variational inference", "abstract": "We develop a general
        variational inference method that preserves dependency among the latent variables.
        Our method uses copulas to augment the families of distributions used in mean-field
        and structured approximations. Copulas model the dependency that is not captured
        by the original variational distribution, and thus the augmented variational
        family guarantees better approximations to the posterior. With stochastic
        optimization, inference on the augmented distribution is scalable. Furthermore,
        our strategy is generic: it can be applied to any inference procedure that
        currently uses the mean-field or structured approach. Copula variational inference
        has many advantages: it reduces bias; it is less sensitive to local optima;
        it is less sensitive to hyperparameters; and it helps characterize and interpret
        the dependency among the latent variables.", "venue": "Neural Information
        Processing Systems", "year": 2015, "referenceCount": 35, "citationCount":
        86, "influentialCitationCount": 5, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics",
        "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2015-06-10",
        "journal": {"pages": "3564-3572"}, "authors": [{"authorId": "47497262", "name":
        "Dustin Tran"}, {"authorId": "1796335", "name": "D. Blei"}, {"authorId": "2247552",
        "name": "E. Airoldi"}]}, {"paperId": "ed032736652ac7e1f36ea17bd253cd1bfdcc3864",
        "externalIds": {"MAG": "2949767141", "DBLP": "journals/corr/TranRB15", "ArXiv":
        "1511.06499", "CorpusId": 661332}, "corpusId": 661332, "publicationVenue":
        {"id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40", "name": "International Conference
        on Learning Representations", "type": "conference", "alternate_names": ["Int
        Conf Learn Represent", "ICLR"], "url": "https://iclr.cc/"}, "url": "https://www.semanticscholar.org/paper/ed032736652ac7e1f36ea17bd253cd1bfdcc3864",
        "title": "Variational Gaussian Process", "abstract": "Abstract: Representations
        offered by deep generative models are fundamentally tied to their inference
        method from data. Variational inference methods require a rich family of approximating
        distributions. We construct the variational Gaussian process (VGP), a Bayesian
        nonparametric model which adapts its shape to match complex posterior distributions.
        The VGP generates approximate posterior samples by generating latent inputs
        and warping them through random non-linear mappings; the distribution over
        random mappings is learned during inference, enabling the transformed outputs
        to adapt to varying complexity. We prove a universal approximation theorem
        for the VGP, demonstrating its representative power for learning any model.
        For inference we present a variational objective inspired by autoencoders
        and perform black box inference over a wide class of models. The VGP achieves
        new state-of-the-art results for unsupervised learning, inferring models such
        as the deep latent Gaussian model and the recently proposed DRAW.", "venue":
        "International Conference on Learning Representations", "year": 2015, "referenceCount":
        51, "citationCount": 160, "influentialCitationCount": 8, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
        "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
        {"category": "Mathematics", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
        "publicationDate": "2015-11-20", "journal": {"volume": "abs/1511.06499", "name":
        "CoRR"}, "authors": [{"authorId": "47497262", "name": "Dustin Tran"}, {"authorId":
        "2615814", "name": "R. Ranganath"}, {"authorId": "1796335", "name": "D. Blei"}]},
        {"paperId": "f31ac36adbd24c43dcd28397081702e98e026b34", "externalIds": {"DBLP":
        "conf/icml/RanganathTB16", "MAG": "2950328408", "ArXiv": "1511.02386", "CorpusId":
        849367}, "corpusId": 849367, "publicationVenue": {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
        "name": "International Conference on Machine Learning", "type": "conference",
        "alternate_names": ["ICML", "Int Conf Mach Learn"], "url": "https://icml.cc/"},
        "url": "https://www.semanticscholar.org/paper/f31ac36adbd24c43dcd28397081702e98e026b34",
        "title": "Hierarchical Variational Models", "abstract": "Black box variational
        inference allows researchers to easily prototype and evaluate an array of
        models. Recent advances allow such algorithms to scale to high dimensions.
        However, a central question remains: How to specify an expressive variational
        distribution that maintains efficient computation? To address this, we develop
        hierarchical variational models (HVMs). HVMs augment a variational approximation
        with a prior on its parameters, which allows it to capture complex structure
        for both discrete and continuous latent variables. The algorithm we develop
        is black box, can be used for any HVM, and has the same computational efficiency
        as the original approximation. We study HVMs on a variety of deep discrete
        latent variable models. HVMs generalize other expressive variational distributions
        and maintains higher fidelity to the posterior.", "venue": "International
        Conference on Machine Learning", "year": 2015, "referenceCount": 60, "citationCount":
        289, "influentialCitationCount": 25, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": ["Mathematics", "Computer Science"], "s2FieldsOfStudy":
        [{"category": "Mathematics", "source": "external"}, {"category": "Computer
        Science", "source": "external"}, {"category": "Computer Science", "source":
        "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate":
        "2015-11-07", "journal": {"volume": "abs/1511.02386", "name": "ArXiv"}, "authors":
        [{"authorId": "2615814", "name": "R. Ranganath"}, {"authorId": "47497262",
        "name": "Dustin Tran"}, {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId":
        "f4a5660fc68339b69289fda1dc4c546d9084748c", "externalIds": {"DBLP": "conf/recsys/ChaneyBE15",
        "MAG": "1984127251", "DOI": "10.1145/2792838.2800193", "CorpusId": 2980873},
        "corpusId": 2980873, "publicationVenue": {"id": "61275a16-1e0d-479f-ac4e-f295310761f0",
        "name": "ACM Conference on Recommender Systems", "type": "conference", "alternate_names":
        ["Conf Recomm Syst", "RecSys", "ACM Conf Recomm Syst", "Conference on Recommender
        Systems"], "url": "http://recsys.acm.org/"}, "url": "https://www.semanticscholar.org/paper/f4a5660fc68339b69289fda1dc4c546d9084748c",
        "title": "A Probabilistic Model for Using Social Networks in Personalized
        Item Recommendation", "abstract": "Preference-based recommendation systems
        have transformed how we consume media. By analyzing usage data, these methods
        uncover our latent preferences for items (such as articles or movies) and
        form recommendations based on the behavior of others with similar tastes.
        But traditional preference-based recommendations do not account for the social
        aspect of consumption, where a trusted friend might point us to an interesting
        item that does not match our typical preferences. In this work, we aim to
        bridge the gap between preference- and social-based recommendations. We develop
        social Poisson factorization (SPF), a probabilistic model that incorporates
        social network information into a traditional factorization method; SPF introduces
        the social aspect to algorithmic recommendation. We develop a scalable algorithm
        for analyzing data with SPF, and demonstrate that it outperforms competing
        methods on six real-world datasets; data sources include a social reader and
        Etsy.", "venue": "ACM Conference on Recommender Systems", "year": 2015, "referenceCount":
        39, "citationCount": 177, "influentialCitationCount": 12, "isOpenAccess":
        false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["Book", "JournalArticle"],
        "publicationDate": "2015-09-16", "journal": {"name": "Proceedings of the 9th
        ACM Conference on Recommender Systems"}, "authors": [{"authorId": "2771308",
        "name": "A. Chaney"}, {"authorId": "1796335", "name": "D. Blei"}, {"authorId":
        "1397398770", "name": "Tina Eliassi-Rad"}]}, {"paperId": "0affd3ffca1707f6800be3c98202daa3b1912d47",
        "externalIds": {"MAG": "1161645253", "DOI": "10.1201/B17520", "CorpusId":
        60956643}, "corpusId": 60956643, "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/0affd3ffca1707f6800be3c98202daa3b1912d47",
        "title": "Handbook of Mixed Membership Models and Their Applications", "abstract":
        "In response to scientific needs for more diverse and structured explanations
        of statistical data, researchers have discovered how to model individual data
        points as belonging to multiple groups. Handbook of Mixed Membership Models
        and Their Applications shows you how to use these flexible modeling tools
        to uncover hidden patterns in modern high-dimensional multivariate data. It
        explores the use of the models in various application settings, including
        survey data, population genetics, text analysis, image processing and annotation,
        and molecular biology. Through examples using real data sets, youll discover
        how to characterize complex multivariate data in: Studies involving genetic
        databases Patterns in the progression of diseases and disabilities Combinations
        of topics covered by text documents Political ideology or electorate voting
        patterns Heterogeneous relationships in networks, and much more The handbook
        spans more than 20 years of the editors and contributors statistical work
        in the field. Top researchers compare partial and mixed membership models,
        explain how to interpret mixed membership, delve into factor analysis, and
        describe nonparametric mixed membership models. They also present extensions
        of the mixed membership model for text analysis, sequence and rank data, and
        network data as well as semi-supervised mixed membership models.", "venue":
        "", "year": 2014, "referenceCount": 0, "citationCount": 50, "influentialCitationCount":
        2, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
        "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["Review"], "publicationDate": "2014-11-06", "journal":
        {"volume": "", "name": ""}, "authors": [{"authorId": "2247552", "name": "E.
        Airoldi"}, {"authorId": "1796335", "name": "D. Blei"}, {"authorId": "1868490",
        "name": "E. Erosheva"}, {"authorId": "1684961", "name": "S. Fienberg"}]},
        {"paperId": "209e1d36f36b8e7db3147b0e424874e54df9012e", "externalIds": {"ArXiv":
        "1411.1810", "DBLP": "conf/aistats/MandtMARB16", "CorpusId": 5763520}, "corpusId":
        5763520, "publicationVenue": {"id": "2d136b11-c2b5-484b-b008-7f4a852fd61e",
        "name": "International Conference on Artificial Intelligence and Statistics",
        "type": "conference", "alternate_names": ["AISTATS", "Int Conf Artif Intell
        Stat"]}, "url": "https://www.semanticscholar.org/paper/209e1d36f36b8e7db3147b0e424874e54df9012e",
        "title": "Variational Tempering", "abstract": "Variational inference (VI)
        combined with data subsampling enables approximate posterior inference over
        large data sets, but suffers from poor local optima. We first formulate a
        deterministic annealing approach for the generic class of conditionally conjugate
        exponential family models. This approach uses a decreasing temperature parameter
        which deterministically deforms the objective during the course of the optimization.
        A well-known drawback to this annealing approach is the choice of the cooling
        schedule. We therefore introduce variational tempering, a variational algorithm
        that introduces a temperature latent variable to the model. In contrast to
        related work in the Markov chain Monte Carlo literature, this algorithm results
        in adaptive annealing schedules. Lastly, we develop local variational tempering,
        which assigns a latent temperature to each data point; this allows for dynamic
        annealing that varies across data. Compared to the traditional VI, all proposed
        approaches find improved predictive likelihoods on held-out data.", "venue":
        "International Conference on Artificial Intelligence and Statistics", "year":
        2014, "referenceCount": 22, "citationCount": 50, "influentialCitationCount":
        3, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science",
        "source": "external"}, {"category": "Mathematics", "source": "external"},
        {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
        ["JournalArticle"], "publicationDate": "2014-11-07", "journal": {"pages":
        "704-712"}, "authors": [{"authorId": "1783468", "name": "S. Mandt"}, {"authorId":
        "2059894216", "name": "James McInerney"}, {"authorId": "2021452", "name":
        "Farhan Abrol"}, {"authorId": "2615814", "name": "R. Ranganath"}, {"authorId":
        "1796335", "name": "D. Blei"}]}, {"paperId": "2a1186207dd9e25b67c89067f0913bb0847d791f",
        "externalIds": {"MAG": "2186654215", "CorpusId": 1027856}, "corpusId": 1027856,
        "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/2a1186207dd9e25b67c89067f0913bb0847d791f",
        "title": "Real-time Topic Models for Crisis Counseling", "abstract": "The
        proliferation of text-based crisis counseling platforms in recent months has
        opened an exciting opportunity for applied machine learning to (1) provide
        practical assistance for human counselors who provide emotional and practical
        support and (2) analyze counselor-caller interactions to build a landscape
        of the distribution of mental health issues experienced by callers on an unprecedented
        scale. We present Fathom, a natural language interface powered by topic models
        to help crisis counselors on Crisis Text Line, a new 911-like crisis hotline
        that takes calls via text messaging. We apply a mixed-initiative labeled LDA
        model to analyze counselor-caller conversations and use them to power real-time
        visualizations aimed at mitigating counselor cognitive load. We discuss three
        key aspects of crisis counseling and why topic models are suitable for mining
        this phenomenon. We propose new variants of topic models inspired by the practical
        constraints posed by their real-time deployment.", "venue": "", "year": 2014,
        "referenceCount": 10, "citationCount": 12, "influentialCitationCount": 0,
        "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Engineering"],
        "s2FieldsOfStudy": [{"category": "Engineering", "source": "external"}, {"category":
        "Computer Science", "source": "s2-fos-model"}], "publicationTypes": null,
        "publicationDate": null, "journal": {"volume": "", "name": ""}, "authors":
        [{"authorId": "2782537", "name": "Karthik Dinakar"}, {"authorId": "2771308",
        "name": "A. Chaney"}, {"authorId": "116288375", "name": "Henry Lieberman Mit"},
        {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId": "33ad5a55670f346716e974221f03330dac6f6c9f",
        "externalIds": {"DBLP": "reference/snam/X14xm", "DOI": "10.1007/978-1-4614-6170-8_100159",
        "CorpusId": 5734144}, "corpusId": 5734144, "publicationVenue": null, "url":
        "https://www.semanticscholar.org/paper/33ad5a55670f346716e974221f03330dac6f6c9f",
        "title": "Statistical Models", "abstract": null, "venue": "Encyclopedia of
        Social Network Analysis and Mining", "year": 2014, "referenceCount": 3, "citationCount":
        435, "influentialCitationCount": 66, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
        "Computer Science", "source": "external"}, {"category": "Computer Science",
        "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": null,
        "journal": {"pages": "2056"}, "authors": [{"authorId": "1796335", "name":
        "D. Blei"}]}, {"paperId": "35409ac9b33f2cc074257018c47fe9c373acb55f", "externalIds":
        {"MAG": "2124450745", "DBLP": "conf/nips/HoulsbyB14", "CorpusId": 16541728},
        "corpusId": 16541728, "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
        "name": "Neural Information Processing Systems", "type": "conference", "alternate_names":
        ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
        "url": "https://www.semanticscholar.org/paper/35409ac9b33f2cc074257018c47fe9c373acb55f",
        "title": "A Filtering Approach to Stochastic Variational Inference", "abstract":
        "Stochastic variational inference (SVI) uses stochastic optimization to scale
        up Bayesian computation to massive data. We present an alternative perspective
        on SVI as approximate parallel coordinate ascent. SVI trades-off bias and
        variance to step close to the unknown true coordinate optimum given by batch
        variational Bayes (VB). We define a model to automate this process. The model
        infers the location of the next VB optimum from a sequence of noisy realizations.
        As a consequence of this construction, we update the variational parameters
        using Bayes rule, rather than a hand-crafted optimization schedule. When our
        model is a Kalman filter this procedure can recover the original SVI algorithm
        and SVI with adaptive steps. We may also encode additional assumptions in
        the model, such as heavy-tailed noise. By doing so, our algorithm outperforms
        the original SVI schedule and a state-of-the-art adaptive SVI algorithm in
        two diverse domains.", "venue": "Neural Information Processing Systems", "year":
        2014, "referenceCount": 27, "citationCount": 3, "influentialCitationCount":
        0, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Mathematics",
        "Computer Science"], "s2FieldsOfStudy": [{"category": "Mathematics", "source":
        "external"}, {"category": "Computer Science", "source": "external"}, {"category":
        "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
        "Conference"], "publicationDate": "2014-12-08", "journal": {"pages": "2114-2122"},
        "authors": [{"authorId": "2815290", "name": "N. Houlsby"}, {"authorId": "1796335",
        "name": "D. Blei"}]}, {"paperId": "5cee21ae4605a330d9977164523be4b865df6ebd",
        "externalIds": {"DBLP": "journals/corr/AbrolMRB14", "MAG": "2407003635", "CorpusId":
        124905151}, "corpusId": 124905151, "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
        "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url":
        "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/5cee21ae4605a330d9977164523be4b865df6ebd",
        "title": "Deterministic Annealing for Stochastic Variational Inference", "abstract":
        "Stochastic variational inference (SVI) maps posterior inference in latent
        variable models to nonconvex stochastic optimization. While they enable approximate
        posterior inference for many otherwise intractable models, variational inference
        methods suffer from local optima. We introduce deterministic annealing for
        SVI to overcome this issue. We introduce a temperature parameter that deterministically
        deforms the objective, and then reduce this parameter over the course of the
        optimization. Initially it encourages high entropy variational distributions,
        which we find eases convergence to better optima. We test our method with
        Latent Dirichlet Allocation on three large corpora. Compared to SVI, we show
        improved predictive likelihoods on held-out data.", "venue": "arXiv.org",
        "year": 2014, "referenceCount": 25, "citationCount": 5, "influentialCitationCount":
        0, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
        "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle"], "publicationDate": "2014-11-07", "journal":
        {"volume": "abs/1411.1810", "name": "ArXiv"}, "authors": [{"authorId": "2021452",
        "name": "Farhan Abrol"}, {"authorId": "1783468", "name": "S. Mandt"}, {"authorId":
        "2615814", "name": "R. Ranganath"}, {"authorId": "1796335", "name": "D. Blei"}]},
        {"paperId": "6cec1861b1717d3ec1f558d48d40ae23e5d42b1e", "externalIds": {"DBLP":
        "reference/crc/AiroldiBEF14", "MAG": "2397077215", "DOI": "10.1201/b17520-3",
        "CorpusId": 37863552}, "corpusId": 37863552, "publicationVenue": null, "url":
        "https://www.semanticscholar.org/paper/6cec1861b1717d3ec1f558d48d40ae23e5d42b1e",
        "title": "Introduction to Mixed Membership Models and Methods", "abstract":
        "1.1 Historical Developments . . . . . . . . . . . . . . . . . . . . . . .
        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
        . . . . . . . . 3 1.2 A General Formulation for Mixed Membership Models .
        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
        1.3 Advantages of Mixed Membership Models in Applied Statistics . . . . .
        . . . . . . . . . . . . . . . . . . . . . . . . 6 1.4 Theoretical Issues with
        Mixed Membership Models . . . . . . . . . . . . . . . . . . . . . . . . .
        . . . . . . . . . . . . . . . . 8 1.4.1 General Issues Inherent to Mixtures
        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
        . . . . . . . . . . . . 8 References . . . . . . . . . . . . . . . . . . .
        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
        . . . . . . . . . . . . . . . . . . . . . . . . . 10", "venue": "Handbook
        of Mixed Membership Models and Their Applications", "year": 2014, "referenceCount":
        54, "citationCount": 34, "influentialCitationCount": 2, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
        "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
        {"category": "Mathematics", "source": "external"}, {"category": "Business",
        "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": "2014-11-06",
        "journal": {"pages": "3-13"}, "authors": [{"authorId": "2247552", "name":
        "E. Airoldi"}, {"authorId": "1796335", "name": "D. Blei"}, {"authorId": "1868490",
        "name": "E. Erosheva"}, {"authorId": "1684961", "name": "S. Fienberg"}]},
        {"paperId": "74771dad32d0b2ab67d901ddcb74d48d6649139b", "externalIds": {"MAG":
        "2806641228", "DBLP": "conf/aistats/GopalanRRB14", "CorpusId": 17736539},
        "corpusId": 17736539, "publicationVenue": {"id": "2d136b11-c2b5-484b-b008-7f4a852fd61e",
        "name": "International Conference on Artificial Intelligence and Statistics",
        "type": "conference", "alternate_names": ["AISTATS", "Int Conf Artif Intell
        Stat"]}, "url": "https://www.semanticscholar.org/paper/74771dad32d0b2ab67d901ddcb74d48d6649139b",
        "title": "Bayesian Nonparametric Poisson Factorization for Recommendation
        Systems", "abstract": "We develop a Bayesian nonparametric Poisson factorization
        model for recommendation systems. Poisson factorization implicitly models
        each user\u2019s limited budget of attention (or money) that allows consumption
        of only a small subset of the available items. In our Bayesian nonparametric
        variant, the number of latent components is theoretically unbounded and eectively
        estimated when computing a posterior with observed user behavior data. To
        approximate the posterior, we develop an ecient variational inference algorithm.
        It adapts the dimensionality of the latent components to the data, only requires
        iteration over the user/item pairs that have been rated, and has computational
        complexity on the same order as for a parametric model with xed dimensionality.
        We studied our model and algorithm with large realworld data sets of user-movie
        preferences. Our model eases the computational burden of searching for the
        number of latent components and gives better predictive performance than its
        parametric counterpart.", "venue": "International Conference on Artificial
        Intelligence and Statistics", "year": 2014, "referenceCount": 35, "citationCount":
        96, "influentialCitationCount": 9, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": ["Mathematics", "Computer Science"], "s2FieldsOfStudy":
        [{"category": "Mathematics", "source": "external"}, {"category": "Computer
        Science", "source": "external"}, {"category": "Computer Science", "source":
        "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
        "2014-04-02", "journal": {"pages": "275-283"}, "authors": [{"authorId": "2141648",
        "name": "Prem Gopalan"}, {"authorId": "144211059", "name": "Francisco J. R.
        Ruiz"}, {"authorId": "2615814", "name": "R. Ranganath"}, {"authorId": "1796335",
        "name": "D. Blei"}]}, {"paperId": "78db7ecd2c2b56db9f2e8a27a36cc7178bdbe34e",
        "externalIds": {"MAG": "2160380549", "DBLP": "conf/prni/ManningRKTCNB14",
        "DOI": "10.1109/PRNI.2014.6858530", "CorpusId": 15738158}, "corpusId": 15738158,
        "publicationVenue": {"id": "f338e737-60fc-41a5-94e6-ecd507f10bf8", "name":
        "International Workshop on Pattern Recognition in NeuroImaging", "type": "conference",
        "alternate_names": ["International Workshop on Pattern Recognition in Neuroimaging",
        "PRN", "PRNI", "Int Workshop Pattern Recognit Neuroimaging"], "issn": "2330-9989"},
        "url": "https://www.semanticscholar.org/paper/78db7ecd2c2b56db9f2e8a27a36cc7178bdbe34e",
        "title": "Hierarchical topographic factor analysis", "abstract": "Recent work
        has revealed that cognitive processes are often reflected in patterns of functional
        connectivity throughout the brain (for review see [16]). However, examining
        functional connectivity patterns using traditional methods carries a substantial
        computational burden (of computing time and memory). Here we present a technique,
        termed Hierarchical topographic factor analysis, for efficiently discovering
        brain networks in large multi-subject neuroimaging datasets.", "venue": "International
        Workshop on Pattern Recognition in NeuroImaging", "year": 2014, "referenceCount":
        22, "citationCount": 11, "influentialCitationCount": 2, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Psychology",
        "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Review"],
        "publicationDate": "2014-06-04", "journal": {"pages": "1-4", "name": "2014
        International Workshop on Pattern Recognition in Neuroimaging"}, "authors":
        [{"authorId": "2586738", "name": "Jeremy R. Manning"}, {"authorId": "2615814",
        "name": "R. Ranganath"}, {"authorId": "40266569", "name": "Waitsang Keung"},
        {"authorId": "1397312794", "name": "N. Turk-Browne"}, {"authorId": "153564781",
        "name": "J. Cohen"}, {"authorId": "1780319", "name": "K. Norman"}, {"authorId":
        "1796335", "name": "D. Blei"}]}, {"paperId": "7a7d9cf8a6e28da11b71057948975fd179ef34be",
        "externalIds": {"MAG": "326790115", "CorpusId": 17052484}, "corpusId": 17052484,
        "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/7a7d9cf8a6e28da11b71057948975fd179ef34be",
        "title": "Multicanonical Stochastic Variational Inference", "abstract": "Stochastic
        variational inference (SVI) enables approximate posterior inference with large
        data sets for otherwise intractable models, but like all variational inference
        algorithms it suffers from local optima. Deterministic annealing, which we
        formulate here for the generic class of conditionally conjugate exponential
        family models, uses a temperature parameter that deterministically deforms
        the objective, and reduce this parameter over the course of the optimization
        to recover the original variational set-up. A well-known drawback in annealing
        approaches is the choice of the annealing schedule. We therefore introduce
        multicanonical variational inference (MVI), a variational algorithm that operates
        at several annealing temperatures simultaneously. This algorithm gives us
        adaptive annealing schedules. Compared to the traditional SVI algorithm, both
        approaches find improved predictive likelihoods on held-out data, with MVI
        being close to the best-tuned annealing schedule.", "venue": "", "year": 2014,
        "referenceCount": 24, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess":
        false, "openAccessPdf": null, "fieldsOfStudy": ["Mathematics"], "s2FieldsOfStudy":
        [{"category": "Mathematics", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate":
        "2014-11-07", "journal": {"volume": "", "name": "arXiv: Machine Learning"},
        "authors": [{"authorId": "1783468", "name": "S. Mandt"}, {"authorId": "2059894216",
        "name": "James McInerney"}, {"authorId": "2021452", "name": "Farhan Abrol"},
        {"authorId": "2615814", "name": "R. Ranganath"}, {"authorId": "1796335", "name":
        "D. Blei"}]}, {"paperId": "7b22ad7650211047208dfa2cd1cc69cea71851f8", "externalIds":
        {"MAG": "2963986141", "DBLP": "conf/uai/KucukelbirB15", "CorpusId": 7917255},
        "corpusId": 7917255, "publicationVenue": {"id": "f9af8000-42f8-410d-a622-e8811e41660a",
        "name": "Conference on Uncertainty in Artificial Intelligence", "type": "conference",
        "alternate_names": ["Uncertainty in Artificial Intelligence", "UAI", "Conf
        Uncertain Artif Intell", "Uncertain Artif Intell"], "url": "http://www.auai.org/"},
        "url": "https://www.semanticscholar.org/paper/7b22ad7650211047208dfa2cd1cc69cea71851f8",
        "title": "Population Empirical Bayes", "abstract": "Bayesian predictive inference
        analyzes a dataset to make predictions about new observations. When a model
        does not match the data, predictive accuracy suffers. We develop population
        empirical Bayes (POP-EB), a hierarchical framework that explicitly models
        the empirical population distribution as part of Bayesian analysis. We introduce
        a new concept, the latent dataset, as a hierarchical variable and set the
        empirical population as its prior. This leads to a new predictive density
        that mitigates model mismatch. We efficiently apply this method to complex
        models by proposing a stochastic variational inference algorithm, called bumping
        variational inference (BUMP-VI). We demonstrate improved predictive accuracy
        over classical Bayesian inference in three models: a linear regression model
        of health data, a Bayesian mixture model of natural images, and a latent Dirichlet
        allocation topic model of scientific documents.", "venue": "Conference on
        Uncertainty in Artificial Intelligence", "year": 2014, "referenceCount": 34,
        "citationCount": 12, "influentialCitationCount": 1, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
        "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
        {"category": "Mathematics", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
        "Conference"], "publicationDate": "2014-11-02", "journal": {"pages": "444-453"},
        "authors": [{"authorId": "3081817", "name": "A. Kucukelbir"}, {"authorId":
        "1796335", "name": "D. Blei"}]}, {"paperId": "7bcfc73c4fbcedf8f290882ddb3d15c70b1eefcb",
        "externalIds": {"CorpusId": 32279480}, "corpusId": 32279480, "publicationVenue":
        null, "url": "https://www.semanticscholar.org/paper/7bcfc73c4fbcedf8f290882ddb3d15c70b1eefcb",
        "title": "A probabilistic approach to full-brain functional connectivity analyses",
        "abstract": "Recent work suggests that our brains\u2019 sub-structures change
        how they communicate with one another depending on the particular functions
        or computations that our ongoing cognitive processes demand (for review see
        [1]). The standard approach to estimating these so called functional connectivity
        patterns in functional magnetic resonance imaging (fMRI) data is to compute
        the correlation between the time series of (pairs of) voxel activations during
        a particular experimental condition. However, this voxel-based approach carries
        a substantial computational burden (of computing time and memory), which has
        led most researchers to focus their connectivity analyses on a small number
        of pre-selected regions of interest (ROIs). Here we present a technique, termed
        Hierarchical Topographic Factor Analysis (HTFA), for efficiently discovering
        full-brain networks (without pre-selecting ROIs) in large multi-subject neuroimaging
        datasets. HTFA approximates each subject\u2019s full-brain functional connectivity
        network through a smaller number of network nodes. The number of nodes, along
        with their locations, sizes, and activations (over time) are determined in
        an unsupervised manner from the dataset. Because the number of nodes is typically
        substantially smaller than the number of voxels in an fMRI dataset, HTFA can
        be orders of magnitude more efficient than voxel-based functional connectivity
        approaches. Among other benefits, this enables researchers to apply polynomial
        time algorithms (which includes many pattern classification algorithms) to
        full-brain functional connectivity networks without paying the typical huge
        increase in computational time and memory that voxel-based methods demand.
        We show that HTFA recovers the connectivity patterns underlying a synthetic
        dataset, and provide a case study illustrating how HTFA may be used to discover
        full-brain connectivity patterns in real fMRI data.", "venue": "", "year":
        2014, "referenceCount": 28, "citationCount": 0, "influentialCitationCount":
        0, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
        ["Review"], "publicationDate": null, "journal": null, "authors": [{"authorId":
        "2586738", "name": "Jeremy R. Manning"}, {"authorId": "2238306108", "name":
        "Kimberly Stachenfeld"}, {"authorId": "2615814", "name": "R. Ranganath"},
        {"authorId": "6845541", "name": "Alexa Tompary"}, {"authorId": "5193045",
        "name": "Naseem Al-Aidroos"}, {"authorId": "2677273", "name": "N. Turk-Browne"},
        {"authorId": "1780319", "name": "K. Norman"}, {"authorId": "1796335", "name":
        "D. Blei"}]}, {"paperId": "84435b23acdd60cd6219bb7fe90405efa4b85d03", "externalIds":
        {"CorpusId": 17746065}, "corpusId": 17746065, "publicationVenue": null, "url":
        "https://www.semanticscholar.org/paper/84435b23acdd60cd6219bb7fe90405efa4b85d03",
        "title": "Supplementary Materials for Distance Dependent Infinite Latent Feature
        Models", "abstract": "In Section 1, we present proofs of the propositions
        and lemmas that appeared in the main paper. In Section 2, we present a Markov
        chain Monte Carlo algorithm for approximate inference. Finally, in Section
        3, we present analysis of data in which a non-exchangeable model might be
        expected to help, but does not.", "venue": "", "year": 2014, "referenceCount":
        4, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category":
        "Mathematics", "source": "s2-fos-model"}, {"category": "Computer Science",
        "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": null,
        "journal": null, "authors": [{"authorId": "1831199", "name": "S. Gershman"},
        {"authorId": "1922747", "name": "P. Frazier"}, {"authorId": "1796335", "name":
        "D. Blei"}]}, {"paperId": "87224164eef14589b137547a3fa81f06eef9bbf4", "externalIds":
        {"MAG": "2397802321", "DBLP": "reference/crc/PaisleyBJ14", "DOI": "10.1201/b17520-15",
        "CorpusId": 9340425}, "corpusId": 9340425, "publicationVenue": null, "url":
        "https://www.semanticscholar.org/paper/87224164eef14589b137547a3fa81f06eef9bbf4",
        "title": "Bayesian Nonnegative Matrix Factorization with Stochastic Variational
        Inference", "abstract": null, "venue": "Handbook of Mixed Membership Models
        and Their Applications", "year": 2014, "referenceCount": 28, "citationCount":
        64, "influentialCitationCount": 5, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
        "Computer Science", "source": "external"}, {"category": "Computer Science",
        "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": "2014-11-06",
        "journal": {"pages": "205-224"}, "authors": [{"authorId": "143855009", "name":
        "J. Paisley"}, {"authorId": "1796335", "name": "D. Blei"}, {"authorId": "1694621",
        "name": "Michael I. Jordan"}]}, {"paperId": "8db0184f4388f71a2a764b8d5a6f51488f691665",
        "externalIds": {"MAG": "2338001230", "CorpusId": 163374411}, "corpusId": 163374411,
        "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/8db0184f4388f71a2a764b8d5a6f51488f691665",
        "title": "Jordan Boyd-Graber, David Mimno, and David Newman. Care and Feeding
        of Topic Models: Problems, Diagnostics, and Improvements. Handbook of Mixed
        Membership Models and Their Applications, 2014.", "abstract": null, "venue":
        "", "year": 2014, "referenceCount": 44, "citationCount": 10, "influentialCitationCount":
        0, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["History"],
        "s2FieldsOfStudy": [{"category": "History", "source": "external"}], "publicationTypes":
        null, "publicationDate": null, "journal": {"volume": "", "name": ""}, "authors":
        [{"authorId": "51981622", "name": "Boca Raton"}, {"authorId": "1389036863",
        "name": "Jordan L. Boyd-Graber"}, {"authorId": "1705700", "name": "David Mimno"},
        {"authorId": "67057961", "name": "D. Newman"}, {"authorId": "2247552", "name":
        "E. Airoldi"}, {"authorId": "1796335", "name": "D. Blei"}, {"authorId": "1868490",
        "name": "E. Erosheva"}, {"authorId": "1684961", "name": "S. Fienberg"}]},
        {"paperId": "9df7ebe156cc341beb75b48e78b06d96ad86572c", "externalIds": {"ArXiv":
        "1407.0050", "MAG": "1894548862", "DOI": "10.1073/pnas.1412301112", "CorpusId":
        577514, "PubMed": "26071445"}, "corpusId": 577514, "publicationVenue": {"id":
        "bb95bf2e-8383-4748-bf9d-d6906d091085", "name": "Proceedings of the National
        Academy of Sciences of the United States of America", "type": "journal", "alternate_names":
        ["PNAS", "PNAS online", "Proceedings of the National Academy of Sciences of
        the United States of America.", "Proc National Acad Sci", "Proceedings of
        the National Academy of Sciences", "Proc National Acad Sci u s Am"], "issn":
        "0027-8424", "alternate_issns": ["1091-6490"], "url": "https://www.jstor.org/journal/procnatiacadscie",
        "alternate_urls": ["http://www.pnas.org/", "https://www.pnas.org/", "http://www.jstor.org/journals/00278424.html",
        "www.pnas.org/"]}, "url": "https://www.semanticscholar.org/paper/9df7ebe156cc341beb75b48e78b06d96ad86572c",
        "title": "Posterior predictive checks to quantify lack-of-fit in admixture
        models of latent population structure", "abstract": "Significance Bayesian
        models, including admixture models, are a powerful framework for articulating
        complex assumptions about large-scale genetic data; such models are widely
        used to explore data or to study population-level statistics of interest.
        However, we assume that a Bayesian model does not oversimplify the complexities
        in the data, to the point of invalidating our analyses. Here, we develop and
        study procedures for quantitatively evaluating admixture models of genetic
        data. Using four large genetic studies, we demonstrate that model checking
        should be an important part of the modern genetic data analysis pipeline.
        Our methods help to support inferences drawn from recovered population structure,
        to protect scientists from being misled by a misspecified model class, and
        to point scientists toward useful model extensions. Admixture models are a
        ubiquitous approach to capture latent population structure in genetic samples.
        Despite the widespread application of admixture models, little thought has
        been devoted to the quality of the model fit or the accuracy of the estimates
        of parameters of interest for a particular study. Here we develop methods
        for validating admixture models based on posterior predictive checks (PPCs),
        a Bayesian method for assessing the quality of fit of a statistical model
        to a specific dataset. We develop PPCs for five population-level statistics
        of interest: within-population genetic variation, background linkage disequilibrium,
        number of ancestral populations, between-population genetic variation, and
        the downstream use of admixture parameters to correct for population structure
        in association studies. Using PPCs, we evaluate the quality of the admixture
        model fit to four qualitatively different population genetic datasets: the
        population reference sample (POPRES) European individuals, the HapMap phase
        3 individuals, continental Indians, and African American individuals. We found
        that the same model fitted to different genomic studies resulted in highly
        study-specific results when evaluated using PPCs, illustrating the utility
        of PPCs for model-based analyses in large genomic studies.", "venue": "Proceedings
        of the National Academy of Sciences of the United States of America", "year":
        2014, "referenceCount": 80, "citationCount": 21, "influentialCitationCount":
        0, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.pnas.org/content/pnas/112/26/E3441.full.pdf",
        "status": null}, "fieldsOfStudy": ["Mathematics", "Biology", "Medicine"],
        "s2FieldsOfStudy": [{"category": "Mathematics", "source": "external"}, {"category":
        "Biology", "source": "external"}, {"category": "Medicine", "source": "external"},
        {"category": "Biology", "source": "s2-fos-model"}, {"category": "Environmental
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
        "publicationDate": "2014-06-30", "journal": {"volume": "112", "pages": "E3441
        - E3450", "name": "Proceedings of the National Academy of Sciences"}, "authors":
        [{"authorId": "38917723", "name": "David M. Mimno"}, {"authorId": "1796335",
        "name": "D. Blei"}, {"authorId": "2053292", "name": "B. Engelhardt"}]}, {"paperId":
        "9f6c0954bd4fa5c8e81924e503d3a652249a3c20", "externalIds": {"MAG": "2787780026",
        "DOI": "10.1201/B17520-9", "CorpusId": 17956728}, "corpusId": 17956728, "publicationVenue":
        null, "url": "https://www.semanticscholar.org/paper/9f6c0954bd4fa5c8e81924e503d3a652249a3c20",
        "title": "A Mixed Membership Approach to the Assessment of Political Ideology
        from Survey Responses", "abstract": "We employ mixed-membership (or grade-of-membership)
        techniques\u2014of growing popularity in medical diagnostics, psychology,
        genetics, and machine learning\u2014in order to identify prototypical profiles
        of survey respondents based on their answers to questions aimed at uncovering
        their basic orientations or ideological predispositions. In contrast with
        factor analytic techniques and IRT approaches, we treat both manifest and
        latent variables as categorical. A mixed membership model may be thought of
        as a generalization of latent class modeling, in which individuals act as
        members of more than one class. This notion is well-aligned with earlier theoretical
        work of Zaller, Feldman, Stimson, and others, who at times envision respondents
        to be internally complex, answering survey questions probabilistically according
        to what Zaller calls varying \u201cconsiderations.\u201d Reanalyzing data
        in this way, we develop new insights into the sorts of constraints that may
        structure mass belief systems.", "venue": "", "year": 2014, "referenceCount":
        31, "citationCount": 18, "influentialCitationCount": 1, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Psychology"], "s2FieldsOfStudy":
        [{"category": "Psychology", "source": "external"}, {"category": "Psychology",
        "source": "s2-fos-model"}], "publicationTypes": ["Review"], "publicationDate":
        "2014-11-06", "journal": {"volume": "", "name": ""}, "authors": [{"authorId":
        "2247552", "name": "E. Airoldi"}, {"authorId": "1796335", "name": "D. Blei"},
        {"authorId": "1868490", "name": "E. Erosheva"}, {"authorId": "1684961", "name":
        "S. Fienberg"}, {"authorId": "2603663", "name": "Justin H. Gross"}, {"authorId":
        "1422126648", "name": "Daniel Manrique-Vallier"}]}, {"paperId": "a46b787c5a9cc883006fd81718d15eaad2fd4197",
        "externalIds": {"MAG": "2949755339", "PubMedCentral": "5127768", "DOI": "10.1101/013227",
        "CorpusId": 1593026, "PubMed": "27819665"}, "corpusId": 1593026, "publicationVenue":
        {"id": "bb27e645-e57c-42c3-bcbc-c7b443c58209", "name": "Nature Genetics",
        "type": "journal", "alternate_names": ["Nat Genet"], "issn": "1061-4036",
        "url": "http://www.nature.com/ng/", "alternate_urls": ["http://www.nature.com/ng/index.html"]},
        "url": "https://www.semanticscholar.org/paper/a46b787c5a9cc883006fd81718d15eaad2fd4197",
        "title": "Scaling probabilistic models of genetic variation to millions of
        humans", "abstract": "A major goal of population genetics is to quantitatively
        understand variation of genetic polymorphisms among individuals. The aggregated
        number of genotyped humans is currently on the order of millions of individuals,
        and existing methods do not scale to data of this size. To solve this problem,
        we developed TeraStructure, an algorithm to fit Bayesian models of genetic
        variation in structured human populations on tera-sample-sized data sets (1012
        observed genotypes; for example, 1 million individuals at 1 million SNPs).
        TeraStructure is a scalable approach to Bayesian inference in which subsamples
        of markers are used to update an estimate of the latent population structure
        among individuals. We demonstrate that TeraStructure performs as well as existing
        methods on current globally sampled data, and we show using simulations that
        TeraStructure continues to be accurate and is the only method that can scale
        to tera-sample sizes.", "venue": "Nature Genetics", "year": 2014, "referenceCount":
        36, "citationCount": 43, "influentialCitationCount": 5, "isOpenAccess": true,
        "openAccessPdf": {"url": "https://europepmc.org/articles/pmc5127768?pdf=render",
        "status": null}, "fieldsOfStudy": ["Biology", "Medicine"], "s2FieldsOfStudy":
        [{"category": "Biology", "source": "external"}, {"category": "Medicine", "source":
        "external"}, {"category": "Biology", "source": "s2-fos-model"}], "publicationTypes":
        ["JournalArticle"], "publicationDate": "2014-12-24", "journal": {"volume":
        "48", "pages": "1587 - 1590", "name": "Nature genetics"}, "authors": [{"authorId":
        "2141648", "name": "Prem Gopalan"}, {"authorId": "116058157", "name": "Wei-Qi
        Hao"}, {"authorId": "1796335", "name": "D. Blei"}, {"authorId": "1717327",
        "name": "John D. Storey"}]}, {"paperId": "a5b50170e7270d20c03fed29602f1fccf4552965",
        "externalIds": {"DBLP": "journals/neuroimage/GershmanBNS14", "MAG": "2009094355",
        "DOI": "10.1016/j.neuroimage.2014.04.055", "CorpusId": 1750398, "PubMed":
        "24791745"}, "corpusId": 1750398, "publicationVenue": {"id": "fd4c7628-c16e-4b50-8555-3ac3ad6da2d7",
        "name": "NeuroImage", "type": "journal", "issn": "1053-8119", "url": "http://www.elsevier.com/locate/ynimg",
        "alternate_urls": ["http://www.elsevier.com/wps/find/journaldescription.cws_home/622925/description#description",
        "https://www.journals.elsevier.com/neuroimage", "http://www.sciencedirect.com/science/journal/10538119",
        "http://www.idealibrary.com/"]}, "url": "https://www.semanticscholar.org/paper/a5b50170e7270d20c03fed29602f1fccf4552965",
        "title": "Decomposing spatiotemporal brain patterns into topographic latent
        sources", "abstract": null, "venue": "NeuroImage", "year": 2014, "referenceCount":
        33, "citationCount": 13, "influentialCitationCount": 0, "isOpenAccess": true,
        "openAccessPdf": {"url": "https://europepmc.org/articles/pmc4099317?pdf=render",
        "status": null}, "fieldsOfStudy": ["Computer Science", "Psychology", "Medicine"],
        "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
        {"category": "Psychology", "source": "external"}, {"category": "Medicine",
        "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle"], "publicationDate": "2014-09-01", "journal":
        {"volume": "98", "pages": "91-102", "name": "NeuroImage"}, "authors": [{"authorId":
        "1831199", "name": "S. Gershman"}, {"authorId": "1796335", "name": "D. Blei"},
        {"authorId": "1780319", "name": "K. Norman"}, {"authorId": "2196760", "name":
        "P. Sederberg"}]}, {"paperId": "b21fd95f438792add77d1d35132a01b895648c17",
        "externalIds": {"DBLP": "journals/corr/MandtB14", "ArXiv": "1406.3650", "MAG":
        "2950413897", "CorpusId": 6587981}, "corpusId": 6587981, "publicationVenue":
        {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd", "name": "Neural Information
        Processing Systems", "type": "conference", "alternate_names": ["Neural Inf
        Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"}, "url": "https://www.semanticscholar.org/paper/b21fd95f438792add77d1d35132a01b895648c17",
        "title": "Smoothed Gradients for Stochastic Variational Inference", "abstract":
        "Stochastic variational inference (SVI) lets us scale up Bayesian computation
        to massive data. It uses stochastic optimization to fit a variational distribution,
        following easy-to-compute noisy natural gradients. As with most traditional
        stochastic optimization methods, SVI takes precautions to use unbiased stochastic
        gradients whose expectations are equal to the true gradients. In this paper,
        we explore the idea of following biased stochastic gradients in SVI. Our method
        replaces the natural gradient with a similarly constructed vector that uses
        a fixed-window moving average of some of its previous terms. We will demonstrate
        the many advantages of this technique. First, its computational cost is the
        same as for SVI and storage requirements only multiply by a constant factor.
        Second, it enjoys significant variance reduction over the unbiased estimates,
        smaller bias than averaged gradients, and leads to smaller mean-squared error
        against the full gradient. We test our method on latent Dirichlet allocation
        with three large corpora.", "venue": "Neural Information Processing Systems",
        "year": 2014, "referenceCount": 22, "citationCount": 28, "influentialCitationCount":
        1, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Mathematics",
        "Computer Science"], "s2FieldsOfStudy": [{"category": "Mathematics", "source":
        "external"}, {"category": "Computer Science", "source": "external"}, {"category":
        "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
        "Conference"], "publicationDate": "2014-06-13", "journal": {"pages": "2438-2446"},
        "authors": [{"authorId": "1783468", "name": "S. Mandt"}, {"authorId": "1796335",
        "name": "D. Blei"}]}, {"paperId": "bc0d7df38dd99d19ea586ecea1e478c4305993fc",
        "externalIds": {"DBLP": "conf/icml/RabinovichB14", "MAG": "2120926292", "CorpusId":
        11924245}, "corpusId": 11924245, "publicationVenue": {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
        "name": "International Conference on Machine Learning", "type": "conference",
        "alternate_names": ["ICML", "Int Conf Mach Learn"], "url": "https://icml.cc/"},
        "url": "https://www.semanticscholar.org/paper/bc0d7df38dd99d19ea586ecea1e478c4305993fc",
        "title": "The Inverse Regression Topic Model", "abstract": "Taddy (2013) proposed
        multinomial inverse regression (MNIR) as a new model of annotated text based
        on the influence of metadata and response variables on the distribution of
        words in a document. While effective, MNIR has no way to exploit structure
        in the corpus to improve its predictions or facilitate exploratory data analysis.
        On the other hand, traditional probabilistic topic models (like latent Dirichlet
        allocation) capture natural heterogeneity in a collection but do not account
        for external variables. In this paper, we introduce the inverse regression
        topic model (IRTM), a mixed-membership extension of MNIR that combines the
        strengths of both methodologies. We present two inference algorithms for the
        IRTM: an efficient batch estimation algorithm and an online variant, which
        is suitable for large corpora. We apply these methods to a corpus of 73K Congressional
        press releases and another of 150K Yelp reviews, demonstrating that the IRTM
        outperforms both MNIR and supervised topic models on the prediction task.
        Further, we give examples showing that the IRTM enables systematic discovery
        of in-topic lexical variation, which is not possible with previous supervised
        topic models.", "venue": "International Conference on Machine Learning", "year":
        2014, "referenceCount": 15, "citationCount": 49, "influentialCitationCount":
        2, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
        "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle", "Conference", "Review"], "publicationDate":
        "2014-06-21", "journal": {"pages": "199-207"}, "authors": [{"authorId": "40354648",
        "name": "Maxim Rabinovich"}, {"authorId": "1796335", "name": "D. Blei"}]},
        {"paperId": "c3c3e2e0f6e54b4da70ceeb81cc5c274731c88b6", "externalIds": {"CorpusId":
        27063058}, "corpusId": 27063058, "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/c3c3e2e0f6e54b4da70ceeb81cc5c274731c88b6",
        "title": "Interpretability Constraints and Trade-offs in Using Mixed Membership
        Models", "abstract": "8.", "venue": "", "year": 2014, "referenceCount": 0,
        "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category":
        "Economics", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate":
        null, "journal": null, "authors": [{"authorId": "2247552", "name": "E. Airoldi"},
        {"authorId": "1796335", "name": "D. Blei"}, {"authorId": "1868490", "name":
        "E. Erosheva"}, {"authorId": "1684961", "name": "S. Fienberg"}, {"authorId":
        "1744700", "name": "Zoubin Ghahramani"}, {"authorId": "14594344", "name":
        "S. Mohamed"}]}, {"paperId": "c8a9b0b5612b3d733203e03ce2aae3c7f7eacdb4", "externalIds":
        {"CorpusId": 263271680}, "corpusId": 263271680, "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/c8a9b0b5612b3d733203e03ce2aae3c7f7eacdb4",
        "title": "PUTOP : Turning Predominant Senses into a Topic Model for Word Sense
        Disambiguation", "abstract": "We extend on McCarthy et al.\u2019s predominant
        sense method to create an unsupervised method of word sense disambiguation
        that uses automatically derived topics using Latent Dirichlet allocation.
        Using topicspecific synset similarity measures, we create predictions for
        each word in each document using only word frequency information. It is hoped
        that this procedure can improve upon the method for larger numbers of topics
        by providing more relevant training corpora for the individual topics. This
        method is evaluated on SemEval-2007 Task 1 and Task 17. 1 Generative Model
        of WSD Word Sense Disambiguation (WSD) is the problem of labeling text with
        the appropriate semantic labels automatically. Although WSD is claimed to
        be an essential step in information retrieval and machine translation, it
        has not seen effective practical application because the dearth of labeled
        data has prevented the use of established supervised statistical methods that
        have been successfully applied to other natural language problems. Unsupervised
        methods have been developed for WSD, but despite modest success have not always
        been well understood statistically (Abney, 2004). Unsupervised methods are
        particularly appealing because they do not require expensive senseannotated
        data and can use the ever-increasing amount of raw text freely available.
        This paper expands on an effective unsupervised method for WSD and embeds
        it into a topic model, thus allowing an algorithm trained on a single, monolithic
        corpora to instead hand-pick relevant documents in choosing a disambiguation.
        After developing this generative statistical model, we present its performance
        on a number of tasks. 1.1 The Intersection of Syntactic and Semantic Similarity
        McCarthy et al. (2004) outlined a method for learning a word\u2019s most-used
        sense given an untagged corpus that ranks each sense wsi using a distributional
        syntactic similarity \u03b3 and a WORDNET-derived semantic similarity \u03b1.
        This process for a word w uses its distributional neighbors Nw, the possible
        senses of not only the word in question, Sw, and also those of the distributionally
        similar words, Snj . Thus, P (wsi) =", "venue": "", "year": 2014, "referenceCount":
        9, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category":
        "Computer Science", "source": "s2-fos-model"}], "publicationTypes": null,
        "publicationDate": null, "journal": null, "authors": [{"authorId": "2240779865",
        "name": "Jordan L. Boyd-Graber"}, {"authorId": "1796335", "name": "D. Blei"}]},
        {"paperId": "d01508b7f10468b47712cfdc19c028997b541dd1", "externalIds": {"DBLP":
        "conf/nips/GopalanCB14", "MAG": "2125261539", "CorpusId": 2064461}, "corpusId":
        2064461, "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
        "name": "Neural Information Processing Systems", "type": "conference", "alternate_names":
        ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
        "url": "https://www.semanticscholar.org/paper/d01508b7f10468b47712cfdc19c028997b541dd1",
        "title": "Content-based recommendations with Poisson factorization", "abstract":
        "We develop collaborative topic Poisson factorization (CTPF), a generative
        model of articles and reader preferences. CTPF can be used to build recommender
        systems by learning from reader histories and content to recommend personalized
        articles of interest. In detail, CTPF models both reader behavior and article
        texts with Poisson distributions, connecting the latent topics that represent
        the texts with the latent preferences that represent the readers. This provides
        better recommendations than competing methods and gives an interpretable latent
        space for understanding patterns of readership. Further, we exploit stochastic
        variational inference to model massive real-world datasets. For example, we
        can fit CPTF to the full arXiv usage dataset, which contains over 43 million
        ratings and 42 million word counts, within a day. We demonstrate empirically
        that our model outperforms several baselines, including the previous state-of-the
        art approach.", "venue": "Neural Information Processing Systems", "year":
        2014, "referenceCount": 25, "citationCount": 175, "influentialCitationCount":
        29, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
        "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2014-12-08",
        "journal": {"pages": "3176-3184"}, "authors": [{"authorId": "2141648", "name":
        "Prem Gopalan"}, {"authorId": "1778839", "name": "Laurent Charlin"}, {"authorId":
        "1796335", "name": "D. Blei"}]}, {"paperId": "d6559f35be0679c6b3371a2e44e3be293704b600",
        "externalIds": {"DBLP": "journals/corr/RanganathTCB14", "MAG": "2169751147",
        "ArXiv": "1411.2581", "CorpusId": 341902}, "corpusId": 341902, "publicationVenue":
        {"id": "2d136b11-c2b5-484b-b008-7f4a852fd61e", "name": "International Conference
        on Artificial Intelligence and Statistics", "type": "conference", "alternate_names":
        ["AISTATS", "Int Conf Artif Intell Stat"]}, "url": "https://www.semanticscholar.org/paper/d6559f35be0679c6b3371a2e44e3be293704b600",
        "title": "Deep Exponential Families", "abstract": "We describe \\textit{deep
        exponential families} (DEFs), a class of latent variable models that are inspired
        by the hidden structures used in deep neural networks. DEFs capture a hierarchy
        of dependencies between latent variables, and are easily generalized to many
        settings through exponential families. We perform inference using recent \"black
        box\" variational inference techniques. We then evaluate various DEFs on text
        and combine multiple DEFs into a model for pairwise recommendation data. In
        an extensive study, we show that going beyond one layer improves predictions
        for DEFs. We demonstrate that DEFs find interesting exploratory structure
        in large data sets, and give better predictive performance than state-of-the-art
        models.", "venue": "International Conference on Artificial Intelligence and
        Statistics", "year": 2014, "referenceCount": 42, "citationCount": 144, "influentialCitationCount":
        17, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Mathematics",
        "Computer Science"], "s2FieldsOfStudy": [{"category": "Mathematics", "source":
        "external"}, {"category": "Computer Science", "source": "external"}, {"category":
        "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
        "publicationDate": "2014-11-10", "journal": {"volume": "abs/1411.2581", "name":
        "ArXiv"}, "authors": [{"authorId": "2615814", "name": "R. Ranganath"}, {"authorId":
        "1950072", "name": "Linpeng Tang"}, {"authorId": "1778839", "name": "Laurent
        Charlin"}, {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId": "d78ebd47d60675b89853bd362051e4d9b1ca09ee",
        "externalIds": {"DOI": "10.4135/9781506326139.n304", "CorpusId": 13118624},
        "corpusId": 13118624, "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/d78ebd47d60675b89853bd362051e4d9b1ca09ee",
        "title": "Hierarchical Regression", "abstract": "There isn\u2019t a single
        authorative definition of a hierarchical model. Gelman et al. (1995) discuss
        two definitions: 1. \u201cEstimating the population distribution of unonobserved
        parameters\u201d 2. \u201cMultiple parameters related by the structure of
        the problem\u201d Intuitively, knowing something about one \u201cexperiment\u201d
        tells us something about another. For example: Multiple similar experiments
        Similar measurements from different locations Several tasks to perform on
        the same set of images We\u2019ve seen the last case when we talked about
        mixed-membership models. These are one type of hierarchical model. When talking
        about hierarchical models, statiticians sometimes use the phrase \u201csharing
        statistical strength.\u201d The idea is that something we can infer well in
        one group of data can help us with something we cannot infer well in another.
        For example, we may have a lot of data from California but much less data
        from Oregon. What we learn from California should help us learn in Oregon.
        The key idea is: Inference about one unobserved quantity affects inference
        about another unobserved quantity.", "venue": "", "year": 2014, "referenceCount":
        2, "citationCount": 8, "influentialCitationCount": 0, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category":
        "Computer Science", "source": "s2-fos-model"}], "publicationTypes": null,
        "publicationDate": null, "journal": null, "authors": [{"authorId": "1796335",
        "name": "D. Blei"}]}, {"paperId": "deda6f8719d751402307512dcf1fb2e4014010b5",
        "externalIds": {"MAG": "2103878673", "DOI": "10.1146/ANNUREV-STATISTICS-022513-115657",
        "CorpusId": 6172646}, "corpusId": 6172646, "publicationVenue": null, "url":
        "https://www.semanticscholar.org/paper/deda6f8719d751402307512dcf1fb2e4014010b5",
        "title": "Build, Compute, Critique, Repeat: Data Analysis with Latent Variable
        Models", "abstract": "We survey latent variable models for solving data-analysis
        problems. A latent variable model is a probabilistic model that encodes hidden
        patterns in the data. We uncover these patterns from their conditional distribution
        and use them to summarize data and form predictions. Latent variable models
        are important in many fields, including computational biology, natural language
        processing, and social network analysis. Our perspective is that models are
        developed iteratively: We build a model, use it to analyze data, assess how
        it succeeds and fails, revise it, and repeat. We describe how new research
        has transformed these essential activities. First, we describe probabilistic
        graphical models, a language for formulating latent variable models. Second,
        we describe mean field variational inference, a generic algorithm for approximating
        conditional distributions. Third, we describe how to use our analyses to solve
        problems: exploring the data, forming predictions, and pointing us in the
        direction of improved mo...", "venue": "", "year": 2014, "referenceCount":
        118, "citationCount": 207, "influentialCitationCount": 21, "isOpenAccess":
        true, "openAccessPdf": {"url": "https://www.annualreviews.org/doi/pdf/10.1146/annurev-statistics-022513-115657",
        "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["Review"], "publicationDate":
        "2014-01-03", "journal": {"volume": "1", "pages": "203-232", "name": ""},
        "authors": [{"authorId": "1796335", "name": "D. Blei"}]}, {"paperId": "e84a29252dabbd2680fb9ebe65bc79bc3f52a73c",
        "externalIds": {"ArXiv": "1404.4114", "MAG": "1795258949", "CorpusId": 8607134},
        "corpusId": 8607134, "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/e84a29252dabbd2680fb9ebe65bc79bc3f52a73c",
        "title": "Structured Stochastic Variational Inference", "abstract": "Stochastic
        variational inference makes it possible to approximate posterior distributions
        induced by large datasets quickly using stochastic optimization. The algorithm
        relies on the use of fully factorized variational distributions. However,
        this \"mean-field\" independence approximation limits the fidelity of the
        posterior approximation, and introduces local optima. We show how to relax
        the mean-field approximation to allow arbitrary dependencies between global
        parameters and local hidden variables, producing better parameter estimates
        by reducing bias, sensitivity to local optima, and sensitivity to hyperparameters.",
        "venue": "", "year": 2014, "referenceCount": 32, "citationCount": 79, "influentialCitationCount":
        8, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Mathematics",
        "Computer Science"], "s2FieldsOfStudy": [{"category": "Mathematics", "source":
        "external"}, {"category": "Computer Science", "source": "external"}, {"category":
        "Computer Science", "source": "s2-fos-model"}], "publicationTypes": null,
        "publicationDate": "2014-04-15", "journal": {"volume": "", "name": "arXiv:
        Learning"}, "authors": [{"authorId": "28552618", "name": "M. Hoffman"}, {"authorId":
        "1796335", "name": "D. Blei"}]}, {"paperId": "e97be50cc895bc1b819fac457a6203bb066a5ae4",
        "externalIds": {"DBLP": "conf/aistats/HoffmanB15", "MAG": "2963267993", "CorpusId":
        27468943}, "corpusId": 27468943, "publicationVenue": {"id": "2d136b11-c2b5-484b-b008-7f4a852fd61e",
        "name": "International Conference on Artificial Intelligence and Statistics",
        "type": "conference", "alternate_names": ["AISTATS", "Int Conf Artif Intell
        Stat"]}, "url": "https://www.semanticscholar.org/paper/e97be50cc895bc1b819fac457a6203bb066a5ae4",
        "title": "Stochastic Structured Variational Inference", "abstract": null,
        "venue": "International Conference on Artificial Intelligence and Statistics",
        "year": 2014, "referenceCount": 0, "citationCount": 66, "influentialCitationCount":
        5, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
        "external"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2014-04-16",
        "journal": {"volume": "", "pages": "361-369", "name": ""}, "authors": [{"authorId":
        "28552618", "name": "M. Hoffman"}, {"authorId": "1796335", "name": "D. Blei"}]},
        {"paperId": "ef2911c4a5b458ac57c2a30c2e2446ed0af7e762", "externalIds": {"MAG":
        "2402203825", "ArXiv": "1411.0292", "DBLP": "journals/corr/KucukelbirB14",
        "CorpusId": 195345797}, "corpusId": 195345797, "publicationVenue": {"id":
        "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names":
        ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/ef2911c4a5b458ac57c2a30c2e2446ed0af7e762",
        "title": "Profile Predictive Inference", "abstract": "Predictive inference
        uses a model to analyze a dataset and make predictions about new observations.
        When a model does not match the data, predictive accuracy suffers. To mitigate
        this effect, we develop the profile predictive, a predictive density that
        incorporates the population distribution of data into Bayesian inference.
        This leads to a practical method for reducing the effect of model mismatch.
        We extend this method into variational inference and propose a stochastic
        optimization algorithm, called bumping variational inference (bump-vi). We
        demonstrate improved predictive accuracy over classical variational inference
        in two models: a Bayesian mixture model of image histograms and a latent Dirichlet
        allocation topic model of a text corpus.", "venue": "arXiv.org", "year": 2014,
        "referenceCount": 27, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess":
        false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
        "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
        {"category": "Mathematics", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
        "publicationDate": "2014-11-02", "journal": {"volume": "abs/1411.0292", "name":
        "ArXiv"}, "authors": [{"authorId": "3081817", "name": "A. Kucukelbir"}, {"authorId":
        "1796335", "name": "D. Blei"}]}, {"paperId": "f0f1292719f02a59990a7fd4a35b97de85be1c11",
        "externalIds": {"CorpusId": 11660040}, "corpusId": 11660040, "publicationVenue":
        null, "url": "https://www.semanticscholar.org/paper/f0f1292719f02a59990a7fd4a35b97de85be1c11",
        "title": "1 Introduction to Mixed Membership Models and Methods", "abstract":
        "1.1 Historical Developments . . . . . . . . . . . . . . . . . . . . . . .
        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
        . . . . . . . . 3 1.2 A General Formulation for Mixed Membership Models .
        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
        1.3 Advantages of Mixed Membership Models in Applied Statistics . . . . .
        . . . . . . . . . . . . . . . . . . . . . . . . 6 1.4 Theoretical Issues with
        Mixed Membership Models . . . . . . . . . . . . . . . . . . . . . . . . .
        . . . . . . . . . . . . . . . . 8 1.4.1 General Issues Inherent to Mixtures
        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
        . . . . . . . . . . . . 8 References . . . . . . . . . . . . . . . . . . .
        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
        . . . . . . . . . . . . . . . . . . . . . . . . . 10", "venue": "", "year":
        2014, "referenceCount": 59, "citationCount": 0, "influentialCitationCount":
        0, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy":
        [{"category": "Business", "source": "s2-fos-model"}], "publicationTypes":
        null, "publicationDate": null, "journal": null, "authors": [{"authorId": "2247552",
        "name": "E. Airoldi"}, {"authorId": "1796335", "name": "D. Blei"}, {"authorId":
        "1868490", "name": "E. Erosheva"}]}, {"paperId": "f479505354f332f692c4775dc91d90758e8379f4",
        "externalIds": {"PubMedCentral": "4012983", "MAG": "2009849957", "DOI": "10.1371/journal.pone.0094914",
        "CorpusId": 16320846, "PubMed": "24804795"}, "corpusId": 16320846, "publicationVenue":
        {"id": "0aed7a40-85f3-4c66-9e1b-c1556c57001b", "name": "PLoS ONE", "type":
        "journal", "alternate_names": ["Plo ONE", "PLOS ONE", "PLO ONE"], "issn":
        "1932-6203", "url": "https://journals.plos.org/plosone/", "alternate_urls":
        ["http://www.plosone.org/"]}, "url": "https://www.semanticscholar.org/paper/f479505354f332f692c4775dc91d90758e8379f4",
        "title": "Topographic Factor Analysis: A Bayesian Model for Inferring Brain
        Networks from Neural Data", "abstract": "The neural patterns recorded during
        a neuroscientific experiment reflect complex interactions between many brain
        regions, each comprising millions of neurons. However, the measurements themselves
        are typically abstracted from that underlying structure. For example, functional
        magnetic resonance imaging (fMRI) datasets comprise a time series of three-dimensional
        images, where each voxel in an image (roughly) reflects the activity of the
        brain structure(s)\u2013located at the corresponding point in space\u2013at
        the time the image was collected. FMRI data often exhibit strong spatial correlations,
        whereby nearby voxels behave similarly over time as the underlying brain structure
        modulates its activity. Here we develop topographic factor analysis (TFA),
        a technique that exploits spatial correlations in fMRI data to recover the
        underlying structure that the images reflect. Specifically, TFA casts each
        brain image as a weighted sum of spatial functions. The parameters of those
        spatial functions, which may be learned by applying TFA to an fMRI dataset,
        reveal the locations and sizes of the brain structures activated while the
        data were collected, as well as the interactions between those structures.",
        "venue": "PLoS ONE", "year": 2014, "referenceCount": 28, "citationCount":
        42, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf":
        {"url": "https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0094914&type=printable",
        "status": null}, "fieldsOfStudy": ["Medicine", "Physics"], "s2FieldsOfStudy":
        [{"category": "Medicine", "source": "external"}, {"category": "Physics", "source":
        "external"}, {"category": "Biology", "source": "s2-fos-model"}, {"category":
        "Psychology", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
        "publicationDate": "2014-05-07", "journal": {"volume": "9", "name": "PLoS
        ONE"}, "authors": [{"authorId": "2586738", "name": "Jeremy R. Manning"}, {"authorId":
        "2615814", "name": "R. Ranganath"}, {"authorId": "1780319", "name": "K. Norman"},
        {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId": "f47af707941a43fae8b58066eab91ee12ea7a72a",
        "externalIds": {"DBLP": "reference/snam/X14zz", "DOI": "10.1007/978-1-4614-6170-8_100963",
        "CorpusId": 2880040}, "corpusId": 2880040, "publicationVenue": null, "url":
        "https://www.semanticscholar.org/paper/f47af707941a43fae8b58066eab91ee12ea7a72a",
        "title": "Topic Model", "abstract": null, "venue": "Encyclopedia of Social
        Network Analysis and Mining", "year": 2014, "referenceCount": 6, "citationCount":
        9, "influentialCitationCount": 0, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
        "Computer Science", "source": "external"}, {"category": "Computer Science",
        "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": null,
        "journal": {"pages": "2178"}, "authors": [{"authorId": "2227410", "name":
        "Yueshen Xu"}, {"authorId": "1397470438", "name": "Ccnt"}, {"authorId": "1397470432",
        "name": "Zju"}, {"authorId": "1796335", "name": "D. Blei"}, {"authorId": "1736467",
        "name": "ChengXiang Zhai"}, {"authorId": "1739581", "name": "J. Lafferty"}]},
        {"paperId": "f5c4c718a0d1705dd9960cd4d3768c0697c5a604", "externalIds": {"CorpusId":
        18409790}, "corpusId": 18409790, "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/f5c4c718a0d1705dd9960cd4d3768c0697c5a604",
        "title": "Foundations of Graphical Models", "abstract": "Description. Foundations
        of Graphical Models is a PhD-level course about how to design and use probability
        models. We study their mathematical properties, algorithms for computing with
        them, and applications to real problems. We study both the foundations and
        modern methods in this \ufb01eld. Our goals are to understand probabilistic
        modeling, to begin research that makes contributions to this \ufb01eld, and
        to develop good practices for building and applying probabilistic models.
        Prerequisites. The prerequisites are: knowledge of basic probability and statistics,
        calculus, and some optimization; comfort writing software to analyze data;
        familiarity with a good programming language for statistics and machine learning,
        such as R or Python.", "venue": "", "year": 2014, "referenceCount": 27, "citationCount":
        0, "influentialCitationCount": 0, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category": "Computer Science",
        "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": null,
        "journal": null, "authors": [{"authorId": "1796335", "name": "D. Blei"}]},
        {"paperId": "fc72472b075215b633d0f52f97789ba05097ed6c", "externalIds": {"CorpusId":
        263271679}, "corpusId": 263271679, "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/fc72472b075215b633d0f52f97789ba05097ed6c",
        "title": "PUTOP: Turning Predominant Senses into a Topic Model for Word Sense
        Disambiguation", "abstract": "We extend on McCarthy et al.\u2019s predominant
        sense method to create an unsupervised method of word sense disambiguation
        that uses automatically derived topics using Latent Dirichlet allocation.
        Using topicspecific synset similarity measures, we create predictions for
        each word in each document using only word frequency information. It is hoped
        that this procedure can improve upon the method for larger numbers of topics
        by providing more relevant training corpora for the individual topics. This
        method is evaluated on SemEval-2007 Task 1 and Task 17. 1 Generative Model
        of WSD Word Sense Disambiguation (WSD) is the problem of labeling text with
        the appropriate semantic labels automatically. Although WSD is claimed to
        be an essential step in information retrieval and machine translation, it
        has not seen effective practical application because the dearth of labeled
        data has prevented the use of established supervised statistical methods that
        have been successfully applied to other natural language problems. Unsupervised
        methods have been developed for WSD, but despite modest success have not always
        been well understood statistically (Abney, 2004). Unsupervised methods are
        particularly appealing because they do not require expensive senseannotated
        data and can use the ever-increasing amount of raw text freely available.
        This paper expands on an effective unsupervised method for WSD and embeds
        it into a topic model, thus allowing an algorithm trained on a single, monolithic
        corpora to instead hand-pick relevant documents in choosing a disambiguation.
        After developing this generative statistical model, we present its performance
        on a number of tasks. 1.1 The Intersection of Syntactic and Semantic Similarity
        McCarthy et al. (2004) outlined a method for learning a word\u2019s most-used
        sense given an untagged corpus that ranks each sense wsi using a distributional
        syntactic similarity \u03b3 and a WORDNET-derived semantic similarity \u03b1.
        This process for a word w uses its distributional neighbors Nw, the possible
        senses of not only the word in question, Sw, and also those of the distributionally
        similar words, Snj . Thus, P (wsi) =", "venue": "", "year": 2014, "referenceCount":
        9, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category":
        "Computer Science", "source": "s2-fos-model"}], "publicationTypes": null,
        "publicationDate": null, "journal": null, "authors": [{"authorId": "1796335",
        "name": "D. Blei"}]}, {"paperId": "1b145525d29300f47330d2486f2cf3509fe2308a",
        "externalIds": {"ArXiv": "1311.1704", "DBLP": "journals/corr/GopalanHB13",
        "MAG": "2123528936", "CorpusId": 8278616}, "corpusId": 8278616, "publicationVenue":
        {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names":
        ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/1b145525d29300f47330d2486f2cf3509fe2308a",
        "title": "Scalable Recommendation with Poisson Factorization", "abstract":
        "We develop a Bayesian Poisson matrix factorization model for forming recommendations
        from sparse user behavior data. These data are large user/item matrices where
        each user has provided feedback on only a small subset of items, either explicitly
        (e.g., through star ratings) or implicitly (e.g., through views or purchases).
        In contrast to traditional matrix factorization approaches, Poisson factorization
        implicitly models each user''s limited attention to consume items. Moreover,
        because of the mathematical form of the Poisson likelihood, the model needs
        only to explicitly consider the observed entries in the matrix, leading to
        both scalable computation and good predictive performance. We develop a variational
        inference algorithm for approximate posterior inference that scales up to
        massive data sets. This is an efficient algorithm that iterates over the observed
        entries and adjusts an approximate posterior over the user/item representations.
        We apply our method to large real-world user data containing users rating
        movies, users listening to songs, and users reading scientific papers. In
        all these settings, Bayesian Poisson factorization outperforms state-of-the-art
        matrix factorization methods.", "venue": "arXiv.org", "year": 2013, "referenceCount":
        58, "citationCount": 154, "influentialCitationCount": 25, "isOpenAccess":
        false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
        "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
        {"category": "Mathematics", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
        "publicationDate": "2013-11-07", "journal": {"volume": "abs/1311.1704", "name":
        "ArXiv"}, "authors": [{"authorId": "2141648", "name": "Prem Gopalan"}, {"authorId":
        "40368603", "name": "J. Hofman"}, {"authorId": "1796335", "name": "D. Blei"}]},
        {"paperId": "204711170720d254b19b4141ac994beeaf87f5c6", "externalIds": {"MAG":
        "2963181793", "ArXiv": "1301.3570", "DBLP": "journals/corr/abs-1301-3570",
        "CorpusId": 61857474}, "corpusId": 61857474, "publicationVenue": {"id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
        "name": "International Conference on Learning Representations", "type": "conference",
        "alternate_names": ["Int Conf Learn Represent", "ICLR"], "url": "https://iclr.cc/"},
        "url": "https://www.semanticscholar.org/paper/204711170720d254b19b4141ac994beeaf87f5c6",
        "title": "A Nested HDP for Hierarchical Topic Models", "abstract": "We develop
        a nested hierarchical Dirichlet process (nHDP) for hierarchical topic modeling.
        The nHDP is a generalization of the nested Chinese restaurant process (nCRP)
        that allows each word to follow its own path to a topic node according to
        a document-specific distribution on a shared tree. This alleviates the rigid,
        single-path formulation of the nCRP, allowing a document to more easily express
        thematic borrowings as a random effect. We demonstrate our algorithm on 1.8
        million documents from The New York Times.", "venue": "International Conference
        on Learning Representations", "year": 2013, "referenceCount": 4, "citationCount":
        8, "influentialCitationCount": 1, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": ["Mathematics", "Computer Science"], "s2FieldsOfStudy":
        [{"category": "Mathematics", "source": "external"}, {"category": "Computer
        Science", "source": "external"}, {"category": "Computer Science", "source":
        "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
        "2013-01-16", "journal": {"volume": "", "name": "arXiv: Machine Learning"},
        "authors": [{"authorId": "143855009", "name": "J. Paisley"}, {"authorId":
        "2108881999", "name": "Chong Wang"}, {"authorId": "1796335", "name": "D. Blei"},
        {"authorId": "1694621", "name": "Michael I. Jordan"}]}, {"paperId": "235721e4e986d5dc5e00a2e0431ffeabd8d3ea0f",
        "externalIds": {"MAG": "2950084973", "DOI": "10.1007/springerreference_205449",
        "CorpusId": 17742579}, "corpusId": 17742579, "publicationVenue": null, "url":
        "https://www.semanticscholar.org/paper/235721e4e986d5dc5e00a2e0431ffeabd8d3ea0f",
        "title": "Mixed Membership Models", "abstract": null, "venue": "", "year":
        2013, "referenceCount": 5, "citationCount": 11, "influentialCitationCount":
        0, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
        "external"}], "publicationTypes": null, "publicationDate": null, "journal":
        {"volume": "", "name": ""}, "authors": [{"authorId": "1796335", "name": "D.
        Blei"}]}, {"paperId": "682000be6be3094c97d64e9a33921abc1fb5b1ca", "externalIds":
        {"DOI": "10.1080/01621459.2013.827983", "CorpusId": 219599274}, "corpusId":
        219599274, "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/682000be6be3094c97d64e9a33921abc1fb5b1ca",
        "title": "Comment", "abstract": "Republican and Democratic representatives,
        the goal appears to be to measure words that are indicative of partisan conflict.
        But because Republicans and Democrats tend to come from different parts of
        the country, the party labels are conflated with language that is particular
        to the geographic region. The author recommends including covariates to mitigate
        this particular problem, but without clarifying the goal of measurement, it
        is hard to know what covariates should be included or excluded from the model.
        The obscured goals also complicate the evaluation of any covariate\u2019s
        value to the model. Thanks to impressive articles like the author\u2019s,
        the application of machine learning techniques in the social sciences is growing.
        With the application of the methods there has also been an impressive number
        of new methods introduced. My hope is that social scientists will develop
        a parallel literature on the evaluation of the models. Together, this will
        lead to the accurate measurement of sentiment across large datasets and facilitate
        the discovery of new facts about the world.", "venue": "", "year": 2013, "referenceCount":
        5, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category":
        "Sociology", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate":
        "2013-09-01", "journal": {"volume": "108", "pages": "771 - 772", "name": "Journal
        of the American Statistical Association"}, "authors": [{"authorId": "1796335",
        "name": "D. Blei"}]}, {"paperId": "6a667700100e228cb30a5d884258a0db921603fe",
        "externalIds": {"MAG": "2153185114", "ArXiv": "1401.0118", "DBLP": "journals/corr/RanganathGB14",
        "CorpusId": 1580089}, "corpusId": 1580089, "publicationVenue": {"id": "2d136b11-c2b5-484b-b008-7f4a852fd61e",
        "name": "International Conference on Artificial Intelligence and Statistics",
        "type": "conference", "alternate_names": ["AISTATS", "Int Conf Artif Intell
        Stat"]}, "url": "https://www.semanticscholar.org/paper/6a667700100e228cb30a5d884258a0db921603fe",
        "title": "Black Box Variational Inference", "abstract": "Variational inference
        has become a widely used method to approximate posteriors in complex latent
        variables models. However, deriving a variational inference algorithm generally
        requires significant model-specific analysis, and these efforts can hinder
        and deter us from quickly developing and exploring a variety of models for
        a problem at hand. In this paper, we present a \"black box\" variational inference
        algorithm, one that can be quickly applied to many models with little additional
        derivation. Our method is based on a stochastic optimization of the variational
        objective where the noisy gradient is computed from Monte Carlo samples from
        the variational distribution. We develop a number of methods to reduce the
        variance of the gradient, always maintaining the criterion that we want to
        avoid difficult model-based derivations. We evaluate our method against the
        corresponding black box sampling based methods. We find that our method reaches
        better predictive likelihoods much faster than sampling methods. Finally,
        we demonstrate that Black Box Variational Inference lets us easily explore
        a wide space of models by quickly constructing and evaluating several models
        of longitudinal healthcare data.", "venue": "International Conference on Artificial
        Intelligence and Statistics", "year": 2013, "referenceCount": 25, "citationCount":
        1007, "influentialCitationCount": 116, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics",
        "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle"], "publicationDate": "2013-12-31", "journal":
        {"pages": "814-822"}, "authors": [{"authorId": "2615814", "name": "R. Ranganath"},
        {"authorId": "21007048", "name": "S. Gerrish"}, {"authorId": "1796335", "name":
        "D. Blei"}]}, {"paperId": "7b3373f90e691c2ba1a2f383ae38334a1f74e651", "externalIds":
        {"DBLP": "journals/pnas/GopalanB13", "MAG": "2066828202", "DOI": "10.1073/pnas.1221839110",
        "CorpusId": 17578820, "PubMed": "23950224"}, "corpusId": 17578820, "publicationVenue":
        {"id": "bb95bf2e-8383-4748-bf9d-d6906d091085", "name": "Proceedings of the
        National Academy of Sciences of the United States of America", "type": "journal",
        "alternate_names": ["PNAS", "PNAS online", "Proceedings of the National Academy
        of Sciences of the United States of America.", "Proc National Acad Sci", "Proceedings
        of the National Academy of Sciences", "Proc National Acad Sci u s Am"], "issn":
        "0027-8424", "alternate_issns": ["1091-6490"], "url": "https://www.jstor.org/journal/procnatiacadscie",
        "alternate_urls": ["http://www.pnas.org/", "https://www.pnas.org/", "http://www.jstor.org/journals/00278424.html",
        "www.pnas.org/"]}, "url": "https://www.semanticscholar.org/paper/7b3373f90e691c2ba1a2f383ae38334a1f74e651",
        "title": "Efficient discovery of overlapping communities in massive networks",
        "abstract": "Detecting overlapping communities is essential to analyzing and
        exploring natural networks such as social networks, biological networks, and
        citation networks. However, most existing approaches do not scale to the size
        of networks that we regularly observe in the real world. In this paper, we
        develop a scalable approach to community detection that discovers overlapping
        communities in massive real-world networks. Our approach is based on a Bayesian
        model of networks that allows nodes to participate in multiple communities,
        and a corresponding algorithm that naturally interleaves subsampling from
        the network and updating an estimate of its communities. We demonstrate how
        we can discover the hidden community structure of several real-world networks,
        including 3.7 million US patents, 575,000 physics articles from the arXiv
        preprint server, and 875,000 connected Web pages from the Internet. Furthermore,
        we demonstrate on large simulated networks that our algorithm accurately discovers
        the true community structure. This paper opens the door to using sophisticated
        statistical models to analyze massive networks.", "venue": "Proceedings of
        the National Academy of Sciences of the United States of America", "year":
        2013, "referenceCount": 47, "citationCount": 294, "influentialCitationCount":
        26, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.pnas.org/content/pnas/110/36/14534.full.pdf",
        "status": null}, "fieldsOfStudy": ["Computer Science", "Medicine"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Medicine",
        "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle"], "publicationDate": "2013-08-15", "journal":
        {"volume": "110", "pages": "14534 - 14539", "name": "Proceedings of the National
        Academy of Sciences"}, "authors": [{"authorId": "2141648", "name": "Prem Gopalan"},
        {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId": "7ce557aa5ee42846061a7ee5344ee56b43775ee0",
        "externalIds": {"DBLP": "conf/icml/RanganathWBX13", "MAG": "2148721460", "CorpusId":
        1898841}, "corpusId": 1898841, "publicationVenue": {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
        "name": "International Conference on Machine Learning", "type": "conference",
        "alternate_names": ["ICML", "Int Conf Mach Learn"], "url": "https://icml.cc/"},
        "url": "https://www.semanticscholar.org/paper/7ce557aa5ee42846061a7ee5344ee56b43775ee0",
        "title": "An Adaptive Learning Rate for Stochastic Variational Inference",
        "abstract": "Stochastic variational inference finds good posterior approximations
        of probabilistic models with very large data sets. It optimizes the variational
        objective with stochastic optimization, following noisy estimates of the natural
        gradient. Operationally, stochastic inference iteratively subsamples from
        the data, analyzes the subsample, and updates parameters with a decreasing
        learning rate. However, the algorithm is sensitive to that rate, which usually
        requires hand-tuning to each application. We solve this problem by developing
        an adaptive learning rate for stochastic variational inference. Our method
        requires no tuning and is easily implemented with computations already made
        in the algorithm. We demonstrate our approach with latent Dirichlet allocation
        applied to three large text corpora. Inference with the adaptive learning
        rate converges faster and to a better approximation than the best settings
        of hand-tuned rates.", "venue": "International Conference on Machine Learning",
        "year": 2013, "referenceCount": 24, "citationCount": 91, "influentialCitationCount":
        6, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Mathematics",
        "Computer Science"], "s2FieldsOfStudy": [{"category": "Mathematics", "source":
        "external"}, {"category": "Computer Science", "source": "external"}, {"category":
        "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
        "Conference"], "publicationDate": "2013-06-16", "journal": {"pages": "298-306"},
        "authors": [{"authorId": "2615814", "name": "R. Ranganath"}, {"authorId":
        "2108881999", "name": "Chong Wang"}, {"authorId": "1796335", "name": "D. Blei"},
        {"authorId": "143977260", "name": "E. Xing"}]}, {"paperId": "8d6227e26a4bfc5482c12b8f072496ac6e97ed21",
        "externalIds": {"DBLP": "journals/pami/ChenPSBDC13", "MAG": "2134905716",
        "DOI": "10.1109/TPAMI.2013.19", "CorpusId": 7373060, "PubMed": "23787342"},
        "corpusId": 7373060, "publicationVenue": {"id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
        "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
        "type": "journal", "alternate_names": ["IEEE Trans Pattern Anal Mach Intell"],
        "issn": "0162-8828", "url": "http://www.computer.org/tpami/", "alternate_urls":
        ["http://www.computer.org/portal/web/tpami", "http://ieeexplore.ieee.org/servlet/opac?punumber=34"]},
        "url": "https://www.semanticscholar.org/paper/8d6227e26a4bfc5482c12b8f072496ac6e97ed21",
        "title": "Deep Learning with Hierarchical Convolutional Factor Analysis",
        "abstract": "Unsupervised multilayered (\u201cdeep\u201d) models are considered
        for imagery. The model is represented using a hierarchical convolutional factor-analysis
        construction, with sparse factor loadings and scores. The computation of layer-dependent
        model parameters is implemented within a Bayesian setting, employing a Gibbs
        sampler and variational Bayesian (VB) analysis that explicitly exploit the
        convolutional nature of the expansion. To address large-scale and streaming
        data, an online version of VB is also developed. The number of dictionary
        elements at each layer is inferred from the data, based on a beta-Bernoulli
        implementation of the Indian buffet process. Example results are presented
        for several image-processing applications, with comparisons to related models
        in the literature.", "venue": "IEEE Transactions on Pattern Analysis and Machine
        Intelligence", "year": 2013, "referenceCount": 55, "citationCount": 107, "influentialCitationCount":
        6, "isOpenAccess": true, "openAccessPdf": {"url": "http://europepmc.org/articles/pmc3683114?pdf=render",
        "status": null}, "fieldsOfStudy": ["Computer Science", "Medicine"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Medicine",
        "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle"], "publicationDate": "2013-08-01", "journal":
        {"volume": "35", "pages": "1887-1901", "name": "IEEE Transactions on Pattern
        Analysis and Machine Intelligence"}, "authors": [{"authorId": null, "name":
        "Bo Chen"}, {"authorId": "2767134", "name": "Gungor Polatkan"}, {"authorId":
        "1699339", "name": "G. Sapiro"}, {"authorId": "1796335", "name": "D. Blei"},
        {"authorId": "39775017", "name": "D. Dunson"}, {"authorId": "145006560", "name":
        "L. Carin"}]}, {"paperId": "8d67f3d8924316dc999360dcc0e0a17047be3e46", "externalIds":
        {"CorpusId": 28923229}, "corpusId": 28923229, "publicationVenue": null, "url":
        "https://www.semanticscholar.org/paper/8d67f3d8924316dc999360dcc0e0a17047be3e46",
        "title": "Poisson Trust Factorization for Incorporating Social Networks into
        Personalized Item Recommendation", "abstract": "Many web users are faced with
        the problem of selecting which books to read and movies to watch. Traditionally,
        we ask our trusted friends for recommendations, but algorithmic recommendation
        models make those choices even easier, saving us time and effort by steering
        us towards media we are more likely to enjoy. The downside to most algorithmic
        recommendations is that, for some people, part of the appeal of reading or
        consuming other media is in creating shared experiences with friends. In this
        work, we aim to bridge this gap. We present a model that incorporates social
        network information into recommendation models, reintroducing the social aspect
        to recommendation and having the potential to improve overall recommendations.
        Further, our model discovers the latent trust that exists between users in
        a network and allows us to consider which users are more trustworthy than
        others, providing us insight into the social network\u2019s dynamics.", "venue":
        "", "year": 2013, "referenceCount": 7, "citationCount": 2, "influentialCitationCount":
        0, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
        null, "publicationDate": null, "journal": null, "authors": [{"authorId": "2771308",
        "name": "A. Chaney"}, {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId":
        "91608e4567959f331f3b231f4f8fb5e33ce71a15", "externalIds": {"MAG": "2164773304",
        "DBLP": "conf/nips/KimGBS13", "CorpusId": 6513464}, "corpusId": 6513464, "publicationVenue":
        {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd", "name": "Neural Information
        Processing Systems", "type": "conference", "alternate_names": ["Neural Inf
        Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"}, "url": "https://www.semanticscholar.org/paper/91608e4567959f331f3b231f4f8fb5e33ce71a15",
        "title": "Efficient Online Inference for Bayesian Nonparametric Relational
        Models", "abstract": "Stochastic block models characterize observed network
        relationships via latent community memberships. In large social networks,
        we expect entities to participate in multiple communities, and the number
        of communities to grow with the network size. We introduce a new model for
        these phenomena, the hierarchical Dirichlet process relational model, which
        allows nodes to have mixed membership in an unbounded set of communities.
        To allow scalable learning, we derive an online stochastic variational inference
        algorithm. Focusing on assortative models of undirected networks, we also
        propose an efficient structured mean field variational bound, and online methods
        for automatically pruning unused communities. Compared to state-of-the-art
        online learning methods for parametric relational models, we show significantly
        improved perplexity and link prediction accuracy for sparse networks with
        tens of thousands of nodes. We also showcase an analysis of Little-Sis, a
        large network of who-knows-who at the heights of business and government.",
        "venue": "Neural Information Processing Systems", "year": 2013, "referenceCount":
        20, "citationCount": 33, "influentialCitationCount": 5, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
        "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
        {"category": "Mathematics", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
        "Conference"], "publicationDate": "2013-12-05", "journal": {"pages": "962-970"},
        "authors": [{"authorId": "2111888647", "name": "D. Kim"}, {"authorId": "2141648",
        "name": "Prem Gopalan"}, {"authorId": "1796335", "name": "D. Blei"}, {"authorId":
        "1799035", "name": "Erik B. Sudderth"}]}, {"paperId": "adc53e88c061a6f871c0a6aa653de443fdee21e2",
        "externalIds": {"MAG": "2102658707", "DOI": "10.21236/ada582116", "CorpusId":
        62661165}, "corpusId": 62661165, "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/adc53e88c061a6f871c0a6aa653de443fdee21e2",
        "title": "NON-PARAMETRIC BAYESIAN ANALYSIS OF HETEROGENEOUS DATA", "abstract":
        "Abstract : Under this grant, my research focused on fusing heterogenous sources
        of data with Bayesian nonparametric models. We published many papers in the
        service of this goal. I would like to highlight the following papers about
        furthering Bayesian nonparametrics and examining the fusion of heterogenous
        data types in a diversity of settings. This is an extension of last year s
        report. It is my final report.", "venue": "", "year": 2013, "referenceCount":
        20, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate":
        "2013-03-01", "journal": {"volume": "", "name": ""}, "authors": [{"authorId":
        "1796335", "name": "D. Blei"}]}, {"paperId": "e2df74941a0a3f11f6126b009e4c154f97600d13",
        "externalIds": {"DBLP": "conf/nips/GopalanWB13", "MAG": "2118085414", "CorpusId":
        10140528}, "corpusId": 10140528, "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
        "name": "Neural Information Processing Systems", "type": "conference", "alternate_names":
        ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
        "url": "https://www.semanticscholar.org/paper/e2df74941a0a3f11f6126b009e4c154f97600d13",
        "title": "Modeling Overlapping Communities with Node Popularities", "abstract":
        "We develop a probabilistic approach for accurate network modeling using node
        popularities within the framework of the mixed-membership stochastic block-model
        (MMSB). Our model integrates two basic properties of nodes in social networks:
        homophily and preferential connection to popular nodes. We develop a scalable
        algorithm for posterior inference, based on a novel nonconjugate variant of
        stochastic variational inference. We evaluate the link prediction accuracy
        of our algorithm on nine real-world networks with up to 60,000 nodes, and
        on simulated networks with degree distributions that follow a power law. We
        demonstrate that the AMP predicts significantly better than the MMSB.", "venue":
        "Neural Information Processing Systems", "year": 2013, "referenceCount": 27,
        "citationCount": 20, "influentialCitationCount": 3, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
        "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
        {"category": "Mathematics", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
        "Conference"], "publicationDate": "2013-12-05", "journal": {"pages": "2850-2858"},
        "authors": [{"authorId": "2141648", "name": "Prem Gopalan"}, {"authorId":
        "2108881999", "name": "Chong Wang"}, {"authorId": "1796335", "name": "D. Blei"}]},
        {"paperId": "e2f61db561f1bc1c11b031ca33049d70196a6f95", "externalIds": {"MAG":
        "2117667023", "DOI": "10.1016/J.POETIC.2013.08.004", "CorpusId": 18181114},
        "corpusId": 18181114, "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/e2f61db561f1bc1c11b031ca33049d70196a6f95",
        "title": "Exploiting affinities between topic modeling and the sociological
        perspective on culture: Application to newspaper coverage of U.S. government
        arts funding", "abstract": null, "venue": "", "year": 2013, "referenceCount":
        79, "citationCount": 666, "influentialCitationCount": 42, "isOpenAccess":
        false, "openAccessPdf": null, "fieldsOfStudy": ["Sociology"], "s2FieldsOfStudy":
        [{"category": "Sociology", "source": "external"}, {"category": "Sociology",
        "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": "2013-12-01",
        "journal": {"volume": "41", "pages": "570-606", "name": "Poetics"}, "authors":
        [{"authorId": "69383880", "name": "Paul DiMaggio"}, {"authorId": "2353412",
        "name": "Manish Nag"}, {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId":
        "e5d1b7dfe1327599a871f73431ced4cf444ab03b", "externalIds": {"MAG": "56307640",
        "CorpusId": 59750368}, "corpusId": 59750368, "publicationVenue": null, "url":
        "https://www.semanticscholar.org/paper/e5d1b7dfe1327599a871f73431ced4cf444ab03b",
        "title": "Applications of latent variable models in modeling influence and
        decision making", "abstract": "The past twenty years have seen an avalanche
        of digital information which is overwhelming people in industry, government,
        and academics. This avalanche is two-sided: while the past decade has seen
        an onslaught of digitized records \u2013 as governments, publishers, and researchers
        race to make their records digital, the electronic and software tools for
        computationally analyzing this data have quickly evolved to face this challenge.
        \nMany of these challenges evolve around recurring patterns, including the
        presence of text, bits of information about pairs of items, and sequential
        observations. In this work we present several methods to address these challenges
        in data analysis which take advantage of these recurring patterns. \nWe begin
        with a method for identifying influential documents in a collection which
        evolves over time. We demonstrate that by encoding our assumptions about influential
        documents in a statistical model of the changes in textual themes, we are
        able to provide an alternative bibliometric which provides results consistent
        with\u2014yet different from\u2014traditional metrics of influence such as
        citation counts. \nWe then introduce a model for measuring the relationships
        between pairs of countries over time. We will demonstrate that this model
        is able to learn meaningful relationships between countries which is extraordinarily
        consistent across different human labels. \nWe next address limitations in
        existing models of legislative voting. In one extention we predict legislators''
        votes by using the text of the bills they are voting on combined with individual
        legislators'' past voting behavior. We then introduce a method for inferring
        these lawmakers'' positions on specific issues. \nA recurring theme in the
        methods we present is that by using a small set of statistical primitives,
        we are able to apply known (or mildly adapted) methods to new problems. Several
        advances in the past few decades in statistical modeling will make the development
        and discussion of our models easier, as they will provide both this set of
        primitives (which can be interchanged easily) and the tools for working with
        them. As a final contribution, we describe a new method for fitting a statistical
        model with variational inference, without the time investment typically required
        of practitioners.", "venue": "", "year": 2013, "referenceCount": 93, "citationCount":
        8, "influentialCitationCount": 0, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
        "Computer Science", "source": "external"}, {"category": "Computer Science",
        "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": null,
        "journal": {"volume": "", "name": ""}, "authors": [{"authorId": "1796335",
        "name": "D. Blei"}, {"authorId": "21007048", "name": "S. Gerrish"}]}, {"paperId":
        "05a55d4d3935c9517c91deef8df76447cb93b7f4", "externalIds": {"MAG": "2123348968",
        "DBLP": "conf/nips/WangB12", "CorpusId": 42656525}, "corpusId": 42656525,
        "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd", "name":
        "Neural Information Processing Systems", "type": "conference", "alternate_names":
        ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
        "url": "https://www.semanticscholar.org/paper/05a55d4d3935c9517c91deef8df76447cb93b7f4",
        "title": "Truncation-free Online Variational Inference for Bayesian Nonparametric
        Models", "abstract": "We present a truncation-free online variational inference
        algorithm for Bayesian nonparametric models. Unlike traditional (online) variational
        inference algorithms that require truncations for the model or the variational
        distribution, our method adapts model complexity on the fly. Our experiments
        for Dirichlet process mixture models and hierarchical Dirichlet process topic
        models on two large-scale data sets show better performance than previous
        online variational inference algorithms.", "venue": "Neural Information Processing
        Systems", "year": 2012, "referenceCount": 38, "citationCount": 63, "influentialCitationCount":
        11, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science",
        "source": "external"}, {"category": "Mathematics", "source": "external"},
        {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
        ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages":
        "422-430"}, "authors": [{"authorId": "2108881999", "name": "Chong Wang"},
        {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId": "0a8f089741ef2785dd02e6d8ca48c023e322a025",
        "externalIds": {"MAG": "962617391", "CorpusId": 60501814}, "corpusId": 60501814,
        "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/0a8f089741ef2785dd02e6d8ca48c023e322a025",
        "title": "Hierarchical bayesian modeling: efficient inference and applications",
        "abstract": "Appropriate tools for managing large-scale data, like online
        texts, images and user profiles, are becoming increasingly important. Hierarchical
        Bayesian models provide a natural framework for building these tools due to
        their flexibility in modeling real-world data. In this thesis, we describe
        a suite of efficient inference algorithms and novel models under the hierarchical
        Bayesian modeling framework. \nWe first present a novel online inference algorithm
        for the hierarchical Dirichlet process. The hierarchical Dirichlet process
        (HDP) is a Bayesian nonparametric model that can be used to model mixed-membership
        data with a potentially infinite number of components. Our online variational
        inference algorithm is easily applicable to massive and streaming data and
        significantly faster than traditional inference algorithms. \nSecond, we present
        a generic approximation framework for variational inference in a large family
        of nonconjugate models. For example, this includes multi-level logistic regression/generalized
        linear models and correlated topic models. With this, developing variational
        inference algorithm for many nonconjugate models is much easier. \nFinally,
        we describe two novel models for real-world applications. This first application
        is about simultaneous image classification and annotation. We show that image
        classification and annotation can be integrated together using the same underlying
        probabilistic model. The second application is to better disseminate scientific
        information using recommendations. Compared with traditional recommendation
        algorithms, our algorithm not only improves the recommendation accuracy, but
        also provides interpretable structure of users and scientific articles. This
        interpretability provides lots of potential for designing better recommender
        systems.", "venue": "", "year": 2012, "referenceCount": 0, "citationCount":
        0, "influentialCitationCount": 0, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
        "Computer Science", "source": "external"}, {"category": "Computer Science",
        "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": null,
        "journal": {"volume": "", "name": ""}, "authors": [{"authorId": "1796335",
        "name": "D. Blei"}, {"authorId": "2108881999", "name": "Chong Wang"}]}, {"paperId":
        "1658c6306d019819abe8f70eb2cb4df1465745dd", "externalIds": {"MAG": "1550123145",
        "ArXiv": "1209.6004", "DBLP": "journals/corr/abs-1209-6004", "CorpusId": 5844321},
        "corpusId": 5844321, "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
        "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url":
        "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/1658c6306d019819abe8f70eb2cb4df1465745dd",
        "title": "The Issue-Adjusted Ideal Point Model", "abstract": "We develop a
        model of issue-specific voting behavior. This model can be used to explore
        lawmakers'' personal voting patterns of voting by issue area, providing an
        exploratory window into how the language of the law is correlated with political
        support. We derive approximate posterior inference algorithms based on variational
        methods. Across 12 years of legislative data, we demonstrate both improvement
        in heldout prediction performance and the model''s utility in interpreting
        an inherently multi-dimensional space.", "venue": "arXiv.org", "year": 2012,
        "referenceCount": 42, "citationCount": 4, "influentialCitationCount": 0, "isOpenAccess":
        false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
        "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
        {"category": "Mathematics", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
        "publicationDate": "2012-09-26", "journal": {"volume": "abs/1209.6004", "name":
        "ArXiv"}, "authors": [{"authorId": "21007048", "name": "S. Gerrish"}, {"authorId":
        "1796335", "name": "D. Blei"}]}, {"paperId": "24fc405aea2faf86ac27f51fbe403433c8fddc7c",
        "externalIds": {"CorpusId": 9594232}, "corpusId": 9594232, "publicationVenue":
        null, "url": "https://www.semanticscholar.org/paper/24fc405aea2faf86ac27f51fbe403433c8fddc7c",
        "title": "Factor Topographic Latent Source Analysis : Factor Analysis for
        Brain Images ?", "abstract": "Traditional approaches to analyzing experimental
        functional magnetic resonance imaging (fMRI) data entail fitting per-voxel
        parameters to explain how the observed images reflect the thoughts and stimuli
        a participant experienced during the experiment. These methods implicitly
        assume that voxel responses are independent and that the unit of analysis
        should be the voxel. However, both of these assumptions are known to be untrue:
        it is well known that voxel activations exhibit strong spatial correlations,
        and common sense tells us that the true underlying brain activations are independent
        of the resolution at which the brain image happened to be taken. Here we propose
        a fundamentally different approach, whereby brain images are represented as
        weighted sums of spatial functions. Our technique yields compact representations
        of the brain images that leverage spatial correlations in the data and are
        independent of the image resolution.", "venue": "", "year": 2012, "referenceCount":
        17, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category":
        "Psychology", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate":
        null, "journal": null, "authors": [{"authorId": "2586738", "name": "Jeremy
        R. Manning"}, {"authorId": "1831199", "name": "S. Gershman"}, {"authorId":
        "1780319", "name": "K. Norman"}, {"authorId": "1796335", "name": "D. Blei"}]},
        {"paperId": "2ecf9a574ef868a48fadd94bd5ca45f6a051b49a", "externalIds": {"ArXiv":
        "1210.6738", "DBLP": "journals/pami/PaisleyWBJ15", "MAG": "2005564522", "DOI":
        "10.1109/TPAMI.2014.2318728", "CorpusId": 9214301, "PubMed": "26353240"},
        "corpusId": 9214301, "publicationVenue": {"id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
        "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
        "type": "journal", "alternate_names": ["IEEE Trans Pattern Anal Mach Intell"],
        "issn": "0162-8828", "url": "http://www.computer.org/tpami/", "alternate_urls":
        ["http://www.computer.org/portal/web/tpami", "http://ieeexplore.ieee.org/servlet/opac?punumber=34"]},
        "url": "https://www.semanticscholar.org/paper/2ecf9a574ef868a48fadd94bd5ca45f6a051b49a",
        "title": "Nested Hierarchical Dirichlet Processes", "abstract": "We develop
        a nested hierarchical Dirichlet process (nHDP) for hierarchical topic modeling.
        The nHDP generalizes the nested Chinese restaurant process (nCRP) to allow
        each word to follow its own path to a topic node according to a per-document
        distribution over the paths on a shared tree. This alleviates the rigid, single-path
        formulation assumed by the nCRP, allowing documents to easily express complex
        thematic borrowings. We derive a stochastic variational inference algorithm
        for the model, which enables efficient inference for massive collections of
        text documents. We demonstrate our algorithm on 1.8 million documents from
        The New York Times and 2.7 million documents from Wikipedia.", "venue": "IEEE
        Transactions on Pattern Analysis and Machine Intelligence", "year": 2012,
        "referenceCount": 29, "citationCount": 210, "influentialCitationCount": 18,
        "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/1210.6738",
        "status": null}, "fieldsOfStudy": ["Medicine", "Computer Science", "Mathematics"],
        "s2FieldsOfStudy": [{"category": "Medicine", "source": "external"}, {"category":
        "Computer Science", "source": "external"}, {"category": "Mathematics", "source":
        "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle"], "publicationDate": "2012-10-25", "journal":
        {"volume": "37", "pages": "256-270", "name": "IEEE Transactions on Pattern
        Analysis and Machine Intelligence"}, "authors": [{"authorId": "143855009",
        "name": "J. Paisley"}, {"authorId": "2108881999", "name": "Chong Wang"}, {"authorId":
        "1796335", "name": "D. Blei"}, {"authorId": "1694621", "name": "Michael I.
        Jordan"}]}, {"paperId": "481eb978677eaae4e01639f03212fd81d1a5a448", "externalIds":
        {"DBLP": "conf/icml/MimnoHB12", "MAG": "2950770596", "ArXiv": "1206.6425",
        "CorpusId": 10236656}, "corpusId": 10236656, "publicationVenue": {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
        "name": "International Conference on Machine Learning", "type": "conference",
        "alternate_names": ["ICML", "Int Conf Mach Learn"], "url": "https://icml.cc/"},
        "url": "https://www.semanticscholar.org/paper/481eb978677eaae4e01639f03212fd81d1a5a448",
        "title": "Sparse stochastic inference for latent Dirichlet allocation", "abstract":
        "We present a hybrid algorithm for Bayesian topic models that combines the
        efficiency of sparse Gibbs sampling with the scalability of online stochastic
        inference. We used our algorithm to analyze a corpus of 1.2 million books
        (33 billion words) with thousands of topics. Our approach reduces the bias
        of variational inference and generalizes to many Bayesian hidden-variable
        models.", "venue": "International Conference on Machine Learning", "year":
        2012, "referenceCount": 19, "citationCount": 148, "influentialCitationCount":
        28, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science",
        "source": "external"}, {"category": "Mathematics", "source": "external"},
        {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
        ["JournalArticle", "Conference"], "publicationDate": "2012-06-26", "journal":
        {"volume": "", "pages": "1515-1522", "name": ""}, "authors": [{"authorId":
        "1705700", "name": "David Mimno"}, {"authorId": "28552618", "name": "M. Hoffman"},
        {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId": "59d7d8415dacd300eb4d98b0da3cb32d27503b36",
        "externalIds": {"DBLP": "conf/icwsm/ChaneyB12", "MAG": "76930416", "DOI":
        "10.1609/icwsm.v6i1.14321", "CorpusId": 14182654}, "corpusId": 14182654, "publicationVenue":
        {"id": "7dc964d5-49e6-4c37-b1c4-a7f0de1fa425", "name": "International Conference
        on Web and Social Media", "type": "conference", "alternate_names": ["Int Conf
        Weblogs Soc Media", "International Conference on Weblogs and Social Media",
        "Int Conf Web Soc Media", "ICWSM"], "url": "http://www.aaai.org/Library/ICWSM/icwsm-library.php"},
        "url": "https://www.semanticscholar.org/paper/59d7d8415dacd300eb4d98b0da3cb32d27503b36",
        "title": "Visualizing Topic Models", "abstract": "\n \n Managing large collections
        of documents is an important problem for many areas of science, industry,
        and culture. Probabilistic topic modeling offers a promising solution. Topic
        modeling is an unsupervised machine learning method that learns the underlying
        themes in a large collection of otherwise unorganized documents. This discovered
        structure summarizes and organizes the documents. However, topic models are
        high-level statistical tools\u2014a user must scrutinize numerical distributions
        to understand and explore their results. In this paper, we present a method
        for visualizing topic models. Our method creates a navigator of the documents,
        allowing users to explore the hidden structure that a topic model discovers.
        These browsing interfaces reveal meaningful patterns in a collection, helping
        end-users explore and understand its contents in new ways. We provide open
        source software of our method.\n \n", "venue": "International Conference on
        Web and Social Media", "year": 2012, "referenceCount": 11, "citationCount":
        225, "influentialCitationCount": 13, "isOpenAccess": true, "openAccessPdf":
        {"url": "https://ojs.aaai.org/index.php/ICWSM/article/download/14321/14170",
        "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
        "publicationDate": "2012-05-20", "journal": {"name": "Proceedings of the International
        AAAI Conference on Web and Social Media"}, "authors": [{"authorId": "2771308",
        "name": "A. Chaney"}, {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId":
        "6a4a39c40e2e54395ab69977331ccfcff11afa87", "externalIds": {"MAG": "1480633635",
        "DBLP": "journals/corr/abs-1201-1657", "ArXiv": "1201.1657", "CorpusId": 2835342},
        "corpusId": 2835342, "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
        "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url":
        "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/6a4a39c40e2e54395ab69977331ccfcff11afa87",
        "title": "A Split-Merge MCMC Algorithm for the Hierarchical Dirichlet Process",
        "abstract": "The hierarchical Dirichlet process (HDP) has become an important
        Bayesian nonparametric model for grouped data, such as document collections.
        The HDP is used to construct a flexible mixed-membership model where the number
        of components is determined by the data. As for most Bayesian nonparametric
        models, exact posterior inference is intractable---practitioners use Markov
        chain Monte Carlo (MCMC) or variational inference. Inspired by the split-merge
        MCMC algorithm for the Dirichlet process (DP) mixture model, we describe a
        novel split-merge MCMC sampling algorithm for posterior inference in the HDP.
        We study its properties on both synthetic data and text corpora. We find that
        split-merge MCMC for the HDP can provide significant improvements over traditional
        Gibbs sampling, and we give some understanding of the data properties that
        give rise to larger improvements.", "venue": "arXiv.org", "year": 2012, "referenceCount":
        19, "citationCount": 49, "influentialCitationCount": 7, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Mathematics", "Computer Science"],
        "s2FieldsOfStudy": [{"category": "Mathematics", "source": "external"}, {"category":
        "Computer Science", "source": "external"}, {"category": "Computer Science",
        "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
        "2012-01-08", "journal": {"volume": "abs/1201.1657", "name": "ArXiv"}, "authors":
        [{"authorId": "2108881999", "name": "Chong Wang"}, {"authorId": "1796335",
        "name": "D. Blei"}]}, {"paperId": "6ba0491f9dde8ea042ea4a49df34838b345f23c2",
        "externalIds": {"MAG": "2164613257", "DBLP": "journals/corr/abs-1206-4665",
        "ArXiv": "1206.4665", "CorpusId": 10633824}, "corpusId": 10633824, "publicationVenue":
        {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29", "name": "International Conference
        on Machine Learning", "type": "conference", "alternate_names": ["ICML", "Int
        Conf Mach Learn"], "url": "https://icml.cc/"}, "url": "https://www.semanticscholar.org/paper/6ba0491f9dde8ea042ea4a49df34838b345f23c2",
        "title": "Nonparametric variational inference", "abstract": "Variational methods
        are widely used for approximate posterior inference. However, their use is
        typically limited to families of distributions that enjoy particular conjugacy
        properties. To circumvent this limitation, we propose a family of variational
        approximations inspired by nonparametric kernel density estimation. The locations
        of these kernels and their bandwidth are treated as variational parameters
        and optimized to improve an approximate lower bound on the marginal likelihood
        of the data. Unlike most other variational approximations, using multiple
        kernels allows the approximation to capture multiple modes of the posterior.
        We demonstrate the efficacy of the nonparametric approximation with a hierarchical
        logistic regression model and a nonlinear matrix factorization model. We obtain
        predictive performance as good as or better than more specialized variational
        methods and MCMC approximations. The method is easy to apply to graphical
        models for which standard variational methods are difficult to derive.", "venue":
        "International Conference on Machine Learning", "year": 2012, "referenceCount":
        23, "citationCount": 139, "influentialCitationCount": 25, "isOpenAccess":
        false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
        "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
        {"category": "Mathematics", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
        "Conference"], "publicationDate": "2012-06-18", "journal": {"volume": "abs/1206.4665",
        "name": "ArXiv"}, "authors": [{"authorId": "1831199", "name": "S. Gershman"},
        {"authorId": "28552618", "name": "M. Hoffman"}, {"authorId": "1796335", "name":
        "D. Blei"}]}, {"paperId": "7314be5cd836c8f06bd1ecab565b00b65259eac6", "externalIds":
        {"DBLP": "journals/cacm/Blei12", "MAG": "2174706414", "DOI": "10.1145/2133806.2133826",
        "CorpusId": 753304}, "corpusId": 753304, "publicationVenue": {"id": "4d9ce1c4-dc84-46b9-903e-e3751c00c7dd",
        "name": "Communications of the ACM", "type": "journal", "alternate_names":
        ["Commun ACM", "Communications of The ACM"], "issn": "0001-0782", "url": "http://www.acm.org/pubs/cacm/",
        "alternate_urls": ["http://portal.acm.org/cacm", "http://www.acm.org/pubs/contents/journals/cacm/",
        "https://cacm.acm.org/"]}, "url": "https://www.semanticscholar.org/paper/7314be5cd836c8f06bd1ecab565b00b65259eac6",
        "title": "Probabilistic topic models", "abstract": "Surveying a suite of algorithms
        that offer a solution to managing large document archives.", "venue": "Communications
        of the ACM", "year": 2012, "referenceCount": 46, "citationCount": 5437, "influentialCitationCount":
        502, "isOpenAccess": true, "openAccessPdf": {"url": "http://dl.acm.org/ft_gateway.cfm?id=2133826&type=pdf",
        "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
        "Review"], "publicationDate": "2012-04-01", "journal": {"volume": "55", "pages":
        "77 - 84", "name": "Communications of the ACM"}, "authors": [{"authorId":
        "1796335", "name": "D. Blei"}]}, {"paperId": "8b9b56af372e4fa50ee616ddf200254ddd315cb7",
        "externalIds": {"MAG": "2187868366", "CorpusId": 263885427}, "corpusId": 263885427,
        "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/8b9b56af372e4fa50ee616ddf200254ddd315cb7",
        "title": "Surveying a suite of algorithms that offer a solution to managing
        large document archives.", "abstract": null, "venue": "", "year": 2012, "referenceCount":
        25, "citationCount": 43, "influentialCitationCount": 2, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Engineering"], "s2FieldsOfStudy":
        [{"category": "Engineering", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["Review"], "publicationDate":
        null, "journal": {"volume": "", "name": ""}, "authors": [{"authorId": "1796335",
        "name": "D. Blei"}]}, {"paperId": "8f302be0d4f9914dc21769c8b1f1aa36bb4cb7f3",
        "externalIds": {"MAG": "2564953340", "CorpusId": 15908645}, "corpusId": 15908645,
        "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd", "name":
        "Neural Information Processing Systems", "type": "conference", "alternate_names":
        ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
        "url": "https://www.semanticscholar.org/paper/8f302be0d4f9914dc21769c8b1f1aa36bb4cb7f3",
        "title": "Truncation-free stochastic variational inference for Bayesian nonparametric
        models", "abstract": "We present a truncation-free stochastic variational
        inference algorithm for Bayesian nonparametric models. While traditional variational
        inference algorithms require truncations for the model or the variational
        distribution, our method adapts model complexity on the fly. We studied our
        method with Dirichlet process mixture models and hierarchical Dirichlet process
        topic models on two large data sets. Our method performs better than previous
        stochastic variational inference algorithms.", "venue": "Neural Information
        Processing Systems", "year": 2012, "referenceCount": 46, "citationCount":
        38, "influentialCitationCount": 4, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": ["Mathematics"], "s2FieldsOfStudy": [{"category": "Mathematics",
        "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"},
        {"category": "Mathematics", "source": "s2-fos-model"}], "publicationTypes":
        null, "publicationDate": "2012-12-03", "journal": {"volume": "", "pages":
        "413-421", "name": ""}, "authors": [{"authorId": "2108881999", "name": "Chong
        Wang"}, {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId": "9805ddd8b0a92764b6df18b0408e8ef78d505e15",
        "externalIds": {"MAG": "2103841034", "DBLP": "conf/nips/GerrishB12", "CorpusId":
        8218322}, "corpusId": 8218322, "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
        "name": "Neural Information Processing Systems", "type": "conference", "alternate_names":
        ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
        "url": "https://www.semanticscholar.org/paper/9805ddd8b0a92764b6df18b0408e8ef78d505e15",
        "title": "How They Vote: Issue-Adjusted Models of Legislative Behavior", "abstract":
        "We develop a probabilistic model of legislative data that uses the text of
        the bills to uncover lawmakers'' positions on specific political issues. Our
        model can be used to explore how a lawmaker''s voting patterns deviate from
        what is expected and how that deviation depends on what is being voted on.
        We derive approximate posterior inference algorithms based on variational
        methods. Across 12 years of legislative data, we demonstrate both improvement
        in heldout predictive performance and the model''s utility in interpreting
        an inherently multi-dimensional space.", "venue": "Neural Information Processing
        Systems", "year": 2012, "referenceCount": 27, "citationCount": 107, "influentialCitationCount":
        4, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
        "external"}, {"category": "Economics", "source": "s2-fos-model"}], "publicationTypes":
        ["JournalArticle", "Conference"], "publicationDate": "2012-12-03", "journal":
        {"pages": "2762-2770"}, "authors": [{"authorId": "21007048", "name": "S. Gerrish"},
        {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId": "9a5eb8fa8df5d3311b27b895ea1af67b5e35f7ff",
        "externalIds": {"MAG": "2102111939", "DBLP": "conf/nips/GopalanMGFB12", "CorpusId":
        10058448}, "corpusId": 10058448, "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
        "name": "Neural Information Processing Systems", "type": "conference", "alternate_names":
        ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
        "url": "https://www.semanticscholar.org/paper/9a5eb8fa8df5d3311b27b895ea1af67b5e35f7ff",
        "title": "Scalable Inference of Overlapping Communities", "abstract": "We
        develop a scalable algorithm for posterior inference of overlapping communities
        in large networks. Our algorithm is based on stochastic variational inference
        in the mixed-membership stochastic blockmodel (MMSB). It naturally interleaves
        subsampling the network with estimating its community structure. We apply
        our algorithm on ten large, real-world networks with up to 60,000 nodes. It
        converges several orders of magnitude faster than the state-of-the-art algorithm
        for MMSB, finds hundreds of communities in large real-world networks, and
        detects the true communities in 280 benchmark networks with equal or better
        accuracy compared to other scalable algorithms.", "venue": "Neural Information
        Processing Systems", "year": 2012, "referenceCount": 28, "citationCount":
        110, "influentialCitationCount": 24, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics",
        "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2012-12-03",
        "journal": {"pages": "2258-2266"}, "authors": [{"authorId": "2141648", "name":
        "Prem Gopalan"}, {"authorId": "1705700", "name": "David Mimno"}, {"authorId":
        "21007048", "name": "S. Gerrish"}, {"authorId": "3122063", "name": "M. Freedman"},
        {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId": "9ceb1dea15ac3df3d610fd0b3cc52b9a4e9141a3",
        "externalIds": {"DBLP": "conf/icml/PaisleyBJ12", "MAG": "2951493172", "ArXiv":
        "1206.6430", "CorpusId": 1758804}, "corpusId": 1758804, "publicationVenue":
        {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29", "name": "International Conference
        on Machine Learning", "type": "conference", "alternate_names": ["ICML", "Int
        Conf Mach Learn"], "url": "https://icml.cc/"}, "url": "https://www.semanticscholar.org/paper/9ceb1dea15ac3df3d610fd0b3cc52b9a4e9141a3",
        "title": "Variational Bayesian Inference with Stochastic Search", "abstract":
        "Mean-field variational inference is a method for approximate Bayesian posterior
        inference. It approximates a full posterior distribution with a factorized
        set of distributions by maximizing a lower bound on the marginal likelihood.
        This requires the ability to integrate a sum of terms in the log joint likelihood
        using this factorized distribution. Often not all integrals are in closed
        form, which is typically handled by using a lower bound. We present an alternative
        algorithm based on stochastic optimization that allows for direct optimization
        of the variational lower bound. This method uses control variates to reduce
        the variance of the stochastic search gradient, in which existing lower bounds
        can play an important role. We demonstrate the approach on two non-conjugate
        models: logistic regression and an approximation to the HDP.", "venue": "International
        Conference on Machine Learning", "year": 2012, "referenceCount": 14, "citationCount":
        461, "influentialCitationCount": 33, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics",
        "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2012-06-26",
        "journal": {"volume": "", "pages": "1363-1370", "name": ""}, "authors": [{"authorId":
        "143855009", "name": "J. Paisley"}, {"authorId": "1796335", "name": "D. Blei"},
        {"authorId": "1694621", "name": "Michael I. Jordan"}]}, {"paperId": "bccb2f99a9d1c105699f5d88c479569085e2c7ba",
        "externalIds": {"MAG": "2166851633", "DBLP": "journals/jmlr/HoffmanBWP13",
        "ArXiv": "1206.7051", "DOI": "10.5555/2567709.2502622", "CorpusId": 5652538},
        "corpusId": 5652538, "publicationVenue": {"id": "c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
        "name": "Journal of machine learning research", "type": "journal", "alternate_names":
        ["Journal of Machine Learning Research", "J mach learn res", "J Mach Learn
        Res"], "issn": "1532-4435", "alternate_issns": ["1533-7928"], "url": "http://www.ai.mit.edu/projects/jmlr/",
        "alternate_urls": ["http://jmlr.csail.mit.edu/", "http://www.jmlr.org/", "http://portal.acm.org/affiliated/jmlr"]},
        "url": "https://www.semanticscholar.org/paper/bccb2f99a9d1c105699f5d88c479569085e2c7ba",
        "title": "Stochastic variational inference", "abstract": "We develop stochastic
        variational inference, a scalable algorithm for approximating posterior distributions.
        We develop this technique for a large class of probabilistic models and we
        demonstrate it with two probabilistic topic models, latent Dirichlet allocation
        and the hierarchical Dirichlet process topic model. Using stochastic variational
        inference, we analyze several large collections of documents: 300K articles
        from Nature, 1.8M articles from The New York Times, and 3.8M articles from
        Wikipedia. Stochastic inference can easily handle data sets of this size and
        outperforms traditional variational inference, which can only handle a smaller
        subset. (We also show that the Bayesian nonparametric topic model outperforms
        its parametric counterpart.) Stochastic variational inference lets us apply
        complex Bayesian models to massive data sets.", "venue": "Journal of machine
        learning research", "year": 2012, "referenceCount": 116, "citationCount":
        2304, "influentialCitationCount": 344, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": ["Mathematics", "Computer Science"], "s2FieldsOfStudy":
        [{"category": "Mathematics", "source": "external"}, {"category": "Computer
        Science", "source": "external"}, {"category": "Computer Science", "source":
        "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
        "2012-06-29", "journal": {"volume": "14", "pages": "1303-1347", "name": "J.
        Mach. Learn. Res."}, "authors": [{"authorId": "28552618", "name": "M. Hoffman"},
        {"authorId": "1796335", "name": "D. Blei"}, {"authorId": "2108881999", "name":
        "Chong Wang"}, {"authorId": "143855009", "name": "J. Paisley"}]}, {"paperId":
        "c95af7ce5d123e48191d5ea256029a1542558095", "externalIds": {"MAG": "2204613168",
        "DBLP": "journals/jmlr/PaisleyBJ12", "CorpusId": 16166621}, "corpusId": 16166621,
        "publicationVenue": {"id": "2d136b11-c2b5-484b-b008-7f4a852fd61e", "name":
        "International Conference on Artificial Intelligence and Statistics", "type":
        "conference", "alternate_names": ["AISTATS", "Int Conf Artif Intell Stat"]},
        "url": "https://www.semanticscholar.org/paper/c95af7ce5d123e48191d5ea256029a1542558095",
        "title": "Stick-Breaking Beta Processes and the Poisson Process", "abstract":
        "We show that the stick-breaking construction of the beta process due to Paisley
        et al. (2010) can be obtained from the characterization of the beta process
        as a Poisson process. Specifically, we show that the mean measure of the underlying
        Poisson process is equal to that of the beta process. We use this underlying
        representation to derive error bounds on truncated beta processes that are
        tighter than those in the literature. We also develop a new MCMC inference
        algorithm for beta processes, based in part on our new Poisson process construction.",
        "venue": "International Conference on Artificial Intelligence and Statistics",
        "year": 2012, "referenceCount": 24, "citationCount": 53, "influentialCitationCount":
        5, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
        "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category":
        "Mathematics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
        "publicationDate": "2012-03-21", "journal": {"pages": "850-858"}, "authors":
        [{"authorId": "143855009", "name": "J. Paisley"}, {"authorId": "1796335",
        "name": "D. Blei"}, {"authorId": "1694621", "name": "Michael I. Jordan"}]},
        {"paperId": "c9db97118af813a87eb355c5a80671364c9ebbe1", "externalIds": {"MAG":
        "2952304124", "ArXiv": "1209.4360", "DBLP": "journals/jmlr/WangB13", "DOI":
        "10.5555/2567709.2502613", "CorpusId": 17702332}, "corpusId": 17702332, "publicationVenue":
        {"id": "c22e7c36-3bfa-43e1-bb7b-edccdea2a780", "name": "Journal of machine
        learning research", "type": "journal", "alternate_names": ["Journal of Machine
        Learning Research", "J mach learn res", "J Mach Learn Res"], "issn": "1532-4435",
        "alternate_issns": ["1533-7928"], "url": "http://www.ai.mit.edu/projects/jmlr/",
        "alternate_urls": ["http://jmlr.csail.mit.edu/", "http://www.jmlr.org/", "http://portal.acm.org/affiliated/jmlr"]},
        "url": "https://www.semanticscholar.org/paper/c9db97118af813a87eb355c5a80671364c9ebbe1",
        "title": "Variational inference in nonconjugate models", "abstract": "Mean-field
        variational methods are widely used for approximate posterior inference in
        many probabilistic models. In a typical application, mean-field methods approximately
        compute the posterior with a coordinate-ascent optimization algorithm. When
        the model is conditionally conjugate, the coordinate updates are easily derived
        and in closed form. However, many models of interest--like the correlated
        topic model and Bayesian logistic regression--are nonconjugate. In these models,
        mean-field methods cannot be directly applied and practitioners have had to
        develop variational algorithms on a case-by-case basis. In this paper, we
        develop two generic methods for nonconjugate models, Laplace variational inference
        and delta method variational inference. Our methods have several advantages:
        they allow for easily derived variational algorithms with a wide class of
        nonconjugate models; they extend and unify some of the existing algorithms
        that have been derived for specific models; and they work well on real-world
        data sets. We studied our methods on the correlated topic model, Bayesian
        logistic regression, and hierarchical Bayesian logistic regression.", "venue":
        "Journal of machine learning research", "year": 2012, "referenceCount": 58,
        "citationCount": 207, "influentialCitationCount": 17, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Mathematics", "Computer Science"],
        "s2FieldsOfStudy": [{"category": "Mathematics", "source": "external"}, {"category":
        "Computer Science", "source": "external"}, {"category": "Computer Science",
        "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
        "2012-09-19", "journal": {"volume": "14", "pages": "1005-1031", "name": "J.
        Mach. Learn. Res."}, "authors": [{"authorId": "2108881999", "name": "Chong
        Wang"}, {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId": "cff80c31103745ae319bbd62a8d226343d3e4daf",
        "externalIds": {"ArXiv": "1209.1996", "MAG": "1756403341", "CorpusId": 88519221},
        "corpusId": 88519221, "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/cff80c31103745ae319bbd62a8d226343d3e4daf",
        "title": "A Bayesian Boosting Model", "abstract": "We offer a novel view of
        AdaBoost in a statistical setting. We propose a Bayesian model for binary
        classification in which label noise is modeled hierarchically. Using variational
        inference to optimize a dynamic evidence lower bound, we derive a new boosting-like
        algorithm called VIBoost. We show its close connections to AdaBoost and give
        experimental results from four datasets.", "venue": "", "year": 2012, "referenceCount":
        25, "citationCount": 6, "influentialCitationCount": 0, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Mathematics"], "s2FieldsOfStudy":
        [{"category": "Mathematics", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate":
        "2012-09-10", "journal": {"volume": "", "name": "arXiv: Machine Learning"},
        "authors": [{"authorId": "1863189", "name": "Alexander Lorbert"}, {"authorId":
        "1796335", "name": "D. Blei"}, {"authorId": "1716301", "name": "R. Schapire"},
        {"authorId": "1693135", "name": "P. Ramadge"}]}, {"paperId": "dd6def73c4dfeb8c314f77b2fe9119c52f36871b",
        "externalIds": {"MAG": "2275207081", "CorpusId": 61317510}, "corpusId": 61317510,
        "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/dd6def73c4dfeb8c314f77b2fe9119c52f36871b",
        "title": "Extracting information from high-dimensional data: probabilistic
        modeling, inference and evaluation", "abstract": "In this thesis, we shall
        derive, in a variety of settings, and for different applications, efficient
        posterior inference algorithms handling large data sets, and use side information
        to derive superior inference techniques. We demonstrate the efficiency and
        accuracy of those models and algorithms in the different applications, on
        both real and synthetic data sets. We evaluate the quality of the results,
        with both quantitative and human evaluation experiments. \nIn the first part
        of the thesis the general framework is that of sparsity: we assume the data
        have a sparse representation; the application on which we focus is image super-resolution,
        in which one seeks to \"up-scale images\", i.e. \"reconstruct\" finer detail
        in an image than given in the data. Image super-resolution has been tackled
        successfully via sparse coding but not, so far, by Bayesian nonparametric
        methods (BNM). In other contexts, BNMs were shown to be powerful because they
        infer parameters that otherwise have to be assigned a priori. We build here
        the tools enabling such a BNM for the super-resolution of images. We start
        with building a sparse nonparametric factor analysis model for image super-resolution,
        more precisely, a model with a beta-Bernoulli process to learn the number
        of dictionary elements from the data. We test the results on both benchmark
        and natural images, comparing with the models in the literature. Then, we
        perform large-scale human evaluation experiments to explicitly assess the
        visual quality of the results. In a first implementation, we use Gibbs sampling,
        operating on the data in batch mode, and assess its performance. However,
        for large-scale data, such a Gibbs sampling approach is typically not feasible.
        To circumvent this, we develop an online variational Bayes (VB) algorithm
        that can deal with larger-scale data in a fraction of the time needed by traditional
        inference. \nIn the second part of the thesis we consider data sets with rich
        side information. We study 2 different frameworks that have such side information:
        relational information and group information. To handle relational information,
        we build a relational factor analysis (rFA) model which incorporates this
        into the dictionary learning. We show that the use of relational information
        (e.g. spatial location), helps learning higher quality dictionaries and improves
        the recommendation systems in a social network and the image analysis algorithms
        (e.g. image inpainting). To handle group information, we propose a multi-task
        learning framework for image super-resolution problem using a hierarchical
        beta-process as a prior to dictionary assignments. In this model, we study
        grouped data and we build a model incorporating the group information. We
        show that by incorporating group information in this way the algorithm avoids
        erroneous selection of dictionary elements. \nFinally, in the third part of
        the thesis, we study latent sequential information between observations. We
        use this information to build a novel dynamic programming algorithm for sequential
        models. Hidden Markov models (HMMs) and conditional random fields (CRFs) are
        two popular techniques for modeling sequential data. Inference algorithms
        designed over CRFs and HMMs allow estimation of the state sequence, given
        the observations. In several applications, the end goal is not the estimation
        of the state sequence, but rather the estimation of the value of some function
        of the state sequence. In such scenarios, estimating the state sequence by
        conventional inference techniques, followed by computing the functional mapping
        from this estimate, is not necessarily optimal; it may be more efficient to
        directly infer the final outcome from the observations. In particular, we
        consider the specific instantiation of the problem where the goal is to find
        the state trajectories without exact transition points; we derive a novel
        polynomial time inference algorithm that outperforms vanilla inference techniques.
        We show that this particular problem arises commonly in many disparate applications
        and present the results for experiments on three different applications: (1)
        Toy robot tracking; (2) Single stroke character recognition; (3) Handwritten
        word recognition. (Abstract shortened by UMI.)", "venue": "", "year": 2012,
        "referenceCount": 0, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess":
        false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate":
        null, "journal": {"volume": "", "pages": "179-179", "name": ""}, "authors":
        [{"authorId": "1737063", "name": "I. Daubechies"}, {"authorId": "1796335",
        "name": "D. Blei"}, {"authorId": "2767134", "name": "Gungor Polatkan"}]},
        {"paperId": "19a40edd529a857db684843f738de13f7be6b09c", "externalIds": {"MAG":
        "96658113", "CorpusId": 35192312}, "corpusId": 35192312, "publicationVenue":
        null, "url": "https://www.semanticscholar.org/paper/19a40edd529a857db684843f738de13f7be6b09c",
        "title": "An Approach to Discovery and Re-ranking of Educational content from
        the World Wide Web using Latent Dirichlet Allocation", "abstract": "With tremendous
        increase in the amount of digital data available educators are forced to author
        content for learning and teaching for use in their classes. With that there
        has emerged a need to facilitate automatic discovery of learning resources
        from the World Wide Web. In this work, we present a novel approach for discovering
        content from the web for e-learning. We argue that for an e-learning scenario,
        retrieval of the redundant content from the web is a serious problem to be
        addressed as it does not satisfy the requirements of a typical learner. Furthermore,
        the content retrieved should cover all topics as in his syllabus. Sense-disambiguation
        should be performed during information retrieval from the web so that it corresponds
        to the learner\u201fs actual domain of interest. This work presents a domain
        ontology based re-querying approach for query expansion to discover content
        from open corpus sources. We use the Latent Dirichlet Allocation Model for
        unsupervised classification of document segments to aid students and educators.
        Having identified the topics at the granularity of document segments in an
        unsupervised fashion, we state that internal topic transitions in a resource
        retrieved from the web can be exploited for providing relevant and personalized
        content. In addition to this, we propose a re-ranking scheme for ordering
        results from search engines to maximize topic coverage and minimize redundancy
        among retrieved results. We also evaluate the effectiveness of our proposed
        method for information retrieval and show that our work results in greater
        coverage of topics from the web without redundancy.", "venue": "", "year":
        2011, "referenceCount": 20, "citationCount": 0, "influentialCitationCount":
        0, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
        "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": null, "publicationDate": "2011-06-15", "journal": {"volume":
        "", "pages": "24-30", "name": "International Journal of Computer Applications"},
        "authors": [{"authorId": "1796335", "name": "D. Blei"}, {"authorId": "34699434",
        "name": "A. Ng"}, {"authorId": "1694621", "name": "Michael I. Jordan"}]},
        {"paperId": "2ac8aea06688995e14fc0b8a1bb19cbfc1d4f974", "externalIds": {"MAG":
        "1646684083", "ArXiv": "1109.0343", "CorpusId": 88518622}, "corpusId": 88518622,
        "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/2ac8aea06688995e14fc0b8a1bb19cbfc1d4f974",
        "title": "The Stick-Breaking Construction of the Beta Process as a Poisson
        Process", "abstract": "We show that the stick-breaking construction of the
        beta process due to Paisley, et al. (2010) can be obtained from the characterization
        of the beta process as a Poisson process. Specifically, we show that the mean
        measure of the underlying Poisson process is equal to that of the beta process.
        We use this underlying representation to derive error bounds on truncated
        beta processes that are tighter than those in the literature. We also develop
        a new MCMC inference algorithm for beta processes, based in part on our new
        Poisson process construction.", "venue": "", "year": 2011, "referenceCount":
        15, "citationCount": 4, "influentialCitationCount": 1, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Mathematics"], "s2FieldsOfStudy":
        [{"category": "Mathematics", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}, {"category": "Mathematics", "source":
        "s2-fos-model"}], "publicationTypes": null, "publicationDate": "2011-09-02",
        "journal": {"volume": "", "name": "arXiv: Statistics Theory"}, "authors":
        [{"authorId": "143855009", "name": "J. Paisley"}, {"authorId": "1796335",
        "name": "D. Blei"}, {"authorId": "1694621", "name": "Michael I. Jordan"}]},
        {"paperId": "4ad4fa27aa1cd113b1e50e1e542cad845e34dcae", "externalIds": {"MAG":
        "278695144", "DOI": "10.21236/ada554210", "CorpusId": 55859388}, "corpusId":
        55859388, "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/4ad4fa27aa1cd113b1e50e1e542cad845e34dcae",
        "title": "Dynamic and Supervised Topic Models for Literature-Based Discovery",
        "abstract": "Abstract : Under the support of the ONR my research focused on
        extending the state ot the an or probabilistic topic modeling, algorithms
        for making discoveries from and predictions about large collections of texts.
        For the past three years, my group has published many papers in the service
        of this goal. In this report, I will highlight some of the themes and publications
        that represent this work. Thanks to the support of the ONR, we have made excellent
        progress in our stated goals.", "venue": "", "year": 2011, "referenceCount":
        23, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate":
        "2011-12-21", "journal": {"volume": "", "name": ""}, "authors": [{"authorId":
        "1796335", "name": "D. Blei"}]}, {"paperId": "57df462188c1b97bd3898f54161ba85f474116b6",
        "externalIds": {"ArXiv": "1106.2697", "MAG": "2949943633", "DOI": "10.1016/J.JMP.2011.08.004",
        "CorpusId": 16074636}, "corpusId": 16074636, "publicationVenue": null, "url":
        "https://www.semanticscholar.org/paper/57df462188c1b97bd3898f54161ba85f474116b6",
        "title": "A Tutorial on Bayesian Nonparametric Models", "abstract": null,
        "venue": "", "year": 2011, "referenceCount": 93, "citationCount": 542, "influentialCitationCount":
        37, "isOpenAccess": true, "openAccessPdf": {"url": "http://arxiv.org/pdf/1106.2697",
        "status": null}, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics",
        "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["Review"], "publicationDate": "2011-06-14", "journal":
        {"volume": "56", "pages": "1-12", "name": "Journal of Mathematical Psychology"},
        "authors": [{"authorId": "1831199", "name": "S. Gershman"}, {"authorId": "1796335",
        "name": "D. Blei"}]}, {"paperId": "593cb435702d2e6e7ed8c84dec1c3154bcc11df9",
        "externalIds": {"MAG": "2185743839", "ArXiv": "1103.4789", "DBLP": "journals/jmlr/PaisleyWB11",
        "DOI": "10.1214/12-BA734", "CorpusId": 6458254}, "corpusId": 6458254, "publicationVenue":
        {"id": "2d136b11-c2b5-484b-b008-7f4a852fd61e", "name": "International Conference
        on Artificial Intelligence and Statistics", "type": "conference", "alternate_names":
        ["AISTATS", "Int Conf Artif Intell Stat"]}, "url": "https://www.semanticscholar.org/paper/593cb435702d2e6e7ed8c84dec1c3154bcc11df9",
        "title": "The Discrete Infinite Logistic Normal Distribution for Mixed-Membership
        Modeling", "abstract": "We present the discrete innite logistic normal distribution
        (DILN), a Bayesian nonparametric prior for mixed membership models. DILN generalizes
        the hierarchical Dirichlet process (HDP) to model correlation structure between
        the weights of the atoms at the group level. We derive a representation of
        DILN as a normalized collection of gamma-distributed random variables and
        study its statistical properties. We derive a variational inference algorithm
        for approximate posterior inference. We apply DILN to topic modeling of documents
        and study its empirical performance on four corpora, comparing performance
        with the HDP and the correlated topic model (CTM). To compute with large-scale
        data, we develop a stochastic variational inference algorithm for DILN and
        compare with similar algorithms for HDP and latent Dirichlet allocation (LDA)
        on a collection of 350; 000 articles from Nature.", "venue": "International
        Conference on Artificial Intelligence and Statistics", "year": 2011, "referenceCount":
        53, "citationCount": 85, "influentialCitationCount": 14, "isOpenAccess": true,
        "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
        "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
        {"category": "Mathematics", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
        "publicationDate": "2011-03-24", "journal": {"pages": "74-82"}, "authors":
        [{"authorId": "143855009", "name": "J. Paisley"}, {"authorId": "2108881999",
        "name": "Chong Wang"}, {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId":
        "5d311ef363f831a903ead8997c493be51609346a", "externalIds": {"DBLP": "journals/neuroimage/GershmanBPN11",
        "MAG": "2043686249", "DOI": "10.1016/j.neuroimage.2011.04.042", "CorpusId":
        5864838, "PubMed": "21549204"}, "corpusId": 5864838, "publicationVenue": {"id":
        "fd4c7628-c16e-4b50-8555-3ac3ad6da2d7", "name": "NeuroImage", "type": "journal",
        "issn": "1053-8119", "url": "http://www.elsevier.com/locate/ynimg", "alternate_urls":
        ["http://www.elsevier.com/wps/find/journaldescription.cws_home/622925/description#description",
        "https://www.journals.elsevier.com/neuroimage", "http://www.sciencedirect.com/science/journal/10538119",
        "http://www.idealibrary.com/"]}, "url": "https://www.semanticscholar.org/paper/5d311ef363f831a903ead8997c493be51609346a",
        "title": "A topographic latent source model for fMRI data", "abstract": null,
        "venue": "NeuroImage", "year": 2011, "referenceCount": 47, "citationCount":
        31, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf":
        {"url": "http://europepmc.org/articles/pmc3101582?pdf=render", "status": null},
        "fieldsOfStudy": ["Computer Science", "Medicine"], "s2FieldsOfStudy": [{"category":
        "Computer Science", "source": "external"}, {"category": "Medicine", "source":
        "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle"], "publicationDate": "2011-07-01", "journal":
        {"volume": "57", "pages": "89-100", "name": "NeuroImage"}, "authors": [{"authorId":
        "1831199", "name": "S. Gershman"}, {"authorId": "1796335", "name": "D. Blei"},
        {"authorId": "144637670", "name": "Francisco Pereira"}, {"authorId": "1780319",
        "name": "K. Norman"}]}, {"paperId": "62e14dec73970514a5e3f81b059d63b34e9ad37c",
        "externalIds": {"MAG": "2145767445", "DBLP": "conf/icml/GerrishB11", "CorpusId":
        9758297}, "corpusId": 9758297, "publicationVenue": {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
        "name": "International Conference on Machine Learning", "type": "conference",
        "alternate_names": ["ICML", "Int Conf Mach Learn"], "url": "https://icml.cc/"},
        "url": "https://www.semanticscholar.org/paper/62e14dec73970514a5e3f81b059d63b34e9ad37c",
        "title": "Predicting Legislative Roll Calls from Text", "abstract": "We develop
        several predictive models linking legislative sentiment to legislative text.
        Our models, which draw on ideas from ideal point estimation and topic models,
        predict voting patterns based on the contents of bills and infer the political
        leanings of legislators. With supervised topics, we provide an exploratory
        window into how the language of the law is correlated with political support.
        We also derive approximate posterior inference algorithms based on variational
        methods. Across 12 years of legislative data, we predict specific voting patterns
        with high accuracy.", "venue": "International Conference on Machine Learning",
        "year": 2011, "referenceCount": 20, "citationCount": 185, "influentialCitationCount":
        18, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
        "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2011-06-28",
        "journal": {"pages": "489-496"}, "authors": [{"authorId": "21007048", "name":
        "S. Gerrish"}, {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId": "7bbd25e76044e3bb7c3a9dfe4dba5ebeb4e95925",
        "externalIds": {"DBLP": "conf/icml/PaisleyCB11", "MAG": "2219920167", "CorpusId":
        10549399}, "corpusId": 10549399, "publicationVenue": {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
        "name": "International Conference on Machine Learning", "type": "conference",
        "alternate_names": ["ICML", "Int Conf Mach Learn"], "url": "https://icml.cc/"},
        "url": "https://www.semanticscholar.org/paper/7bbd25e76044e3bb7c3a9dfe4dba5ebeb4e95925",
        "title": "Variational Inference for Stick-Breaking Beta Process Priors", "abstract":
        "We present a variational Bayesian inference algorithm for the stick-breaking
        construction of the beta process. We derive an alternate representation of
        the beta process that is amenable to variational inference, and present a
        bound relating the truncated beta process to its infinite counterpart. We
        assess performance on two matrix factorization problems, using a non-negative
        factorization model and a linear-Gaussian model.", "venue": "International
        Conference on Machine Learning", "year": 2011, "referenceCount": 16, "citationCount":
        40, "influentialCitationCount": 6, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": ["Mathematics", "Computer Science"], "s2FieldsOfStudy":
        [{"category": "Mathematics", "source": "external"}, {"category": "Computer
        Science", "source": "external"}, {"category": "Computer Science", "source":
        "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate":
        "2011-06-28", "journal": {"pages": "889-896"}, "authors": [{"authorId": "143855009",
        "name": "J. Paisley"}, {"authorId": "145006560", "name": "L. Carin"}, {"authorId":
        "1796335", "name": "D. Blei"}]}, {"paperId": "7d9184edefd59b8692f9f41c038446afacea931f",
        "externalIds": {"ArXiv": "1110.5454", "MAG": "2091979392", "DBLP": "journals/pami/GershmanFB15",
        "DOI": "10.1109/TPAMI.2014.2321387", "CorpusId": 1636177, "PubMed": "26353245"},
        "corpusId": 1636177, "publicationVenue": {"id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
        "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
        "type": "journal", "alternate_names": ["IEEE Trans Pattern Anal Mach Intell"],
        "issn": "0162-8828", "url": "http://www.computer.org/tpami/", "alternate_urls":
        ["http://www.computer.org/portal/web/tpami", "http://ieeexplore.ieee.org/servlet/opac?punumber=34"]},
        "url": "https://www.semanticscholar.org/paper/7d9184edefd59b8692f9f41c038446afacea931f",
        "title": "Distance Dependent Infinite Latent Feature Models", "abstract":
        "Latent feature models are widely used to decompose data into a small number
        of components. Bayesian nonparametric variants of these models, which use
        the Indian buffet process (IBP) as a prior over latent features, allow the
        number of features to be determined from the data. We present a generalization
        of the IBP, the distance dependent Indian buffet process (dd-IBP), for modeling
        non-exchangeable data. It relies on distances defined between data points,
        biasing nearby data to share more features. The choice of distance measure
        allows for many kinds of dependencies, including temporal and spatial. Further,
        the original IBP is a special case of the dd-IBP. We develop the dd-IBP and
        theoretically characterize its feature-sharing properties. We derive a Markov
        chain Monte Carlo sampler for a linear Gaussian model with a dd-IBP prior
        and study its performance on real-world non-exchangeable data.", "venue":
        "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year":
        2011, "referenceCount": 37, "citationCount": 33, "influentialCitationCount":
        4, "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/1110.5454",
        "status": null}, "fieldsOfStudy": ["Mathematics", "Computer Science", "Medicine"],
        "s2FieldsOfStudy": [{"category": "Mathematics", "source": "external"}, {"category":
        "Computer Science", "source": "external"}, {"category": "Medicine", "source":
        "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle"], "publicationDate": "2011-10-25", "journal":
        {"volume": "37", "pages": "334-345", "name": "IEEE Transactions on Pattern
        Analysis and Machine Intelligence"}, "authors": [{"authorId": "1831199", "name":
        "S. Gershman"}, {"authorId": "1922747", "name": "P. Frazier"}, {"authorId":
        "1796335", "name": "D. Blei"}]}, {"paperId": "87f553e5b5cd1f1cc833cb28235889ee8c08be36",
        "externalIds": {"MAG": "2138107145", "DOI": "10.1145/2107736.2107741", "CorpusId":
        260454704}, "corpusId": 260454704, "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/87f553e5b5cd1f1cc833cb28235889ee8c08be36",
        "title": "Probabilistic topic models", "abstract": "Probabilistic topic modeling
        provides a suite of tools for the unsupervised analysis of large collections
        of documents. Topic modeling algorithms can uncover the underlying themes
        of a collection and decompose its documents according to those themes. This
        analysis can be used for corpus exploration, document search, and a variety
        of prediction problems.\n In this tutorial, I will review the state-of-the-art
        in probabilistic topic models. I will describe the three components of topic
        modeling:\n (1) Topic modeling assumptions\n (2) Algorithms for computing
        with topic models\n (3) Applications of topic models\n In (1), I will describe
        latent Dirichlet allocation (LDA), which is one of the simplest topic models,
        and then describe a variety of ways that we can build on it. These include
        dynamic topic models, correlated topic models, supervised topic models, author-topic
        models, bursty topic models, Bayesian nonparametric topic models, and others.
        I will also discuss some of the fundamental statistical ideas that are used
        in building topic models, such as distributions on the simplex, hierarchical
        Bayesian modeling, and models of mixed-membership.\n In (2), I will review
        how we compute with topic models. I will describe approximate posterior inference
        for directed graphical models using both sampling and variational inference,
        and I will discuss the practical issues and pitfalls in developing these algorithms
        for topic models. Finally, I will describe some of our most recent work on
        building algorithms that can scale to millions of documents and documents
        arriving in a stream.\n In (3), I will discuss applications of topic models.
        These include applications to images, music, social networks, and other data
        in which we hope to uncover hidden patterns. I will describe some of our recent
        work on adapting topic modeling algorithms to collaborative filtering, legislative
        modeling, and bibliometrics without citations.\n Finally, I will discuss some
        future directions and open research problems in topic models.", "venue": "KDD
        ''11 Tutorials", "year": 2011, "referenceCount": 0, "citationCount": 9, "influentialCitationCount":
        0, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
        "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["Review"], "publicationDate": "2011-08-21", "journal":
        {"volume": "", "pages": "5", "name": ""}, "authors": [{"authorId": "1796335",
        "name": "D. Blei"}]}, {"paperId": "92eb167f30ad59f6949667021760eb41078cf85c",
        "externalIds": {"DBLP": "conf/kdd/WangB11", "MAG": "2135790056", "DOI": "10.1145/2020408.2020480",
        "CorpusId": 96163}, "corpusId": 96163, "publicationVenue": {"id": "a0edb93b-1e95-4128-a295-6b1659149cef",
        "name": "Knowledge Discovery and Data Mining", "type": "conference", "alternate_names":
        ["KDD", "Knowl Discov Data Min"], "url": "http://www.acm.org/sigkdd/"}, "url":
        "https://www.semanticscholar.org/paper/92eb167f30ad59f6949667021760eb41078cf85c",
        "title": "Collaborative topic modeling for recommending scientific articles",
        "abstract": "Researchers have access to large online archives of scientific
        articles. As a consequence, finding relevant papers has become more difficult.
        Newly formed online communities of researchers sharing citations provides
        a new way to solve this problem. In this paper, we develop an algorithm to
        recommend scientific articles to users of an online community. Our approach
        combines the merits of traditional collaborative filtering and probabilistic
        topic modeling. It provides an interpretable latent structure for users and
        items, and can form recommendations about both existing and newly published
        articles. We study a large subset of data from CiteULike, a bibliography sharing
        service, and show that our algorithm provides a more effective recommender
        system than traditional collaborative filtering.", "venue": "Knowledge Discovery
        and Data Mining", "year": 2011, "referenceCount": 33, "citationCount": 1593,
        "influentialCitationCount": 225, "isOpenAccess": false, "openAccessPdf": null,
        "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer
        Science", "source": "external"}, {"category": "Computer Science", "source":
        "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate":
        "2011-08-21", "journal": {"pages": "448-456"}, "authors": [{"authorId": "2108881999",
        "name": "Chong Wang"}, {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId":
        "93792105974f4d42c83172c4fc9f24be77fe781b", "externalIds": {"MAG": "2187741934",
        "DBLP": "journals/jmlr/WangPB11", "CorpusId": 8092936}, "corpusId": 8092936,
        "publicationVenue": {"id": "2d136b11-c2b5-484b-b008-7f4a852fd61e", "name":
        "International Conference on Artificial Intelligence and Statistics", "type":
        "conference", "alternate_names": ["AISTATS", "Int Conf Artif Intell Stat"]},
        "url": "https://www.semanticscholar.org/paper/93792105974f4d42c83172c4fc9f24be77fe781b",
        "title": "Online Variational Inference for the Hierarchical Dirichlet Process",
        "abstract": "The hierarchical Dirichlet process (HDP) is a Bayesian nonparametric
        model that can be used to model mixed-membership data with a potentially infinite
        number of components. It has been applied widely in probabilistic topic modeling,
        where the data are documents and the components are distributions of terms
        that reflect recurring patterns (or \u201ctopics\u201d) in the collection.
        Given a document collection, posterior inference is used to determine the
        number of topics needed and to characterize their distributions. One limitation
        of HDP analysis is that existing posterior inference algorithms require multiple
        passes through all the data\u2014these algorithms are intractable for very
        large scale applications. We propose an online variational inference algorithm
        for the HDP, an algorithm that is easily applicable to massive and streaming
        data. Our algorithm is significantly faster than traditional inference algorithms
        for the HDP, and lets us analyze much larger data sets. We illustrate the
        approach on two large collections of text, showing improved performance over
        online LDA, the finite counterpart to the HDP topic model.", "venue": "International
        Conference on Artificial Intelligence and Statistics", "year": 2011, "referenceCount":
        28, "citationCount": 387, "influentialCitationCount": 63, "isOpenAccess":
        false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
        "publicationDate": "2011-06-14", "journal": {"pages": "752-760"}, "authors":
        [{"authorId": "2108881999", "name": "Chong Wang"}, {"authorId": "143855009",
        "name": "J. Paisley"}, {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId":
        "93da6dee4834b424af81065a0a1720e8d0463802", "externalIds": {"CorpusId": 14342242},
        "corpusId": 14342242, "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/93da6dee4834b424af81065a0a1720e8d0463802",
        "title": "Hierarchical Dirichlet Processes Author ( s ) :", "abstract": "JSTOR
        is a not-for-profit service that helps scholars, researchers, and students
        discover, use, and build upon a wide range of content in a trusted digital
        archive. We use information technology and tools to increase productivity
        and facilitate new forms of scholarship. For more information about JSTOR,
        please contact support@jstor.org.. American Statistical Association is collaborating
        with JSTOR to digitize, preserve and extend access to Journal of the American
        Statistical Association. We consider problems involving groups of data where
        each observation within a group is a draw from a mixture model and where it
        is desirable to share mixture components between groups. We assume that the
        number of mixture components is unknown a priori and is to be inferred from
        the data. In this setting it is natural to consider sets of Dirichlet processes,
        one for each group, where the well-known clustering property of the Dirichlet
        process provides a nonparametric prior for the number of mixture components
        within each group. Given our desire to tie the mixture models in the various
        groups, we consider a hierarchical model, specifically one in which the base
        measure for the child Dirichlet processes is itself distributed according
        to a Dirichlet process. Such a base measure being discrete, the child Dirichlet
        processes necessarily share atoms. Thus, as desired, the mixture models in
        the different groups necessarily share mixture components. We discuss representations
        of hierarchical Dirichlet processes in terms of a stick-breaking process,
        and a generalization of the Chinese restaurant process that we refer to as
        the \"Chinese restaurant franchise.\" We present Markov chain Monte Carlo
        algorithms for posterior inference in hierarchical Dirichlet process mixtures
        and describe applications to problems in information retrieval and text modeling.",
        "venue": "", "year": 2011, "referenceCount": 45, "citationCount": 0, "influentialCitationCount":
        0, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
        null, "publicationDate": null, "journal": null, "authors": [{"authorId": "1725303",
        "name": "Y. Teh"}, {"authorId": "1694621", "name": "Michael I. Jordan"}, {"authorId":
        "1773821", "name": "Matthew J. Beal"}, {"authorId": "1796335", "name": "D.
        Blei"}]}, {"paperId": "9b652a613bf96fdcd04137270e12d0a7cf15519f", "externalIds":
        {"MAG": "1486371509", "DOI": "10.1002/9781119995678.CH7", "CorpusId": 116886491},
        "corpusId": 116886491, "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/9b652a613bf96fdcd04137270e12d0a7cf15519f",
        "title": "Nonparametric Mixed Membership Modelling Using the IBP Compound
        Dirichlet Process", "abstract": null, "venue": "", "year": 2011, "referenceCount":
        10, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Mathematics"], "s2FieldsOfStudy":
        [{"category": "Mathematics", "source": "external"}], "publicationTypes": null,
        "publicationDate": "2011-04-24", "journal": {"volume": "", "pages": "145-160",
        "name": ""}, "authors": [{"authorId": "40075113", "name": "Sinead Williamson"},
        {"authorId": "2108881999", "name": "Chong Wang"}, {"authorId": "145993598",
        "name": "K. Heller"}, {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId":
        "9e09017ba9f478da7163d667186dd6c47e678d7e", "externalIds": {"MAG": "2259123132",
        "CorpusId": 61824371}, "corpusId": 61824371, "publicationVenue": null, "url":
        "https://www.semanticscholar.org/paper/9e09017ba9f478da7163d667186dd6c47e678d7e",
        "title": "Uncovering, understanding, and predicting links", "abstract": "Network
        data, such as citation networks of documents, hyperlinked networks of web
        pages, and social networks of friends, are pervasive in applied statistics
        and machine learning. The statistical analysis of network data can provide
        both useful predictive models and descriptive statistics. Predictive models
        can point social network members towards new friends, scientific papers towards
        relevant citations, and web pages towards other related pages. Descriptive
        statistics can uncover the hidden community structure underlying a network
        data set. \nIn this work we develop new models of network data that account
        for both links and attributes. We also develop the inferential and predictive
        tools around these models to make them widely applicable to large, real-world
        data sets. One such model, the Relational Topic Model can predict links using
        only a new node''s attributes. Thus, we can suggest citations of newly written
        papers, predict the likely hyperlinks of a web page in development, or suggest
        friendships in a social network based only on a new user''s profile of interests.
        Moreover, given a new node and its links, the model provides a predictive
        distribution of node attributes. This mechanism can be used to predict keywords
        from citations or a user''s interests from his or her social connections.
        \nWhile explicit network data\u2014network data in which the connections between
        people, places, genes, corporations, etc. are explicitly encoded\u2014are
        already ubiquitous, most of these can only annotate connections in a limited
        fashion. Although relationships between entities are rich, it is impractical
        to manually devise complete characterizations of these relationships for every
        pair of entities on large, real-world corpora. To resolve this we present
        a probabilistic topic model to analyze text corpora and infer descriptions
        of its entities and of relationships between those entities. We show qualitatively
        and quantitatively that our model can construct and annotate graphs of relationships
        and make useful predictions.", "venue": "", "year": 2011, "referenceCount":
        88, "citationCount": 5, "influentialCitationCount": 1, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate":
        null, "journal": {"volume": "", "name": ""}, "authors": [{"authorId": "1796335",
        "name": "D. Blei"}, {"authorId": "80936017", "name": "Jonathan D. Chang"}]},
        {"paperId": "d9e650bebc559b34a4b5ae379d20933a8d0c9335", "externalIds": {"MAG":
        "184934546", "DBLP": "conf/emnlp/MimnoB11", "ACL": "D11-1021", "CorpusId":
        6717687}, "corpusId": 6717687, "publicationVenue": {"id": "41bf9ed3-85b3-4c90-b015-150e31690253",
        "name": "Conference on Empirical Methods in Natural Language Processing",
        "type": "conference", "alternate_names": ["Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing", "Conf Empir Method Nat
        Lang Process", "EMNLP"], "url": "https://www.aclweb.org/portal/emnlp"}, "url":
        "https://www.semanticscholar.org/paper/d9e650bebc559b34a4b5ae379d20933a8d0c9335",
        "title": "Bayesian Checking for Topic Models", "abstract": "Real document
        collections do not fit the independence assumptions asserted by most statistical
        topic models, but how badly do they violate them? We present a Bayesian method
        for measuring how well a topic model fits a corpus. Our approach is based
        on posterior predictive checking, a method for diagnosing Bayesian models
        in user-defined ways. Our method can identify where a topic model fits the
        data, where it falls short, and in which directions it might be improved.",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2011, "referenceCount": 20, "citationCount": 96, "influentialCitationCount":
        3, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
        "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2011-07-27",
        "journal": {"pages": "227-237"}, "authors": [{"authorId": "1705700", "name":
        "David Mimno"}, {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId":
        "ecfbd1a30a243f15610fa6b76907f8455560da3a", "externalIds": {"DBLP": "conf/nips/GhoshUSB11",
        "MAG": "2122740842", "CorpusId": 940617}, "corpusId": 940617, "publicationVenue":
        {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd", "name": "Neural Information
        Processing Systems", "type": "conference", "alternate_names": ["Neural Inf
        Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"}, "url": "https://www.semanticscholar.org/paper/ecfbd1a30a243f15610fa6b76907f8455560da3a",
        "title": "Spatial distance dependent Chinese restaurant processes for image
        segmentation", "abstract": "The distance dependent Chinese restaurant process
        (ddCRP) was recently introduced to accommodate random partitions of non-exchangeable
        data [1]. The dd-CRP clusters data in a biased way: each data point is more
        likely to be clustered with other data that are near it in an external sense.
        This paper examines the dd-CRP in a spatial setting with the goal of natural
        image segmentation. We explore the biases of the spatial ddCRP model and propose
        a novel hierarchical extension better suited for producing \"human-like\"
        segmentations. We then study the sensitivity of the models to various distance
        and appearance hyperparameters, and provide the first rigorous comparison
        of nonparametric Bayesian models in the image segmentation domain. On unsupervised
        image segmentation, we demonstrate that similar performance to existing nonparametric
        Bayesian models is possible with substantially simpler models and algorithms.",
        "venue": "Neural Information Processing Systems", "year": 2011, "referenceCount":
        24, "citationCount": 79, "influentialCitationCount": 7, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
        "Conference"], "publicationDate": "2011-12-12", "journal": {"pages": "1476-1484"},
        "authors": [{"authorId": "143864453", "name": "S. Ghosh"}, {"authorId": "12878444",
        "name": "Andrei Ungureanu"}, {"authorId": "1799035", "name": "Erik B. Sudderth"},
        {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId": "097ade72f43a979e652ed5a6aa801f5e8dfb9066",
        "externalIds": {"MAG": "2086710210", "DOI": "10.1037/a0017808", "CorpusId":
        18132921, "PubMed": "20063968"}, "corpusId": 18132921, "publicationVenue":
        {"id": "dfb7f114-609c-4c34-9ee7-8cb1fc3e4a6b", "name": "Psychology Review",
        "type": "journal", "alternate_names": ["Psychol rev", "Psychological review",
        "Psychol Rev", "Psychological Review"], "issn": "1354-1129", "alternate_issns":
        ["0033-295X"], "url": "http://www.apa.org/journals/rev/", "alternate_urls":
        ["http://www.apa.org/pubs/journals/rev/", "http://www.apa.org/pubs/journals/rev/index.aspx",
        "https://www.apa.org/pubs/journals/rev/"]}, "url": "https://www.semanticscholar.org/paper/097ade72f43a979e652ed5a6aa801f5e8dfb9066",
        "title": "Context, learning, and extinction.", "abstract": "A. Redish et al.
        (2007) proposed a reinforcement learning model of context-dependent learning
        and extinction in conditioning experiments, using the idea of \"state classification\"
        to categorize new observations into states. In the current article, the authors
        propose an interpretation of this idea in terms of normative statistical inference.
        They focus on renewal and latent inhibition, 2 conditioning paradigms in which
        contextual manipulations have been studied extensively, and show that online
        Bayesian inference within a model that assumes an unbounded number of latent
        causes can characterize a diverse set of behavioral results from such manipulations,
        some of which pose problems for the model of Redish et al. Moreover, in both
        paradigms, context dependence is absent in younger animals, or if hippocampal
        lesions are made prior to training. The authors suggest an explanation in
        terms of a restricted capacity to infer new causes.", "venue": "Psychology
        Review", "year": 2010, "referenceCount": 79, "citationCount": 287, "influentialCitationCount":
        19, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Psychology",
        "Medicine"], "s2FieldsOfStudy": [{"category": "Psychology", "source": "external"},
        {"category": "Medicine", "source": "external"}, {"category": "Psychology",
        "source": "s2-fos-model"}], "publicationTypes": ["Review", "JournalArticle",
        "LettersAndComments"], "publicationDate": null, "journal": {"volume": "117
        1", "pages": "\n          197-209\n        ", "name": "Psychological review"},
        "authors": [{"authorId": "1831199", "name": "S. Gershman"}, {"authorId": "1796335",
        "name": "D. Blei"}, {"authorId": "9796712", "name": "Y. Niv"}]}, {"paperId":
        "1dda1a4414675729f46594a5e609938ef3a48382", "externalIds": {"DBLP": "conf/icml/HoffmanBC10",
        "MAG": "2113526703", "CorpusId": 14996848}, "corpusId": 14996848, "publicationVenue":
        {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29", "name": "International Conference
        on Machine Learning", "type": "conference", "alternate_names": ["ICML", "Int
        Conf Mach Learn"], "url": "https://icml.cc/"}, "url": "https://www.semanticscholar.org/paper/1dda1a4414675729f46594a5e609938ef3a48382",
        "title": "Bayesian Nonparametric Matrix Factorization for Recorded Music",
        "abstract": "Recent research in machine learning has focused on breaking audio
        spectrograms into separate sources of sound using latent variable decompositions.
        These methods require that the number of sources be specified in advance,
        which is not always possible. To address this problem, we develop Gamma Process
        Nonnegative Matrix Factorization (GaP-NMF), a Bayesian nonparametric approach
        to decomposing spectrograms. The assumptions behind GaP-NMF are based on research
        in signal processing regarding the expected distributions of spectrogram data,
        and GaP-NMF automatically discovers the number of latent sources. We derive
        a mean-field variational inference algorithm and evaluate GaP-NMF on both
        synthetic data and recorded music.", "venue": "International Conference on
        Machine Learning", "year": 2010, "referenceCount": 18, "citationCount": 165,
        "influentialCitationCount": 18, "isOpenAccess": false, "openAccessPdf": null,
        "fieldsOfStudy": ["Mathematics", "Computer Science"], "s2FieldsOfStudy": [{"category":
        "Mathematics", "source": "external"}, {"category": "Computer Science", "source":
        "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2010-06-21",
        "journal": {"pages": "439-446"}, "authors": [{"authorId": "28552618", "name":
        "M. Hoffman"}, {"authorId": "1796335", "name": "D. Blei"}, {"authorId": "1716507",
        "name": "P. Cook"}]}, {"paperId": "2d8cbd7370b4ce666edd864e66f83ebf20963516",
        "externalIds": {"MAG": "2165599843", "DBLP": "conf/nips/HoffmanBB10", "CorpusId":
        15674552}, "corpusId": 15674552, "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
        "name": "Neural Information Processing Systems", "type": "conference", "alternate_names":
        ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
        "url": "https://www.semanticscholar.org/paper/2d8cbd7370b4ce666edd864e66f83ebf20963516",
        "title": "Online Learning for Latent Dirichlet Allocation", "abstract": "We
        develop an online variational Bayes (VB) algorithm for Latent Dirichlet Allocation
        (LDA). Online LDA is based on online stochastic optimization with a natural
        gradient step, which we show converges to a local optimum of the VB objective
        function. It can handily analyze massive document collections, including those
        arriving in a stream. We study the performance of online LDA in several ways,
        including by fitting a 100-topic topic model to 3.3M articles from Wikipedia
        in a single pass. We demonstrate that online LDA finds topic models as good
        or better than those found with batch VB, and in a fraction of the time.",
        "venue": "Neural Information Processing Systems", "year": 2010, "referenceCount":
        28, "citationCount": 1601, "influentialCitationCount": 209, "isOpenAccess":
        false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
        "Conference"], "publicationDate": "2010-12-06", "journal": {"pages": "856-864"},
        "authors": [{"authorId": "28552618", "name": "M. Hoffman"}, {"authorId": "1796335",
        "name": "D. Blei"}, {"authorId": "144570279", "name": "F. Bach"}]}, {"paperId":
        "44ab78d0605f2b4905e19ba56d6e51529da4bed9", "externalIds": {"MAG": "2001932471",
        "DBLP": "journals/spm/BleiCD10", "DOI": "10.1109/MSP.2010.938079", "CorpusId":
        27522323, "PubMed": "25104898"}, "corpusId": 27522323, "publicationVenue":
        {"id": "f62e5eab-173a-4e0a-a963-ed8de9835d22", "name": "IEEE Signal Processing
        Magazine", "type": "journal", "alternate_names": ["IEEE Signal Process Mag"],
        "issn": "1053-5888", "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=79",
        "alternate_urls": ["https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=79"]},
        "url": "https://www.semanticscholar.org/paper/44ab78d0605f2b4905e19ba56d6e51529da4bed9",
        "title": "Probabilistic Topic Models", "abstract": "In this article, we review
        probabilistic topic models: graphical models that can be used to summarize
        a large collection of documents with a smaller number of distributions over
        words. Those distributions are called \"topics\" because, when fit to data,
        they capture the salient themes that run through the collection. We describe
        both finite-dimensional parametric topic models and their Bayesian nonparametric
        counterparts, which are based on the hierarchical Dirichlet process (HDP).
        We discuss two extensions of topic models to time-series data-one that lets
        the topics slowly change over time and one that lets the assumed prevalence
        of the topics change. Finally, we illustrate the application of topic models
        to nontext data, summarizing some recent research results in image analysis.",
        "venue": "IEEE Signal Processing Magazine", "year": 2010, "referenceCount":
        61, "citationCount": 53, "influentialCitationCount": 2, "isOpenAccess": true,
        "openAccessPdf": {"url": "https://europepmc.org/articles/pmc4122269?pdf=render",
        "status": null}, "fieldsOfStudy": ["Computer Science", "Medicine"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Medicine",
        "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2010-10-18",
        "journal": {"volume": "27", "pages": "55-65", "name": "IEEE Signal Processing
        Magazine"}, "authors": [{"authorId": "1796335", "name": "D. Blei"}, {"authorId":
        "145006560", "name": "L. Carin"}, {"authorId": "39775017", "name": "D. Dunson"}]},
        {"paperId": "5f1038ad42ed8a4428e395c96d57f83d201ef3b3", "externalIds": {"MAG":
        "2889495008", "CorpusId": 56964528}, "corpusId": 56964528, "publicationVenue":
        null, "url": "https://www.semanticscholar.org/paper/5f1038ad42ed8a4428e395c96d57f83d201ef3b3",
        "title": "Introduction to Probabilistic Topic Models", "abstract": "Probabilistic
        topic models are a suite of algorithms whose aim is to discover the hidden
        thematic structure in large archives of documents. In this article, we review
        the main ideas of this \ufb01eld, survey the current state-of-the-art, and
        describe some promising future directions. We \ufb01rst describe latent Dirichlet
        allocation (LDA) [8], which is the simplest kind of topic model. We discuss
        its connections to probabilistic modeling, and describe two kinds of algorithms
        for topic discovery. We then survey the growing body of research that extends
        and applies topic models in interesting ways. These extensions have been developed
        by relaxing some of the statistical assumptions of LDA, incorporating meta-data
        into the analysis of the documents, and using similar kinds of models on a
        diversity of data types such as social networks, images and genetics. Finally,
        we give our thoughts as to some of the important unexplored directions for
        topic modeling. These include rigorous methods for checking models built for
        data exploration, new approaches to visualizing text and other high dimensional
        data, and moving beyond traditional information engineering applications towards
        using topic models for more scienti\ufb01c ends.", "venue": "", "year": 2010,
        "referenceCount": 37, "citationCount": 601, "influentialCitationCount": 52,
        "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
        "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["Review"], "publicationDate": null, "journal": {"volume":
        "", "name": ""}, "authors": [{"authorId": "1796335", "name": "D. Blei"}]},
        {"paperId": "7c25a758844bf2adfbb348806ff12f20a049e796", "externalIds": {"MAG":
        "1915450281", "ArXiv": "1006.4338", "CorpusId": 1015554}, "corpusId": 1015554,
        "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/7c25a758844bf2adfbb348806ff12f20a049e796",
        "title": "Stochastic Search with an Observable State Variable", "abstract":
        "In this paper we study convex stochastic search problems where a noisy objective
        function value is observed after a decision is made. There are many stochastic
        search problems whose behavior depends on an exogenous state variable which
        affects the shape of the objective function. Currently, there is no general
        purpose algorithm to solve this class of problems. We use nonparametric density
        estimation to take observations from the joint state-outcome distribution
        and use them to infer the optimal decision for a given query state. We propose
        two solution methods that depend on the problem characteristics: function-based
        and gradient-based optimization. We examine two weighting schemes, kernel-based
        weights and Dirichlet process-based weights, for use with the solution methods.
        The weights and solution methods are tested on a synthetic multi-product newsvendor
        problem and the hour-ahead wind commitment problem. Our results show that
        in some cases Dirichlet process weights offer substantial benefits over kernel
        based weights and more generally that nonparametric estimation methods provide
        good solutions to otherwise intractable problems.", "venue": "", "year": 2010,
        "referenceCount": 69, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess":
        false, "openAccessPdf": null, "fieldsOfStudy": ["Mathematics"], "s2FieldsOfStudy":
        [{"category": "Mathematics", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate":
        "2010-06-22", "journal": {"volume": "", "name": "arXiv: Optimization and Control"},
        "authors": [{"authorId": "48019890", "name": "Lauren Hannah"}, {"authorId":
        "1852241", "name": "Warrren B Powell"}, {"authorId": "1796335", "name": "D.
        Blei"}]}, {"paperId": "a0bcdf26fcd382dd54ac38be07e6b33179bd52f4", "externalIds":
        {"DBLP": "conf/icml/WilliamsonWHB10", "MAG": "2167810193", "CorpusId": 6023511},
        "corpusId": 6023511, "publicationVenue": {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
        "name": "International Conference on Machine Learning", "type": "conference",
        "alternate_names": ["ICML", "Int Conf Mach Learn"], "url": "https://icml.cc/"},
        "url": "https://www.semanticscholar.org/paper/a0bcdf26fcd382dd54ac38be07e6b33179bd52f4",
        "title": "The IBP Compound Dirichlet Process and its Application to Focused
        Topic Modeling", "abstract": "The hierarchical Dirichlet process (HDP) is
        a Bayesian nonparametric mixed membership model\u2014each data point is modeled
        with a collection of components of different proportions. Though powerful,
        the HDP makes an assumption that the probability of a component being exhibited
        by a data point is positively correlated with its proportion within that data
        point. This might be an undesirable assumption. For example, in topic modeling,
        a topic (component) might be rare throughout the corpus but dominant within
        those documents (data points) where it occurs. We develop the IBP compound
        Dirichlet process (ICD), a Bayesian nonparametric prior that decouples across-data
        prevalence and within-data proportion in a mixed membership model. The ICD
        combines properties from the HDP and the Indian buffet process (IBP), a Bayesian
        nonparametric prior on binary matrices. The ICD assigns a subset of the shared
        mixture components to each data point. This subset, the data point''s \"focus\",
        is determined independently from the amount that each of its components contribute.
        We develop an ICD mixture model for text, the focused topic model (FTM), and
        show superior performance over the HDP-based topic model.", "venue": "International
        Conference on Machine Learning", "year": 2010, "referenceCount": 17, "citationCount":
        161, "influentialCitationCount": 30, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
        "Computer Science", "source": "external"}, {"category": "Computer Science",
        "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"],
        "publicationDate": "2010-06-21", "journal": {"pages": "1151-1158"}, "authors":
        [{"authorId": "40075113", "name": "Sinead Williamson"}, {"authorId": "2108881999",
        "name": "Chong Wang"}, {"authorId": "145993598", "name": "K. Heller"}, {"authorId":
        "1796335", "name": "D. Blei"}]}, {"paperId": "a96e3a4a3af1de3c16a0914c5304db186fde6af0",
        "externalIds": {"MAG": "2185591340", "CorpusId": 263885422}, "corpusId": 263885422,
        "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/a96e3a4a3af1de3c16a0914c5304db186fde6af0",
        "title": "A focus on graphical model design and applications to document and
        image analysis )", "abstract": "In this article, we review probabilistic topic
        models: graphical models that can be used to summarize a large collection
        of documents with a smaller number of distributions over words. Those distributions
        are called \u201ctopics\u201d because, when fit to data, they capture the
        salient themes that run through the collection. We describe both finite-dimensional
        parametric topic models and their Bayesian nonparametric counterparts, which
        are based on the hierarchical Dirichlet process (HDP). We discuss two extensions
        of topic models to time-series data\u2014one that lets the topics slowly change
        over time and one that lets the assumed prevalence of the topics change. Finally,
        we illustrate the application of topic models to nontext data, summarizing
        some recent research results in image analysis.", "venue": "", "year": 2010,
        "referenceCount": 55, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess":
        false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["Review"], "publicationDate":
        null, "journal": {"volume": "", "name": ""}, "authors": [{"authorId": "1796335",
        "name": "D. Blei"}, {"authorId": "2185755024", "name": "L. Carin"}, {"authorId":
        "39775017", "name": "D. Dunson"}]}, {"paperId": "be98bc6a2694b150fff2394f88c2b84e06e5a510",
        "externalIds": {"CorpusId": 43734484}, "corpusId": 43734484, "publicationVenue":
        null, "url": "https://www.semanticscholar.org/paper/be98bc6a2694b150fff2394f88c2b84e06e5a510",
        "title": "The Ideal Point Topic Model : Predicting Legislative Roll Calls
        from Text", "abstract": "We develop the ideal point topic model, a probabilistic
        model of legislative text. Our model \u2013 drawing on ideas from ideal point
        estimation and topic modeling \u2013 predicts voting patterns based on the
        contents of bills and the inferred political leanings of legislators. It also
        provides an exploratory window into how legislative language is correlated
        with political support. Across 14 years of legislative data, we predict specific
        voting patterns with high accuracy.", "venue": "", "year": 2010, "referenceCount":
        11, "citationCount": 5, "influentialCitationCount": 0, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category":
        "Computer Science", "source": "s2-fos-model"}], "publicationTypes": null,
        "publicationDate": null, "journal": null, "authors": [{"authorId": "21007048",
        "name": "S. Gerrish"}, {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId":
        "c645d99bf9b3f496a561818b37350d7a1834b92b", "externalIds": {"MAG": "75368837",
        "DBLP": "conf/icml/GerrishB10", "CorpusId": 2297133}, "corpusId": 2297133,
        "publicationVenue": {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29", "name":
        "International Conference on Machine Learning", "type": "conference", "alternate_names":
        ["ICML", "Int Conf Mach Learn"], "url": "https://icml.cc/"}, "url": "https://www.semanticscholar.org/paper/c645d99bf9b3f496a561818b37350d7a1834b92b",
        "title": "A Language-based Approach to Measuring Scholarly Impact", "abstract":
        "Identifying the most influential documents in a corpus is an important problem
        in many fields, from information science and historiography to text summarization
        and news aggregation. Unfortunately, traditional bibliometrics such as citations
        are often not available. We propose using changes in the thematic content
        of documents over time to measure the importance of individual documents within
        the collection. We describe a dynamic topic model for both quantifying and
        qualifying the impact of these documents. We validate the model by analyzing
        three large corpora of scientific articles. Our measurement of a document''s
        impact correlates significantly with its number of citations.", "venue": "International
        Conference on Machine Learning", "year": 2010, "referenceCount": 34, "citationCount":
        162, "influentialCitationCount": 12, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
        "Computer Science", "source": "external"}, {"category": "Computer Science",
        "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"],
        "publicationDate": "2010-06-21", "journal": {"pages": "375-382"}, "authors":
        [{"authorId": "21007048", "name": "S. Gerrish"}, {"authorId": "1796335", "name":
        "D. Blei"}]}, {"paperId": "d3e3affa94244fd3d6d0253a0cce0949a596b24a", "externalIds":
        {"MAG": "24083617", "CorpusId": 220950635}, "corpusId": 220950635, "publicationVenue":
        null, "url": "https://www.semanticscholar.org/paper/d3e3affa94244fd3d6d0253a0cce0949a596b24a",
        "title": "Linguistic extensions of topic models", "abstract": "Topic models
        like latent Dirichlet allocation (LDA) provide a framework for analyzing large
        datasets where observations are collected into groups. Although topic modeling
        has been fruitfully applied to problems social science, biology, and computer
        vision, it has been most widely used to model datasets where documents are
        modeled as exchangeable groups of words. In this context, topic models discover
        topics, distributions over words that express a coherent theme like \u201cbusiness\u201d
        or \u201cpolitics.\u201d While one of the strengths of topic models is that
        they make few assumptions about the underlying data, such a general approach
        sometimes limits the type of problems topic models can solve. \nWhen we restrict
        our focus to natural language datasets, we can use insights from linguistics
        to create models that understand and discover richer language patterns. In
        this thesis, we extend LDA in three different ways: adding knowledge of word
        meaning, modeling multiple languages, and incorporating local syntactic context.
        These extensions apply topic models to new problems, such as discovering the
        meaning of ambiguous words, extend topic models for new datasets, such as
        unaligned multilingual corpora, and combine topic models with other sources
        of information about documents'' context. \nIn Chapter 2, we present latent
        Dirichlet allocation with WordNet (LDAWN), an unsupervised probabilistic topic
        model that includes word sense as a hidden variable. LDAWN replaces the multinomial
        topics of LDA with Abney and Light''s distribution over meanings. Thus, posterior
        inference in this model discovers not only the topical domains of each token,
        as in LDA, but also the meaning associated with each token. We show that considering
        more topics improves the problem of word sense disambiguation. \nLDAWN allows
        us to separate the representation of meaning from how that meaning is expressed
        as word forms. In Chapter 3, we extend LDAWN to allow meanings to be expressed
        using different word forms in different languages. In addition to the disambiguation
        provided by LDAWN, this offers a new method of using topic models on corpora
        with multiple languages. \nIn Chapter 4, we relax the assumptions of multilingual
        LDAWN. We present the multilingual topic model for unaligned text (MuTo).
        Like multilingual LDAWN, it is a probabilistic model of text that is designed
        to analyze corpora composed of documents in multiple languages. Unlike multilingual
        LDAWN, which requires the correspondence between languages to be painstakingly
        annotated, MuTo also uses stochastic EM to simultaneously discover both a
        matching between the languages while it simultaneously learns multilingual
        topics. We demonstrate that MuTo allows the meaning of similar documents to
        be recovered across languages. \nIn Chapter 5, we address a recurring problem
        that hindered the performance of the models presented in the previous chapters:
        the lack of a local context. We develop the syntactic topic model (STM), a
        non-parametric Bayesian model of parsed documents. The STM generates words
        that are both thematically and syntactically constrained, which combines the
        semantic insights of topic models with the syntactic information available
        from parse trees. Each word of a sentence is generated by a distribution that
        combines document-specific topic weights and parse-tree-specific syntactic
        transitions. Words are assumed to be generated in an order that respects the
        parse tree. We derive an approximate posterior inference method based on variational
        methods for hierarchical Dirichlet processes, and we report qualitative and
        quantitative results on both synthetic data and hand-parsed documents. \nIn
        Chapter 6, we conclude with a discussion of how the models presented in this
        thesis can be applied in real world applications such as sentiment analysis
        and how the models can be extended to capture even richer linguistic information
        from text.", "venue": "", "year": 2010, "referenceCount": 143, "citationCount":
        3, "influentialCitationCount": 0, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
        "Computer Science", "source": "external"}, {"category": "Computer Science",
        "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": null,
        "journal": {"volume": "", "name": ""}, "authors": [{"authorId": "1796335",
        "name": "D. Blei"}, {"authorId": "1389036863", "name": "Jordan L. Boyd-Graber"}]},
        {"paperId": "d82d35c96fb2aa5980b34a6312c7caf3b772ec7c", "externalIds": {"DBLP":
        "journals/jmlr/LorbertEKBR10", "MAG": "2606640352", "CorpusId": 8397226},
        "corpusId": 8397226, "publicationVenue": {"id": "2d136b11-c2b5-484b-b008-7f4a852fd61e",
        "name": "International Conference on Artificial Intelligence and Statistics",
        "type": "conference", "alternate_names": ["AISTATS", "Int Conf Artif Intell
        Stat"]}, "url": "https://www.semanticscholar.org/paper/d82d35c96fb2aa5980b34a6312c7caf3b772ec7c",
        "title": "Exploiting Covariate Similarity in Sparse Regression via the Pairwise
        Elastic Net", "abstract": "A new approach to regression regularization called
        the Pairwise Elastic Net is proposed. Like the Elastic Net, it simultaneously
        performs automatic variable selection and continuous shrinkage. In addition,
        the Pairwise Elastic Net encourages the grouping of strongly correlated predictors
        based on a pairwise similarity measure. We give examples of how the approach
        can be used to achieve the objectives of Ridge regression, the Lasso, the
        Elastic Net, and Group Lasso. Finally, we present a coordinate descent algorithm
        to solve the Pairwise Elastic Net.", "venue": "International Conference on
        Artificial Intelligence and Statistics", "year": 2010, "referenceCount": 22,
        "citationCount": 35, "influentialCitationCount": 6, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Mathematics", "Computer Science"],
        "s2FieldsOfStudy": [{"category": "Mathematics", "source": "external"}, {"category":
        "Computer Science", "source": "external"}, {"category": "Computer Science",
        "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
        "2010-03-31", "journal": {"pages": "477-484"}, "authors": [{"authorId": "1863189",
        "name": "Alexander Lorbert"}, {"authorId": "48751495", "name": "David J. Eis"},
        {"authorId": "2597473", "name": "V. Kostina"}, {"authorId": "1796335", "name":
        "D. Blei"}, {"authorId": "1693135", "name": "P. Ramadge"}]}, {"paperId": "e700bd326cd50ac0154f6e58f2adab9fa51b3128",
        "externalIds": {"DBLP": "conf/nips/HannahPB10", "MAG": "2153034112", "CorpusId":
        2240137}, "corpusId": 2240137, "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
        "name": "Neural Information Processing Systems", "type": "conference", "alternate_names":
        ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
        "url": "https://www.semanticscholar.org/paper/e700bd326cd50ac0154f6e58f2adab9fa51b3128",
        "title": "Nonparametric Density Estimation for Stochastic Optimization with
        an Observable State Variable", "abstract": "In this paper we study convex
        stochastic optimization problems where a noisy objective function value is
        observed after a decision is made. There are many stochastic optimization
        problems whose behavior depends on an exogenous state variable which affects
        the shape of the objective function. Currently, there is no general purpose
        algorithm to solve this class of problems. We use nonparametric density estimation
        to take observations from the joint state-outcome distribution and use them
        to infer the optimal decision for a given query state s. We propose two solution
        methods that depend on the problem characteristics: function-based and gradient-based
        optimization. We examine two weighting schemes, kernel based weights and Dirichlet
        process based weights, for use with the solution methods. The weights and
        solution methods are tested on a synthetic multi-product newsvendor problem
        and the hour ahead wind commitment problem. Our results show that in some
        cases Dirichlet process weights offer substantial benefits over kernel based
        weights and more generally that nonparametric estimation methods provide good
        solutions to otherwise intractable problems.", "venue": "Neural Information
        Processing Systems", "year": 2010, "referenceCount": 32, "citationCount":
        35, "influentialCitationCount": 4, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics",
        "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2010-12-06",
        "journal": {"pages": "820-828"}, "authors": [{"authorId": "48019890", "name":
        "Lauren Hannah"}, {"authorId": "1852241", "name": "Warrren B Powell"}, {"authorId":
        "1796335", "name": "D. Blei"}]}, {"paperId": "ebcd8b40a3d1d08bade75a30a5adddea423dd073",
        "externalIds": {"ACL": "N10-1081", "DBLP": "conf/naacl/CohenBS10", "MAG":
        "1892363745", "CorpusId": 1483983}, "corpusId": 1483983, "publicationVenue":
        {"id": "01103732-3808-4930-b8e4-7e9e68d5c68d", "name": "North American Chapter
        of the Association for Computational Linguistics", "type": "conference", "alternate_names":
        ["North Am Chapter Assoc Comput Linguistics", "NAACL"], "url": "https://www.aclweb.org/portal/naacl"},
        "url": "https://www.semanticscholar.org/paper/ebcd8b40a3d1d08bade75a30a5adddea423dd073",
        "title": "Variational Inference for Adaptor Grammars", "abstract": "Adaptor
        grammars extend probabilistic context-free grammars to define prior distributions
        over trees with \"rich get richer\" dynamics. Inference for adaptor grammars
        seeks to find parse trees for raw text. This paper describes a variational
        inference algorithm for adaptor grammars, providing an alternative to Markov
        chain Monte Carlo methods. To derive this method, we develop a stick-breaking
        representation of adaptor grammars, a representation that enables us to define
        adaptor grammars with recursion. We report experimental results on a word
        segmentation task, showing that variational inference performs comparably
        to MCMC. Further, we show a significant speed-up when parallelizing the algorithm.
        Finally, we report promising results for a new application for adaptor grammars,
        dependency grammar induction.", "venue": "North American Chapter of the Association
        for Computational Linguistics", "year": 2010, "referenceCount": 33, "citationCount":
        34, "influentialCitationCount": 6, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
        "Computer Science", "source": "external"}, {"category": "Computer Science",
        "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"],
        "publicationDate": "2010-06-02", "journal": {"pages": "564-572"}, "authors":
        [{"authorId": "40146204", "name": "Shay B. Cohen"}, {"authorId": "1796335",
        "name": "D. Blei"}, {"authorId": "144365875", "name": "Noah A. Smith"}]},
        {"paperId": "fbac75009db521a7dbe08344316bb538f039ac00", "externalIds": {"MAG":
        "1979936637", "DBLP": "conf/cvpr/LiWLBL10", "DOI": "10.1109/CVPR.2010.5540027",
        "CorpusId": 12482867}, "corpusId": 12482867, "publicationVenue": null, "url":
        "https://www.semanticscholar.org/paper/fbac75009db521a7dbe08344316bb538f039ac00",
        "title": "Building and using a semantivisual image hierarchy", "abstract":
        "A semantically meaningful image hierarchy can ease the human effort in organizing
        thousands and millions of pictures (e.g., personal albums), and help to improve
        performance of end tasks such as image annotation and classification. Previous
        work has focused on using either low-level image features or textual tags
        to build image hierarchies, resulting in limited success in their general
        usage. In this paper, we propose a method to automatically discover the \u201csemantivisual\u201d
        image hierarchy by incorporating both image and tag information. This hierarchy
        encodes a general-to-specific image relationship. We pay particular attention
        to quantifying the effectiveness of the learned hierarchy, as well as comparing
        our method with others in the end-task applications. Our experiments show
        that humans find our semantivisual image hierarchy more effective than those
        solely based on texts or low-level visual features. And using the constructed
        image hierarchy as a knowledge ontology, our algorithm can perform challenging
        image classification and annotation tasks more accurately.", "venue": "2010
        IEEE Computer Society Conference on Computer Vision and Pattern Recognition",
        "year": 2010, "referenceCount": 34, "citationCount": 131, "influentialCitationCount":
        10, "isOpenAccess": true, "openAccessPdf": {"url": "http://vision.stanford.edu/documents/LiWangLimBleiFei-Fei_CVPR2010.pdf",
        "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
        "Conference"], "publicationDate": "2010-06-13", "journal": {"pages": "3336-3343",
        "name": "2010 IEEE Computer Society Conference on Computer Vision and Pattern
        Recognition"}, "authors": [{"authorId": "2040091191", "name": "Li-Jia Li"},
        {"authorId": "2108883097", "name": "Chong Wang"}, {"authorId": "7892285",
        "name": "Yongwhan Lim"}, {"authorId": "1796335", "name": "D. Blei"}, {"authorId":
        "48004138", "name": "Li Fei-Fei"}]}, {"paperId": "0229a79f9d19a99510813ea61351a12efd037744",
        "externalIds": {"CorpusId": 18929702}, "corpusId": 18929702, "publicationVenue":
        null, "url": "https://www.semanticscholar.org/paper/0229a79f9d19a99510813ea61351a12efd037744",
        "title": "FINDING LATENT SOURCES IN RECORDED MUSIC WITH A SHIFT-INVARIANT
        HDP", "abstract": "We present the Shift-Invariant Hierarchical Dirichlet Process
        (SIHDP), a nonparametric Bayesian model for modeling multiple songs in terms
        of a shared vocabulary of latent sound sources. The SIHDP is an extension
        of the Hierarchical Dirichlet Process (HDP) that explicitly models the times
        at which each latent component appears in each song. This extension allows
        us to model how sound sources evolve over time, which is critical to the human
        ability to recognize and interpret sounds. To make inference on large datasets
        possible, we develop an exact distributed Gibbs sampling algorithm to do posterior
        inference. We evaluate the SIHDP\u2019s ability to model audio using a dataset
        of real popular music, and measure its ability to accurately find patterns
        in music using a set of synthesized drum loops. Ultimately, our model produces
        a rich representation of a set of songs consisting of a set of short sound
        sources and when they appear in each song.", "venue": "", "year": 2009, "referenceCount":
        7, "citationCount": 21, "influentialCitationCount": 0, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category":
        "Computer Science", "source": "s2-fos-model"}], "publicationTypes": null,
        "publicationDate": null, "journal": null, "authors": [{"authorId": "28552618",
        "name": "M. Hoffman"}, {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId":
        "161cc1dc14a4c30f16b35fac1868f4b9b9ad7f1d", "externalIds": {"DBLP": "conf/kdd/ChangBB09",
        "MAG": "2073792299", "DOI": "10.1145/1557019.1557044", "CorpusId": 215806424},
        "corpusId": 215806424, "publicationVenue": {"id": "a0edb93b-1e95-4128-a295-6b1659149cef",
        "name": "Knowledge Discovery and Data Mining", "type": "conference", "alternate_names":
        ["KDD", "Knowl Discov Data Min"], "url": "http://www.acm.org/sigkdd/"}, "url":
        "https://www.semanticscholar.org/paper/161cc1dc14a4c30f16b35fac1868f4b9b9ad7f1d",
        "title": "Connections between the lines: augmenting social networks with text",
        "abstract": "Network data is ubiquitous, encoding collections of relationships
        between entities such as people, places, genes, or corporations. While many
        resources for networks of interesting entities are emerging, most of these
        can only annotate connections in a limited fashion. Although relationships
        between entities are rich, it is impractical to manually devise complete characterizations
        of these relationships for every pair of entities on large, real-world corpora.\n
        In this paper we present a novel probabilistic topic model to analyze text
        corpora and infer descriptions of its entities and of relationships between
        those entities. We develop variational methods for performing approximate
        inference on our model and demonstrate that our model can be practically deployed
        on large corpora such as Wikipedia. We show qualitatively and quantitatively
        that our model can construct and annotate graphs of relationships and make
        useful predictions.", "venue": "Knowledge Discovery and Data Mining", "year":
        2009, "referenceCount": 35, "citationCount": 142, "influentialCitationCount":
        6, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
        "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2009-06-28",
        "journal": {"pages": "169-178"}, "authors": [{"authorId": "80936017", "name":
        "Jonathan D. Chang"}, {"authorId": "1389036863", "name": "Jordan L. Boyd-Graber"},
        {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId": "1e7d12d0782b700af534ad56f888903d6d80431b",
        "externalIds": {"DBLP": "journals/jmlr/WangTMB09", "MAG": "153090119", "CorpusId":
        5642980}, "corpusId": 5642980, "publicationVenue": {"id": "2d136b11-c2b5-484b-b008-7f4a852fd61e",
        "name": "International Conference on Artificial Intelligence and Statistics",
        "type": "conference", "alternate_names": ["AISTATS", "Int Conf Artif Intell
        Stat"]}, "url": "https://www.semanticscholar.org/paper/1e7d12d0782b700af534ad56f888903d6d80431b",
        "title": "Markov Topic Models", "abstract": "We develop Markov topic models
        (MTMs), a novel family of generative probabilistic models that can learn topics
        simultaneously from multiple corpora, such as papers from different conferences.
        We apply Gaussian (Markov) random fields to model the correlations of different
        corpora. MTMs capture both the internal topic structure within each corpus
        and the relationships between topics across the corpora. We derive an efficient
        estimation procedure with variational expectation-maximization. We study the
        performance of our models on a corpus of abstracts from six different computer
        science conferences. Our analysis reveals qualitative discoveries that are
        not possible with traditional topic models, and improved quantitative performance
        over the state of the art.", "venue": "International Conference on Artificial
        Intelligence and Statistics", "year": 2009, "referenceCount": 19, "citationCount":
        58, "influentialCitationCount": 4, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
        "Computer Science", "source": "external"}, {"category": "Computer Science",
        "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
        "2009-04-15", "journal": {"pages": "583-590"}, "authors": [{"authorId": "2108881999",
        "name": "Chong Wang"}, {"authorId": "2868274", "name": "B. Thiesson"}, {"authorId":
        "50004012", "name": "Christopher Meek"}, {"authorId": "1796335", "name": "D.
        Blei"}]}, {"paperId": "2f14e3b459dc78868851c372ae00a74519c3e1f4", "externalIds":
        {"MAG": "2144148101", "DBLP": "conf/nips/WangB09", "CorpusId": 9893225}, "corpusId":
        9893225, "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
        "name": "Neural Information Processing Systems", "type": "conference", "alternate_names":
        ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
        "url": "https://www.semanticscholar.org/paper/2f14e3b459dc78868851c372ae00a74519c3e1f4",
        "title": "Variational Inference for the Nested Chinese Restaurant Process",
        "abstract": "The nested Chinese restaurant process (nCRP) is a powerful nonparametric
        Bayesian model for learning tree-based hierarchies from data. Since its posterior
        distribution is intractable, current inference methods have all relied on
        MCMC sampling. In this paper, we develop an alternative inference technique
        based on variational methods. To employ variational methods, we derive a tree-based
        stick-breaking construction of the nCRP mixture model, and a novel variational
        algorithm that efficiently explores a posterior over a large set of combinatorial
        structures. We demonstrate the use of this approach for text and hand written
        digits modeling, where we show we can adapt the nCRP to continuous data as
        well.", "venue": "Neural Information Processing Systems", "year": 2009, "referenceCount":
        29, "citationCount": 63, "influentialCitationCount": 7, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
        "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
        {"category": "Mathematics", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
        "Conference"], "publicationDate": "2009-12-07", "journal": {"pages": "1990-1998"},
        "authors": [{"authorId": "2108881999", "name": "Chong Wang"}, {"authorId":
        "1796335", "name": "D. Blei"}]}, {"paperId": "2fe0a1994adeee963c895e20566c24eb7b25d516",
        "externalIds": {"MAG": "2097394335", "ArXiv": "0907.1013", "CorpusId": 18352657},
        "corpusId": 18352657, "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/2fe0a1994adeee963c895e20566c24eb7b25d516",
        "title": "Visualizing Topics with Multi-Word Expressions", "abstract": "We
        describe a new method for visualizing topics, the distributions over terms
        that are automatically extracted from large text corpora using latent variable
        models. Our method finds significant $n$-grams related to a topic, which are
        then used to help understand and interpret the underlying distribution. Compared
        with the usual visualization, which simply lists the most probable topical
        terms, the multi-word expressions provide a better intuitive impression for
        what a topic is \"about.\" Our approach is based on a language model of arbitrary
        length expressions, for which we develop a new methodology based on nested
        permutation tests to find significant phrases. We show that this method outperforms
        the more standard use of $\\chi^2$ and likelihood ratio tests. We illustrate
        the topic presentations on corpora of scientific abstracts and news articles.",
        "venue": "", "year": 2009, "referenceCount": 25, "citationCount": 100, "influentialCitationCount":
        5, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science",
        "source": "external"}, {"category": "Mathematics", "source": "external"},
        {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
        null, "publicationDate": "2009-07-06", "journal": {"volume": "", "name": "arXiv:
        Machine Learning"}, "authors": [{"authorId": "1796335", "name": "D. Blei"},
        {"authorId": "1739581", "name": "J. Lafferty"}]}, {"paperId": "470555cbaa483af32fd1a5cd7ca314554e91a61a",
        "externalIds": {"CorpusId": 13810641}, "corpusId": 13810641, "publicationVenue":
        null, "url": "https://www.semanticscholar.org/paper/470555cbaa483af32fd1a5cd7ca314554e91a61a",
        "title": "Syntactic Topic Models Supplement", "abstract": "1. Choose global
        topic weights \u03b2 \u223c GEM(\u03b1) 2. For each topic index k = {1, .
        . . }: (a) Choose topic transition distribution \u03c0k \u223c DP(\u03b1T
        , \u03b2). (b) Choose topic \u03c4k \u223c Dir(\u03c3) 3. For each document
        d = {1, . . . M}: (a) Choose topic weights \u03b8d \u223c DP(\u03b1D, \u03b2).
        (b) For each sentence in the document: i. Choose topic assignment z0 \u221d
        \u03b8d\u03c0start ii. Choose root word w0 \u223c mult(1, \u03c4z0) iii. For
        each additional word wn and parent pn, n \u2208 {1, . . . dn} \u2022 Choose
        topic assignment zn \u221d \u03b8d\u03c0zp(n) \u2022 Choose word wn \u223c
        mult(1, \u03c4z0)", "venue": "", "year": 2009, "referenceCount": 4, "citationCount":
        0, "influentialCitationCount": 0, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category": "Computer Science",
        "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": null,
        "journal": null, "authors": [{"authorId": "1389036863", "name": "Jordan L.
        Boyd-Graber"}, {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId": "49be9a3bd3d59a1ff2399213594af003eec82d45",
        "externalIds": {"ArXiv": "0909.5194", "MAG": "2605968092", "DBLP": "journals/jmlr/HannahBP11",
        "DOI": "10.5555/1953048.2021061", "CorpusId": 1520660}, "corpusId": 1520660,
        "publicationVenue": {"id": "c22e7c36-3bfa-43e1-bb7b-edccdea2a780", "name":
        "Journal of machine learning research", "type": "journal", "alternate_names":
        ["Journal of Machine Learning Research", "J mach learn res", "J Mach Learn
        Res"], "issn": "1532-4435", "alternate_issns": ["1533-7928"], "url": "http://www.ai.mit.edu/projects/jmlr/",
        "alternate_urls": ["http://jmlr.csail.mit.edu/", "http://www.jmlr.org/", "http://portal.acm.org/affiliated/jmlr"]},
        "url": "https://www.semanticscholar.org/paper/49be9a3bd3d59a1ff2399213594af003eec82d45",
        "title": "Dirichlet Process Mixtures of Generalized Linear Models", "abstract":
        "We propose Dirichlet Process mixtures of Generalized Linear Models (DP-GLM),
        a new class of methods for nonparametric regression. Given a data set of input-response
        pairs, the DP-GLM produces a global model of the joint distribution through
        a mixture of local generalized linear models. DP-GLMs allow both continuous
        and categorical inputs, and can model the same class of responses that can
        be modeled with a generalized linear model. We study the properties of the
        DP-GLM, and show why it provides better predictions and density estimates
        than existing Dirichlet process mixture regression models. We give conditions
        for weak consistency of the joint distribution and pointwise consistency of
        the regression estimate.", "venue": "Journal of machine learning research",
        "year": 2009, "referenceCount": 78, "citationCount": 189, "influentialCitationCount":
        13, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science", "Mathematics", "Physics"], "s2FieldsOfStudy": [{"category": "Computer
        Science", "source": "external"}, {"category": "Mathematics", "source": "external"},
        {"category": "Physics", "source": "external"}, {"category": "Mathematics",
        "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle"], "publicationDate": "2009-09-28", "journal":
        {"volume": "12", "pages": "1923-1953", "name": "J. Mach. Learn. Res."}, "authors":
        [{"authorId": "48019890", "name": "Lauren Hannah"}, {"authorId": "1796335",
        "name": "D. Blei"}, {"authorId": "1852241", "name": "Warrren B Powell"}]},
        {"paperId": "49d942620f925305b583cf4153f1aec88639fbe0", "externalIds": {"CorpusId":
        14400773}, "corpusId": 14400773, "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/49d942620f925305b583cf4153f1aec88639fbe0",
        "title": "Focused Topic Models", "abstract": "We present the focused topic
        model (FTM), a family of nonparametric Bayesian models for learning sparse
        topic mixture patterns. The FTM integrates desirable features from both the
        hierarchical Dirichlet process (HDP) and the Indian buffet process (IBP) \u2013
        allowing an unbounded number of topics for the entire corpus, while each document
        maintains a sparse distribution over these topics. We observe that the HDP
        assumes correlation between the global and within-documant prevalences of
        a topic, and note that such a relationship may be undesirable. By using an
        IBP to select which topics contribute to a document, and an unnormalized Dirichlet
        Process to determine how much of the document is generated by that topic,
        the FTM decouples these probabilities, allowing for more flexible modeling.
        Experimental results on three text corpora demonstrate superior performance
        over the hierarchical Dirichlet process topic model.", "venue": "", "year":
        2009, "referenceCount": 5, "citationCount": 12, "influentialCitationCount":
        2, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
        null, "publicationDate": null, "journal": null, "authors": [{"authorId": "2108881999",
        "name": "Chong Wang"}, {"authorId": "145993598", "name": "K. Heller"}, {"authorId":
        "1796335", "name": "D. Blei"}]}, {"paperId": "72cc610bcdaf4166839eeff04776adcea225439f",
        "externalIds": {"MAG": "2949634144", "ArXiv": "0910.1022", "DBLP": "journals/jmlr/BleiF11",
        "DOI": "10.5555/1953048.2078184", "CorpusId": 1513141}, "corpusId": 1513141,
        "publicationVenue": {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29", "name":
        "International Conference on Machine Learning", "type": "conference", "alternate_names":
        ["ICML", "Int Conf Mach Learn"], "url": "https://icml.cc/"}, "url": "https://www.semanticscholar.org/paper/72cc610bcdaf4166839eeff04776adcea225439f",
        "title": "Distance dependent Chinese restaurant processes", "abstract": "We
        develop the distance dependent Chinese restaurant process (CRP), a flexible
        class of distributions over partitions that allows for non-exchangeability.
        This class can be used to model dependencies between data in infinite clustering
        models, including dependencies across time or space. We examine the properties
        of the distance dependent CRP, discuss its connections to Bayesian nonparametric
        mixture models, and derive a Gibbs sampler for both observed and mixture settings.
        We study its performance with time-dependent models and three text corpora.
        We show that relaxing the assumption of exchangeability with distance dependent
        CRPs can provide a better fit to sequential data. We also show its alternative
        formulation of the traditional CRP leads to a faster-mixing Gibbs sampling
        algorithm than the one based on the original formulation.", "venue": "International
        Conference on Machine Learning", "year": 2009, "referenceCount": 47, "citationCount":
        383, "influentialCitationCount": 72, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": ["Mathematics", "Computer Science"], "s2FieldsOfStudy":
        [{"category": "Mathematics", "source": "external"}, {"category": "Computer
        Science", "source": "external"}, {"category": "Computer Science", "source":
        "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate":
        "2009-10-06", "journal": {"pages": "87-94"}, "authors": [{"authorId": "1796335",
        "name": "D. Blei"}, {"authorId": "1922747", "name": "P. Frazier"}]}, {"paperId":
        "7b1d9dd5be58acb2d25d27a0dfa40c0088f3df77", "externalIds": {"DBLP": "conf/icmc/HoffmanCB09",
        "MAG": "2408713380", "CorpusId": 14584367}, "corpusId": 14584367, "publicationVenue":
        {"id": "6612c9a2-e1af-470e-a13d-eb7d38055139", "name": "International Conference
        on Mathematics and Computing", "type": "conference", "alternate_names": ["Int
        Conf Math Comput", "Int Conf Mechatronics Control", "International Computer
        Music Conference", "International Crimean Microwave Conference", "Int Crime
        Microw Conf", "Int Comput Music Conf", "ICMC", "International Conference on
        Mechatronics and Control"], "url": "http://www.computermusic.org/"}, "url":
        "https://www.semanticscholar.org/paper/7b1d9dd5be58acb2d25d27a0dfa40c0088f3df77",
        "title": "Bayesian Spectral Matching: Turning Young MC into MC Hammer via
        MCMC Sampling", "abstract": "In this paper, we introduce an audio mosaicing
        technique based on performing posterior inference on a probabilistic generative
        model. Whereas previous approaches to concatenative synthesis and audio mosaicing
        have mostly tried to match higher-level descriptors of audio or individual
        STFT frames, we try to directly match the magnitude spectrogram of a target
        sound by combining and overlapping a set of short samples at different times
        and amplitudes. Our use of the graphical modeling formalism allows us to use
        a standard Markov Chain Monte Carlo (MCMC) posterior inference algorithm to
        find a set of time shifts and amplitudes for each sample that results in a
        layered composite sound whose spectrogram approximately matches the target
        spectrogram.", "venue": "International Conference on Mathematics and Computing",
        "year": 2009, "referenceCount": 9, "citationCount": 12, "influentialCitationCount":
        2, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
        "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal":
        {"volume": "2009", "name": ""}, "authors": [{"authorId": "28552618", "name":
        "M. Hoffman"}, {"authorId": "1716507", "name": "P. Cook"}, {"authorId": "1796335",
        "name": "D. Blei"}]}, {"paperId": "7fdf31d5ebdd293b3027e6555e256a936ff5515a",
        "externalIds": {"DBLP": "conf/cvpr/WangBL09", "MAG": "2161050705", "DOI":
        "10.1109/CVPR.2009.5206800", "CorpusId": 14362511}, "corpusId": 14362511,
        "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/7fdf31d5ebdd293b3027e6555e256a936ff5515a",
        "title": "Simultaneous image classification and annotation", "abstract": "Image
        classification and annotation are important problems in computer vision, but
        rarely considered together. Intuitively, annotations provide evidence for
        the class label, and the class label provides evidence for annotations. For
        example, an image of class highway is more likely annotated with words \u201croad,\u201d
        \u201ccar,\u201d and \u201ctraffic\u201d than words \u201cfish,\u201d \u201cboat,\u201d
        and \u201cscuba.\u201d In this paper, we develop a new probabilistic model
        for jointly modeling the image, its class label, and its annotations. Our
        model treats the class label as a global description of the image, and treats
        annotation terms as local descriptions of parts of the image. Its underlying
        probabilistic assumptions naturally integrate these two sources of information.
        We derive an approximate inference and estimation algorithms based on variational
        methods, as well as efficient approximations for classifying and annotating
        new images. We examine the performance of our model on two real-world image
        data sets, illustrating that a single model provides competitive annotation
        performance, and superior classification performance.", "venue": "2009 IEEE
        Conference on Computer Vision and Pattern Recognition", "year": 2009, "referenceCount":
        28, "citationCount": 613, "influentialCitationCount": 77, "isOpenAccess":
        false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
        "Conference"], "publicationDate": "2009-06-20", "journal": {"pages": "1903-1910",
        "name": "2009 IEEE Conference on Computer Vision and Pattern Recognition"},
        "authors": [{"authorId": "2108881999", "name": "Chong Wang"}, {"authorId":
        "1796335", "name": "D. Blei"}, {"authorId": "48004138", "name": "Li Fei-Fei"}]},
        {"paperId": "87156b2e9b7eadd04af1438d0c7d3e733a2ecb84", "externalIds": {"DBLP":
        "conf/nips/ChangBGWB09", "MAG": "2159426623", "CorpusId": 215812433}, "corpusId":
        215812433, "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
        "name": "Neural Information Processing Systems", "type": "conference", "alternate_names":
        ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
        "url": "https://www.semanticscholar.org/paper/87156b2e9b7eadd04af1438d0c7d3e733a2ecb84",
        "title": "Reading Tea Leaves: How Humans Interpret Topic Models", "abstract":
        "Probabilistic topic models are a popular tool for the unsupervised analysis
        of text, providing both a predictive model of future text and a latent topic
        representation of the corpus. Practitioners typically assume that the latent
        space is semantically meaningful. It is used to check models, summarize the
        corpus, and guide exploration of its contents. However, whether the latent
        space is interpretable is in need of quantitative evaluation. In this paper,
        we present new quantitative methods for measuring semantic meaning in inferred
        topics. We back these measures with large-scale user studies, showing that
        they capture aspects of the model that are undetected by previous measures
        of model quality based on held-out likelihood. Surprisingly, topic models
        which perform better on held-out likelihood may infer less semantically meaningful
        topics.", "venue": "Neural Information Processing Systems", "year": 2009,
        "referenceCount": 27, "citationCount": 2063, "influentialCitationCount": 152,
        "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
        "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2009-12-07",
        "journal": {"pages": "288-296"}, "authors": [{"authorId": "80936017", "name":
        "Jonathan D. Chang"}, {"authorId": "1389036863", "name": "Jordan L. Boyd-Graber"},
        {"authorId": "21007048", "name": "S. Gerrish"}, {"authorId": "2108881999",
        "name": "Chong Wang"}, {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId":
        "9dc9f0551b9dcb5837b75a8286fc0d8121d0de7a", "externalIds": {"CorpusId": 208781901},
        "corpusId": 208781901, "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/9dc9f0551b9dcb5837b75a8286fc0d8121d0de7a",
        "title": "Modeling Influence in Text Corpora", "abstract": "Identifying the
        most influential documents in a corpus is an important problem in a wide range
        of fields, ranging from information science and historiography to text summarization
        and news aggregation. We propose using changes in the linguistic content of
        these documents over time to predict the importance of individual documents
        within the collection and describe a dynamic topic model for both quantifying
        and qualifying the impact of each document in the corpus. Introduction In
        many fields, identifying the most influential documents is an important challenge;
        researchers in information science, historiography, text summarization, and
        news aggregation, for example, are all interested in identifying influential
        documents in their respective fields. Researchers in fields like these often
        use traditional methods of assessing the impact of an article such as analyzing
        the citations to it: the impact factor of a journal, for example, is based
        largely on academic citation analysis; and Google\u2019s successful PageRank
        algorithm is based on hyperlink citations between Webpages [1]. Often, however,
        citation information is not present: certain legal documents, news stories,
        blog posts, and email, for example, all might lack such citation metadata,
        while there is a clear notion of influence among articles in these collections.
        The goal of this work is to develop an unsupervised method for determining
        the influence of a document in the absense of citations. Our intuition is
        that language changes over time, and that influential documents contribute
        to this change. We formalize this intuition and present an algorithm which
        takes a sequence of documents as input and computes a vector of \u201dinfluence\u201d
        for each document. This vector characterizes the document\u2019s influence
        in terms of themes discovered using a topic model we have developed. We validate
        this method by measuring how well the computed impact predicts citations and
        demonstrate that this method provides a citation-free measure of bibliometric
        impact. The Document Influence Model We base our model, the Document Influence
        Model (DIM), on the the Dynamic Topic Model (DTM) [2]. The DTM models corpora
        by assuming that documents are mixtures of themes; it models these themes,
        in turn, by allowing them to change over time. The concept of a theme in the
        DTM is formalized as a topic using Latent Dirichlet Allocation (LDA) [3].
        Figure 1: The Document Influence Model. We extend the DTM by associating each
        document dt with a vector of topic weights ldt ,k (see Figure 1). These weights
        express how much the language used in document dt affects the drift of these
        topics over a period of time. The more influential a document is in a topic
        k, the larger its topic weights should be, making it more likely that future
        documents about topic k will use the same language. The influence of a document
        on each topic k works as follows. We assume that document dt at time t may
        have some influence on the language used within each topic. The more influential
        dt is on topic k (i.e., the higher its weight for this topic)\u2013and the
        more its words are \u201dabout\u201d topic k in the first place\u2013the more
        it \u201cnudges\u201d this topic\u2019s natural parameters \u03b2t,k in log
        space. In LDA, and hence the DIM, the topic assignment for word n is given
        by the random variable zn; its role is clarified below the generative model.
        The full generative model at time t is then: 1. For topic k = 1, . . . ,K:
        Draw natural parameters \u03b2t,k|\u03b2t\u22121,k,zs", "venue": "", "year":
        2009, "referenceCount": 4, "citationCount": 1, "influentialCitationCount":
        0, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
        null, "publicationDate": null, "journal": null, "authors": [{"authorId": "21007048",
        "name": "S. Gerrish"}, {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId":
        "9dfcaad0c019a67c46e0156bc8da52e32628d446", "externalIds": {"MAG": "2114352343",
        "DBLP": "conf/nips/SocherGPSBN09", "CorpusId": 14285119}, "corpusId": 14285119,
        "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd", "name":
        "Neural Information Processing Systems", "type": "conference", "alternate_names":
        ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
        "url": "https://www.semanticscholar.org/paper/9dfcaad0c019a67c46e0156bc8da52e32628d446",
        "title": "A Bayesian Analysis of Dynamics in Free Recall", "abstract": "We
        develop a probabilistic model of human memory performance in free recall experiments.
        In these experiments, a subject first studies a list of words and then tries
        to recall them. To model these data, we draw on both previous psychological
        research and statistical topic models of text documents. We assume that memories
        are formed by assimilating the semantic meaning of studied words (represented
        as a distribution over topics) into a slowly changing latent context (represented
        in the same space). During recall, this context is reinstated and used as
        a cue for retrieving studied words. By conceptualizing memory retrieval as
        a dynamic latent variable model, we are able to use Bayesian inference to
        represent uncertainty and reason about the cognitive processes underlying
        memory. We present a particle filter algorithm for performing approximate
        posterior inference, and evaluate our model on the prediction of recalled
        words in experimental data. By specifying the model hierarchically, we are
        also able to capture inter-subject variability.", "venue": "Neural Information
        Processing Systems", "year": 2009, "referenceCount": 20, "citationCount":
        32, "influentialCitationCount": 3, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
        "Computer Science", "source": "external"}, {"category": "Computer Science",
        "source": "s2-fos-model"}, {"category": "Psychology", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2009-12-07",
        "journal": {"pages": "1714-1722"}, "authors": [{"authorId": "2166511", "name":
        "R. Socher"}, {"authorId": "1831199", "name": "S. Gershman"}, {"authorId":
        "3237761", "name": "A. Perotte"}, {"authorId": "2196760", "name": "P. Sederberg"},
        {"authorId": "1796335", "name": "D. Blei"}, {"authorId": "1780319", "name":
        "K. Norman"}]}, {"paperId": "9f68d27df3a4c4be8636f376cb15f77e55a2f496", "externalIds":
        {"DBLP": "journals/jmlr/ChangB09", "MAG": "2123549998", "CorpusId": 312394},
        "corpusId": 312394, "publicationVenue": {"id": "2d136b11-c2b5-484b-b008-7f4a852fd61e",
        "name": "International Conference on Artificial Intelligence and Statistics",
        "type": "conference", "alternate_names": ["AISTATS", "Int Conf Artif Intell
        Stat"]}, "url": "https://www.semanticscholar.org/paper/9f68d27df3a4c4be8636f376cb15f77e55a2f496",
        "title": "Relational Topic Models for Document Networks", "abstract": "We
        develop the relational topic model (RTM), a model of documents and the links
        between them. For each pair of documents, the RTM models their link as a binary
        random variable that is conditioned on their contents. The model can be used
        to summarize a network of documents, predict links between them, and predict
        words within them. We derive efficient inference and learning algorithms based
        on variational methods and evaluate the predictive performance of the RTM
        for large networks of scientific abstracts and web documents.", "venue": "International
        Conference on Artificial Intelligence and Statistics", "year": 2009, "referenceCount":
        26, "citationCount": 567, "influentialCitationCount": 93, "isOpenAccess":
        false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
        "publicationDate": "2009-04-15", "journal": {"pages": "81-88"}, "authors":
        [{"authorId": "80936017", "name": "Jonathan D. Chang"}, {"authorId": "1796335",
        "name": "D. Blei"}]}, {"paperId": "a23f77cb550362995409646d3ff65d769009ef3a",
        "externalIds": {"MAG": "2124672527", "ArXiv": "0909.4331", "DOI": "10.1214/09-AOAS309",
        "CorpusId": 8896596}, "corpusId": 8896596, "publicationVenue": null, "url":
        "https://www.semanticscholar.org/paper/a23f77cb550362995409646d3ff65d769009ef3a",
        "title": "Hierarchical relational models for document networks", "abstract":
        "We develop the relational topic model (RTM), a hierarchical model of both
        network structure and node attributes. We focus on document networks, where
        the attributes of each document are its words, that is, discrete observations
        taken from a fixed vocabulary. For each pair of documents, the RTM models
        their link as a binary random variable that is conditioned on their contents.
        The model can be used to summarize a network of documents, predict links between
        them, and predict words within them. We derive efficient inference and estimation
        algorithms based on variational methods that take advantage of sparsity and
        scale with the number of links. We evaluate the predictive performance of
        the RTM for large networks of scientific abstracts, web documents, and geographically
        tagged news.", "venue": "", "year": 2009, "referenceCount": 52, "citationCount":
        252, "influentialCitationCount": 35, "isOpenAccess": true, "openAccessPdf":
        null, "fieldsOfStudy": ["Mathematics", "Computer Science"], "s2FieldsOfStudy":
        [{"category": "Mathematics", "source": "external"}, {"category": "Computer
        Science", "source": "external"}, {"category": "Computer Science", "source":
        "s2-fos-model"}], "publicationTypes": null, "publicationDate": "2009-09-23",
        "journal": {"volume": "4", "pages": "124-150", "name": "The Annals of Applied
        Statistics"}, "authors": [{"authorId": "80936017", "name": "Jonathan D. Chang"},
        {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId": "b5026e3fc62a9d7b244fe9a88d22907aa976a7e4",
        "externalIds": {"DBLP": "conf/nips/WangB09a", "MAG": "2098047130", "CorpusId":
        6704990}, "corpusId": 6704990, "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
        "name": "Neural Information Processing Systems", "type": "conference", "alternate_names":
        ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
        "url": "https://www.semanticscholar.org/paper/b5026e3fc62a9d7b244fe9a88d22907aa976a7e4",
        "title": "Decoupling Sparsity and Smoothness in the Discrete Hierarchical
        Dirichlet Process", "abstract": "We present a nonparametric hierarchical Bayesian
        model of document collections that decouples sparsity and smoothness in the
        component distributions (i.e., the \"topics\"). In the sparse topic model
        (sparseTM), each topic is represented by a bank of selector variables that
        determine which terms appear in the topic. Thus each topic is associated with
        a subset of the vocabulary, and topic smoothness is modeled on this subset.
        We develop an efficient Gibbs sampler for the sparseTM that includes a general-purpose
        method for sampling from a Dirichlet mixture with a combinatorial number of
        components. We demonstrate the sparseTM on four real-world datasets. Compared
        to traditional approaches, the empirical results will show that sparseTMs
        give better predictive performance with simpler inferred models.", "venue":
        "Neural Information Processing Systems", "year": 2009, "referenceCount": 11,
        "citationCount": 130, "influentialCitationCount": 13, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
        "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
        {"category": "Mathematics", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
        "Conference"], "publicationDate": "2009-12-07", "journal": {"pages": "1982-1989"},
        "authors": [{"authorId": "2108881999", "name": "Chong Wang"}, {"authorId":
        "1796335", "name": "D. Blei"}]}, {"paperId": "b737b228865eefb4a5627d77a0c0984c64fb98ed",
        "externalIds": {"DBLP": "journals/corr/abs-1205-2657", "MAG": "2962684168",
        "ArXiv": "1205.2657", "CorpusId": 215825009}, "corpusId": 215825009, "publicationVenue":
        {"id": "f9af8000-42f8-410d-a622-e8811e41660a", "name": "Conference on Uncertainty
        in Artificial Intelligence", "type": "conference", "alternate_names": ["Uncertainty
        in Artificial Intelligence", "UAI", "Conf Uncertain Artif Intell", "Uncertain
        Artif Intell"], "url": "http://www.auai.org/"}, "url": "https://www.semanticscholar.org/paper/b737b228865eefb4a5627d77a0c0984c64fb98ed",
        "title": "Multilingual Topic Models for Unaligned Text", "abstract": "We develop
        the multilingual topic model for unaligned text (MuTo), a probabilistic model
        of text that is designed to analyze corpora composed of documents in two languages.
        From these documents, MuTo uses stochastic EM to simultaneously discover both
        a matching between the languages and multilingual latent topics. We demonstrate
        that MuTo is able to find shared topics on real-world multilingual corpora,
        successfully pairing related documents across languages. MuTo provides a new
        framework for creating multilingual topic models without needing carefully
        curated parallel corpora and allows applications built using the topic model
        formalism to be applied to a much wider class of corpora.", "venue": "Conference
        on Uncertainty in Artificial Intelligence", "year": 2009, "referenceCount":
        29, "citationCount": 172, "influentialCitationCount": 15, "isOpenAccess":
        false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
        "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
        {"category": "Mathematics", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
        "Conference"], "publicationDate": "2009-06-18", "journal": {"volume": "abs/1205.2657",
        "name": "ArXiv"}, "authors": [{"authorId": "1389036863", "name": "Jordan L.
        Boyd-Graber"}, {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId": "c34898a44917a0f6f1018c5db6e534deec58aea5",
        "externalIds": {"DBLP": "conf/ismir/HoffmanBC09", "MAG": "2113229563", "CorpusId":
        13471538}, "corpusId": 13471538, "publicationVenue": {"id": "cfc287e4-4c04-4848-ab16-633b33a61a09",
        "name": "International Society for Music Information Retrieval Conference",
        "type": "conference", "alternate_names": ["International Symposium/Conference
        on Music Information Retrieval", "ISMIR", "Int Soc Music Inf Retr Conf", "Int
        Symp Music Inf Retr"], "url": "http://www.ismir.net/"}, "url": "https://www.semanticscholar.org/paper/c34898a44917a0f6f1018c5db6e534deec58aea5",
        "title": "Easy As CBA: A Simple Probabilistic Model for Tagging Music", "abstract":
        "Many songs in large music databases are not labeled with semantic tags that
        could help users sort out the songs they want to listen to from those they
        do not. If the words that apply to a song can be predicted from audio, then
        those predictions can be used both to automatically annotate a song with tags,
        allowing users to get a sense of what qualities characterize a song at a glance.
        Automatic tag prediction can also drive retrieval by allowing users to search
        for the songs most strongly characterized by a particular word. We present
        a probabilistic model that learns to predict the probability that a word applies
        to a song from audio. Our model is simple to implement, fast to train, predicts
        tags for new songs quickly, and achieves state-of-the-art performance on annotation
        and retrieval tasks.", "venue": "International Society for Music Information
        Retrieval Conference", "year": 2009, "referenceCount": 10, "citationCount":
        98, "influentialCitationCount": 17, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
        "Computer Science", "source": "external"}, {"category": "Computer Science",
        "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
        null, "journal": {"pages": "369-374"}, "authors": [{"authorId": "28552618",
        "name": "M. Hoffman"}, {"authorId": "1796335", "name": "D. Blei"}, {"authorId":
        "1716507", "name": "P. Cook"}]}, {"paperId": "e651bf5fbf5aa2ca7fc3c7ce05d7c02f6fed1509",
        "externalIds": {"DBLP": "conf/icmc/HoffmanCB08", "MAG": "1594968668", "CorpusId":
        1405115}, "corpusId": 1405115, "publicationVenue": {"id": "6612c9a2-e1af-470e-a13d-eb7d38055139",
        "name": "International Conference on Mathematics and Computing", "type": "conference",
        "alternate_names": ["Int Conf Math Comput", "Int Conf Mechatronics Control",
        "International Computer Music Conference", "International Crimean Microwave
        Conference", "Int Crime Microw Conf", "Int Comput Music Conf", "ICMC", "International
        Conference on Mechatronics and Control"], "url": "http://www.computermusic.org/"},
        "url": "https://www.semanticscholar.org/paper/e651bf5fbf5aa2ca7fc3c7ce05d7c02f6fed1509",
        "title": "Data-Driven Recomposition using the Hierarchical Dirichlet Process
        Hidden Markov Model", "abstract": "Hidden Markov Models (HMMs) have been widely
        used in various audio analysis tasks such as speech recognition and genre
        classification. In this paper we show how HMMs can be used to synthesize new
        audio clips of unlimited length inspired by the temporal structure and perceptual
        content of a training recording or set of such recordings. We use Markov chain
        techniques similar to those that have long been used to generate symbolic
        data such as text and musical scores to instead generate sequences of continuous
        audio feature data that can then be transformed into audio using feature-based
        and concatenative synthesis techniques. Additionally, we explore the use of
        the Hierarchical Dirichlet Process HMM (HDP-HMM) for music, which sidesteps
        some difficulties with traditional HMMs, and extend the HDP-HMM to allow multiple
        song models to be trained simultaneously in a way that allows the blending
        of different models to produce output that is a hybrid of multiple input recordings.",
        "venue": "International Conference on Mathematics and Computing", "year":
        2009, "referenceCount": 20, "citationCount": 19, "influentialCitationCount":
        0, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
        "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle"], "publicationDate": "2009-10-01", "journal":
        {"volume": "2008", "name": ""}, "authors": [{"authorId": "28552618", "name":
        "M. Hoffman"}, {"authorId": "1716507", "name": "P. Cook"}, {"authorId": "1796335",
        "name": "D. Blei"}]}, {"paperId": "411277565d6182b40d301df1d2179f5c2b634bb3",
        "externalIds": {"ArXiv": "1206.3298", "MAG": "1915315806", "DBLP": "journals/corr/abs-1206-3298",
        "CorpusId": 1866513}, "corpusId": 1866513, "publicationVenue": {"id": "f9af8000-42f8-410d-a622-e8811e41660a",
        "name": "Conference on Uncertainty in Artificial Intelligence", "type": "conference",
        "alternate_names": ["Uncertainty in Artificial Intelligence", "UAI", "Conf
        Uncertain Artif Intell", "Uncertain Artif Intell"], "url": "http://www.auai.org/"},
        "url": "https://www.semanticscholar.org/paper/411277565d6182b40d301df1d2179f5c2b634bb3",
        "title": "Continuous Time Dynamic Topic Models", "abstract": "In this paper,
        we develop the continuous time dynamic topic model (cDTM). The cDTM is a dynamic
        topic model that uses Brownian motion to model the latent topics through a
        sequential collection of documents, where a \"topic\" is a pattern of word
        use that we expect to evolve over the course of the collection. We derive
        an efficient variational approximate inference algorithm that takes advantage
        of the sparsity of observations in text, a property that lets us easily handle
        many time points. In contrast to the cDTM, the original discrete-time dynamic
        topic model (dDTM) requires that time be discretized. Moreover, the complexity
        of variational inference for the dDTM grows quickly as time granularity increases,
        a drawback which limits fine-grained discretization. We demonstrate the cDTM
        on two news corpora, reporting both predictive perplexity and the novel task
        of time stamp prediction.", "venue": "Conference on Uncertainty in Artificial
        Intelligence", "year": 2008, "referenceCount": 27, "citationCount": 479, "influentialCitationCount":
        21, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science",
        "source": "external"}, {"category": "Mathematics", "source": "external"},
        {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
        ["JournalArticle", "Conference"], "publicationDate": "2008-07-09", "journal":
        {"pages": "579-586"}, "authors": [{"authorId": "2108881999", "name": "Chong
        Wang"}, {"authorId": "1796335", "name": "D. Blei"}, {"authorId": "48099028",
        "name": "D. Heckerman"}]}, {"paperId": "457628a1c232bb48acc2db8440571e289cc80e15",
        "externalIds": {"DBLP": "conf/nips/Boyd-GraberB08", "ArXiv": "1002.4665",
        "MAG": "2949592739", "CorpusId": 215824810}, "corpusId": 215824810, "publicationVenue":
        {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd", "name": "Neural Information
        Processing Systems", "type": "conference", "alternate_names": ["Neural Inf
        Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"}, "url": "https://www.semanticscholar.org/paper/457628a1c232bb48acc2db8440571e289cc80e15",
        "title": "Syntactic Topic Models", "abstract": "We develop the syntactic topic
        model (STM), a nonparametric Bayesian model of parsed documents. The STM generates
        words that are both thematically and syntactically constrained, which combines
        the semantic insights of topic models with the syntactic information available
        from parse trees. Each word of a sentence is generated by a distribution that
        combines document-specific topic weights and parse-tree-specific syntactic
        transitions. Words are assumed to be generated in an order that respects the
        parse tree. We derive an approximate posterior inference method based on variational
        methods for hierarchical Dirichlet processes, and we report qualitative and
        quantitative results on both synthetic data and hand-parsed documents.", "venue":
        "Neural Information Processing Systems", "year": 2008, "referenceCount": 75,
        "citationCount": 216, "influentialCitationCount": 8, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
        "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
        {"category": "Mathematics", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
        "Conference"], "publicationDate": "2008-12-08", "journal": {"volume": "abs/1002.4665",
        "name": "ArXiv"}, "authors": [{"authorId": "1389036863", "name": "Jordan L.
        Boyd-Graber"}, {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId": "497e22323137e909f64dccd8695dcfb72e82e032",
        "externalIds": {"MAG": "2148111240", "DBLP": "conf/nips/MukherjeeB08", "CorpusId":
        1132471}, "corpusId": 1132471, "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
        "name": "Neural Information Processing Systems", "type": "conference", "alternate_names":
        ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
        "url": "https://www.semanticscholar.org/paper/497e22323137e909f64dccd8695dcfb72e82e032",
        "title": "Relative Performance Guarantees for Approximate Inference in Latent
        Dirichlet Allocation", "abstract": "Hierarchical probabilistic modeling of
        discrete data has emerged as a powerful tool for text analysis. Posterior
        inference in such models is intractable, and practitioners rely on approximate
        posterior inference methods such as variational inference or Gibbs sampling.
        There has been much research in designing better approximations, but there
        is yet little theoretical understanding of which of the available techniques
        are appropriate, and in which data analysis settings. In this paper we provide
        the beginnings of such understanding. We analyze the improvement that the
        recently proposed collapsed variational inference (CVB) provides over mean
        field variational inference (VB) in latent Dirichlet allocation. We prove
        that the difference in the tightness of the bound on the likelihood of a document
        decreases as O(k - 1) + \u221alog m/m, where k is the number of topics in
        the model and m is the number of words in a document. As a consequence, the
        advantage of CVB over VB is lost for long documents but increases with the
        number of topics. We demonstrate empirically that the theory holds, using
        simulated text data and two text corpora. We provide practical guidelines
        for choosing an approximation.", "venue": "Neural Information Processing Systems",
        "year": 2008, "referenceCount": 24, "citationCount": 25, "influentialCitationCount":
        1, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science",
        "source": "external"}, {"category": "Mathematics", "source": "external"},
        {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
        ["JournalArticle", "Conference"], "publicationDate": "2008-12-08", "journal":
        {"pages": "1129-1136"}, "authors": [{"authorId": "1943285", "name": "Indraneel
        Mukherjee"}, {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId": "e82307b16df5046d166a53f55212dfc70b469f8f",
        "externalIds": {"DBLP": "conf/ismir/HoffmanBC08", "MAG": "190299453", "CorpusId":
        624263}, "corpusId": 624263, "publicationVenue": {"id": "cfc287e4-4c04-4848-ab16-633b33a61a09",
        "name": "International Society for Music Information Retrieval Conference",
        "type": "conference", "alternate_names": ["International Symposium/Conference
        on Music Information Retrieval", "ISMIR", "Int Soc Music Inf Retr Conf", "Int
        Symp Music Inf Retr"], "url": "http://www.ismir.net/"}, "url": "https://www.semanticscholar.org/paper/e82307b16df5046d166a53f55212dfc70b469f8f",
        "title": "Content-Based Musical Similarity Computation using the Hierarchical
        Dirichlet Process", "abstract": "We develop a method for discovering the latent
        structure in MFCC feature data using the Hierarchical Dirichlet Process (HDP).
        Based on this structure, we compute timbral similarity between recorded songs.
        The HDP is a nonparametric Bayesian model. Like the Gaussian Mixture Model
        (GMM), it represents each song as a mixture of some number of multivariate
        Gaussian distributions However, the number of mixture components is not fixed
        in the HDP, but is determined as part of the posterior inference process.
        Moreover, in the HDP the same set of Gaussians is used to model all songs,
        with only the mixture weights varying from song to song. We compute the similarity
        of songs based on these weights, which is faster than previous approaches
        that compare single Gaussian distributions directly. Experimental results
        on a genre-based retrieval task illustrate that our HDPbased method is both
        faster and produces better retrieval quality than such previous approaches.",
        "venue": "International Society for Music Information Retrieval Conference",
        "year": 2008, "referenceCount": 13, "citationCount": 75, "influentialCitationCount":
        1, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
        "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal":
        {"pages": "349-354"}, "authors": [{"authorId": "28552618", "name": "M. Hoffman"},
        {"authorId": "1796335", "name": "D. Blei"}, {"authorId": "1716507", "name":
        "P. Cook"}]}, {"paperId": "0af8d3bc3f2d8ac3171dd3114af0b69a4d488cd1", "externalIds":
        {"MAG": "1555796047", "ArXiv": "0706.0294", "CorpusId": 17449358}, "corpusId":
        17449358, "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/0af8d3bc3f2d8ac3171dd3114af0b69a4d488cd1",
        "title": "Mixed membership analysis of high-throughput interaction studies:
        Relational data", "abstract": "In this paper, we consider the statistical
        analysis of a protein interaction network. We propose a Bayesian model that
        uses a hierarchy of probabilistic assumptions about the way proteins interact
        with one another in order to: (i) identify the number of non-observable functional
        modules; (ii) estimate the degree of membership of proteins to modules; and
        (iii) estimate typical interaction patterns among the functional modules themselves.
        Our model describes large amount of (relational) data using a relatively small
        set of parameters that we can reliably estimate with an efficient inference
        algorithm. We apply our methodology to data on protein-to-protein interactions
        in saccharomyces cerevisiae to reveal proteins'' diverse functional roles.
        The case study provides the basis for an overview of which scientific questions
        can be addressed using our methods, and for a discussion of technical issues.",
        "venue": "", "year": 2007, "referenceCount": 30, "citationCount": 11, "influentialCitationCount":
        0, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Biology",
        "Computer Science"], "s2FieldsOfStudy": [{"category": "Biology", "source":
        "external"}, {"category": "Computer Science", "source": "external"}, {"category":
        "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["Review"],
        "publicationDate": "2007-06-03", "journal": {"volume": "", "name": "arXiv:
        Molecular Networks"}, "authors": [{"authorId": "2247552", "name": "E. Airoldi"},
        {"authorId": "1796335", "name": "D. Blei"}, {"authorId": "1684961", "name":
        "S. Fienberg"}, {"authorId": "143977260", "name": "E. Xing"}]}, {"paperId":
        "1df5e35f08cfb8105d34bd97848b877ebfa6ccf6", "externalIds": {"MAG": "1995843818",
        "ACL": "W07-2059", "DBLP": "conf/semeval/ChangDB07", "DOI": "10.3115/1621474.1621533",
        "CorpusId": 16192346}, "corpusId": 16192346, "publicationVenue": {"id": "70713d09-6e4b-4554-9d3f-94d08aba320c",
        "name": "International Workshop on Semantic Evaluation", "type": "conference",
        "alternate_names": ["SemEval ", "Int Workshop Semantic Evaluation"]}, "url":
        "https://www.semanticscholar.org/paper/1df5e35f08cfb8105d34bd97848b877ebfa6ccf6",
        "title": "PU-BCD: Exponential Family Models for the Coarse- and Fine-Grained
        All-Words Tasks", "abstract": "This paper describes an exponential family
        model of word sense which captures both occurrences and co-occurrences of
        words and senses in a joint probability distribution. This statistical framework
        lends itself to the task of word sense disambiguation. We evaluate the performance
        of the model in its participation on the SemEval-2007 coarse- and fine-grained
        all-words tasks under a variety of parameters.", "venue": "International Workshop
        on Semantic Evaluation", "year": 2007, "referenceCount": 10, "citationCount":
        0, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url":
        "https://dl.acm.org/doi/pdf/10.5555/1621474.1621533", "status": null}, "fieldsOfStudy":
        ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science",
        "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle"], "publicationDate": "2007-06-23", "journal":
        {"pages": "272-276"}, "authors": [{"authorId": "80936017", "name": "Jonathan
        D. Chang"}, {"authorId": "144652072", "name": "Miroslav Dud\u00edk"}, {"authorId":
        "1796335", "name": "D. Blei"}]}, {"paperId": "276579dd0d88a0046f58586db0047fe7e71c3571",
        "externalIds": {"MAG": "2433569575", "CorpusId": 2562333}, "corpusId": 2562333,
        "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/276579dd0d88a0046f58586db0047fe7e71c3571",
        "title": "The nested Chinese restaurant process and Bayesian inference of
        topic hierarchies", "abstract": "We present the nested Chinese restaurant
        process (nCRP), a stochastic process which assigns probability distributions
        to infinitely-deep, infinitely-branching trees. We show how this stochastic
        process can be used as a prior distribution in a Bayesian nonparametric model
        of document collections. Specifically, we present an application to information
        retrieval in which documents are modeled as paths down a random tree, and
        the preferential attachment dynamics of the nCRP leads to clustering of documents
        according to sharing of topics at multiple levels of abstraction. Given a
        corpus of documents, a posterior inference algorithm finds an approximation
        to a posterior distribution over trees, topics and allocations of words to
        levels of the tree. We demonstrate this algorithm on collections of scientific
        abstracts from several journals. This model exemplifies a recent trend in
        statistical machine learning--the use of Bayesian nonparametric methods to
        infer distributions on flexible data structures.", "venue": "", "year": 2007,
        "referenceCount": 72, "citationCount": 15, "influentialCitationCount": 0,
        "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
        "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": null, "publicationDate": "2007-10-03", "journal": {"volume":
        "", "name": ""}, "authors": [{"authorId": "1796335", "name": "D. Blei"}, {"authorId":
        "1799860", "name": "T. Griffiths"}, {"authorId": "1694621", "name": "Michael
        I. Jordan"}]}, {"paperId": "46c825c9c5c6ce278671419ce76ea9c71d4dcbc7", "externalIds":
        {"DBLP": "conf/icml/2006sna", "MAG": "2493516651", "DOI": "10.1007/978-3-540-73133-7",
        "CorpusId": 263880100}, "corpusId": 263880100, "publicationVenue": null, "url":
        "https://www.semanticscholar.org/paper/46c825c9c5c6ce278671419ce76ea9c71d4dcbc7",
        "title": "Statistical Network Analysis: Models, Issues, and New Directions
        - ICML 2006 Workshop on Statistical Network Analysis, Pittsburgh, PA, USA,
        June 29, 2006, Revised Selected Papers", "abstract": null, "venue": "SNA@ICML",
        "year": 2007, "referenceCount": 163, "citationCount": 29, "influentialCitationCount":
        2, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
        "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": null, "publicationDate": null, "journal": {"volume": "4503"},
        "authors": [{"authorId": "2247552", "name": "E. Airoldi"}, {"authorId": "1796335",
        "name": "D. Blei"}, {"authorId": "1684961", "name": "S. Fienberg"}, {"authorId":
        "2257283676", "name": "Anna Goldenberg"}, {"authorId": "2257164680", "name":
        "Eric P. Xing"}, {"authorId": "2257285911", "name": "Alice X. Zheng"}]}, {"paperId":
        "4854e506a1b19e6158bce5e5b43697d4aaa1c12a", "externalIds": {"MAG": "1974778955",
        "DBLP": "conf/icml/DudikBS07", "DOI": "10.1145/1273496.1273528", "CorpusId":
        10267369}, "corpusId": 10267369, "publicationVenue": {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
        "name": "International Conference on Machine Learning", "type": "conference",
        "alternate_names": ["ICML", "Int Conf Mach Learn"], "url": "https://icml.cc/"},
        "url": "https://www.semanticscholar.org/paper/4854e506a1b19e6158bce5e5b43697d4aaa1c12a",
        "title": "Hierarchical maximum entropy density estimation", "abstract": "We
        study the problem of simultaneously estimating several densities where the
        datasets are organized into overlapping groups, such as a hierarchy. For this
        problem, we propose a maximum entropy formulation, which systematically incorporates
        the groups and allows us to share the strength of prediction across similar
        datasets. We derive general performance guarantees, and show how some previous
        approaches, such as hierarchical shrinkage and hierarchical priors, can be
        derived as special cases. We demonstrate the proposed technique on synthetic
        data and in a real-world application to modeling the geographic distributions
        of species hierarchically grouped in a taxonomy. Specifically, we model the
        geographic distributions of species in the Australian wet tropics and Northeast
        New South Wales. In these regions, small numbers of samples per species significantly
        hinder effective prediction. Substantial benefits are obtained by combining
        information across taxonomic groups.", "venue": "International Conference
        on Machine Learning", "year": 2007, "referenceCount": 17, "citationCount":
        25, "influentialCitationCount": 0, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": ["Mathematics", "Computer Science"], "s2FieldsOfStudy":
        [{"category": "Mathematics", "source": "external"}, {"category": "Computer
        Science", "source": "external"}, {"category": "Computer Science", "source":
        "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
        "2007-06-20", "journal": {"pages": "249-256"}, "authors": [{"authorId": "144652072",
        "name": "Miroslav Dud\u00edk"}, {"authorId": "1796335", "name": "D. Blei"},
        {"authorId": "1716301", "name": "R. Schapire"}]}, {"paperId": "59fc7efc9cfe92c90704ef32017162a7fc23ff06",
        "externalIds": {"MAG": "2290807181", "CorpusId": 14904661}, "corpusId": 14904661,
        "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/59fc7efc9cfe92c90704ef32017162a7fc23ff06",
        "title": "Admixtures of latent blocks with application to protein interaction
        networks", "abstract": "In this paper, we consider the statistical analysis
        of a protein interaction network. We propose a Bayesian model that uses a
        hierarchy of probabilistic assumptions about the way proteins interact with
        one another in order to: (i) identify the number of non-observable functional
        modules; (ii) estimate the degree of membership of proteins to modules; and
        (iii) estimate typical interaction patterns among the functional modules themselves.
        Our model describes large amount of (relational) data using a relatively small
        set of parameters that we can reliably estimate with an efficient inference
        algorithm. We apply our methodology to data on protein-to-protein interactions
        in saccharomyces cerevisiae to reveal proteins\u2019 diverse functional roles.
        The case study provides the basis for an overview of which scientific questions
        can be addressed using our methods, and for a discussion of technical issues.",
        "venue": "", "year": 2007, "referenceCount": 28, "citationCount": 6, "influentialCitationCount":
        0, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
        "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["Review"], "publicationDate": "2007-06-03", "journal":
        {"volume": "", "name": ""}, "authors": [{"authorId": "9642064", "name": "E.
        Airoldi"}, {"authorId": "1796335", "name": "D. Blei"}, {"authorId": "1684961",
        "name": "S. Fienberg"}, {"authorId": "143977260", "name": "E. Xing"}]}, {"paperId":
        "6d296b269991f165b650a6360254a7413e966d27", "externalIds": {"DBLP": "journals/jacm/BleiGJ10",
        "MAG": "2150286230", "ArXiv": "0710.0845", "DOI": "10.1145/1667053.1667056",
        "CorpusId": 9735250}, "corpusId": 9735250, "publicationVenue": null, "url":
        "https://www.semanticscholar.org/paper/6d296b269991f165b650a6360254a7413e966d27",
        "title": "The nested chinese restaurant process and bayesian nonparametric
        inference of topic hierarchies", "abstract": "We present the nested Chinese
        restaurant process (nCRP), a stochastic process that assigns probability distributions
        to ensembles of infinitely deep, infinitely branching trees. We show how this
        stochastic process can be used as a prior distribution in a Bayesian nonparametric
        model of document collections. Specifically, we present an application to
        information retrieval in which documents are modeled as paths down a random
        tree, and the preferential attachment dynamics of the nCRP leads to clustering
        of documents according to sharing of topics at multiple levels of abstraction.
        Given a corpus of documents, a posterior inference algorithm finds an approximation
        to a posterior distribution over trees, topics and allocations of words to
        levels of the tree. We demonstrate this algorithm on collections of scientific
        abstracts from several journals. This model exemplifies a recent trend in
        statistical machine learning\u2014the use of Bayesian nonparametric methods
        to infer distributions on flexible data structures.", "venue": "JACM", "year":
        2007, "referenceCount": 100, "citationCount": 670, "influentialCitationCount":
        81, "isOpenAccess": true, "openAccessPdf": {"url": "http://arxiv.org/pdf/0710.0845",
        "status": null}, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics",
        "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle"], "publicationDate": "2007-10-03", "journal":
        {"volume": "57", "pages": "7:1-7:30", "name": "J. ACM"}, "authors": [{"authorId":
        "1796335", "name": "D. Blei"}, {"authorId": "1799860", "name": "T. Griffiths"},
        {"authorId": "1694621", "name": "Michael I. Jordan"}]}, {"paperId": "87ce59b72873d2b77ab29611a8b448dd571f40ed",
        "externalIds": {"MAG": "1915058860", "DBLP": "journals/corr/abs-1206-5270",
        "ArXiv": "1206.5270", "DOI": "10.5555/3020488.3020518", "CorpusId": 250246},
        "corpusId": 250246, "publicationVenue": {"id": "f9af8000-42f8-410d-a622-e8811e41660a",
        "name": "Conference on Uncertainty in Artificial Intelligence", "type": "conference",
        "alternate_names": ["Uncertainty in Artificial Intelligence", "UAI", "Conf
        Uncertain Artif Intell", "Uncertain Artif Intell"], "url": "http://www.auai.org/"},
        "url": "https://www.semanticscholar.org/paper/87ce59b72873d2b77ab29611a8b448dd571f40ed",
        "title": "Nonparametric Bayes Pachinko Allocation", "abstract": "Recent advances
        in topic models have explored complicated structured distributions to represent
        topic correlation. For example, the pachinko allocation model (PAM) captures
        arbitrary, nested, and possibly sparse correlations between topics using a
        directed acyclic graph (DAG). While PAM provides more flexibility and greater
        expressive power than previous models like latent Dirichlet allocation (LDA),
        it is also more difficult to determine the appropriate topic structure for
        a specific dataset. In this paper, we propose a nonparametric Bayesian prior
        for PAM based on a variant of the hierarchical Dirichlet process (HDP). Although
        the HDP can capture topic correlations defined by nested data structure, it
        does not automatically discover such correlations from unstructured data.
        By assuming an HDP-based prior for PAM, we are able to learn both the number
        of topics and how the topics are correlated. We evaluate our model on synthetic
        and real-world text datasets, and show that nonparametric PAM achieves performance
        matching the best of PAM without manually tuning the number of topics.", "venue":
        "Conference on Uncertainty in Artificial Intelligence", "year": 2007, "referenceCount":
        13, "citationCount": 87, "influentialCitationCount": 3, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
        "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
        {"category": "Mathematics", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
        "Conference"], "publicationDate": "2007-07-19", "journal": {"pages": "243-250"},
        "authors": [{"authorId": "40400230", "name": "Wei Li"}, {"authorId": "1796335",
        "name": "D. Blei"}, {"authorId": "143753639", "name": "A. McCallum"}]}, {"paperId":
        "9c514afc65342e71adc84ed9b46fda3a54a3d127", "externalIds": {"DBLP": "conf/icdm/KaplanB07",
        "ArXiv": "2310.09357", "MAG": "2145071590", "DOI": "10.1109/ICDM.2007.76",
        "CorpusId": 14797449}, "corpusId": 14797449, "publicationVenue": {"id": "67d15a94-d523-4b5f-be58-03fe2ef9dcfb",
        "name": "Industrial Conference on Data Mining", "type": "conference", "alternate_names":
        ["Ind Conf Data Min", "ICDM"], "url": "http://www.data-mining-forum.de/"},
        "url": "https://www.semanticscholar.org/paper/9c514afc65342e71adc84ed9b46fda3a54a3d127",
        "title": "A Computational Approach to Style in American Poetry", "abstract":
        "We develop a quantitative method to assess the style of American poems and
        to visualize a collection of poems in relation to one another. Qualitative
        poetry criticism helped guide our development of metrics that analyze various
        orthographic, syntactic, and phonemic features. These features are used to
        discover comprehensive stylistic information from a poem''s multi-layered
        latent structure, and to compute distances between poems in this space. Visualizations
        provide ready access to the analytical components. We demonstrate our method
        on several collections of poetry, showing that it better delineates poetry
        style than the traditional word-occurrence features that are used in typical
        text analysis algorithms. Our method has potential applications to academic
        research of texts, to research of the intuitive personal response to poetry,
        and to making recommendations to readers based on their favorite poems.",
        "venue": "Industrial Conference on Data Mining", "year": 2007, "referenceCount":
        20, "citationCount": 51, "influentialCitationCount": 5, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Education",
        "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"],
        "publicationDate": "2007-10-28", "journal": {"pages": "553-558", "name": "Seventh
        IEEE International Conference on Data Mining (ICDM 2007)"}, "authors": [{"authorId":
        "145258658", "name": "D. M. Kaplan"}, {"authorId": "1796335", "name": "D.
        Blei"}]}, {"paperId": "a91760aca33559a6c7703c0fccf3289e1c4dd729", "externalIds":
        {"MAG": "2094951132", "ACL": "D07-1109", "DBLP": "conf/emnlp/Boyd-GraberBZ07",
        "CorpusId": 216637309}, "corpusId": 216637309, "publicationVenue": {"id":
        "41bf9ed3-85b3-4c90-b015-150e31690253", "name": "Conference on Empirical Methods
        in Natural Language Processing", "type": "conference", "alternate_names":
        ["Empir Method Nat Lang Process", "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process", "EMNLP"], "url": "https://www.aclweb.org/portal/emnlp"},
        "url": "https://www.semanticscholar.org/paper/a91760aca33559a6c7703c0fccf3289e1c4dd729",
        "title": "A Topic Model for Word Sense Disambiguation", "abstract": "We develop
        latent Dirichlet allocation with WORDNET (LDAWN), an unsupervised probabilistic
        topic model that includes word sense as a hidden variable. We develop a probabilistic
        posterior inference algorithm for simultaneously disambiguating a corpus and
        learning the domains in which to consider each word. Using the WORDNET hierarchy,
        we embed the construction of Abney and Light (1999) in the topic model and
        show that automatically learned domains improve WSD accuracy compared to alternative
        contexts.", "venue": "Conference on Empirical Methods in Natural Language
        Processing", "year": 2007, "referenceCount": 20, "citationCount": 270, "influentialCitationCount":
        12, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
        "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2007-06-01",
        "journal": {"pages": "1024-1033"}, "authors": [{"authorId": "1389036863",
        "name": "Jordan L. Boyd-Graber"}, {"authorId": "1796335", "name": "D. Blei"},
        {"authorId": "1832364", "name": "Xiaojin Zhu"}]}, {"paperId": "ad3ca6979f82d7926dfcae949b099b2aa36f3df1",
        "externalIds": {"ACL": "W07-2060", "DBLP": "conf/semeval/Boyd-GraberB07",
        "MAG": "2025441317", "DOI": "10.3115/1621474.1621534", "CorpusId": 215747638},
        "corpusId": 215747638, "publicationVenue": {"id": "70713d09-6e4b-4554-9d3f-94d08aba320c",
        "name": "International Workshop on Semantic Evaluation", "type": "conference",
        "alternate_names": ["SemEval ", "Int Workshop Semantic Evaluation"]}, "url":
        "https://www.semanticscholar.org/paper/ad3ca6979f82d7926dfcae949b099b2aa36f3df1",
        "title": "PUTOP: Turning Predominant Senses into a Topic Model for Word Sense
        Disambiguation", "abstract": "We extend on McCarthy et al.''s predominant
        sense method to create an unsupervised method of word sense disambiguation
        that uses automatically derived topics using Latent Dirichlet allocation.
        Using topic-specific synset similarity measures, we create predictions for
        each word in each document using only word frequency information. It is hoped
        that this procedure can improve upon the method for larger numbers of topics
        by providing more relevant training corpora for the individual topics. This
        method is evaluated on SemEval-2007 Task 1 and Task 17.", "venue": "International
        Workshop on Semantic Evaluation", "year": 2007, "referenceCount": 10, "citationCount":
        27, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf":
        {"url": "https://dl.acm.org/doi/pdf/10.5555/1621474.1621534", "status": null},
        "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer
        Science", "source": "external"}, {"category": "Computer Science", "source":
        "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
        "2007-06-23", "journal": {"pages": "277-281"}, "authors": [{"authorId": "1389036863",
        "name": "Jordan L. Boyd-Graber"}, {"authorId": "1796335", "name": "D. Blei"}]},
        {"paperId": "c13aa63ccd5cf972a0a8c6b236c1dfad95b19b4e", "externalIds": {"MAG":
        "2950477597", "DBLP": "conf/nips/BleiM07", "ArXiv": "1003.0783", "CorpusId":
        7375081}, "corpusId": 7375081, "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
        "name": "Neural Information Processing Systems", "type": "conference", "alternate_names":
        ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
        "url": "https://www.semanticscholar.org/paper/c13aa63ccd5cf972a0a8c6b236c1dfad95b19b4e",
        "title": "Supervised Topic Models", "abstract": "We introduce supervised latent
        Dirichlet allocation (sLDA), a statistical model of labelled documents. The
        model accommodates a variety of response types. We derive a maximum-likelihood
        procedure for parameter estimation, which relies on variational approximations
        to handle intractable posterior expectations. Prediction problems motivate
        this research: we use the fitted model to predict response values for new
        documents. We test sLDA on two real-world problems: movie ratings predicted
        from reviews, and web page popularity predicted from text descriptions. We
        illustrate the benefits of sLDA versus modern regularized regression, as well
        as versus an unsupervised LDA analysis followed by a separate regression.",
        "venue": "Neural Information Processing Systems", "year": 2007, "referenceCount":
        38, "citationCount": 1719, "influentialCitationCount": 204, "isOpenAccess":
        false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
        "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
        {"category": "Mathematics", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
        "Conference", "Review"], "publicationDate": "2007-12-03", "journal": {"pages":
        "121-128"}, "authors": [{"authorId": "1796335", "name": "D. Blei"}, {"authorId":
        "40411909", "name": "Jon D. McAuliffe"}]}, {"paperId": "c5bd589b5ab7287151c21e482597a19a58ffb35b",
        "externalIds": {"CorpusId": 15049574}, "corpusId": 15049574, "publicationVenue":
        null, "url": "https://www.semanticscholar.org/paper/c5bd589b5ab7287151c21e482597a19a58ffb35b",
        "title": "A dynamic theory of social failure in isolated communities", "abstract":
        "We introduce a statistical model for dynamic network analysis and we apply
        it to Sampson\u2019s longitudinal collection of the sociometric relation trust,
        among 18 novices in a monastery (Sampson, 1968). A preliminary analysis offers
        a suggestive characterization of the social dynamics that led to the collapse
        of the congregation\u2014purportedly over religious differences. Such a characterization
        provides the elements for a theory of failure in isolated communities that
        is rooted in latent aspects of the dynamics of social interaction. Namely,
        the existence of tight informal groups as a precondition, the progressive
        polarization of such groups as a signal of increasing differences, and the
        emergence of an interstitial group of individuals over time as a signal of
        imminent conflict.", "venue": "", "year": 2007, "referenceCount": 7, "citationCount":
        0, "influentialCitationCount": 0, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category": "Economics",
        "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": null,
        "journal": null, "authors": [{"authorId": "9642064", "name": "E. Airoldi"},
        {"authorId": "1796335", "name": "D. Blei"}, {"authorId": "143977260", "name":
        "E. Xing"}, {"authorId": "1684961", "name": "S. Fienberg"}]}, {"paperId":
        "cf69a1ef575077115e3542bebf975666f7884845", "externalIds": {"CorpusId": 18153304},
        "corpusId": 18153304, "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/cf69a1ef575077115e3542bebf975666f7884845",
        "title": "Statistical discovery of signaling pathways from an ensemble of
        weakly informative data sources", "abstract": "Brief introduction. Signaling
        pathways are complex biological mechanisms. For instance, consider the Mitogen-activated
        protein (MAP) kinase pathway, available on Kyoto Encyclopedia of Genes and
        Genomes (KEGG) (Kanehisa et al., 2006). It involves 55 genes/proteins, it
        sub-divides into four interconnected modules (pheromone response, high osmolarity
        glycerol response, hypotonic shock response, and starvation response), and
        their circuitry is currently instantiated by 104 directed interactions with
        various meanings (e.g. activation, phosphorylation, binding, and inhibition)",
        "venue": "", "year": 2007, "referenceCount": 9, "citationCount": 1, "influentialCitationCount":
        0, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy":
        [{"category": "Biology", "source": "s2-fos-model"}], "publicationTypes": null,
        "publicationDate": null, "journal": null, "authors": [{"authorId": "2247552",
        "name": "E. Airoldi"}, {"authorId": "1796516", "name": "F. Markowetz"}, {"authorId":
        "1796335", "name": "D. Blei"}, {"authorId": "1754521", "name": "O. Troyanskaya"}]},
        {"paperId": "d9b9fb207013bf8afb064f23f3dffc7edd005f73", "externalIds": {"MAG":
        "2111565546", "ArXiv": "0705.4485", "DBLP": "conf/nips/AiroldiBFX08", "DOI":
        "10.5555/1390681.1442798", "CorpusId": 12105527, "PubMed": "21701698"}, "corpusId":
        12105527, "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
        "name": "Neural Information Processing Systems", "type": "conference", "alternate_names":
        ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
        "url": "https://www.semanticscholar.org/paper/d9b9fb207013bf8afb064f23f3dffc7edd005f73",
        "title": "Mixed Membership Stochastic Blockmodels", "abstract": "Observations
        consisting of measurements on relationships for pairs of objects arise in
        many settings, such as protein interaction and gene regulatory networks, collections
        of author-recipient email, and social networks. Analyzing such data with probabilisic
        models can be delicate because the simple exchangeability assumptions underlying
        many boilerplate models no longer hold. In this paper, we describe a latent
        variable model of such data called the mixed membership stochastic blockmodel.
        This model extends blockmodels for relational data to ones which capture mixed
        membership latent relational structure, thus providing an object-specific
        low-dimensional representation. We develop a general variational inference
        algorithm for fast approximate posterior inference. We explore applications
        to social and protein interaction networks.", "venue": "Neural Information
        Processing Systems", "year": 2007, "referenceCount": 65, "citationCount":
        2045, "influentialCitationCount": 334, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": ["Computer Science", "Mathematics", "Physics", "Medicine"],
        "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
        {"category": "Mathematics", "source": "external"}, {"category": "Physics",
        "source": "external"}, {"category": "Medicine", "source": "external"}, {"category":
        "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
        "Conference"], "publicationDate": "2007-05-30", "journal": {"volume": "9",
        "pages": "\n          1981-2014\n        ", "name": "Journal of machine learning
        research : JMLR"}, "authors": [{"authorId": "2247552", "name": "E. Airoldi"},
        {"authorId": "1796335", "name": "D. Blei"}, {"authorId": "1684961", "name":
        "S. Fienberg"}, {"authorId": "143977260", "name": "E. Xing"}]}, {"paperId":
        "dc6107c5507d252e10a28ae68405c7f64bf8c089", "externalIds": {"ACL": "S07-1059",
        "CorpusId": 219307111}, "corpusId": 219307111, "publicationVenue": {"id":
        "70713d09-6e4b-4554-9d3f-94d08aba320c", "name": "International Workshop on
        Semantic Evaluation", "type": "conference", "alternate_names": ["SemEval ",
        "Int Workshop Semantic Evaluation"]}, "url": "https://www.semanticscholar.org/paper/dc6107c5507d252e10a28ae68405c7f64bf8c089",
        "title": "PU-BCD: Exponential Family Models for the Coarse- and Fine-Grained
        All-Words Tasks", "abstract": null, "venue": "International Workshop on Semantic
        Evaluation", "year": 2007, "referenceCount": 0, "citationCount": 0, "influentialCitationCount":
        0, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy":
        [], "publicationTypes": null, "publicationDate": null, "journal": null, "authors":
        [{"authorId": "80936017", "name": "Jonathan D. Chang"}, {"authorId": "144652072",
        "name": "Miroslav Dud\u00edk"}, {"authorId": "1796335", "name": "D. Blei"}]},
        {"paperId": "e18a0e2f03807c998c92a3b4c68bf031bbb4cec7", "externalIds": {"ArXiv":
        "0712.1486", "MAG": "3104103042", "DOI": "10.1214/07-AOAS136", "CorpusId":
        88512254}, "corpusId": 88512254, "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/e18a0e2f03807c998c92a3b4c68bf031bbb4cec7",
        "title": "Correction: A correlated topic model of Science", "abstract": "Correction
        to Annals of Applied Statistics 1 (2007) 17--35 [doi:10.1214/07-AOAS114]",
        "venue": "", "year": 2007, "referenceCount": 0, "citationCount": 29, "influentialCitationCount":
        4, "isOpenAccess": true, "openAccessPdf": {"url": "https://projecteuclid.org/journals/annals-of-applied-statistics/volume-1/issue-2/Correction-A-correlated-topic-model-of-Science/10.1214/07-AOAS136.pdf",
        "status": null}, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics",
        "source": "external"}, {"category": "Psychology", "source": "s2-fos-model"}],
        "publicationTypes": null, "publicationDate": "2007-12-01", "journal": {"volume":
        "1", "pages": "634-634", "name": "The Annals of Applied Statistics"}, "authors":
        [{"authorId": "1796335", "name": "D. Blei"}, {"authorId": "1739581", "name":
        "J. Lafferty"}]}, {"paperId": "e981f16fde9185373634b53d94baa1f9185ff890",
        "externalIds": {"MAG": "3099640513", "ArXiv": "0708.3601", "DOI": "10.1214/07-AOAS114",
        "CorpusId": 8872108}, "corpusId": 8872108, "publicationVenue": null, "url":
        "https://www.semanticscholar.org/paper/e981f16fde9185373634b53d94baa1f9185ff890",
        "title": "A correlated topic model of Science", "abstract": "Topic models,
        such as latent Dirichlet allocation (LDA), can be useful tools for the statistical
        analysis of document collections and other discrete data. The LDA model assumes
        that the words of each document arise from a mixture of topics, each of which
        is a distribution over the vocabulary. A limitation of LDA is the inability
        to model topic correlation even though, for example, a document about genetics
        is more likely to also be about disease than X-ray astronomy. This limitation
        stems from the use of the Dirichlet distribution to model the variability
        among the topic proportions. In this paper we develop the correlated topic
        model (CTM), where the topic proportions exhibit correlation via the logistic
        normal distribution [J. Roy. Statist. Soc. Ser. B 44 (1982) 139--177]. We
        derive a fast variational inference algorithm for approximate posterior inference
        in this model, which is complicated by the fact that the logistic normal is
        not conjugate to the multinomial. We apply the CTM to the articles from Science
        published from 1990--1999, a data set that comprises 57M words. The CTM gives
        a better fit of the data than LDA, and we demonstrate its use as an exploratory
        tool of large document collections.", "venue": "", "year": 2007, "referenceCount":
        36, "citationCount": 1364, "influentialCitationCount": 120, "isOpenAccess":
        true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
        "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
        {"category": "Mathematics", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate":
        "2007-06-01", "journal": {"volume": "1", "pages": "17-35", "name": "The Annals
        of Applied Statistics"}, "authors": [{"authorId": "1796335", "name": "D. Blei"},
        {"authorId": "1739581", "name": "J. Lafferty"}]}, {"paperId": "f6990f3cfe5828cbcd6c431b24f921e20f83a57c",
        "externalIds": {"DOI": "10.1111/j.1467-985X.2007.00507.x", "CorpusId": 247698846},
        "corpusId": 247698846, "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/f6990f3cfe5828cbcd6c431b24f921e20f83a57c",
        "title": "Index of authors, volume 170, 2007", "abstract": null, "venue":
        "", "year": 2007, "referenceCount": 43, "citationCount": 0, "influentialCitationCount":
        0, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy":
        [{"category": "Medicine", "source": "s2-fos-model"}], "publicationTypes":
        null, "publicationDate": "2007-10-01", "journal": {"volume": "170", "name":
        "Journal of the Royal Statistical Society: Series A (Statistics in Society)"},
        "authors": [{"authorId": "1753785", "name": "V. Batagelj"}, {"authorId": "3072213",
        "name": "J. Besag"}, {"authorId": "1796335", "name": "D. Blei"}, {"authorId":
        "2667570", "name": "R. Breiger"}, {"authorId": "3014984", "name": "C. Butts"},
        {"authorId": "3162415", "name": "P. Doreian"}, {"authorId": "2071165664",
        "name": "D. Draper"}, {"authorId": "103809199", "name": "M. Duijn"}, {"authorId":
        "103610827", "name": "K. Faust"}, {"authorId": "108177516", "name": "A. Ferligoj"},
        {"authorId": "1684961", "name": "S. Fienberg"}, {"authorId": "50276397", "name":
        "J. Forster"}, {"authorId": "1483996241", "name": "B. Francis"}, {"authorId":
        "2063960672", "name": "A. Gelman"}, {"authorId": "5640063", "name": "P. Ghazal"},
        {"authorId": "3151920", "name": "C. Glasbey"}, {"authorId": "2180408", "name":
        "S. Goodreau"}, {"authorId": "2487859", "name": "I. C. Gormley"}, {"authorId":
        "38695283", "name": "P. Greenwood"}, {"authorId": "36713816", "name": "K.
        Gruenberg"}, {"authorId": "1778954", "name": "M. Handcock"}, {"authorId":
        "2058538931", "name": "C. Hennig"}, {"authorId": "1518373938", "name": "P.
        Hoff"}, {"authorId": "49406642", "name": "D. Hunter"}, {"authorId": "1702356",
        "name": "D. Husmeier"}, {"authorId": "2587774", "name": "J. Kent"}, {"authorId":
        "2344553", "name": "D. Krackhardt"}, {"authorId": "2120105", "name": "J. Kuha"},
        {"authorId": "119090697", "name": "T. Lawrance"}, {"authorId": "91947430",
        "name": "A. Lawson"}, {"authorId": "69843121", "name": "D. S. Leslie"}, {"authorId":
        "2068338909", "name": "A. Lewin"}, {"authorId": "77046151", "name": "T. Liao"},
        {"authorId": "66571981", "name": "N. Longford"}, {"authorId": "2059839583",
        "name": "B. Mendes"}, {"authorId": "2061522135", "name": "T. B. Murphy"},
        {"authorId": "1399007149", "name": "M. Petrescu-Prahova"}, {"authorId": "1804386",
        "name": "A. Raftery"}, {"authorId": "112938000", "name": "G. Reinert"}, {"authorId":
        "2092206002", "name": "M. Riani"}, {"authorId": "121997977", "name": "S. Richardson"},
        {"authorId": "1400024425", "name": "T. Robinson"}, {"authorId": "6226523",
        "name": "A. Skrondal"}, {"authorId": "48244312", "name": "T. Snijders"}, {"authorId":
        "118982400", "name": "T. Sweeting"}, {"authorId": "2961007", "name": "J. Tantrum"},
        {"authorId": "2076987752", "name": "D. M. Titterington"}, {"authorId": "48523918",
        "name": "S. Wasserman"}, {"authorId": "2125863375", "name": "V. A."}, {"authorId":
        "2160291477", "name": "Werhli"}]}, {"paperId": "ff81ed2ee2d5a67a95d371240f4ba8d01c0ff6da",
        "externalIds": {"MAG": "1747752640", "CorpusId": 17399660}, "corpusId": 17399660,
        "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/ff81ed2ee2d5a67a95d371240f4ba8d01c0ff6da",
        "title": "The nested Chinese restaurant process and hierarchical topic models",
        "abstract": "We present the nested Chinese restaurant process (nCRP), a stochastic
        process which assigns probability distributions to infinitely-deep, infinitely-branching
        trees. We show how this stochastic process can be used as a prior distribution
        in a Bayesian nonparametric model of document collections. Specifically, we
        present an application to information retrieval in which documents are modeled
        as paths down a random tree, and the preferential attachment dynamics of the
        nCRP leads to clustering of documents according to sharing of topics at multiple
        levels of abstraction. Given a corpus of documents, a posterior inference
        algorithm finds an approximation to a posterior distribution over trees, topics
        and allocations of words to levels of the tree. We demonstrate this algorithm
        on collections of scientific abstracts from several journals. This model exemplifies
        a recent trend in statistical machine learning--the use of Bayesian nonparametric
        methods to infer distributions on flexible data structures.", "venue": "",
        "year": 2007, "referenceCount": 47, "citationCount": 17, "influentialCitationCount":
        0, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
        "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": null, "publicationDate": "2007-10-03", "journal": {"volume":
        "", "name": ""}, "authors": [{"authorId": "1796335", "name": "D. Blei"}, {"authorId":
        "1799860", "name": "T. Griffiths"}, {"authorId": "1694621", "name": "Michael
        I. Jordan"}]}, {"paperId": "0c5eb26a1cf9e6ca8a7580a8a4bfbd743cbb7ed6", "externalIds":
        {"MAG": "2911563498", "CorpusId": 57181140}, "corpusId": 57181140, "publicationVenue":
        null, "url": "https://www.semanticscholar.org/paper/0c5eb26a1cf9e6ca8a7580a8a4bfbd743cbb7ed6",
        "title": "Proceedings of the 2006 conference on Statistical network analysis",
        "abstract": null, "venue": "", "year": 2006, "referenceCount": 0, "citationCount":
        0, "influentialCitationCount": 0, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
        "Computer Science", "source": "external"}, {"category": "Computer Science",
        "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": "2006-06-29",
        "journal": {"volume": "", "name": ""}, "authors": [{"authorId": "2247552",
        "name": "E. Airoldi"}, {"authorId": "1796335", "name": "D. Blei"}, {"authorId":
        "1684961", "name": "S. Fienberg"}, {"authorId": "49800482", "name": "A. Goldenberg"},
        {"authorId": "143977260", "name": "E. Xing"}]}, {"paperId": "2205bc21bf3fb324bb96fad5c32c3e2cfd7304ee",
        "externalIds": {"DOI": "10.1186/1471-2105-7-250", "CorpusId": 54566755, "PubMed":
        "16681860"}, "corpusId": 54566755, "publicationVenue": {"id": "be3f884c-b44a-496a-a593-1cad3f89d254",
        "name": "BMC Bioinformatics", "type": "journal", "alternate_names": ["BMC
        Bioinform"], "issn": "1471-2105", "url": "http://www.biomedcentral.com/bmcbioinformatics",
        "alternate_urls": ["http://www.pubmedcentral.nih.gov/tocrender.fcgi?journal=13",
        "http://www.biomedcentral.com/bmcbioinformatics/"]}, "url": "https://www.semanticscholar.org/paper/2205bc21bf3fb324bb96fad5c32c3e2cfd7304ee",
        "title": "Statistical modeling of biomedical corpora: mining the Caenorhabditis
        Genetic Center Bibliography for genes related to life span", "abstract": null,
        "venue": "BMC Bioinformatics", "year": 2006, "referenceCount": 0, "citationCount":
        9, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url":
        "https://bmcbioinformatics.biomedcentral.com/counter/pdf/10.1186/1471-2105-7-250",
        "status": null}, "fieldsOfStudy": ["Medicine"], "s2FieldsOfStudy": [{"category":
        "Medicine", "source": "external"}, {"category": "Biology", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle", "Study"], "publicationDate": null,
        "journal": {"volume": "7", "pages": "1-19", "name": "BMC Bioinformatics"},
        "authors": [{"authorId": "1796335", "name": "D. Blei"}, {"authorId": "2072748134",
        "name": "K. Franks"}, {"authorId": "1694621", "name": "Michael I. Jordan"},
        {"authorId": "1863326", "name": "I. Mian"}]}, {"paperId": "2dd53efa74850c6b60bc7bb0dcfca049a4a71474",
        "externalIds": {"DBLP": "conf/icml/BleiL06", "MAG": "2072644219", "DOI": "10.1145/1143844.1143859",
        "CorpusId": 263020656}, "corpusId": 263020656, "publicationVenue": {"id":
        "fc0a208c-acb7-47dc-a0d4-af8190e21d29", "name": "International Conference
        on Machine Learning", "type": "conference", "alternate_names": ["ICML", "Int
        Conf Mach Learn"], "url": "https://icml.cc/"}, "url": "https://www.semanticscholar.org/paper/2dd53efa74850c6b60bc7bb0dcfca049a4a71474",
        "title": "Dynamic topic models", "abstract": "A family of probabilistic time
        series models is developed to analyze the time evolution of topics in large
        document collections. The approach is to use state space models on the natural
        parameters of the multinomial distributions that represent the topics. Variational
        approximations based on Kalman filters and nonparametric wavelet regression
        are developed to carry out approximate posterior inference over the latent
        topics. In addition to giving quantitative, predictive models of a sequential
        corpus, dynamic topic models provide a qualitative window into the contents
        of a large document collection. The models are demonstrated by analyzing the
        OCR''ed archives of the journal Science from 1880 through 2000.", "venue":
        "International Conference on Machine Learning", "year": 2006, "referenceCount":
        48, "citationCount": 410, "influentialCitationCount": 10, "isOpenAccess":
        false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["Book", "JournalArticle",
        "Conference"], "publicationDate": "2006-06-25", "journal": {"name": "Proceedings
        of the 23rd international conference on Machine learning"}, "authors": [{"authorId":
        "1796335", "name": "D. Blei"}, {"authorId": "2247573710", "name": "John D.
        Lafferty"}]}, {"paperId": "31cbc51a4a441d2ca6668e0e0614f5abfcb88194", "externalIds":
        {"DBLP": "journals/sac/McAuliffeBJ06", "MAG": "2020213096", "DOI": "10.1007/s11222-006-5196-2",
        "CorpusId": 7998884}, "corpusId": 7998884, "publicationVenue": {"id": "dd522c0a-e715-4d95-9ed7-e60346254ddf",
        "name": "Statistics and computing", "type": "journal", "alternate_names":
        ["Stat comput", "Stat Comput", "Statistics and Computing"], "issn": "0960-3174",
        "alternate_issns": ["1431-8784"], "url": "https://link.springer.com/journal/11222",
        "alternate_urls": ["http://www.springerlink.com/content/100219/"]}, "url":
        "https://www.semanticscholar.org/paper/31cbc51a4a441d2ca6668e0e0614f5abfcb88194",
        "title": "Nonparametric empirical Bayes for the Dirichlet process mixture
        model", "abstract": null, "venue": "Statistics and computing", "year": 2006,
        "referenceCount": 39, "citationCount": 137, "influentialCitationCount": 6,
        "isOpenAccess": true, "openAccessPdf": {"url": "http://www.cs.berkeley.edu/~jordan/papers/675.pdf",
        "status": null}, "fieldsOfStudy": ["Mathematics", "Computer Science"], "s2FieldsOfStudy":
        [{"category": "Mathematics", "source": "external"}, {"category": "Computer
        Science", "source": "external"}, {"category": "Mathematics", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle"], "publicationDate": "2006-03-01", "journal":
        {"volume": "16", "pages": "5-14", "name": "Statistics and Computing"}, "authors":
        [{"authorId": "40411909", "name": "Jon D. McAuliffe"}, {"authorId": "1796335",
        "name": "D. Blei"}, {"authorId": "1694621", "name": "Michael I. Jordan"}]},
        {"paperId": "4d63618acc0bc6ecb1b3e88d5050b1cef06c3bed", "externalIds": {"MAG":
        "327576307", "CorpusId": 2706207}, "corpusId": 2706207, "publicationVenue":
        null, "url": "https://www.semanticscholar.org/paper/4d63618acc0bc6ecb1b3e88d5050b1cef06c3bed",
        "title": "Mixed Membership Stochastic Block Models for Relational Data with
        Application to Protein-Protein Interactions", "abstract": "Modeling relational
        data is an important problem for modern data analysis and machine learning.
        In this paper we propose a Bayesian model that uses a hierarchy of probabilistic
        assumptions about the way objects interact with one another in order to learn
        latent groups, their typical interaction patterns, and the degree of membership
        of objects to groups. Our model explains the data using a small set of parameters
        that can be reliably estimated with an efficient inference algorithm. In our
        approach, the set of probabilistic assumptions may be tailored to a specific
        application domain in order to incorporate intuitions and/or semantics of
        interest. We demonstrate our methods on simulated data, where they outperform
        spectral clustering techniques, and we apply our model to a data set of protein-to-protein
        interactions, to reveal proteins\u2019 diverse functional roles.", "venue":
        "", "year": 2006, "referenceCount": 26, "citationCount": 136, "influentialCitationCount":
        12, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Mathematics"],
        "s2FieldsOfStudy": [{"category": "Mathematics", "source": "external"}, {"category":
        "Computer Science", "source": "s2-fos-model"}], "publicationTypes": null,
        "publicationDate": null, "journal": {"volume": "", "name": ""}, "authors":
        [{"authorId": "2247552", "name": "E. Airoldi"}, {"authorId": "1796335", "name":
        "D. Blei"}, {"authorId": "1684961", "name": "S. Fienberg"}, {"authorId": "143977260",
        "name": "E. Xing"}]}, {"paperId": "78de77e1332ff56275f85f4c0de239c55269f93f",
        "externalIds": {"MAG": "587082925", "CorpusId": 1980950}, "corpusId": 1980950,
        "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/78de77e1332ff56275f85f4c0de239c55269f93f",
        "title": "Variational inference and learning for a unified model of syntax,
        semantics and morphology", "abstract": "There have been recent a t t empts
        to produce trainable (unsupervised) models of human-language syntax and semantics,
        as well as morphology. To our knowledge, there has not been an a t t empt
        to produce a generative model tha t encorporates semantic, syntactic, and
        morphological elements. Some immediate applications of this tool axe stemming,
        word clustering by root, and disambiguation (at the syntactic, semantic, and
        morphological levels). In this work, we propose a hierarchical topics-syntax-morphology
        model. We provide the variational inference and upda te rules for this model
        (exact inference is intractable) . We show some preliminary results on segmentation
        tasks. * Current address: Computer Science Depar tment , Pr inceton University,
        Princeton, NJ 08540 Research supported in part by NSF grants IIS-0312814 and
        IIS0427206 and by the DARPA CALO project. University Libraries Carnegie Mellon
        University Pittsburgh, PA 152 i 3-3890 1. b a t t e r a. [ b a t t e r ] \u2014
        liquidy dough b . [ b a t ] [ e r ] \u2014 one who bats (in baseball) 2. r
        u n g a. [ rung] \u2014 a horizontal bar in a ladder b . [ r i n g ] [PP]
        \u2014 past participle of '' r ing'' 3. d i c e a. [ d i c e ] [PRES] to chop
        finely b . [ d i e ] [PLU] the plural form of ''die '' 4. r e a r r a n g
        e a. [ r e ] [ a r r a n g e ] [PRES] \u2014 to arrange anew b . [ r e a r
        ] [ r a n g e ] the range of the rear 5. r e s e n t a. [ r e s e n t ] [PRES]
        to dislike b . [ r e ] [ send] [PAST] \u2014 past tense of ''resend'' to send
        again Figure 1: Morphological ambiguity: analysis", "venue": "", "year": 2006,
        "referenceCount": 18, "citationCount": 2, "influentialCitationCount": 0, "isOpenAccess":
        false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate":
        null, "journal": {"volume": "", "name": ""}, "authors": [{"authorId": "1791882",
        "name": "L. Kontorovich"}, {"authorId": "1739581", "name": "J. Lafferty"},
        {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId": "793a76d1025eed16abf92a22300ec6964227aa9a",
        "externalIds": {"DBLP": "conf/icml/Blei06", "DOI": "10.1007/978-3-540-73133-7_17",
        "CorpusId": 1189328}, "corpusId": 1189328, "publicationVenue": null, "url":
        "https://www.semanticscholar.org/paper/793a76d1025eed16abf92a22300ec6964227aa9a",
        "title": "Panel Discussion", "abstract": null, "venue": "SNA@ICML", "year":
        2006, "referenceCount": 0, "citationCount": 151, "influentialCitationCount":
        10, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
        "external"}], "publicationTypes": ["JournalArticle"], "publicationDate": null,
        "journal": {"pages": "186-194"}, "authors": [{"authorId": "1796335", "name":
        "D. Blei"}]}, {"paperId": "92da26eff6632a343ddd613a4c34ec96c5bc40ee", "externalIds":
        {"MAG": "2898986748", "CorpusId": 9773544}, "corpusId": 9773544, "publicationVenue":
        null, "url": "https://www.semanticscholar.org/paper/92da26eff6632a343ddd613a4c34ec96c5bc40ee",
        "title": "TagLDA: Bringing a document structure knowledge into topic models",
        "abstract": "Latent Dirichlet Allocation models a document by a mixture of
        topics, where each topic itself is typically modeled by a unigram word distribution.
        Documents however often have known structures, and the same topic can exhibit
        dif ferent word distributions under different parts of the structure. We extend
        laten t Dirichlet allocation model by replacing the unigram word distributions
        with a factored r epresentation conditioned on both the topic and the structure.
        In the resultant m odel each topic is equivalent to a set of unigrams, reflecting
        the structure a wo rd is in. The proposed model is more flexible in modeling
        the corpus. The factore d representation prevents combinatorial explosion
        and leads to efficient parame terization. We derive the variational optimization
        algorithm for the new model. The mode l shows improved perplexity on text
        and image data, but not significant ac cur y improvement when used for classification.",
        "venue": "", "year": 2006, "referenceCount": 4, "citationCount": 25, "influentialCitationCount":
        1, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
        "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": null, "publicationDate": null, "journal": {"volume": "",
        "name": ""}, "authors": [{"authorId": "1832364", "name": "Xiaojin Zhu"}, {"authorId":
        "1796335", "name": "D. Blei"}, {"authorId": "1739581", "name": "J. Lafferty"}]},
        {"paperId": "9b4a7e2d19c531f72bf7d40b4cfa2dbc37496cea", "externalIds": {"MAG":
        "120654303", "CorpusId": 12618083}, "corpusId": 12618083, "publicationVenue":
        null, "url": "https://www.semanticscholar.org/paper/9b4a7e2d19c531f72bf7d40b4cfa2dbc37496cea",
        "title": "Stochastic Block Models of Mixed Membership", "abstract": "We consider
        the statistical analysis of a collection of unipartite graphs, i.e., multiple
        matrices of relations among objects of a single type. Such data arise, for
        example, in biological settings, collections of author-recipient email, and
        social networks. In many applications, clustering the objects of study or
        situating them in a low dimensional space (e.g., a simplex) is only one of
        the goals of the analysis. Begin able to estimate relational structures among
        the clusters themselves is often times as important. For example, in biological
        applications we are interested in estimating how stable protein complexes
        (i.e., clusters of proteins) interact. To support such integrated data analyses,
        we develop the family of \u201cstochastic block models of mixed membership\u201d.
        Our models combine features of mixed-membership models (Erosheva & Fienberg,
        2005) and block models for relational data (Holland et al., 1983) in a hierarchical
        Bayesian framework. We develop a novel \u201cnested\u201d variational inference
        scheme, which is necessary to successfully perform fast approximate posterior
        inference in our models of relational data. We present evidence to support
        our claims, using both synthetic data and biological case study.", "venue":
        "", "year": 2006, "referenceCount": 36, "citationCount": 56, "influentialCitationCount":
        1, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
        "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": null, "publicationDate": null, "journal": {"volume": "",
        "name": ""}, "authors": [{"authorId": "2247552", "name": "E. Airoldi"}, {"authorId":
        "1796335", "name": "D. Blei"}, {"authorId": "1684961", "name": "S. Fienberg"},
        {"authorId": "143977260", "name": "E. Xing"}]}, {"paperId": "a465c9b65f3f99ec6ee9cb1bea0da9b5c46bd94a",
        "externalIds": {"MAG": "2288793237", "CorpusId": 59487590}, "corpusId": 59487590,
        "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/a465c9b65f3f99ec6ee9cb1bea0da9b5c46bd94a",
        "title": "Computational Analysis and Visualized Comparison of Style in American
        Poetry", "abstract": "This thesis describes the creation of software to quantitatively
        compute the style of American poems and accordingly visualize a collection
        of poems in relation to one another. While certain components have been and
        are being researched, a comparable comprehensive system had not previously
        been attempted. Such a system has potential applications in academic research
        of texts and of the intuitive personal response to poetry, in making recommendations
        to readers based on their favorite poems, and more. Sample applications were
        developed and run to better assess the accuracy and usefulness of the software.",
        "venue": "", "year": 2006, "referenceCount": 17, "citationCount": 6, "influentialCitationCount":
        0, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Art"],
        "s2FieldsOfStudy": [{"category": "Art", "source": "external"}, {"category":
        "Education", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate":
        null, "journal": {"volume": "", "name": ""}, "authors": [{"authorId": "145258658",
        "name": "D. M. Kaplan"}, {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId":
        "afd354506626d4b9ee30de86239a82d586f5800c", "externalIds": {"DBLP": "journals/bmcbi/BleiFJM06",
        "MAG": "2163679724", "PubMedCentral": "1533868", "DOI": "10.1186/1471-2105-7-250",
        "CorpusId": 1946548, "PubMed": "3663481"}, "corpusId": 1946548, "publicationVenue":
        {"id": "be3f884c-b44a-496a-a593-1cad3f89d254", "name": "BMC Bioinformatics",
        "type": "journal", "alternate_names": ["BMC Bioinform"], "issn": "1471-2105",
        "url": "http://www.biomedcentral.com/bmcbioinformatics", "alternate_urls":
        ["http://www.pubmedcentral.nih.gov/tocrender.fcgi?journal=13", "http://www.biomedcentral.com/bmcbioinformatics/"]},
        "url": "https://www.semanticscholar.org/paper/afd354506626d4b9ee30de86239a82d586f5800c",
        "title": "Statistical modeling of biomedical corpora: mining the Caenorhabditis
        Genetic Center Bibliography for genes related to life span", "abstract": null,
        "venue": "BMC Bioinformatics", "year": 2006, "referenceCount": 59, "citationCount":
        32, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf":
        {"url": "https://bmcbioinformatics.biomedcentral.com/counter/pdf/10.1186/1471-2105-7-250",
        "status": null}, "fieldsOfStudy": ["Computer Science", "Medicine"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Medicine",
        "source": "external"}, {"category": "Biology", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle"], "publicationDate": "2006-05-08", "journal":
        {"volume": "7", "pages": "250 - 250", "name": "BMC Bioinformatics"}, "authors":
        [{"authorId": "1796335", "name": "D. Blei"}, {"authorId": "2072748134", "name":
        "K. Franks"}, {"authorId": "49071005", "name": "Michael I. Jordan"}, {"authorId":
        "1863326", "name": "I. Mian"}]}, {"paperId": "b27f9ee1d24407a88681e1346b6a9075d43e05a6",
        "externalIds": {"CorpusId": 2715844}, "corpusId": 2715844, "publicationVenue":
        null, "url": "https://www.semanticscholar.org/paper/b27f9ee1d24407a88681e1346b6a9075d43e05a6",
        "title": "Generative Models for Decoding Real-Valued Natural Experience in
        FMRI", "abstract": "Functional Magnetic Resonance Imaging (FMRI) provides
        an unprecedented window into the complex functioning of the human brain, typically
        detailing the activity of thousands of voxels for hundreds of time points.
        The interpretation of FMRI is complicated, however, because of the unknown
        connection between the hemodynamic response and neural activity, and the unknown
        spatiotemporal characteristics of the cognitive patterns themselves.", "venue":
        "", "year": 2006, "referenceCount": 3, "citationCount": 0, "influentialCitationCount":
        0, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy":
        [{"category": "Biology", "source": "s2-fos-model"}, {"category": "Psychology",
        "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": null,
        "journal": null, "authors": [{"authorId": "3096034", "name": "G. Stephens"},
        {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId": "b82c2c26c44358e140b6e2eb5537563939cd3ab9",
        "externalIds": {"MAG": "1542746141", "DBLP": "conf/icml/AiroldiBFX06", "DOI":
        "10.1007/978-3-540-73133-7_5", "CorpusId": 14972968}, "corpusId": 14972968,
        "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/b82c2c26c44358e140b6e2eb5537563939cd3ab9",
        "title": "Combining Stochastic Block Models and Mixed Membership for Statistical
        Network Analysis", "abstract": null, "venue": "SNA@ICML", "year": 2006, "referenceCount":
        39, "citationCount": 25, "influentialCitationCount": 1, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Mathematics", "Computer Science"],
        "s2FieldsOfStudy": [{"category": "Mathematics", "source": "external"}, {"category":
        "Computer Science", "source": "external"}, {"category": "Computer Science",
        "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
        "2006-06-29", "journal": {"pages": "57-74"}, "authors": [{"authorId": "2247552",
        "name": "E. Airoldi"}, {"authorId": "1796335", "name": "D. Blei"}, {"authorId":
        "1684961", "name": "S. Fienberg"}, {"authorId": "143977260", "name": "E. Xing"}]},
        {"paperId": "b90d922ff07d0eb8d77b8687aba7f55bd3926436", "externalIds": {"MAG":
        "2158266063", "DOI": "10.1198/016214506000000302", "CorpusId": 7934949}, "corpusId":
        7934949, "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/b90d922ff07d0eb8d77b8687aba7f55bd3926436",
        "title": "Hierarchical Dirichlet Processes", "abstract": "We consider problems
        involving groups of data where each observation within a group is a draw from
        a mixture model and where it is desirable to share mixture components between
        groups. We assume that the number of mixture components is unknown a priori
        and is to be inferred from the data. In this setting it is natural to consider
        sets of Dirichlet processes, one for each group, where the well-known clustering
        property of the Dirichlet process provides a nonparametric prior for the number
        of mixture components within each group. Given our desire to tie the mixture
        models in the various groups, we consider a hierarchical model, specifically
        one in which the base measure for the child Dirichlet processes is itself
        distributed according to a Dirichlet process. Such a base measure being discrete,
        the child Dirichlet processes necessarily share atoms. Thus, as desired, the
        mixture models in the different groups necessarily share mixture components.
        We discuss representations of hierarchical Dirichlet processes in terms of
        a stick-breaking process, and a generalization of the Chinese restaurant process
        that we refer to as the \u201cChinese restaurant franchise.\u201d We present
        Markov chain Monte Carlo algorithms for posterior inference in hierarchical
        Dirichlet process mixtures and describe applications to problems in information
        retrieval and text modeling.", "venue": "", "year": 2006, "referenceCount":
        63, "citationCount": 3759, "influentialCitationCount": 694, "isOpenAccess":
        true, "openAccessPdf": {"url": "http://www.gatsby.ucl.ac.uk/~ywteh/research/npbayes/jasa2006-print.pdf",
        "status": null}, "fieldsOfStudy": ["Mathematics"], "s2FieldsOfStudy": [{"category":
        "Mathematics", "source": "external"}, {"category": "Mathematics", "source":
        "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": null, "publicationDate": "2006-12-01", "journal": {"volume":
        "101", "pages": "1566 - 1581", "name": "Journal of the American Statistical
        Association"}, "authors": [{"authorId": "1725303", "name": "Y. Teh"}, {"authorId":
        "1694621", "name": "Michael I. Jordan"}, {"authorId": "1773821", "name": "Matthew
        J. Beal"}, {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId": "48291923daa6a5d3174c3150444d2996141f78b2",
        "externalIds": {"MAG": "2049701036", "DBLP": "conf/kdd/AiroldiBXF05", "DOI":
        "10.1145/1134271.1134283", "CorpusId": 15635285}, "corpusId": 15635285, "publicationVenue":
        null, "url": "https://www.semanticscholar.org/paper/48291923daa6a5d3174c3150444d2996141f78b2",
        "title": "A latent mixed membership model for relational data", "abstract":
        "Modeling relational data is an important problem for modern data analysis
        and machine learning. In this paper we propose a Bayesian model that uses
        a hierarchy of probabilistic assumptions about the way objects interact with
        one another in order to learn latent groups, their typical interaction patterns,
        and the degree of membership of objects to groups. Our model explains the
        data using a small set of parameters that can be reliably estimated with an
        efficient inference algorithm. In our approach, the set of probabilistic assumptions
        may be tailored to a specific application domain in order to incorporate intuitions
        and/or semantics of interest. We demonstrate our methods on simulated data
        and we successfully apply our model to a data set of protein-to-protein interactions.",
        "venue": "LinkKDD ''05", "year": 2005, "referenceCount": 24, "citationCount":
        58, "influentialCitationCount": 5, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
        "Computer Science", "source": "external"}, {"category": "Computer Science",
        "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
        "2005-08-21", "journal": {"pages": "82-89"}, "authors": [{"authorId": "2247552",
        "name": "E. Airoldi"}, {"authorId": "1796335", "name": "D. Blei"}, {"authorId":
        "143977260", "name": "E. Xing"}, {"authorId": "1684961", "name": "S. Fienberg"}]},
        {"paperId": "e49da956b23ed295541c80939d4a1261d0a1022f", "externalIds": {"DBLP":
        "conf/nips/BleiL05", "MAG": "2112050062", "CorpusId": 263132326}, "corpusId":
        263132326, "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
        "name": "Neural Information Processing Systems", "type": "conference", "alternate_names":
        ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
        "url": "https://www.semanticscholar.org/paper/e49da956b23ed295541c80939d4a1261d0a1022f",
        "title": "Correlated Topic Models", "abstract": "Topic models, such as latent
        Dirichlet allocation (LDA), can be useful tools for the statistical analysis
        of document collections and other discrete data. The LDA model assumes that
        the words of each document arise from a mixture of topics, each of which is
        a distribution over the vocabulary. A limitation of LDA is the inability to
        model topic correlation even though, for example, a document about genetics
        is more likely to also be about disease than x-ray astronomy. This limitation
        stems from the use of the Dirichlet distribution to model the variability
        among the topic proportions. In this paper we develop the correlated topic
        model (CTM), where the topic proportions exhibit correlation via the logistic
        normal distribution [1]. We derive a mean-field variational inference algorithm
        for approximate posterior inference in this model, which is complicated by
        the fact that the logistic normal is not conjugate to the multinomial. The
        CTM gives a better fit than LDA on a collection of OCRed articles from the
        journal Science. Furthermore, the CTM provides a natural way of visualizing
        and exploring this and other unstructured data sets.", "venue": "Neural Information
        Processing Systems", "year": 2005, "referenceCount": 14, "citationCount":
        617, "influentialCitationCount": 27, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": ["Mathematics", "Computer Science"], "s2FieldsOfStudy":
        [{"category": "Mathematics", "source": "external"}, {"category": "Computer
        Science", "source": "external"}, {"category": "Computer Science", "source":
        "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate":
        "2005-12-05", "journal": {"pages": "147-154"}, "authors": [{"authorId": "1796335",
        "name": "D. Blei"}, {"authorId": "2247573710", "name": "John D. Lafferty"}]},
        {"paperId": "0ecc5ffeae38689dd2fe6ed4c32a6745744d7641", "externalIds": {"MAG":
        "2112971401", "DBLP": "conf/nips/GriffithsSBT04", "CorpusId": 11408454}, "corpusId":
        11408454, "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
        "name": "Neural Information Processing Systems", "type": "conference", "alternate_names":
        ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
        "url": "https://www.semanticscholar.org/paper/0ecc5ffeae38689dd2fe6ed4c32a6745744d7641",
        "title": "Integrating Topics and Syntax", "abstract": "Statistical approaches
        to language learning typically focus on either short-range syntactic dependencies
        or long-range semantic dependencies between words. We present a generative
        model that uses both kinds of dependencies, and can be used to simultaneously
        find syntactic classes and semantic topics despite having no representation
        of syntax or semantics beyond statistical dependency. This model is competitive
        on tasks like part-of-speech tagging and document classification with models
        that exclusively use short- and long-range dependencies respectively.", "venue":
        "Neural Information Processing Systems", "year": 2004, "referenceCount": 11,
        "citationCount": 618, "influentialCitationCount": 37, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
        "Conference"], "publicationDate": "2004-12-01", "journal": {"pages": "537-544"},
        "authors": [{"authorId": "1799860", "name": "T. Griffiths"}, {"authorId":
        "1804885", "name": "M. Steyvers"}, {"authorId": "1796335", "name": "D. Blei"},
        {"authorId": "1763295", "name": "J. Tenenbaum"}]}, {"paperId": "42fddb742959e80cefe3932475a3c2cad97cba8d",
        "externalIds": {"MAG": "2109176692", "DBLP": "conf/icml/BleiJ04", "DOI": "10.1145/1015330.1015439",
        "CorpusId": 15569336}, "corpusId": 15569336, "publicationVenue": {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
        "name": "International Conference on Machine Learning", "type": "conference",
        "alternate_names": ["ICML", "Int Conf Mach Learn"], "url": "https://icml.cc/"},
        "url": "https://www.semanticscholar.org/paper/42fddb742959e80cefe3932475a3c2cad97cba8d",
        "title": "Variational methods for the Dirichlet process", "abstract": "Variational
        inference methods, including mean field methods and loopy belief propagation,
        have been widely used for approximate probabilistic inference in graphical
        models. While often less accurate than MCMC, variational methods provide a
        fast deterministic approximation to marginal and conditional probabilities.
        Such approximations can be particularly useful in high dimensional problems
        where sampling methods are too slow to be effective. A limitation of current
        methods, however, is that they are restricted to parametric probabilistic
        models. MCMC does not have such a limitation; indeed, MCMC samplers have been
        developed for the Dirichlet process (DP), a nonparametric distribution on
        distributions (Ferguson, 1973) that is the cornerstone of Bayesian nonparametric
        statistics (Escobar & West, 1995; Neal, 2000). In this paper, we develop a
        mean-field variational approach to approximate inference for the Dirichlet
        process, where the approximate posterior is based on the truncated stick-breaking
        construction (Ishwaran & James, 2001). We compare our approach to DP samplers
        for Gaussian DP mixture models.", "venue": "International Conference on Machine
        Learning", "year": 2004, "referenceCount": 17, "citationCount": 184, "influentialCitationCount":
        12, "isOpenAccess": true, "openAccessPdf": {"url": "http://www.ee.duke.edu/~lcarin/emag/seminar_presentations/blei04variational.pdf",
        "status": null}, "fieldsOfStudy": ["Mathematics", "Computer Science"], "s2FieldsOfStudy":
        [{"category": "Mathematics", "source": "external"}, {"category": "Computer
        Science", "source": "external"}, {"category": "Computer Science", "source":
        "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Book", "Conference"],
        "publicationDate": "2004-07-04", "journal": {"name": "Proceedings of the twenty-first
        international conference on Machine learning"}, "authors": [{"authorId": "1796335",
        "name": "D. Blei"}, {"authorId": "1694621", "name": "Michael I. Jordan"}]},
        {"paperId": "adcec120a442af0d58f7a1d2dc175b58ff5a32e2", "externalIds": {"MAG":
        "2100163972", "DBLP": "conf/nips/TehJBB04", "CorpusId": 13156740}, "corpusId":
        13156740, "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
        "name": "Neural Information Processing Systems", "type": "conference", "alternate_names":
        ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
        "url": "https://www.semanticscholar.org/paper/adcec120a442af0d58f7a1d2dc175b58ff5a32e2",
        "title": "Sharing Clusters among Related Groups: Hierarchical Dirichlet Processes",
        "abstract": "We propose the hierarchical Dirichlet process (HDP), a nonparametric
        Bayesian model for clustering problems involving multiple groups of data.
        Each group of data is modeled with a mixture, with the number of components
        being open-ended and inferred automatically by the model. Further, components
        can be shared across groups, allowing dependencies across groups to be modeled
        effectively as well as conferring generalization to new groups. Such grouped
        clustering problems occur often in practice, e.g. in the problem of topic
        discovery in document corpora. We report experimental results on three text
        corpora showing the effective and superior performance of the HDP over previous
        models.", "venue": "Neural Information Processing Systems", "year": 2004,
        "referenceCount": 13, "citationCount": 506, "influentialCitationCount": 67,
        "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Mathematics",
        "Computer Science"], "s2FieldsOfStudy": [{"category": "Mathematics", "source":
        "external"}, {"category": "Computer Science", "source": "external"}, {"category":
        "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
        "Conference"], "publicationDate": "2004-12-01", "journal": {"pages": "1385-1392"},
        "authors": [{"authorId": "1725303", "name": "Y. Teh"}, {"authorId": "1694621",
        "name": "Michael I. Jordan"}, {"authorId": "1773821", "name": "Matthew J.
        Beal"}, {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId": "28e245ce0e06f398dd26a9d6ab6bb04ef4c016e6",
        "externalIds": {"MAG": "2132827946", "DBLP": "conf/nips/BleiGJT03", "CorpusId":
        1269561}, "corpusId": 1269561, "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
        "name": "Neural Information Processing Systems", "type": "conference", "alternate_names":
        ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
        "url": "https://www.semanticscholar.org/paper/28e245ce0e06f398dd26a9d6ab6bb04ef4c016e6",
        "title": "Hierarchical Topic Models and the Nested Chinese Restaurant Process",
        "abstract": "We address the problem of learning topic hierarchies from data.
        The model selection problem in this domain is daunting\u2014which of the large
        collection of possible trees to use? We take a Bayesian approach, generating
        an appropriate prior via a distribution on partitions that we refer to as
        the nested Chinese restaurant process. This nonparametric prior allows arbitrarily
        large branching factors and readily accommodates growing data collections.
        We build a hierarchical topic model by combining this prior with a likelihood
        that is based on a hierarchical variant of latent Dirichlet allocation. We
        illustrate our approach on simulated data and with an application to the modeling
        of NIPS abstracts.", "venue": "Neural Information Processing Systems", "year":
        2003, "referenceCount": 13, "citationCount": 1128, "influentialCitationCount":
        107, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
        "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2003-12-09",
        "journal": {"pages": "17-24"}, "authors": [{"authorId": "1796335", "name":
        "D. Blei"}, {"authorId": "1799860", "name": "T. Griffiths"}, {"authorId":
        "1694621", "name": "Michael I. Jordan"}, {"authorId": "1763295", "name": "J.
        Tenenbaum"}]}, {"paperId": "401680497e8144dfeef032f5a3393e869180c7b7", "externalIds":
        {"MAG": "2158150544", "CorpusId": 7115335}, "corpusId": 7115335, "publicationVenue":
        null, "url": "https://www.semanticscholar.org/paper/401680497e8144dfeef032f5a3393e869180c7b7",
        "title": "Hierarchical Bayesian Models for Applications in Information Retrieval",
        "abstract": "SUMMARY We present a simple hierarchical Bayesian approach to
        the modeling collections of texts and other large-scale data collections.
        For text collections, we posit that a document is generated by choosing a
        random set of multinomial probabilities for a set of possible \u201ctopics,\u201d
        and then repeatedly generating words by sampling from the topic mixture. This
        model is intractable for exact probabilistic inference, but approximate posterior
        probabilities and marginal likelihoods can be obtained via fast variational
        methods. We also present extensions to coupled models for joint text/image
        data and multiresolution models for topic hierarchies.", "venue": "", "year":
        2003, "referenceCount": 25, "citationCount": 51, "influentialCitationCount":
        1, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Mathematics"],
        "s2FieldsOfStudy": [{"category": "Mathematics", "source": "external"}, {"category":
        "Computer Science", "source": "s2-fos-model"}], "publicationTypes": null,
        "publicationDate": null, "journal": {"volume": "", "name": ""}, "authors":
        [{"authorId": "1796335", "name": "D. Blei"}, {"authorId": "1694621", "name":
        "Michael I. Jordan"}, {"authorId": "34699434", "name": "A. Ng"}]}, {"paperId":
        "473f4b7f8ae2b03dda2593f54b316ff7d55db26b", "externalIds": {"DBLP": "conf/sigir/BleiJ03",
        "MAG": "2020842694", "DOI": "10.1145/860435.860460", "CorpusId": 207561477},
        "corpusId": 207561477, "publicationVenue": {"id": "8dce23a9-44e0-4381-a39e-2acc1edff700",
        "name": "Annual International ACM SIGIR Conference on Research and Development
        in Information Retrieval", "type": "conference", "alternate_names": ["International
        ACM SIGIR Conference on Research and Development in Information Retrieval",
        "Int ACM SIGIR Conf Res Dev Inf Retr", "SIGIR", "Annu Int ACM SIGIR Conf Res
        Dev Inf Retr"], "url": "http://www.acm.org/sigir/"}, "url": "https://www.semanticscholar.org/paper/473f4b7f8ae2b03dda2593f54b316ff7d55db26b",
        "title": "Modeling annotated data", "abstract": "We consider the problem of
        modeling annotated data---data with multiple types where the instance of one
        type (such as a caption) serves as a description of the other type (such as
        an image). We describe three hierarchical probabilistic mixture models which
        aim to describe such data, culminating in correspondence latent Dirichlet
        allocation, a latent variable model that is effective at modeling the joint
        distribution of both types and the conditional distribution of the annotation
        given the primary type. We conduct experiments on the Corel database of images
        and captions, assessing performance in terms of held-out likelihood, automatic
        annotation, and text-based image retrieval.", "venue": "Annual International
        ACM SIGIR Conference on Research and Development in Information Retrieval",
        "year": 2003, "referenceCount": 17, "citationCount": 1246, "influentialCitationCount":
        160, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
        "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle", "Book", "Conference"], "publicationDate":
        "2003-07-28", "journal": {"name": "Proceedings of the 26th annual international
        ACM SIGIR conference on Research and development in informaion retrieval"},
        "authors": [{"authorId": "1796335", "name": "D. Blei"}, {"authorId": "1694621",
        "name": "Michael I. Jordan"}]}, {"paperId": "68338de07d3a097920ac74118c00539b9c411a69",
        "externalIds": {"CorpusId": 15720307}, "corpusId": 15720307, "publicationVenue":
        null, "url": "https://www.semanticscholar.org/paper/68338de07d3a097920ac74118c00539b9c411a69",
        "title": "1 Matching Words and Pictures", "abstract": "We present a new and
        very rich approach for modeling multi-modal data sets, focusing on the specific
        case of segmented images with associated text. Learning the joint distribution
        of image regions and words has many applications. We consider in detail predicting
        words associated with whole images (auto-annotation) and corresponding to
        particular image regions (region naming). Auto-annotation might help organize
        and access large collections of images. Region naming is a model of object
        recognition as a process of translating image regions to words, much as one
        might translate from one language to another. Learning the relationships between
        image regions and semantic correlates (words) is an interesting example of
        multi-modal data mining, particularly because it is typically hard to apply
        data mining techniques to collections of images. We develop a number of models
        for the joint distribution of image regions and words, including several which
        explicitly learn the correspondence between regions and words. We study multi-modal
        and correspondence extensions to Hofmann\u2019s hierarchical clustering/aspect
        model, a translation model adapted from statistical machine translation (Brown
        et al.), and a multi-modal extension to latent dirichlet allocation (LDA).
        All models are assessed using a large collection of annotated images of real
        scenes. We study in depth the difficult problem of measuring performance.
        For the annotation task, we look at prediction performance on held out data.
        We present three alternative measures, oriented toward different types of
        task. Measuring the performance of correspondence methods is harder, because
        one must determine whether a word has been placed on the right region of an
        image. We can use annotation performance as a proxy measure, but accurate
        measurement requires hand labeled data, and thus must occur on a smaller scale.
        We show results using both an annotation proxy, and manually labeled data.",
        "venue": "", "year": 2003, "referenceCount": 40, "citationCount": 12, "influentialCitationCount":
        2, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
        null, "publicationDate": null, "journal": null, "authors": [{"authorId": "145602732",
        "name": "Kobus Barnard"}, {"authorId": "46995592", "name": "Pinar Duygulu"},
        {"authorId": "144016256", "name": "D. Forsyth"}, {"authorId": "1737568", "name":
        "Nando de Freitas"}, {"authorId": "1796335", "name": "D. Blei"}, {"authorId":
        "1694621", "name": "Michael I. Jordan"}]}, {"paperId": "6a26268d2ba9d34e5b59ae6e5c11a83cdca1a85e",
        "externalIds": {"DBLP": "journals/jmlr/BarnardDFFBJ03", "MAG": "2137471889",
        "DOI": "10.1162/153244303322533214", "CorpusId": 868535}, "corpusId": 868535,
        "publicationVenue": {"id": "c22e7c36-3bfa-43e1-bb7b-edccdea2a780", "name":
        "Journal of machine learning research", "type": "journal", "alternate_names":
        ["Journal of Machine Learning Research", "J mach learn res", "J Mach Learn
        Res"], "issn": "1532-4435", "alternate_issns": ["1533-7928"], "url": "http://www.ai.mit.edu/projects/jmlr/",
        "alternate_urls": ["http://jmlr.csail.mit.edu/", "http://www.jmlr.org/", "http://portal.acm.org/affiliated/jmlr"]},
        "url": "https://www.semanticscholar.org/paper/6a26268d2ba9d34e5b59ae6e5c11a83cdca1a85e",
        "title": "Matching Words and Pictures", "abstract": "We present a new approach
        for modeling multi-modal data sets, focusing on the specific case of segmented
        images with associated text. Learning the joint distribution of image regions
        and words has many applications. We consider in detail predicting words associated
        with whole images (auto-annotation) and corresponding to particular image
        regions (region naming). Auto-annotation might help organize and access large
        collections of images. Region naming is a model of object recognition as a
        process of translating image regions to words, much as one might translate
        from one language to another. Learning the relationships between image regions
        and semantic correlates (words) is an interesting example of multi-modal data
        mining, particularly because it is typically hard to apply data mining techniques
        to collections of images. We develop a number of models for the joint distribution
        of image regions and words, including several which explicitly learn the correspondence
        between regions and words. We study multi-modal and correspondence extensions
        to Hofmann''s hierarchical clustering/aspect model, a translation model adapted
        from statistical machine translation (Brown et al.), and a multi-modal extension
        to mixture of latent Dirichlet allocation (MoM-LDA). All models are assessed
        using a large collection of annotated images of real scenes. We study in depth
        the difficult problem of measuring performance. For the annotation task, we
        look at prediction performance on held out data. We present three alternative
        measures, oriented toward different types of task. Measuring the performance
        of correspondence methods is harder, because one must determine whether a
        word has been placed on the right region of an image. We can use annotation
        performance as a proxy measure, but accurate measurement requires hand labeled
        data, and thus must occur on a smaller scale. We show results using both an
        annotation proxy, and manually labeled data.", "venue": "Journal of machine
        learning research", "year": 2003, "referenceCount": 44, "citationCount": 1791,
        "influentialCitationCount": 98, "isOpenAccess": false, "openAccessPdf": null,
        "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer
        Science", "source": "external"}, {"category": "Computer Science", "source":
        "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
        "2003-03-01", "journal": {"volume": "3", "pages": "1107-1135", "name": "J.
        Mach. Learn. Res."}, "authors": [{"authorId": "145602732", "name": "Kobus
        Barnard"}, {"authorId": "2446509", "name": "P. D. Sahin"}, {"authorId": "144016256",
        "name": "D. Forsyth"}, {"authorId": "1737568", "name": "Nando de Freitas"},
        {"authorId": "1796335", "name": "D. Blei"}, {"authorId": "1694621", "name":
        "Michael I. Jordan"}]}, {"paperId": "dbcb94bc47d0570cb00fc9d48b948377f588d647",
        "externalIds": {"MAG": "197558907", "CorpusId": 118853416}, "corpusId": 118853416,
        "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/dbcb94bc47d0570cb00fc9d48b948377f588d647",
        "title": "Variational inference in a truncated Dirichlet process", "abstract":
        "The N -component truncated Dirichlet process (DPN ) is defined in Ishwaran
        and James [2001] and converges almost surely to a true Dirichlet process (DP\u221e).
        Like a full Dirichlet process, this distribution can be used as a nonparametric
        Bayesian prior in a mixture model. Ishwaran and James show that this approximation
        allows a blocking strategy in the corresponding Gibbs sampler which can be
        faster than the classical Gibbs samplers developed for the DP\u221e prior
        [Escobar and West, 1995]. In this paper, we develop a variational inference
        algorithm to approximate the posterior in a Bayesian mixture model with a
        DPN prior. An exponential family mixture model with DPN prior on the natural
        parameter of the mixture component is illustrated in Figure 2. The random
        variables are distributed as follows:", "venue": "", "year": 2003, "referenceCount":
        5, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": ["Mathematics"], "s2FieldsOfStudy":
        [{"category": "Mathematics", "source": "external"}, {"category": "Mathematics",
        "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": null,
        "journal": {"volume": "", "name": ""}, "authors": [{"authorId": "1796335",
        "name": "D. Blei"}, {"authorId": "1694621", "name": "Michael I. Jordan"}]},
        {"paperId": "41eba1efcd55beb1b70db260dc6f070f7317b71b", "externalIds": {"MAG":
        "2953377002", "ArXiv": "1301.0556", "DBLP": "journals/corr/abs-1301-0556",
        "CorpusId": 8030563}, "corpusId": 8030563, "publicationVenue": {"id": "f9af8000-42f8-410d-a622-e8811e41660a",
        "name": "Conference on Uncertainty in Artificial Intelligence", "type": "conference",
        "alternate_names": ["Uncertainty in Artificial Intelligence", "UAI", "Conf
        Uncertain Artif Intell", "Uncertain Artif Intell"], "url": "http://www.auai.org/"},
        "url": "https://www.semanticscholar.org/paper/41eba1efcd55beb1b70db260dc6f070f7317b71b",
        "title": "Learning with Scope, with Application to Information Extraction
        and Classification", "abstract": "In probabilistic approaches to classification
        and information extraction, one typically builds a statistical model of words
        under the assumption that future data will exhibit the same regularities as
        the training data. In many data sets, however, there are scope-limited features
        whose predictive power is only applicable to a certain subset of the data.
        For example, in information extraction from web pages, word formatting may
        be indicative of extraction category in different ways on different web pages.
        The difficulty with using such features is capturing and exploiting the new
        regularities encountered in previously unseen data. In this paper, we propose
        a hierarchical probabilistic model that uses both local/scope-limited features,
        such as word formatting, and global features, such as word content. The local
        regularities are modeled as an unobserved random parameter which is drawn
        once for each local data set. This random parameter is estimated during the
        inference process and then used to perform classification with both the local
        and global features--- a procedure which is akin to automatically retuning
        the classifier to the local regularities on each newly encountered web page.
        Exact inference is intractable and we present approximations via point estimates
        and variational methods. Empirical results on large collections of web data
        demonstrate that this method significantly improves performance from traditional
        models of global features alone.", "venue": "Conference on Uncertainty in
        Artificial Intelligence", "year": 2002, "referenceCount": 8, "citationCount":
        37, "influentialCitationCount": 1, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics",
        "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2002-08-01",
        "journal": {"pages": "53-60"}, "authors": [{"authorId": "1796335", "name":
        "D. Blei"}, {"authorId": "1756566", "name": "J. Bagnell"}, {"authorId": "143753639",
        "name": "A. McCallum"}]}, {"paperId": "845c20b482765922c67d3ee0ae650c069d7013b0",
        "externalIds": {"MAG": "2053569739", "DBLP": "conf/sigir/BleiM01", "DOI":
        "10.1145/383952.384021", "CorpusId": 5058852}, "corpusId": 5058852, "publicationVenue":
        {"id": "8dce23a9-44e0-4381-a39e-2acc1edff700", "name": "Annual International
        ACM SIGIR Conference on Research and Development in Information Retrieval",
        "type": "conference", "alternate_names": ["International ACM SIGIR Conference
        on Research and Development in Information Retrieval", "Int ACM SIGIR Conf
        Res Dev Inf Retr", "SIGIR", "Annu Int ACM SIGIR Conf Res Dev Inf Retr"], "url":
        "http://www.acm.org/sigir/"}, "url": "https://www.semanticscholar.org/paper/845c20b482765922c67d3ee0ae650c069d7013b0",
        "title": "Topic segmentation with an aspect hidden Markov model", "abstract":
        "We present a novel probabilistic method for topic segmentation on unstructured
        text. One previous approach to this problem utilizes the hidden Markov model
        (HMM) method for probabilistically modeling sequence data [7]. The HMM treats
        a document as mutually independent sets of words generated by a latent topic
        variable in a time series. We extend this idea by embedding Hofmann''s aspect
        model for text [5] into the segmenting HMM to form an aspect HMM (AHMM). In
        doing so, we provide an intuitive topical dependency between words and a cohesive
        segmentation model. We apply this method to segment unbroken streams of New
        York Times articles as well as noisy transcripts of radio programs on SpeechBot,
        an online audio archive indexed by an automatic speech recognition engine.
        We provide experimental comparisons which show that the AHMM outperforms the
        HMM for this task.", "venue": "Annual International ACM SIGIR Conference on
        Research and Development in Information Retrieval", "year": 2001, "referenceCount":
        18, "citationCount": 194, "influentialCitationCount": 14, "isOpenAccess":
        false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "external"}, {"category": "Computer
        Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
        "publicationDate": "2001-09-01", "journal": {"pages": "343-348"}, "authors":
        [{"authorId": "1796335", "name": "D. Blei"}, {"authorId": "47690405", "name":
        "P. Moreno"}]}, {"paperId": "f198043a866e9187925a8d8db9a55e3bfdd47f2c", "externalIds":
        {"DBLP": "conf/nips/BleiNJ01", "MAG": "1880262756", "DOI": "10.1016/B978-0-12-411519-4.00006-9",
        "CorpusId": 3177797}, "corpusId": 3177797, "publicationVenue": {"id": "c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
        "name": "Journal of machine learning research", "type": "journal", "alternate_names":
        ["Journal of Machine Learning Research", "J mach learn res", "J Mach Learn
        Res"], "issn": "1532-4435", "alternate_issns": ["1533-7928"], "url": "http://www.ai.mit.edu/projects/jmlr/",
        "alternate_urls": ["http://jmlr.csail.mit.edu/", "http://www.jmlr.org/", "http://portal.acm.org/affiliated/jmlr"]},
        "url": "https://www.semanticscholar.org/paper/f198043a866e9187925a8d8db9a55e3bfdd47f2c",
        "title": "Latent Dirichlet Allocation", "abstract": null, "venue": "Journal
        of machine learning research", "year": 2001, "referenceCount": 57, "citationCount":
        34870, "influentialCitationCount": 6777, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
        "Computer Science", "source": "external"}, {"category": "Computer Science",
        "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
        "2001-01-03", "journal": {"volume": "3", "pages": "993-1022", "name": "J.
        Mach. Learn. Res."}, "authors": [{"authorId": "1796335", "name": "D. Blei"},
        {"authorId": "34699434", "name": "A. Ng"}, {"authorId": "1694621", "name":
        "Michael I. Jordan"}]}, {"paperId": "1ababac0c6c3f3858c2d34e3ad486c9a4f388a8d",
        "externalIds": {"MAG": "2355171993", "DOI": "10.1136/bmj.319.7209.0a", "CorpusId":
        63192514}, "corpusId": 63192514, "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/1ababac0c6c3f3858c2d34e3ad486c9a4f388a8d",
        "title": "Deep learning?", "abstract": "Unsupervised multilayered (\"deep\")
        models are considered for imagery. The model is represented using a hierarchical
        convolutional factor-analysis construction, with sparse factor loadings and
        scores. The computation of layer-dependent model parameters is implemented
        within a Bayesian setting, employing a Gibbs sampler and variational Bayesian
        (VB) analysis that explicitly exploit the convolutional nature of the expansion.
        To address large-scale and streaming data, an online version of VB is also
        developed. The number of dictionary elements at each layer is inferred from
        the data, based on a beta-Bernoulli implementation of the Indian buffet process.
        Example results are presented for several image-processing applications, with
        comparisons to related models in the literature. Index Terms\u2014Bayesian,
        deep learning, convolutional, dictionary learning, factor analysis", "venue":
        "", "year": 1999, "referenceCount": 4, "citationCount": 13, "influentialCitationCount":
        2, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
        Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
        "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
        "publicationTypes": null, "publicationDate": "1999-08-28", "journal": {"volume":
        "319", "name": "BMJ"}, "authors": [{"authorId": "2152691280", "name": "Bo
        Chen"}, {"authorId": "1699339", "name": "G. Sapiro"}, {"authorId": "1796335",
        "name": "D. Blei"}, {"authorId": "35108187", "name": "D. Dunson"}, {"authorId":
        "145006560", "name": "L. Carin"}]}, {"paperId": "0b21071aeb96b73aabdf8e6b4c7d961158da4765",
        "externalIds": {"CorpusId": 16199922}, "corpusId": 16199922, "publicationVenue":
        null, "url": "https://www.semanticscholar.org/paper/0b21071aeb96b73aabdf8e6b4c7d961158da4765",
        "title": "v 3 COMS E 6998-002 : Probabilistic Modeling for Discrete Data Lecture
        3 : Word Embeddings III Instructor", "abstract": "If I missed any of your
        comments from class below or misattributed an existing comment, it was not
        intentional, and I''ll fix it. The following scribed notes are provided in
        a stream of consciousness style. If anything is missing or confusing, please
        let me know and I will update the notes. 0 Logistics \u2022 If you are using
        GitHub, please switch to BitBucket. \u2022 Please update your project log
        every time you work on your project. Be more like Tom. At the end of the previous
        class, we asked: Why do likelihood ratios give us analogies? Maybe this isn''t
        the right approach. We should start from the modeling assumption that the
        probability of a word appearing in the context of another word is a log linear
        model, as follows:", "venue": "", "year": null, "referenceCount": 8, "citationCount":
        0, "influentialCitationCount": 0, "isOpenAccess": false, "openAccessPdf":
        null, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category": "Computer Science",
        "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": null,
        "journal": null, "authors": [{"authorId": "1796335", "name": "D. Blei"}]},
        {"paperId": "387f273a54fd7b5795abd6ca6a6ec1acf8755b08", "externalIds": {"CorpusId":
        263420785}, "corpusId": 263420785, "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/387f273a54fd7b5795abd6ca6a6ec1acf8755b08",
        "title": "A Topographic Latent Source Model for Fmri Data", "abstract": "a
        r t i c l e i n f o We describe and evaluate a new statistical generative
        model of functional magnetic resonance imaging (fMRI) data. The model, topographic
        latent source analysis (TLSA), assumes that fMRI images are generated by a
        covariate-dependent superposition of latent sources. These sources are defined
        in terms of basis functions over space. The number of parameters in the model
        does not depend on the number of voxels, enabling a parsimonious description
        of activity patterns that avoids many of the pitfalls of traditional voxel-based
        approaches. We develop a multi-subject extension where latent sources at the
        subject-level are perturbations of a group-level template. We evaluate TLSA
        according to prediction, reconstruction and reproducibility. We show that
        it compares favorably to a Naive Bayes model while using fewer parameters.
        We also describe a hypothesis testing framework that can be used to identify
        significant latent sources. Introduction Most current approaches to functional
        magnetic resonance imaging (fMRI) take the basic spatial unit of analysis
        to be the voxel, and attempt to learn a set of parameters characterizing the
        voxel''s response to a set of covariates (e.g., experimental manipulations).
        Traditionally, each voxel''s response is assumed to be independent of all
        the other voxels, and is modeled as a linear function of the covariates convolved
        with a hemodynamic response function (Friston et al., 1994). Although this
        approach, which we refer to as the mass-univariate general linear model (MU-GLM),
        has been productive, it suffers from two shortcomings. First, the assumption
        that responses of voxels are independent of one another is rarely true, necessitating
        post-hoc correction procedures to account for these dependencies (Friston
        et al., 1996). Second, and more fundamentally, modeling neural responses at
        the voxel level does not enable direct inferences about what are arguably
        the variables of real interest, the responses of the underlying neuroanatomical
        regions. To sidestep this issue, regionally specific activations are typically
        extracted from the voxel-specific parameters by looking for spatially extended
        excursions from a null distribution (Worsley et al. More recently, two modeling
        trends have emerged that attempt to move beyond the mass-univariate GLM towards
        more realistic spatial assumptions. The first retains the GLM, but assumes
        that the parameters vary smoothly over voxels within a spatial neighborhood.
        The smoothness assumption is enforced in a Bayesian framework by encoding
        spatial dependencies between voxels in the prior (Woolrich et al. We refer
        to this approach as the spatially regularized GLM \u2026", "venue": "", "year":
        null, "referenceCount": 43, "citationCount": 0, "influentialCitationCount":
        0, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy":
        [{"category": "Psychology", "source": "s2-fos-model"}], "publicationTypes":
        null, "publicationDate": null, "journal": null, "authors": [{"authorId": "2239150873",
        "name": "Samuel J. Gershman"}, {"authorId": "1796335", "name": "D. Blei"},
        {"authorId": "2256338028", "name": "Francisco Pereira"}, {"authorId": "2250115198",
        "name": "K. A. Norman"}, {"authorId": "2250115198", "name": "K. A. Norman"}]},
        {"paperId": "77081383a7ef9fa3ebe47f7b13230476e9232395", "externalIds": {"CorpusId":
        597358}, "corpusId": 597358, "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/77081383a7ef9fa3ebe47f7b13230476e9232395",
        "title": "Ieee Transaction on Pattern Analysis and Machine Intelligence 1
        a Bayesian Nonparametric Approach to Image Super-resolution", "abstract":
        "\u2014Super-resolution methods form high-resolution images from low-resolution
        images. In this paper, we develop a new Bayesian nonparametric model for super-resolution.
        Our method uses a beta-Bernoulli process to learn a set of recurring visual
        patterns, called dictionary elements, from the data. Because it is nonparametric,
        the number of elements found is also determined from the data. We test the
        results on both benchmark and natural images, comparing with several other
        models from the research literature. We perform large-scale human evaluation
        experiments to assess the visual quality of the results. In a first implementation,
        we use Gibbs sampling to approximate the posterior. However, this algorithm
        is not feasible for large-scale data. To circumvent this, we then develop
        an online variational Bayes (VB) algorithm. This algorithm finds high quality
        dictionaries in a fraction of the time needed by the Gibbs sampler.", "venue":
        "", "year": null, "referenceCount": 49, "citationCount": 83, "influentialCitationCount":
        3, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
        null, "publicationDate": null, "journal": null, "authors": [{"authorId": "2767134",
        "name": "Gungor Polatkan"}, {"authorId": "38026572", "name": "Mingyuan Zhou"},
        {"authorId": "145006560", "name": "L. Carin"}, {"authorId": "1796335", "name":
        "D. Blei"}]}, {"paperId": "e12d5afc5f2b193788a865f4a9afed7b335f00ec", "externalIds":
        {"CorpusId": 260479446}, "corpusId": 260479446, "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/e12d5afc5f2b193788a865f4a9afed7b335f00ec",
        "title": "Equal Opportunity and Af\ufb01rmative Action via Counterfactual
        Predictions", "abstract": "Machine learning ( ML ) can automate decision-making
        by learning to predict decisions from historical data. However, these predictors
        may inherit discriminatory policies from past decisions and reproduce unfair
        decisions. In this paper, we propose two algorithms that adjust \ufb01tted
        ML predictors to make them fair. We focus on two legal notions of fairness:
        (a) providing equal opportunities ( EO ) to individuals regardless of sensitive
        attributes and (b) repairing historical disadvantages through af\ufb01rmative
        action ( AA ). More technically, we produce fair EO and AA predictors by positing
        a causal model and considering counterfactual decisions. We prove that the
        resulting predictors are theoretically optimal in predictive performance while
        satisfying fairness. We evaluate the algorithms, and the trade-offs between
        accuracy and fairness, on datasets about admissions, income, credit, and recidivism.",
        "venue": "", "year": null, "referenceCount": 22, "citationCount": 4, "influentialCitationCount":
        1, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy":
        [{"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
        null, "publicationDate": null, "journal": null, "authors": [{"authorId": "2108734693",
        "name": "Yixin Wang"}, {"authorId": "153485411", "name": "Dhanya Sridhar"},
        {"authorId": "1796335", "name": "D. Blei"}]}]}, {"authorId": "2250987266",
        "externalIds": {}, "url": "https://www.semanticscholar.org/author/2250987266",
        "name": "David Blei", "aliases": null, "affiliations": [], "homepage": null,
        "paperCount": 1, "citationCount": 0, "hIndex": 0, "papers": [{"paperId": "e35ec0af61305de98f47910b893fc746e0eaf6a3",
        "externalIds": {"DOI": "10.1017/cbo9780511804779.025", "CorpusId": 263454112},
        "corpusId": 263454112, "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/e35ec0af61305de98f47910b893fc746e0eaf6a3",
        "title": "Mixture Models", "abstract": "That is find the most likely distribution
        of words B l for each of the L classes and the most likely distribution \u03c0
        of documents among classes, given the N words in each of the D documents.",
        "venue": "Machine Learning Fundamentals", "year": 2021, "referenceCount":
        0, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false,
        "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category":
        "Economics", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate":
        "2021-10-31", "journal": {"name": "Machine Learning Fundamentals"}, "authors":
        [{"authorId": "2250987266", "name": "David Blei"}]}]}]}

        '
    headers:
      Access-Control-Allow-Origin:
      - '*'
      Connection:
      - keep-alive
      Content-Length:
      - '803100'
      Content-Type:
      - application/json
      Date:
      - Thu, 19 Oct 2023 20:32:01 GMT
      Via:
      - 1.1 8af02ce0419e91e83834b7deea9dd962.cloudfront.net (CloudFront)
      X-Amz-Cf-Id:
      - S3DBwHOF1vJpW8VZdlQpfHLyBHbmK7F4hX28ixNM2xcrJNvWVFL34g==
      X-Amz-Cf-Pop:
      - JFK52-P2
      X-Cache:
      - Miss from cloudfront
      x-amz-apigw-id:
      - NEPKREjhPHcF4nw=
      x-amzn-Remapped-Connection:
      - keep-alive
      x-amzn-Remapped-Content-Length:
      - '803100'
      x-amzn-Remapped-Date:
      - Thu, 19 Oct 2023 20:32:01 GMT
      x-amzn-Remapped-Server:
      - gunicorn
      x-amzn-RequestId:
      - a26b33d6-b7b5-4a62-ba7a-b44d5c897a61
    status:
      code: 200
      message: OK
version: 1
