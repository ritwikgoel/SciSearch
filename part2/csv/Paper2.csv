paper_id,title,abstract,date_published,url
2819,Galectins in regulation of apoptosis.,,2011-12-19,https://www.semanticscholar.org/paper/696005a293433b9ad15993ffe99ba0c165d53829,Advances in Experimental Medicine and Biology
3202,Global Marine Fishing across Space and Time,"Human health and livelihoods are threatened by declining marine fisheries catches, causing substantial interest in the sources and dynamics of fishing. Catch analyses in individual exclusive economic zones (EEZs) and the high seas are abundant, and research across multiple EEZs is growing. However, no previous studies have systematically compared catches, intranational versus international fish flows, and fishing nations within all of the world’s EEZs and across adjacent and distant EEZs and the high seas to inform “metacoupled” fisheries management. We use the metacoupling framework—a new approach for evaluating human–nature interactions within and across adjacent and distant systems (metacouplings)—to illustrate how fisheries catches were locally, regionally, and globally interconnected in 1950–2014, totaling 5.8 billion metric tons and increasing by 298% (tonnage) and 431% (monetary value) over this time period. Catches by nations in their own EEZs (largest in Peru) and adjacent EEZs (largest in Indonesia) constituted 86% of worldwide catches, growing in 1950–1996 but declining in 1997–2014. In contrast, catches in distant EEZs and the high seas—largest in Morocco, Mauritania, and Canada—peaked in 1973 and have since represented 9–21% of annual catches. Our 65-year, local–regional–global analysis illustrates how metacoupled fisheries governance—holistic management of multiscalar catches, flows, and tradeoffs within and among fisheries—can improve food and nutrition security, livelihood resilience, and biodiversity conservation across the world.",2020-06-01,https://www.semanticscholar.org/paper/07c4bf054e249e7f4a7d4caca62db7d82f7d0423,Sustainability
2289,Neutrophil infiltration into human gliomas,,1999-10-01,https://www.semanticscholar.org/paper/0cfb22ee9011ba4de118748fe03c18b095a384f5,Acta Neuropathologica
420,Planar map graphs,"We introduce and study a modi ed notion of planarity, in which two regions of a map are considered adjacent when they share any point of their boundaries (not an edge, as standard planarity requires). We seek to characterize the abstract graphs realized by such map adjacencies. We prove some preliminary properties of such graphs, and give a polynomial time algorithm for the following restricted problem: given an abstract graph, decide whether it is realized by a map in which at most four regions meet at any point. The general recognition problem remains open.",1998-05-23,https://www.semanticscholar.org/paper/53ba3ac19073a87241506b2202f98ad439a33c84,Symposium on the Theory of Computing
3174,Divergent water requirements partition exposure risk to parasites in wild equids,"Abstract For grazing herbivores, dung density in feeding areas is an important determinant of exposure risk to fecal‐orally transmitted parasites. When host species share the same parasite species, a nonrandom distribution of their cumulative dung density and/or nonrandom ranging and feeding behavior may skew exposure risk and the relative selection pressure parasites impose on each host. The arid‐adapted Grevy's zebra (Equus grevyi) can range more widely than the water‐dependent plains zebra (Equus quagga), with which it shares the same species of gastrointestinal nematodes. We studied how the spatial distribution of zebra dung relates to ranging and feeding behavior to assess parasite exposure risk in Grevy's and plains zebras at a site inhabited by both zebra species. We found that zebra dung density declined with distance from water, Grevy's zebra home ranges (excluding those of territorial males) were farther from water than those of plains zebras, and plains zebra grazing areas had higher dung density than random points while Grevy's zebra grazing areas did not, suggesting a greater exposure risk in plains zebras associated with their water dependence. Fecal egg counts increased with home range proximity to water for both species, but the response was stronger in plains zebras, indicating that this host species may be particularly vulnerable to the elevated exposure risk close to water. We further ran experiments on microclimatic effects on dung infectivity and showed that fewer nematode eggs embryonated in dung in the sun than in the shade. However, only 5% of the zebra dung on the landscape was in shade, indicating that the microclimatic effects of shade on the density of infective larvae is not a major influence on exposure risk dynamics. Ranging constraints based on water requirements appear to be key mediators of nematode parasite exposure in free‐ranging equids.",2022-02-01,https://www.semanticscholar.org/paper/3de4e95e82efd967b6bb87bd6656ae609ebdcb19,Ecology and Evolution
1900,Hybrid Particle Swarm Optimization Combined With Genetic Operators for Flexible Job-Shop Scheduling Under Uncertain Processing Time for Semiconductor Manufacturing,"Semiconductor manufacturing is a complicated flexible job-shop scheduling problem (FJSP) of combinatorial complexity. Because of the adoption of advanced process control and advanced equipment control, the processing time in advanced wafer fabs become uncertain. Existing approaches considering constant processing time may not be appropriate to address the present problem in a real setting. In practice, processing times can be represented as intervals with the most probable completion time somewhere near the middle of the interval. A fuzzy number that is a generalized interval can represent this processing time interval exactly and naturally. This paper developed a hybrid approach integrating a particle swarm optimization algorithm with a Cauchy distribution and genetic operators (HPSO+GA) for solving an FJSP by finding a job sequence that minimizes the makespan with uncertain processing time. In particular, the proposed hybridized HPSO+GA approach employs PSO for creating operation sequences and assigning the time and resources for each operation, and then uses genetic operators to update the particles for improving the solution. To estimate the validity of the proposed approaches, experiments were conducted to compare the proposed approach with conventional approaches. The results show the practical viability of this approach. This paper concludes with discussions of contributions and recommends directions for future research.",2018-02-01,https://www.semanticscholar.org/paper/22e3ff3ef15e892cc5edd3b493248a173fab5612,IEEE transactions on semiconductor manufacturing
50,Modeling and managing content changes in text databases,"Large amounts of (often valuable) information are stored in Web-accessible text databases. ""Metasearchers"" provide unified interfaces to query multiple such databases at once. For efficiency, metasearchers rely on succinct statistical summaries of the database contents to select the best databases for each query. So far, database selection research has largely assumed that databases are static, so the associated statistical summaries do not need to change over time. However, databases are rarely static and the statistical summaries that describe their contents need to be updated periodically to reflect content changes. In this paper, we first report the results of a study showing how the content summaries of 152 real Web databases evolved over a period of 52 weeks. Then, we show how to use ""survival analysis"" techniques in general, and Cox's proportional hazards regression in particular, to model database changes over time and predict when we should update each content summary. Finally, we exploit our change model to devise update schedules that keep the summaries up to date by contacting databases only when needed, and then we evaluate the quality of our schedules experimentally over real Web databases.",2005-04-05,https://www.semanticscholar.org/paper/09c86f8704518f3cf1d6e559128167be0cfbd6ac,IEEE International Conference on Data Engineering
2294,In vitro effects of GM-CSF on mature peripheral blood neutrophils.,"GM-CSF can play a crucial role in regulating the neutrophil-mediated inflammatory response. This growth factor is a proliferative stimulus for bone marrow neutrophil stem cell precursors and has at least 3 important roles in regulating neutrophil-mediated immunity: a) a direct effect on the proliferation and development of neutrophil progenitors; b) synergistic activity with other haemopoietic growth factors; c) stimulation of the functional activity of mature neutrophils. The production of GM-CSF may be triggered directly by exogenous factors such as antigens and endotoxins, or indirectly through the release of cytokines by a variety of cells including lymphocytes, activated macrophages and endothelial cells exposed to products of mononuclear phagocytes. Such production of GM-CSF may serve to quickly release mature neutrophils from the bone marrow in response to infections. Moreover, enhancement of the function of mature neutrophils may also augment their ability to migrate to infective sites and then phagocytose and kill pathogens. Increased expression of CD11b/CD18 may play a fundamental part in this mechanism because this receptor is essential for the adhesion of neutrophils to the endothelium. Both phagocytosis and oxidative burst activity increase as a result of the action of GM-CSF and the increased expression of complement- and Fc-receptors can augment opsono-phagocytosis. A further level of neutrophil up-regulation occurs by increasing the functional life span of neutrophils by GM-CSF. Thus, by delaying neutrophil apoptosis, GM-CSF greatly extends the time over which neutrophils may function at inflammatory sites. GM-CSF can thus exert a variety of important regulatory controls of neutrophil function during bacterial infections. Both the number and the functional status of neutrophils is highly regulated by GM-CSF. It is also possible that GM-CSF produced within localised sites of acute inflammation or infection may attract, trap and then activate neutrophils within this site.",1998-06-01,https://www.semanticscholar.org/paper/fd8abd62208bc0cdcee3ab2ec72717aad5fe36b9,International Journal of Molecular Medicine
3252,Disruption of a protective ant-plant mutualism by an invasive ant increases elephant damage to savanna trees.,"Invasive species can indirectly affect ecosystem processes via the disruption of mutualisms. The mutualism between the whistling thorn acacia (Acacia drepanolobium) and four species of symbiotic ants is an ecologically important one; ants strongly defend trees against elephants, which can otherwise have dramatic impacts on tree cover. In Laikipia, Kenya, the invasive big-headed ant (Pheidole megacephala) has established itself at numerous locations within the last 10-15 years. In invaded areas on five properties, we found that three species of symbiotic Crematogaster ants were virtually extirpated, whereas Tetraponera penzigi co-occurred with P. megacephala. T. penzigi appears to persist because of its nonaggressive behavior; in a whole-tree translocation experiment, Crematogaster defended host trees against P. megacephala, but were extirpated from trees within hours. In contrast, T. penzigi retreated into domatia and withstood invading ants for >30 days. In the field, the loss of defensive Crematogaster ants in invaded areas led to a five- to sevenfold increase in the number of trees catastrophically damaged by elephants compared to uninvaded areas. In savannas, tree cover drives many ecosystem processes and provides essential forage for many large mammal species; thus, the invasion of big-headed ants may strongly alter the dynamics and diversity of East Africa's whistling thorn savannas by disrupting this system's keystone acaciaant mutualism.",2015-03-01,https://www.semanticscholar.org/paper/162ac08f323a5af5e83975751be8caac932282d3,Ecology
3443,Speed scaling for weighted flow time,"In addition to the traditional goal of efficiently managing time and space, many computers now need to efficiently manage power usage. For example, Intel's SpeedStep and AMD's PowerNOW technologies allow the Windows XP operating system to dynamically change the speed of the processor to prolong battery life. In this setting, the operating system must not only have a job selection policy to determine which job to run, but also a speed scaling policy to determine the speed at which the job will be run. These policies must be online since the operating system does not in general have knowledge of the future. In current CMOS based processors, the speed satisfies the well known cube-root-rule, that the speed is approximately the cube root of the power [Mud01, BBS+00]. Thus, in this work, we make the standard generalization that the power is equal to speed to some power α ≥ 1, where one should think of α as being approximately 3 [YDS95, BKP04]. Energy is power integrated over time. The operating system is faced with a dual objective optimization problem as it both wants to conserve energy, and optimize some Quality of Service (QoS) measure of the resulting schedule.",2007-01-07,https://www.semanticscholar.org/paper/8a1664d7f0ec5591e7193966c1180526355bb291,ACM-SIAM Symposium on Discrete Algorithms
1050,"Constraints on low-mass, relic dark matter candidates from a surface-operated SuperCDMS single-charge sensitive detector","This article presents an analysis and the resulting limits on light dark matter inelastically scattering off of electrons, and on dark photon and axion-like particle absorption, using a second-generation SuperCDMS high-voltage eV-resolution detector. The 0.93 gram Si detector achieved a 3 eV phonon energy resolution; for a detector bias of 100 V, this corresponds to a charge resolution of 3% of a single electron-hole pair. The energy spectrum is reported from a blind analysis with 1.2 gram-days of exposure acquired in an above-ground laboratory. With charge carrier trapping and impact ionization effects incorporated into the dark matter signal models, the dark matter-electron cross section $\bar{\sigma}_{e}$ is constrained for dark matter masses from 0.5--$10^{4} $MeV$/c^{2}$; in the mass range from 1.2--50 eV$/c^{2}$ the dark photon kinetic mixing parameter $\varepsilon$ and the axioelectric coupling constant $g_{ae}$ are constrained. The minimum 90% confidence-level upper limits within the above mentioned mass ranges are $\bar{\sigma}_{e}\,=\,8.7\times10^{-34}$ cm$^{2}$, $\varepsilon\,=\,3.3\times10^{-14}$, and $g_{ae}\,=\,1.0\times10^{-9}$.",2020-05-28,https://www.semanticscholar.org/paper/43dfcef1f5c45674aeceab7f4300b05d3844e3ab,Physical Review D
919,The Clique Problem for Planar Graphs,,,https://www.semanticscholar.org/paper/1af1c38925c2df2d192203a6f19c124b0b5814b6,Information Processing Letters
1610,Dose-response modeling in high-throughput cancer drug screenings: an end-to-end approach.,"Personalized cancer treatments based on the molecular profile of a patient's tumor are an emerging and exciting class of treatments in oncology. As genomic tumor profiling is becoming more common, targeted treatments for specific molecular alterations are gaining traction. To discover new potential therapeutics that may apply to broad classes of tumors matching some molecular pattern, experimentalists and pharmacologists rely on high-throughput, in vitro screens of many compounds against many different cell lines. We propose a hierarchical Bayesian model of how cancer cell lines respond to drugs in these experiments and develop a method for fitting the model to real-world high-throughput screening data. Through a case study, the model is shown to capture nontrivial associations between molecular features and drug response, such as requiring both wild type TP53 and overexpression of MDM2 to be sensitive to Nutlin-3(a). In quantitative benchmarks, the model outperforms a standard approach in biology, with $\approx20\%$ lower predictive error on held out data. When combined with a conditional randomization testing procedure, the model discovers markers of therapeutic response that recapitulate known biology and suggest new avenues for investigation. All code for the article is publicly available at https://github.com/tansey/deep-dose-response.",2018-12-13,https://www.semanticscholar.org/paper/ba84afa9279d794cada027bed70e9a343b9add6e,Biostatistics
257,"Games, algorithms, and the Internet","The advent of the Internet brought parallel paradigm shifts to both Economics and Computer Science. Computer scientists realized that large-scale performing systems can emerge from the interaction of selfish agents and that incentives are a quintessential part of a good system design. And economists saw that the default platforms of economic transactions are computational and interconnected. Algorithmic Game Theory is a subdiscipline that emerged from this turmoil, revisiting some of the most important problems in Economics and Game Theory from a computational and network perspective. This talk will survey some of the major themes, results and challenges in this field.",2011-03-28,https://www.semanticscholar.org/paper/bb149415a6f994f19558b455d1366c429b2249cc,The Web Conference
3007,Formal Verification of a Multiprocessor Hypervisor on Arm Relaxed Memory Hardware,"Concurrent systems software is widely-used, complex, and error-prone, posing a significant security risk. We introduce VRM, a new framework that makes it possible for the first time to verify concurrent systems software, such as operating systems and hypervisors, on Arm relaxed memory hardware. VRM defines a set of synchronization and memory access conditions such that a program that satisfies these conditions can be mostly verified on a sequentially consistent hardware model and the proofs will automatically hold on relaxed memory hardware. VRM can be used to verify concurrent kernel code that is not data race free, including code responsible for managing shared page tables in the presence of relaxed MMU hardware. Using VRM, we verify the security guarantees of a retrofitted implementation of the Linux KVM hypervisor on Arm. For multiple versions of KVM, we prove KVM's security properties on a sequentially consistent model, then prove that KVM satisfies VRM's required program conditions such that its security proofs hold on Arm relaxed memory hardware. Our experimental results show that the retrofit and VRM conditions do not adversely affect the scalability of verified KVM, as it performs similar to unmodified KVM when concurrently running many multiprocessor virtual machines with real application workloads on Arm multiprocessor server hardware. Our work is the first machine-checked proof for concurrent systems software on Arm relaxed memory hardware.",2021-10-26,https://www.semanticscholar.org/paper/59c83a2ced9816343098d85cdc7c12d806838fce,Symposium on Operating Systems Principles
2061,Editorial e-Manufacturing in the Semiconductor Industry,The 11 papers in this special issue focus on e-manufacturing in the semiconductor industry.,2007-10-01,https://www.semanticscholar.org/paper/1dc52187db9b992016e15e1822bca36d7581557b,IEEE Transactions on Automation Science and Engineering
2790,Galectin-7 downregulation in lesional keratinocytes contributes to enhanced IL-17A signaling and skin pathology in psoriasis.,"Psoriasis is a chronic inflammatory skin disease characterized by inflammatory cell infiltration, as well as hyperproliferation of keratinocytes in skin lesions, and is considered a metabolic syndrome. We found that the expression of galectin-7 is reduced in the skin lesions of patients with psoriasis. IL-17A and TNF-α, two cytokines intimately involved in the development of psoriatic lesions, suppressed galectin-7 expression in human primary keratinocytes (HEKn cells) and the immortalized human keratinocyte cell line HaCaT. A galectin-7 knockdown in these cells elevated the production of IL-6 and IL-8 and enhanced ERK signaling when the cells were stimulated with IL-17A. Galectin-7 attenuated IL-17A-induced production of inflammatory mediators by keratinocytes via the miR-146a-ERK pathway. Moreover, galectin-7-deficient mice showed enhanced epidermal hyperplasia and skin inflammation in response to intradermal IL-23 injection. We identified fluvastatin as an inducer of galectin-7 expression by connectivity map (cMAP) analysis, confirmed this effect in keratinocytes, and demonstrated that fluvastatin attenuated IL-6 and IL-8 production induced by IL-17A. Thus, we validate a role of galectin-7 in the pathogenesis of psoriasis, in both epidermal hyperplasia and keratinocyte-mediated inflammatory responses, and formulated a rationale for the use of statins in the treatment of psoriasis.",2020-10-15,https://www.semanticscholar.org/paper/ef1365b63e11d75e6c0474452533535ab90ced13,Journal of Clinical Investigation
1824,The nested chinese restaurant process and bayesian nonparametric inference of topic hierarchies,"We present the nested Chinese restaurant process (nCRP), a stochastic process that assigns probability distributions to ensembles of infinitely deep, infinitely branching trees. We show how this stochastic process can be used as a prior distribution in a Bayesian nonparametric model of document collections. Specifically, we present an application to information retrieval in which documents are modeled as paths down a random tree, and the preferential attachment dynamics of the nCRP leads to clustering of documents according to sharing of topics at multiple levels of abstraction. Given a corpus of documents, a posterior inference algorithm finds an approximation to a posterior distribution over trees, topics and allocations of words to levels of the tree. We demonstrate this algorithm on collections of scientific abstracts from several journals. This model exemplifies a recent trend in statistical machine learning—the use of Bayesian nonparametric methods to infer distributions on flexible data structures.",2007-10-03,https://www.semanticscholar.org/paper/6d296b269991f165b650a6360254a7413e966d27,JACM
3653,Parameterized Types for C++,"Type parameterization is the ability to defrne a type in terms of another, unspecifled, type. Versions of the parameterized type may then be created for several particular parameter types. A language supporting type parameterization allows specification ofgeneral container types such as list, vector, and associative array where the specific type of the elements is left as a parameter. Thus, a parameterized class specifies an unbounded set of related types; for example: list of int, list of name, list of shape, etc. Type parameterization is one way of making a language more extensible. In the context of C++, the problems are",1989-01-03,https://www.semanticscholar.org/paper/b176b1368c966f248e906379b831df5d1b51a7c7,C++ Conference
834,Linear approximation of shortest superstrings,"We consider the following problem: given a collection of strings <italic>s<subscrpt>1</subscrpt></italic>,…, <italic>s<subscrpt>m</subscrpt></italic>, find the shortest string <italic>s</italic> such that each <italic>s<subscrpt>i</subscrpt></italic> appears as a substring (a consecutive block) of <italic>s</italic>. Although this problem is known to be NP-hard, a simple greedy procedure appears to do quite well and is routinely used in DNA sequencing and data compression practice, namely: repeatedly merge the pair of (distinct) strings with maximum overlap until only one string remains. Let <italic>n</italic> denote the length of the optimal superstring. A common conjecture states that the above greedy procedure produces a superstring of length <italic>O(n)</italic> (in fact, 2<italic>n</italic>), yet the only previous nontrivial bound known for any polynomial-time algorithm is a recent <italic>O(n</italic> log <italic>n</italic>) result.
We show that the greedy algorithm does in fact achieve a constant factor approximation, proving an upper bound of 4<italic>n</italic>. Furthermore, we present a simple modified version of the greedy algorithm that we show produces a superstring of length at most 3<italic>n</italic>. We also show the superstring problem to be MAXSNP-hard, which implies that a polynomial-time approximation scheme for this problem is unlikely.",1994-07-01,https://www.semanticscholar.org/paper/b9bd744a4337f4469afaa246cd34990908a2561d,JACM
1578,The Medical Deconfounder: Assessing Treatment Effect with Electronic Health Records (EHRs),"Causal estimation of treatment effect has an important role in guiding physicians' decision process for drug prescription. While treatment effect is classically assessed with randomized controlled trials (RCTs), the availability of electronic health records (EHRs) bring an unprecedented opportunity for more efficient estimation. However, the presence of unobserved confounders makes treatment effect assessment from EHRs a challenging task. Confounders are the variables that affect both drug prescription and the patient's outcome; examples include a patient's gender, race, social economic status and comorbidities. When these confounders are unobserved, they bias the estimation. To adjust for unobserved confounders, we develop the medical deconfounder, a machine learning algorithm that unbiasedly estimates treatment effect from EHRs. The medical deconfounder first constructs a substitute confounder by modeling which drugs were prescribed to each patient; this substitute confounder is guaranteed to capture all multi-drug confounders, observed or unobserved (Wang and Blei, 2018). It then uses this substitute confounder to adjust for the confounding bias in the analysis. We validate the medical deconfounder on simulations and two medical data sets. The medical deconfounder produces closer-to-truth estimates in simulations and identifies effective medications that are more consistent with the findings reported in the medical literature compared to classical approaches.",2019-04-03,https://www.semanticscholar.org/paper/b352c244259c17eecea8dc50a53ca1d694357e4c,arXiv.org
1570,"Bayesian Tensor Filtering: Smooth, Locally-Adaptive Factorization of Functional Matrices","We consider the problem of functional matrix factorization, finding low-dimensional structure in a matrix where every entry is a noisy function evaluated at a set of discrete points. Such problems arise frequently in drug discovery, where biological samples form the rows, candidate drugs form the columns, and entries contain the dose-response curve of a sample treated at different concentrations of a drug. We propose Bayesian Tensor Filtering (BTF), a hierarchical Bayesian model of matrices of functions. BTF captures the smoothness in each individual function while also being locally adaptive to sharp discontinuities. The BTF model is agnostic to the likelihood of the underlying observations, making it flexible enough to handle many different kinds of data. We derive efficient Gibbs samplers for three classes of likelihoods: (i) Gaussian, for which updates are fully conjugate; (ii) Binomial and related likelihoods, for which updates are conditionally conjugate through P{\'o}lya--Gamma augmentation; and (iii) Black-box likelihoods, for which updates are non-conjugate but admit an analytic truncated elliptical slice sampling routine. We compare BTF against a state-of-the-art method for dynamic Poisson matrix factorization, showing BTF better reconstructs held out data in synthetic experiments. Finally, we build a dose-response model around BTF and show on real data from a multi-sample, multi-drug cancer study that BTF outperforms the current standard approach in biology. Code for BTF is available at https://github.com/tansey/functionalmf.",2019-06-10,https://www.semanticscholar.org/paper/47f247ef2fe0308087bc63dc8011e7b63f3d3aa6,arXiv.org
2892,Grain Size and Shape Dependent Crystal Plasticity Finite Element Model and its Application to Electron Beam Welded Ss316l,,2023-06-01,https://www.semanticscholar.org/paper/3f40540c07bc543ee4a4aac46ac8b1f133c87582,Social Science Research Network
386,Deciding stability and mortality of piecewise affine dynamical systems,,2001-03-28,https://www.semanticscholar.org/paper/67ff0a9fa5a1973d7f37178461d47acfd857c35a,Theoretical Computer Science
2175,Modified culture protocol for differentiation of human promyelocytic leukaemia PLB-985 cell-line into mature neutrophil-like granulocytes,"
 Neutrophils are considered to be mature and terminally differentiated. They are short-lived, lack proliferation capacity and are impossible to transfect in vitro to express exogenous genes or proteins. These properties have made their ex vivo experimental manipulation and biochemical studies of their signal activation mechanisms and genetic makeup, challenging. Establishment of cell line models capable of differentiating along the myeloid lineage into mature neutrophil-like granulocytes, with similar morphological and functional properties to blood neutrophils would, therefore, be an important tool to enable transfection of key genes and determine the effects of such manipulations on neutrophil functions. PLB-985 is a recently established cell line that has been demonstrated to show greater levels of differentiation into granulocyte in the presence of appropriate differentiation-inducing agents, but the efficiency of differentiation reported so far was very low, and the differentiated cells only partly resembled mature blood neutrophils. This study developed a modified differentiation protocol and an optimised culture conditions that induced the PLB-985 cell line to differentiate into mature, neutrophil-like granulocytes, Terminally-differentiated, neutrophil-like phenotypes of PLB-985 cells that resembled mature blood neutrophil morphology, and which had an appreciably delayed apoptosis have been consistently cultured. This was evident by the acquisition of multi-lobed nucleus, granulated cytoplasm and delayed apoptosis. The differentiated PLB-985 cells may, therefore, provide an excellent neutrophil-model system for in vitro study of neutrophil differentiation, signalling and functions.",2020-05-01,https://www.semanticscholar.org/paper/afd6c5e13e2272a06a141f421a3a4f70259d077b,Journal of Immunology
2299,"Neutrophils from the synovial fluid of patients with rheumatoid arthritis express the high affinity immunoglobulin G receptor, FcγRI (CD64): role of immune complexes and cytokines in induction of receptor expression","Neutrophils isolated from the synovial fluid of 16/24 patients with rheumatoid arthritis expressed FcγRI (CD64), the high‐affinity receptor for monomeric immunoglobulin G (IgG), on their cell surface. Receptor expression ranged from 17% to 168% of the level of expression obtained after incubation of control blood neutrophils with 100 U/ml interferon‐γ (IFN‐γ) for 24 hr in vitro. Similarly, mRNA for FcγRI was detected in synovial fluid neutrophils from 12/15 patients and transcript levels ranged from 5% to 200% of the values obtained after treatment of blood neutrophils with IFN‐γ for 4 hr in vitro. No surface expression nor mRNA were detected in freshly isolated blood neutrophils from either patients or from healthy controls. Addition of cell‐free synovial fluid to control blood neutrophils induced both mRNA and surface expression of FcγRI to levels that were comparable to those achieved after addition of IFN‐γ. Neither soluble nor insoluble immune complexes appeared to be involved in induction of FcγRI expression in spite of the ability of these complexes to induce protein biosynthesis. Synovial fluid‐induced expression of FcγRI was partially blocked by incubation with neutralizing IFN‐γ antibodies, whilst neutralizing interleukin (IL)‐6 antibodies had little effect. Levels of IFN‐γ measured within these synovial fluids ranged from 0 to 2·7 U/ml, well within the range known to induce neutrophil FcγRI expression. These data thus indicate that gene expression in synovial fluid neutrophils is selectively activated as the cells enter the diseased joint. Furthermore, these data indicate that induced expression of FcγRI may alter the ability of infiltrating neutrophils to respond to IgG‐containing immune complexes present in these joints.",1997-06-01,https://www.semanticscholar.org/paper/8fc14c3ddf3bd1d58b75e213bf67164b318cb458,Immunology
391,Auditing Boolean attributes,"We study the problem of auditing databases which support statistical sum queries to protect the security of sensitive information; we focus on the special case in which the sensitive information is Boolean. Principles and techniques developed for the security of statistical database in the case of continuous attributes do not apply here. We prove certain strong complexity results suggesting that there is no general efficient solution for the auditing problem in this case. We propose two efficient algorithms: The first is applicable when the sum queries are one-dimensional range queries (we prove that the problem is NP-hard even in the two-dimensional case). The second is an approximate algorithm that maintains security, although it may be too restrictive. Finally, we consider a “dual” variant, with continuous data but an aggregate function that is combinatorial in nature. Specifically, we provide algorithms for two natural definitions of the auditing condition when the aggregate function is MAX.",2000-05-01,https://www.semanticscholar.org/paper/333a016bc00ff74da51ebf51f1c4f80298db7122,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems
2387,The mitochondrial adenosine triphosphatase of Acanthamoeba castellanii. Partial characterization and changes in activity during exponential growth.,,,https://www.semanticscholar.org/paper/66d03c3b665332a0befc03bd3d8b4ed7e9254bf0,"Comparative biochemistry and physiology. B, Comparative biochemistry"
3402,Minimizing Maximum Flow Time on Related Machines via Dynamic Posted Pricing,"We consider a setting where selfish agents want to schedule jobs on related machines. The agent submitting a job picks a server that minimizes a linear combination of the server price and the resulting response time for that job on the selected server. The manager’s task is to maintain server prices to (approximately) optimize the maximum response time, which is a measure of social good. We show that the existence of a pricing scheme with certain competitiveness is equivalent to the existence of a monotone immediate-dispatch algorithm. Our main result is a monotone immediate-dispatch algorithm that is O(1)-competitive with respect to the maximum response time. 1998 ACM Subject Classification F.2.2 [Nonnumerical Algorithms and Problem]: Sequencing",,https://www.semanticscholar.org/paper/5368e327f1f41557c69651458c8547c77311c410,Embedded Systems and Applications
1686,The Survival Filter: Joint Survival Analysis with a Latent Time Series,"Survival analysis is a core task in applied statistics, which models time-to-failure or time-to-event data. In the clinical domain, for example, meaningful events are defined as the onset of different diseases for a given patient. Survival analysis is limited, however, for analyzing modern electronic health records. Patients often have a wide range of diseases, and there are complex interactions among the relative risks of different events. To this end, we develop the survival filter model, a time-series model for joint survival analysis that models multiple patients and multiple diseases. We develop a scalable variational inference algorithm and apply our method to a large data set of longitudinal patient records. The survival filter gives good predictive performance when compared to two baselines and identifies clinically meaningful patterns of disease interaction.",2015-07-12,https://www.semanticscholar.org/paper/9f08a5e4360a7823e596ed4567a5f671ec3759a3,Conference on Uncertainty in Artificial Intelligence
2800,Galectin-3 promotes HIV-1 budding via association with Alix and Gag p6.,"Galectin-3 has been reported to regulate the functions of a number of immune cell types. We previously reported that galectin-3 is translocated to immunological synapses in T cells upon T-cell receptor engagement, where it associates with ALG-2-interacting protein X (Alix). Alix is known to coordinate with the endosomal sorting complex required for transport (ESCRT) to promote human immunodeficiency virus (HIV)-1 virion release. We hypothesized that galectin-3 plays a role in HIV-1 viral budding. Cotransfection of cells of the Jurkat T line with galectin-3 and HIV-1 plasmids resulted in increased HIV-1 budding, and suppression of galectin-3 expression by RNAi in Hut78 and primary CD4+ T cells led to reduced HIV-1 budding. We used immunofluorescence microscopy to observe the partial colocalization of galectin-3, Alix and Gag in HIV-1-infected cells. Results from co-immunoprecipitation experiments indicate that galectin-3 expression promotes Alix-Gag p6 association, whereas the results of Alix knockdown suggest that galectin-3 promotes HIV-1 budding through Alix. HIV-1 particles released from galectin-3-expressing cells acquire the galectin-3 protein in an Alix-dependent manner, with proteins primarily residing inside the virions. We also found that the galectin-3 N-terminal domain interacts with the proline-rich region of Alix. Collectively, these results suggest that endogenous galectin-3 facilitates HIV-1 budding by promoting the Alix-Gag p6 association.",2014-11-01,https://www.semanticscholar.org/paper/7f8c1a8f85cce48d532c1573262911d5f07074c5,Glycobiology
156,Online Stochastic Max-Weight Bipartite Matching: Beyond Prophet Inequalities,"The rich literature on online Bayesian selection problems has long focused on so-called prophet inequalities, which compare the gain of an online algorithm to that of a ""prophet"" who knows the future. An equally-natural, though significantly less well-studied benchmark is the optimum online algorithm, which may be omnipotent (i.e., computationally-unbounded), but not omniscient. What is the computational complexity of the optimum online? How well can a polynomial-time algorithm approximate it? Motivated by applications in ride hailing, we study the above questions for the online stochastic maximum-weight matching problem under vertex arrivals. This problem was recently introduced by Ezra, Feldman, Gravin and Tang (EC'20), who gave a 1/2-competitive algorithm for it. This is the best possible ratio, as this problem is a generalization of the original single-item prophet inequality. We present a polynomial-time algorithm which approximates optimal online within a factor of 0.51---beating the best-possible prophet inequality. At the core of our result are a new linear program formulation, an algorithm that tries to match the arriving vertices in two attempts, and an analysis that bounds the correlation resulting from the second attempts. In contrast, we show that it is PSPACE-hard to approximate this problem within some constant α < 1.",2021-02-20,https://www.semanticscholar.org/paper/1c8a05b5f15067d5c40c933539239c5d1c304e8b,ACM Conference on Economics and Computation
2908,"Correlation study on tensile properties of Cu, CuCrZr and W by small punch test and uniaxial tensile test",,2022-04-01,https://www.semanticscholar.org/paper/8d1d5ec9df0707f9193afa157f30cc33420226fd,Fusion engineering and design
2904,Multiset correlation and factor analysis enables exploration of multi-omic data,"Multi-omics datasets are becoming more common, necessitating better integration methods to realize their revolutionary potential. Here, we introduce Multi-set Correlation and Factor Analysis, an unsupervised integration method that enables fast inference of shared and private factors in multi-modal data. Applied to 614 ancestry-diverse participant samples across five ‘omics types, MCFA infers a shared space that captures clinically relevant molecular processes.",2022-07-20,https://www.semanticscholar.org/paper/5873787a7ebfd28080a58df29e9921086a910d2b,bioRxiv
2419,HoloFight: An Augmented Reality Fighting Game,"Augmented Reality (AR) provides opportunities to create exciting new kinds of digital entertainment, such as watching movies on a large virtual screen or playing games that interact with a real physical room. While a number of AR games have been built, many do not build on the control innovations found in modern console, PC, and mobile gaming [Von Itzstein et al. 2019]. To explore the space of immersive multiplayer experiences with support for control innovations found in common non-immersive video games, we present HoloFight, a multiplayer fighting game using two or more Microsoft HoloLens 2s, two or more Xbox controllers, and the various natural user interfaces supported by the Microsoft HoloLens 2.",2021-08-05,https://www.semanticscholar.org/paper/43e05dab2a106c57f64449cdf334355a88fd8493,SIGGRAPH Immersive Pavilion
3717,The Boombox: Visual Reconstruction from Acoustic Vibrations,"Interacting with bins and containers is a fundamental task in robotics, making state estimation of the objects inside the bin critical. While robots often use cameras for state estimation, the visual modality is not always ideal due to occlusions and poor illumination. We introduce The Boombox, a container that uses sound to estimate the state of the contents inside a box. Based on the observation that the collision between objects and its containers will cause an acoustic vibration, we present a convolutional network for learning to reconstruct visual scenes. Although we use low-cost and low-power contact microphones to detect the vibrations, our results show that learning from multimodal data enables state estimation from affordable audio sensors. Due to the many ways that robots use containers, we believe the box will have a number of applications in robotics. Our project website is at: boombox.cs.columbia.edu",2021-05-17,https://www.semanticscholar.org/paper/d666e22aa939a262bcf2a7985bcc3eb16dcb85e4,Conference on Robot Learning
792,Analysis of recursive state machines,"Recursive state machines (RSMs) enhance the power of ordinary state machines by allowing vertices to correspond either to ordinary states or to potentially recursive invocations of other state machines. RSMs can model the control flow in sequential imperative programs containing recursive procedure calls. They can be viewed as a visual notation extending Statecharts-like hierarchical state machines, where concurrency is disallowed but recursion is allowed. They are also related to various models of pushdown systems studied in the verification and program analysis communities.After introducing RSMs and comparing their expressiveness with other models, we focus on whether verification can be efficiently performed for RSMs. Our first goal is to examine the verification of linear time properties of RSMs. We begin this study by dealing with two key components for algorithmic analysis and model checking, namely, reachability (Is a target state reachable from initial states?) and cycle detection (Is there a reachable cycle containing an accepting state?). We show that both these problems can be solved in time O(nθ2) and space O(nθ), where n is the size of the recursive machine and θ is the maximum, over all component state machines, of the minimum of the number of entries and the number of exits of each component. From this, we easily derive algorithms for linear time temporal logic model checking with the same complexity in the model. We then turn to properties in the branching time logic CTL*, and again demonstrate a bound linear in the size of the state machine, but only for the case of RSMs with a single exit node.",2001-07-18,https://www.semanticscholar.org/paper/c33a7f02d85e6e17c8509324f8e95483f9db62bb,TOPL
3141,Low-complexity interpolation coding for server-based computing,"Summary form only given. The growing total cost of ownership has resulted in a shift away from the distributed model of desktop computing toward a more centralized server-based computing (SBC) model. In SBC, all application logic is executed on the server while clients simply process the resulting screen updates sent from the server. To provide good performance, SBC systems employ various techniques to encode the screen updates to minimize the bandwidth and processing requirements of sending the screen updates. However, existing SBC encoding techniques are not able to effectively support multimedia applications. To address this problem, we have developed a family of linear interpolation algorithms for encoding SBC screen updates. We first present an overview of an optimal linear interpolation (OLI) algorithm. Given a rectangular region of pixels to be encoded, OLI represents the region as a one-dimensional function, mapping from the cardinal number of each pixel to the color value of the pixel. To reduce encoding complexity, we developed two linear interpolation algorithms with linear encoding and decoding computational complexity. The algorithms are near optimal linear interpolation (NOLI) and 2-D lossless linear interpolation (2DLI). We have implemented our linear interpolation algorithms and compared their performance with other approaches on discrete-toned and smoothed-toned images.",2002-04-02,https://www.semanticscholar.org/paper/65b8b122c05e7563c6af353e66230ab5495dfc08,Proceedings DCC 2002. Data Compression Conference
376,Market equilibrium via a primal-dual-type algorithm,"Although the study of market equilibria has occupied center stage within mathematical economics for over a century, polynomial time algorithms for such questions have so far evaded researchers. We provide the first such algorithm for the linear version of a problem defined by Irving Fisher in 1891. Our algorithm is modeled after Kuhn's (1995) primal-dual algorithm for bipartite matching.",2002-11-16,https://www.semanticscholar.org/paper/b31c29a4353fdd958ecdb96bdb677c4ba4e8ab69,"The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings."
1512,Evaluating the Moral Beliefs Encoded in LLMs,"This paper presents a case study on the design, administration, post-processing, and evaluation of surveys on large language models (LLMs). It comprises two components: (1) A statistical method for eliciting beliefs encoded in LLMs. We introduce statistical measures and evaluation metrics that quantify the probability of an LLM""making a choice"", the associated uncertainty, and the consistency of that choice. (2) We apply this method to study what moral beliefs are encoded in different LLMs, especially in ambiguous cases where the right choice is not obvious. We design a large-scale survey comprising 680 high-ambiguity moral scenarios (e.g.,""Should I tell a white lie?"") and 687 low-ambiguity moral scenarios (e.g.,""Should I stop for a pedestrian on the road?""). Each scenario includes a description, two possible actions, and auxiliary labels indicating violated rules (e.g.,""do not kill""). We administer the survey to 28 open- and closed-source LLMs. We find that (a) in unambiguous scenarios, most models""choose""actions that align with commonsense. In ambiguous cases, most models express uncertainty. (b) Some models are uncertain about choosing the commonsense action because their responses are sensitive to the question-wording. (c) Some models reflect clear preferences in ambiguous scenarios. Specifically, closed-source models tend to agree with each other.",2023-07-26,https://www.semanticscholar.org/paper/12acdfc7e32e9d603dc108008bb15e65439e7c79,arXiv.org
2636,A graphical user interface toolkit approach to thin-client computing,"Network and server-centric computing paradigms are quickly returning to being the dominant methods by which we use computers. Web applications are so prevalent that the role of a PC today has been largely reduced to a terminal for running a client or viewer such as a Web browser. Implementers of network-centric applications typically rely on the limited capabilities of HTML, employing proprietary ""plug ins"" or transmitting the binary image of an entire application that will be executed on the client. Alternatively, implementers can develop without regard for remote use, requiring users who wish to run such applications on a remote server to rely on a system that creates a virtual frame buffer on the server, and transmits a copy of its raster image to the local client.We review some of the problems that these current approaches pose, and show how they can be solved by developing a distributed user interface toolkit. A distributed user interface toolkit applies techniques to the high level components of a toolkit that are similar to those used at a low level in the X Window System. As an example of this approach, we present RemoteJFC, a working distributed user interface toolkit that makes it possible to develop thin-client applications using a distributed version of the Java Foundation Classes.",2002-05-07,https://www.semanticscholar.org/paper/ac923ea792134f229a4b6519ce0447fb14ca5287,The Web Conference
843,On limited nondeterminism and the complexity of the V-C dimension,"The complexity of several natural computational problems in NP, which have been proposed but not categorized satisfactorily in the literature is characterized precisely. These problems can be solved in n/sup O(logn)/ time, and thus they are probably not NP-complete. Two new complexity classes between P and NP, very much in the spirit of MAXNP and MAXSNP, are defined. It is shown that computing the V-C dimension is complete for the more general class, whereas the other two problems are complete for the weaker class.<<ETX>>",1993-05-18,https://www.semanticscholar.org/paper/d74f8e664e461340ffcbdad9c38d6e619378f024,[1993] Proceedings of the Eigth Annual Structure in Complexity Theory Conference
239,The Complexity of Fairness Through Equilibrium,"Competitive equilibrium from equal incomes (CEEI) is a well-known fair allocation mechanism with desirable fairness and efficiency properties; however, with indivisible resources, a CEEI may not exist [Foley 1967; Varian 1974; Thomson and Varian 1985]. It was shown in Budish [2011] that in the case of indivisible resources, there is always an allocation, called A-CEEI, that is approximately fair, approximately truthful, and approximately efficient for some favorable approximation parameters. A heuristic search that attempts to find this approximation is used in practice to assign business school students to courses. In this article, we show that finding the A-CEEI allocation guaranteed to exist by Budish’s theorem is PPAD-complete. We further show that finding an approximate equilibrium with better approximation guarantees is even harder: NP-complete.",2013-12-21,https://www.semanticscholar.org/paper/8451b013e29d325e795d45b2f9b2105b55d49a9c,ACM Trans. Economics and Comput.
2057,Using DEA for relative efficiency analysis of wafer fabrication facilities,"Semiconductor industry is capital-incentive, competitive, and having complicated manufacturing processes. It is essential to utilize resources efficiently to provide products and services for maintaining competitive advantages. Knowing whether the resource is properly utilized is the foundation for future improvements and/or production decision. Due to the disparate units involved, direct evaluation and comparison among the fabs is difficult. This study aims to develop a two-stage model for relative comparison on fabrication operations by adopting data envelopment analysis (DEA). The proposed model clearly defines and differentiates the performance of fab production, and provides an overall performance index while considering different aspects.",2008-10-01,https://www.semanticscholar.org/paper/d3aaad3e8cac28957dc87b3444c6e067d81b06b5,International Symposium on Semiconductor Manufacturing
530,Scheduling Dags to Minimize Time and Communication,,1988-06-28,https://www.semanticscholar.org/paper/8558a76e1f4607b7a22fd9838345704a42c4f70d,Aegean Workshop on Computing
877,Optimal Scheduling of Products with Two Subassemblies on a Single Machine,We consider a single machine job shop in which subassemblies of two different types are made and then assembled into products. The time required for each type is known. A fixed changeover cost is incurred whenever the machine is switched over from one type to the other. We describe and analyze an efficient algorithm for minimizing the total flow time of the products. Applications to the automated manufacture of circuit boards are noted.,1989-06-01,https://www.semanticscholar.org/paper/3106ac3e78ef35dd2a94acb343ea06929101e369,Operational Research
1647,Automatic Differentiation Variational Inference,"Probabilistic modeling is iterative. A scientist posits a simple model, fits it to her data, refines it according to her analysis, and repeats. However, fitting complex models to large data is a bottleneck in this process. Deriving algorithms for new models can be both mathematically and computationally challenging, which makes it difficult to efficiently cycle through the steps. To this end, we develop automatic differentiation variational inference (ADVI). Using our method, the scientist only provides a probabilistic model and a dataset, nothing else. ADVI automatically derives an efficient variational inference algorithm, freeing the scientist to refine and explore many models. ADVI supports a broad class of models-no conjugacy assumptions are required. We study ADVI across ten different models and apply it to a dataset with millions of observations. ADVI is integrated into Stan, a probabilistic programming system; it is available for immediate use.",2016-03-02,https://www.semanticscholar.org/paper/30691d2a4eb1a6e88116c357e95b49f9573bcdae,Journal of machine learning research
778,"Protocol System Integration, Interface and Interoperability",,2004-12-15,https://www.semanticscholar.org/paper/a63f33aed2e3e686fcd87467dd9c8e7ac7e5ee33,International Conference on Principles of Distributed Systems
1565,Adapting Text Embeddings for Causal Inference,"Does adding a theorem to a paper affect its chance of acceptance? Does labeling a post with the author's gender affect the post popularity? This paper develops a method to estimate such causal effects from observational text data, adjusting for confounding features of the text such as the subject or writing quality. We assume that the text suffices for causal adjustment but that, in practice, it is prohibitively high-dimensional. To address this challenge, we develop causally sufficient embeddings, low-dimensional document representations that preserve sufficient information for causal identification and allow for efficient estimation of causal effects. Causally sufficient embeddings combine two ideas. The first is supervised dimensionality reduction: causal adjustment requires only the aspects of text that are predictive of both the treatment and outcome. The second is efficient language modeling: representations of text are designed to dispose of linguistically irrelevant information, and this information is also causally irrelevant. Our method adapts language models (specifically, word embeddings and topic models) to learn document embeddings that are able to predict both treatment and outcome. We study causally sufficient embeddings with semi-synthetic datasets and find that they improve causal estimation over related embedding methods. We illustrate the methods by answering the two motivating questions---the effect of a theorem on paper acceptance and the effect of a gender label on post popularity. Code and data available at this https URL}{this http URL",2019-05-29,https://www.semanticscholar.org/paper/09428af106c378616d7767a37c4f4070a2664e5a,Conference on Uncertainty in Artificial Intelligence
2643,Recent Advances in Augmented Reality,"In 1997, Azuma published a survey on augmented reality (AR). Our goal is to complement, rather than replace, the original survey by presenting representative examples of the new advances. We refer one to the original survey for descriptions of potential applications (such as medical visualization, maintenance and repair of complex equipment, annotation, and path planning); summaries of AR system characteristics (such as the advantages and disadvantages of optical and video approaches to blending virtual and real, problems in display focus and contrast, and system portability); and an introduction to the crucial problem of registration, including sources of registration error and error-reduction strategies.",2001-11-01,https://www.semanticscholar.org/paper/408b6295e901e3e6d96b55a69ca6f6eda762891e,IEEE Computer Graphics and Applications
1395,Search for the production of single sleptons through R-parity violation in pp; collisions at square root (s) =1.8 TeV.,"We report the first search for supersymmetric particles via s-channel production and decay of smuons or muon sneutrinos at hadronic colliders. The data for the two-muon and two-jets final states were collected by the D0 experiment and correspond to an integrated luminosity of 94+/-5 pb(-1). Assuming that R parity is violated via the single coupling lambda'211, the number of candidate events is in agreement with expectation from the standard model. Exclusion contours are given in the (m(0),m(1/2)) and (m(x),m(v)) planes for lambda(')(211)=0.09, 0.08, and 0.07.",,https://www.semanticscholar.org/paper/95d27ae37d1ebc1b813f2d53ab8d02ccaf1ebc80,Physical Review Letters
1337,Search for large extra spatial dimensions in dimuon production with the d0 detector.,"We present the results of a search for the effects of large extra spatial dimensions in pp collisions at sqrt[s] = 1.96 TeV in events containing a pair of energetic muons. The data correspond to 246 pb(-1) of integrated luminosity collected by the D0 experiment at the Fermilab Tevatron Collider. Good agreement with the expected background was found, yielding no evidence for large extra dimensions. We set 95% C.L. lower limits on the fundamental Planck scale between 0.85 and 1.27 TeV within several formalisms. These are the most stringent limits achieved in the dimuon channel to date.",2005-06-25,https://www.semanticscholar.org/paper/2a860eeb1357a73869642616b20777c6aa9b582d,Physical Review Letters
495,Decision-Making with Incomplete Information,,1991-12-16,https://www.semanticscholar.org/paper/2897cedb05bf8b768d9a0ba03ac5486323145a11,International Symposium on Algorithms
2474,ParaFrustum: visualization techniques for guiding a user to a constrained set of viewing positions and orientations,"Many tasks in real or virtual environments require users to view a target object or location from one of a set of strategic viewpoints to see it in context, avoid occlusions, or view it at an appropriate angle or distance. We introduce ParaFrustum, a geometric construct that represents this set of strategic viewpoints and viewing directions. ParaFrustum is inspired by the look-from and look-at points of a computer graphics camera specification, which precisely delineate a location for the camera and a direction in which it looks. We generalize this approach by defining a ParaFrustum in terms of a look-from volume and a look-at volume, which establish constraints on a range of acceptable locations for the user's eyes and a range of acceptable angles in which the user's head can be oriented. Providing tolerance in the allowable viewing positions and directions avoids burdening the user with the need to assume a tightly constrained 6DoF pose when it is not required by the task. We describe two visualization techniques for virtual or augmented reality that guide a user to assume one of the poses defined by a ParaFrustum, and present the results of a user study measuring the performance of these techniques. The study shows that the constraints of a tightly constrained ParaFrustum (e.g., approximating a conventional camera frustum) require significantly more time to satisfy than those of a loosely constrained one. The study also reveals interesting differences in participant trajectories in response to the two techniques.",2014-10-05,https://www.semanticscholar.org/paper/5bc9cd5b54d9781f3f388673a7102e579b5a176c,ACM Symposium on User Interface Software and Technology
2585,Exploring interaction with a simulated wrist-worn projection display,"One of the major limitations of portable computing devices is the small size of their built-in displays. Fortunately, extremely small projection systems are being developed that can be integrated into devices that are small enough to be body-worn, yet can project a large image onto surfaces in the environment. To explore how a user might interact with this near-horizon technology, we created a functional simulation of a wrist-worn projector. We then developed a set of interaction techniques that assume that the wrist-worn computer and projector are equipped with position and orientation sensors, in addition to a touch-sensitive built-in screen. To complement the techniques that rely on the spatial manipulation of the user's forearm and the device itself, we also describe the use of a cursor less watch user interface that minimizes the need for the user to look down at the device's built-in screen. Finally, we present a sample application that illustrates our interaction techniques.",2005-10-18,https://www.semanticscholar.org/paper/9e6ec12254969377c3133f7ab32beebdc826f8ca,International Semantic Web Conference
2661,"Erratum to ""Efficiently planning coherent visual discourse"" [Knowledge-Based Systems 10 (1998) 275-286]",,1999-06-01,https://www.semanticscholar.org/paper/5a82c961b67a969fdb0ded211f8bd32552c4bc7f,Knowledge-Based Systems
1733,Efficient discovery of overlapping communities in massive networks,"Detecting overlapping communities is essential to analyzing and exploring natural networks such as social networks, biological networks, and citation networks. However, most existing approaches do not scale to the size of networks that we regularly observe in the real world. In this paper, we develop a scalable approach to community detection that discovers overlapping communities in massive real-world networks. Our approach is based on a Bayesian model of networks that allows nodes to participate in multiple communities, and a corresponding algorithm that naturally interleaves subsampling from the network and updating an estimate of its communities. We demonstrate how we can discover the hidden community structure of several real-world networks, including 3.7 million US patents, 575,000 physics articles from the arXiv preprint server, and 875,000 connected Web pages from the Internet. Furthermore, we demonstrate on large simulated networks that our algorithm accurately discovers the true community structure. This paper opens the door to using sophisticated statistical models to analyze massive networks.",2013-08-15,https://www.semanticscholar.org/paper/7b3373f90e691c2ba1a2f383ae38334a1f74e651,Proceedings of the National Academy of Sciences of the United States of America
2493,Workshop 2: Classifying the AR presentation space,"Already 3D visualization environments provide a large design space not being investigated to the same extent as traditional WIMP-spaces. When using this design space in combination with AR, the design space even further grows. Information can not only be presented in a 3D space, AR also puts virtual information in relation to real objects, locations or events. The different properties of presentation in AR need to be investigated to develop a comprehensive set of dimensions of presentation principles.",2012-11-05,https://www.semanticscholar.org/paper/2b674383830eaf50f59399841907966dfbdf6635,International Symposium on Mixed and Augmented Reality
507,On path lengths modulo three,"We show that between any two nodes of a cubic, planar, three-connected graph there are three paths whose lengths are 0, 1, and 2 modulo 3, respectively. The proof is by a rather extensive case analysis. Counterexamples show that all three hypotheses (i.e., planarity, degree-three, and three-connectivity) are necessary.",1991-07-01,https://www.semanticscholar.org/paper/e225f9f2c43fe25773454faac16b68f361c94383,Journal of Graph Theory
3049,Structured linux kernel projects for teaching operating systems concepts,"Linux has emerged as a widely-used platform for enabling hands-on kernel programming experience to learn about operating system concepts. However, developing pedagogically-effective programming projects in the context of a complex, production operating system can be a challenge. We present a structured series of five Linux kernel programming projects suitable for a one semester introductory operating systems course to address this issue. Each assignment introduces students to a core topic and major component of an operating system while implicitly teaching them about various aspects of a real-world operating system. Projects are of modest coding complexity, but require students to understand and leverage core components of the Linux operating system. The learning benefits for students from this approach include learning from real-world operating system code examples by expert kernel designers and gaining software engineering experience managing production code complexity. We have successfully used these structured Linux kernel projects to teach over a thousand students in the introductory operating systems course at Columbia University.",2011-03-09,https://www.semanticscholar.org/paper/3be9c5fa026b3fa887a8652a752d100b84e57451,Technical Symposium on Computer Science Education
274,The complexity of computing a Nash equilibrium,"How long does it take until economic agents converge to an equilibrium? By studying the complexity of the problem of computing a mixed Nash equilibrium in a game, we provide evidence that there are games in which convergence to such an equilibrium takes prohibitively long. Traditionally, computational problems fall into two classes: those that have a polynomial-time algorithm and those that are NP-hard. However, the concept of NP-hardness cannot be applied to the rare problems where ""every instance has a solution""---for example, in the case of games Nash's theorem asserts that every game has a mixed equilibrium (now known as the Nash equilibrium, in honor of that result). We show that finding a Nash equilibrium is complete for a class of problems called PPAD, containing several other known hard problems; all problems in PPAD share the same style of proof that every instance has a solution.",2009-02-01,https://www.semanticscholar.org/paper/469133e860a5c46d524e83a1418420d138f97aea,CACM
2998,UPGRADVISOR: Early Adopting Dependency Updates Using Hybrid Program Analysis and Hardware Tracing,"Applications often have fast-paced release schedules, but adoption of software dependency updates can lag by years, leaving applications susceptible to security risks and unexpected breakage. To address this problem, we present U PGRADVISOR , a system that reduces developer effort in evaluating dependency updates and can, in many cases, automatically determine which updates are backward-compatible versus API-breaking. U PGRADVISOR introduces a novel co-designed static analysis and dynamic tracing mechanism to gauge the scope and effect of dependency updates on an application. Static analysis prunes changes irrelevant to an application and clusters relevant ones into targets . Dynamic tracing needs to focus only on whether targets affect an application, making it fast and accurate. U PGRADVISOR handles dynamic interpreted languages and introduces call graph over-approximation to account for their lack of type information and selective hardware tracing to capture program execution while ignoring interpreter machinery. We have implemented U PGRADVISOR for Python and evaluated it on 172 dependency updates previously blocked from being adopted in widely-used open-source software, including Django , aws-cli , tfx , and Celery . U PGRADVISOR automatically determined that 56% of dependencies were safe to update and reduced by more than an order of magnitude the number of code changes that needed to be considered by dynamic tracing. Evaluating U PGRADVISOR ’s tracer in a production-like environment incurred only 3% overhead on average, making it fast enough to deploy in practice. We submitted safe updates that were previously blocked as pull requests for nine projects, and their developers have already merged most of them. Abstract The version of U PGRADVISOR used to perform the experiments described in the paper may be downloaded from figshare.com. Theartifactcontainsthecodeforthepackagesur-vey, the static analyzer, and the hardware tracer. It also contains scripts to compile the tracer, run the experiments described in the paper, and produce most of the figures. For the most up to We provide the analyzer pre-installed in a docker container. The tracer requires a bare-metal machine. It directly employs a tracing capability found in Intel 5th generation CPUs (Broadwell) and above. Installing the tracer software requires root access to the OS. This artifact will run on a i7-10700 CPU workstation with 16GB RAM. A slower machine may result in reduced performance. We set up the docker container on the tracer machine and encourage you to do the same.",,https://www.semanticscholar.org/paper/11bcdf664ee392ba1cec9a0ecc0f7d59a7f497eb,USENIX Symposium on Operating Systems Design and Implementation
2307,Gene expression by inflammatory neutrophils: stimulation of interleukin-1 beta production by rheumatoid synovial fluid.,"The neutrophl has been thought of as a terminally differentiated cell with little or no capacity or requirement for de novo protein synthesis. It has recently been demonstrated that neutrophils are biosynthetically active and that the biosynthesis of some cellular components is essential to maintain the neutrophil in an active state [I]. It has also been shown that neutrophls can synthesise and secrete a range of cytokines in v i m including Interleukin (1L)-I p, IL-6, IL-8 and tumour necrosis factor-a (TNFa), all of which are present in the synovial fluid of rheumatoid joints [2,3]. L I P has been implicated as a mediator of tissueand bone-damage in rheumatoid arthritis and among its many effects it has been shown to stimulate osteoclasts and chondrocytes to resorb bone 131. As neutrophils can comprise 80% of the cell population in a rheumatoid joint, their production of IL-IP in the joint could be an important contributing factor in the perpetuation of the disease. This production in the joint could have many effects including the stimulation of production of further IL1 P by forming a positive feedback loop. IL-1p has also been shown to enhance adhesion molecule expression on vascular endothelium thereby promoting the accumulation of further neutrophils and other leukocytes [4]. Here we show the synthesis of IL-lp by neutrophils in response to rheumatoid synovial fluid. Neutrophils were isolated from fresh buffy coats as described previously [5] and suspended in RPMI 1640 medium. Suspensions (2x10’ celldml) were re-incubated at 37°C with gentle agitation in the presence of [’ S]-methionine for 15 min. Following this, either granulocyte macrophage-colony stimulating factor (GM-CSF) (14 ng/ml), lipopolysaccharide (LPS) (5 pg/ml), TNFa (25 nglml), synthetic soluble or insoluble immune complexes (10 %, v/v) or cell free synovial fluid (5 %, v/v) were added. Incubation was continued at 37°C with gentle agitation for time periods of up to 24 h. After incubation, cells were pelleted and any IL-Ip present in the cells or culture medium was immunoprecipitated as described previously [6]. The immuno-precipitated samples were then analysed by SDS-PAGE and fluorography. P",1996-08-01,https://www.semanticscholar.org/paper/b286d561e52dfd5d018a0a0f16fbcb5c3887e4ef,Biochemical Society Transactions
3774,HOGgles: Visualizing Object Detection Features,"We introduce algorithms to visualize feature spaces used by object detectors. The tools in this paper allow a human to put on 'HOG goggles' and perceive the visual world as a HOG based object detector sees it. We found that these visualizations allow us to analyze object detection systems in new ways and gain new insight into the detector's failures. For example, when we visualize the features for high scoring false alarms, we discovered that, although they are clearly wrong in image space, they do look deceptively similar to true positives in feature space. This result suggests that many of these false alarms are caused by our choice of feature space, and indicates that creating a better learning algorithm or building bigger datasets is unlikely to correct these errors. By visualizing feature spaces, we can gain a more intuitive understanding of our detection systems.",2013-12-01,https://www.semanticscholar.org/paper/854994253119aa3dbae827a42ca3a6d91d46f215,IEEE International Conference on Computer Vision
2342,Inhibition of neutrophil oxidant secretion by D-penicillamine: scavenging of H2O2 and HOCl.,"D-Penicillamine inhibited oxidant secretion from human neutrophils after activation by the chemotactic peptide N-formyl-methionyl-leucyl-phenylalanine (fMet-Leu-Phe), as assessed by luminol dependent chemiluminescence. In contrast, this drug had little effect on either intracellular oxidant production or lucigenin dependent chemiluminescence activated by the same agonist. The drug was shown to scavenge both H2O2 and HOCl in a cell free luminol chemiluminescence system, though its ability to scavenge HOCl was greater than that for H2O2. Both these oxidants could oxidise the drug, but again HOCl was more potent than H2O2. When D-penicillamine was oxidised by exposure to H2O2 it could no longer serve as a scavenger of secreted oxidants from neutrophils. These data suggest that in vivo the preferential scavenging of HOCl may be important under pathological conditions where secreted myeloperoxidase may be functional.",1992-03-01,https://www.semanticscholar.org/paper/e4ef55269d60c75538951cf5ae2d5ae5afdbb26c,Annals of the Rheumatic Diseases
2259,Neutrophil gene expression in rheumatoid arthritis.,,2005-10-01,https://www.semanticscholar.org/paper/ed017863c971daff319ee566f776ab7e1052f398,Pathophysiology
2361,Interactions between bacterial surfaces and phagocyte plasma membranes.,,1989-06-01,https://www.semanticscholar.org/paper/fe275f0ac1308bef223562d0edae549888d2082b,Biochemical Society Transactions
2730,A history-based macro by example system,"Many tasks performed using computer interfaces are very repetitive. While programmers can write macros or procedures to automate these repetitive tasks, this requires special skills. Demonstrational systems make macro building accessible to all users, but most provide either no visual representation of the macro or only a textual representation. We have developed a history-based visual representation of commands in a graphical user interface. This representation supports the definition of macros by example in several novel ways. At any time, a user can open a history window, review the commands executed in a session, select operations to encapsulate into a macro, and choose objects and their attributes as arguments. The system has facilities to generalize the macro automatically, save it for future use, and edit it.",1992-12-01,https://www.semanticscholar.org/paper/c4e3c03568a88c1497a345a6df023a017f010ef2,ACM Symposium on User Interface Software and Technology
2278,"Differential role of neutrophil Fcgamma receptor IIIB (CD16) in phagocytosis, bacterial killing, and responses to immune complexes.","OBJECTIVE
To determine the roles played by the neutrophil Fcgamma receptor type II (FcgammaRII) (CD32) and FcgammaRIIIb (CD16) in phagocytosis, bacterial killing, and activation by immune complexes (ICs) and to test the hypothesis that inhibition of pathologic effector neutrophil function is possible without compromising host defense.


METHODS
Receptor function was probed by enzymic removal of FcgammaRIIIb from the cell surface and by use of Fab/F(ab')(2) fragments of monoclonal antibodies to block receptor-ligand binding. Cells were challenged with (a) serum-opsonized Staphylococcus aureus, (b) serum- and IgG-opsonized latex particles, and (c) synthetic soluble and insoluble ICs to mimic bacterial and inflammatory stimuli.


RESULTS
Phosphatidylinositol-phospholipase C treatment removed >97% of surface FcgammaRIIIb from neutrophils previously treated with tumor necrosis factor alpha to mobilize intracellular stores of receptor. This treatment profoundly inhibited activation of primed neutrophils by soluble ICs of the type found in diseased rheumatoid joints, but had no effect on phagocytosis and killing of serum-opsonized S aureus.


CONCLUSION
FcgammaRIIIb plays a major role in the secretion of toxic products in response to ICs, but little or no role in the phagocytosis and killing of serum-opsonized bacteria. The selective suppression of effector neutrophil function is therefore possible. FcgammaRIIIb, or its intracellular signaling pathway, is a potential therapeutic target in inflammatory diseases such as rheumatoid arthritis, because disruption of its function should decrease inflammatory tissue damage, but not jeopardize host protection against infection.",2002-05-01,https://www.semanticscholar.org/paper/548a2bb4fd5d2ab36ef5864ae69e0366d9d1fbc5,Arthritis & Rheumatism
2798,Galectin 3 Regulates HCC cell invasion by RhoA and MLCK activation,,2015-05-22,https://www.semanticscholar.org/paper/e7b9fbf35a24240e9c75f1c248c144722922ba10,Laboratory Investigation
1724,Topographic Factor Analysis: A Bayesian Model for Inferring Brain Networks from Neural Data,"The neural patterns recorded during a neuroscientific experiment reflect complex interactions between many brain regions, each comprising millions of neurons. However, the measurements themselves are typically abstracted from that underlying structure. For example, functional magnetic resonance imaging (fMRI) datasets comprise a time series of three-dimensional images, where each voxel in an image (roughly) reflects the activity of the brain structure(s)–located at the corresponding point in space–at the time the image was collected. FMRI data often exhibit strong spatial correlations, whereby nearby voxels behave similarly over time as the underlying brain structure modulates its activity. Here we develop topographic factor analysis (TFA), a technique that exploits spatial correlations in fMRI data to recover the underlying structure that the images reflect. Specifically, TFA casts each brain image as a weighted sum of spatial functions. The parameters of those spatial functions, which may be learned by applying TFA to an fMRI dataset, reveal the locations and sizes of the brain structures activated while the data were collected, as well as the interactions between those structures.",2014-05-07,https://www.semanticscholar.org/paper/f479505354f332f692c4775dc91d90758e8379f4,PLoS ONE
2870,"Galectins-3 and -7, but not Galectin-1, Play a Role in Re-epithelialization of Wounds*","Disorders of wound healing characterized by impaired or delayed re-epithelialization are a serious medical problem. These conditions affect many tissues, are painful, and are difficult to treat. In this study, using cornea as a model, we demonstrate for the first time the importance of carbohydrate-binding proteins galectins-3 and -7 in re-epithelialization of wounds. In two different models of corneal wound healing, re-epithelialization of wounds was significantly slower in galectin-3-deficient (gal3−/−) mice compared with wild-type (gal3+/+) mice. In contrast, there was no difference in corneal epithelial wound closure rates between galectin-1-deficient and wild-type mice. Quantitation of the bromodeoxyuridine-labeled cells in gal3+/+ and gal3−/− corneas revealed that corneal epithelial cell proliferation rate is not perturbed in gal3−/− corneas. Exogenous galectin-3 accelerated re-epithelialization of wounds in gal3+/+ mice but, surprisingly, not in the gal3−/− mice. Gene expression analysis using cDNA microarrays revealed that healing corneas of gal3−/− mice contain markedly reduced levels of galectin-7 compared with those of gal3+/+ mice. More importantly, unlike galectin-3, galectin-7 accelerated re-epithelialization of wounds in both gal3−/− and gal3+/+ mice. In corresponding experiments, recombinant galectin-1 did not stimulate the corneal epithelial wound closure rate. The extent of acceleration of re-epithelialization of wounds with both galectin-3 and galectin-7 was greater than that observed in most of the published studies using growth factors. These findings have broad implications for developing novel therapeutic strategies for treating nonhealing wounds.",2002-11-01,https://www.semanticscholar.org/paper/6264124fdfd598b5564747e43f70e04478af042b,Journal of Biological Chemistry
586,Communication complexity,"In this paper we prove several results concerning this complexity measure. First we establish (in a non-constructive manner) that there exist languages which cannot be recognized with less than <italic>n</italic> communication (obviously, communication <italic>n</italic> is always enough for recognizing any language). In fact, we show that for any function<italic>f(n)-&-lt; n,</italic> there are languages recognizable with communication<italic>f(n)</italic> but not with communication<italic>f (n)</italic>-&-minus;1. In other words, this complexity measure possesses a very dense hierarchy or complexity classes, as miniscule increments in communication add to the languages that can be recognized.",1982-05-05,https://www.semanticscholar.org/paper/0ef9546e001dbdb7ba13818feb1f9b2f442f2ce5,Symposium on the Theory of Computing
3282,HotSpotter — Patterned species instance recognition,"We present HotSpotter, a fast, accurate algorithm for identifying individual animals against a labeled database. It is not species specific and has been applied to Grevy's and plains zebras, giraffes, leopards, and lionfish. We describe two approaches, both based on extracting and matching keypoints or “hotspots”. The first tests each new query image sequentially against each database image, generating a score for each database image in isolation, and ranking the results. The second, building on recent techniques for instance recognition, matches the query image against the database using a fast nearest neighbor search. It uses a competitive scoring mechanism derived from the Local Naive Bayes Nearest Neighbor algorithm recently proposed for category recognition. We demonstrate results on databases of more than 1000 images, producing more accurate matches than published methods and matching each query image in just a few seconds.",2013-01-15,https://www.semanticscholar.org/paper/cf0a5d6667d43f31215a8c92c236e0ddb3ddb8dd,2013 IEEE Workshop on Applications of Computer Vision (WACV)
3606,Abstraction and the C++ Machine Model,,2004-12-09,https://www.semanticscholar.org/paper/dcb38a10f1a634dc82b535527c58791dbae94429,International Conference on Embedded Software and Systems
3131,Web browsing performance of wireless thin-client computing,"Web applications are becoming increasingly popular for mobile wireless systems. However, wireless networks can have high packet loss rates, which can degrade web browsing performance on wireless systems. An alternative approach is wireless thin-client computing, in which the web browser runs on a remote thin server with a more reliable wired connection to the Internet. A mobile client then maintains a connection to the thin server to receive display updates over the lossy wireless network. To assess the viability of this thin-client approach, we compare the web browsing performance of thin clients against fat clients that run the web browser locally in lossy wireless networks. Our results show that thin clients can operate quite effectively over lossy networks. Compared to fat clients running web browsers locally, our results show surprisingly that thin clients can be faster and more resilient on web applications over lossy wireless LANs despite having to send more data over the network. We characterize and analyze different design choices in various thin-client systems and explain why these approaches can yield superior web browsing performance in lossy wireless networks.",2003-05-20,https://www.semanticscholar.org/paper/5bdbd4c2457d0610ead68d064bc26e60654903d6,The Web Conference
2149,Directly Lower Bounding the Information Capacity for Channels With I.I.D. Deletions and Duplications,"In this paper, we directly lower bound the information capacity for channels with independent identically distributed (i.i.d.) deletions and duplications. Our approach differs from previous work in that we focus on the information capacity using ideas from renewal theory, rather than focusing on the transmission capacity by analyzing the error probability of some randomly generated code using a combinatorial argument. Of course, the transmission and information capacities are equal, but our change of perspective allows for a much simpler analysis that gives more general theoretical results. We then apply these results to the binary deletion channel to improve existing lower bounds on its capacity.",2007-06-24,https://www.semanticscholar.org/paper/64d382fb00a3677109dec9bd5d08f45452d74fe0,IEEE Transactions on Information Theory
2248,Replication of Colonic Crohn's Disease Mucosal Escherichia coli Isolates within Macrophages and Their Susceptibility to Antibiotics,"ABSTRACT There is increasing evidence that Escherichia coli organisms are important in Crohn's disease (CD) pathogenesis. In CD tissue they are found within macrophages, and the adherent-invasive CD ileal E. coli isolate LF82 can replicate inside macrophage phagolysosomes. This study investigates replication and antibiotic susceptibility of CD colonic E. coli isolates inside macrophages. Replication of CD colonic E. coli within J774-A1 murine macrophages and human monocyte-derived macrophages (HMDM) was assessed by culture and lysis after gentamicin killing of noninternalized bacteria and verified by electron microscopy (EM). All seven CD colonic isolates tested replicated within J774-A1 macrophages by 3 h (6.36-fold ± 0.7-fold increase; n = 7 isolates) to a similar extent to CD ileal E. coli LF82 (6.8-fold ± 0.8-fold) but significantly more than control patient isolates (5.2-fold ± 0.25-fold; n = 6; P = 0.006) and E. coli K-12 (1.0-fold ± 0.1-fold; P < 0.0001). Replication of CD E. coli HM605 within HMDM (3.9-fold ± 0.7-fold) exceeded that for K-12 (1.4-fold ± 0.2-fold; P = 0.03). EM showed replicating E. coli within macrophage vacuoles. Killing of HM605 within J774-A1 macrophages following a 3-h incubation with antibiotics at published peak serum concentrations (Cmax) was as follows: for ciprofloxacin, 99.5% ± 0.2%; rifampin, 85.1% ± 6.6%; tetracycline, 62.8% ± 6.1%; clarithromycin, 62.1% ± 5.6% (all P < 0.0001); sulfamethoxazole, 61.3% ± 7.0% (P = 0.0007); trimethoprim, 56.3% ± 3.4% (P < 0.0001); and azithromycin, 41.0% ± 10.5% (P = 0.03). Ampicillin was not effective against intracellular E. coli. Triple antibiotic combinations were assessed at 10% Cmax, with ciprofloxacin, tetracycline, and trimethoprim causing 97% ± 0.0% killing versus 86% ± 2.0% for ciprofloxacin alone. Colonic mucosa-associated E. coli, particularly CD isolates, replicate within macrophages. Clinical trials are indicated to assess the efficacy of a combination antibiotic therapy targeting intramacrophage E. coli.",2007-12-10,https://www.semanticscholar.org/paper/b054ce15e0ddbf2485ddf65d8dd5598b617237e0,Antimicrobial Agents and Chemotherapy
2840,Lack of galectin‐3 alters the balance of innate immune cytokines and confers resistance to Rhodococcus equi infection,"Galectin‐3 is a β‐galactoside‐binding lectin implicated in the fine‐tuning of innate immunity. Rhodococcus equi, a facultative intracellular bacterium of macrophages, causes severe granulomatous bronchopneumonia in young horses and immunocompromised humans. The aim of this study is to investigate the role of galectin‐3 in the innate resistance mechanism against R. equi infection. The bacterial challenge of galectin‐3‐deficient mice (gal3−/−) and their wild‐type counterpart (gal3+/+) revealed that the LD50 for the gal3−/− mice was about seven times higher than that for the gal3+/+ mice. When challenged with a sublethal dose, gal3−/− mice showed lower bacteria counts and higher production of IL‐12 and IFN‐γ production, besides exhibiting a delayed although increased inflammatory reaction. Gal3−/− macrophages exhibited a decreased frequency of bacterial replication and survival, and higher transcript levels of IL‐1β, IL‐6, IL‐10, TLR2 and MyD88. R. equi‐infected gal3+/+ macrophages showed decreased expression of TLR2, whereas R. equi‐infected gal3−/− macrophages showed enhanced expression of this receptor. Furthermore, galectin‐3 deficiency in macrophages may be responsible for the higher IL‐1β serum levels detected in infected gal3−/− mice. Therefore galectin‐3 may exert a regulatory role in innate immunity by diminishing IL‐1β production and thus affecting resistance to R. equi infection.",2008-10-01,https://www.semanticscholar.org/paper/4cf31d4d82136ea0fed05cb06f16ba6c74164eed,European Journal of Immunology
82,Simplifying Data Access: The Energy Data Collection Project,"Using technology developed at the Digital Government Research Center, a team of researchers is seeking to make government statistical data more accessible through the Internet. In collaboration with government experts, they are conducting research into advanced information systems, developing standards, interfaces and a shared infrastructure, and building and managing pilot systems.",2001-02-01,https://www.semanticscholar.org/paper/19e71f233fa1f862623d841743f02fd26ad0a07f,Computer
103,Snowball: extracting relations from large plain-text collections,"Text documents often contain valuable structured data that is hidden Yin regular English sentences. This data is best exploited infavailable as arelational table that we could use for answering precise queries or running data mining tasks.We explore a technique for extracting such tables from document collections that requires only a handful of training examples from users. These examples are used to generate extraction patterns, that in turn result in new tuples being extracted from the document collection.We build on this idea and present our Snowball system. Snowball introduces novel strategies for generating patterns and extracting tuples from plain-text documents.At each iteration of the extraction process, Snowball evaluates the quality of these patterns and tuples without human intervention,and keeps only the most reliable ones for the next iteration. In this paper we also develop a scalable evaluation methodology and metrics for our task, and present a thorough experimental evaluation of Snowball and comparable techniques over a collection of more than 300,000 newspaper documents.",2000-06-01,https://www.semanticscholar.org/paper/cee045e890270abae65455667b292db355d53728,Digital library
3009,Encrypted cloud photo storage using Google photos,"Cloud photo services are widely used for persistent, convenient, and often free photo storage, which is especially useful for mobile devices. As users store more and more photos in the cloud, significant privacy concerns arise because even a single compromise of a user's credentials give attackers unfettered access to all of the user's photos. We have created Easy Secure Photos (ESP) to enable users to protect their photos on cloud photo services such as Google Photos. ESP introduces a new client-side encryption architecture that includes a novel format-preserving image encryption algorithm, an encrypted thumbnail display mechanism, and a usable key management system. ESP encrypts image data such that the result is still a standard format image like JPEG that is compatible with cloud photo services. ESP efficiently generates and displays encrypted thumbnails for fast and easy browsing of photo galleries from trusted user devices. ESP's key management makes it simple to authorize multiple user devices to view encrypted image content via a process similar to device pairing, but using the cloud photo service as a QR code communication channel. We have implemented ESP in a popular Android photos app for use with Google Photos and demonstrate that it is easy to use and provides encryption functionality transparently to users, maintains good interactive performance and image quality while providing strong privacy guarantees, and retains the sharing and storage benefits of Google Photos without any changes to the cloud service.",2021-06-24,https://www.semanticscholar.org/paper/73d77e49560e637fe6bb6d82a965bd149a5e47d4,"ACM SIGMOBILE International Conference on Mobile Systems, Applications, and Services"
210,"Evolution and Learning: Used Together, Fused Together. A Response to Watson and Szathmáry.",,2016-12-01,https://www.semanticscholar.org/paper/d7be5f1fef6090298eb9b04d579d852cd744e3f7,Trends in Ecology & Evolution
340,Three-Player Games Are Hard,"We prove that computing a Nash equilibrium in a 3-player game is PPAD-complete, solving a problem left open in (2).",,https://www.semanticscholar.org/paper/ec72f622b8c5041a9ae7150220b5525283d612d1,Electron. Colloquium Comput. Complex.
2937,Opportunities and challenges for transcriptome-wide association studies,,2019-03-29,https://www.semanticscholar.org/paper/e7db05c36efd6aabb0bd793fda02e6f82d5a0d1d,Nature Genetics
1044,Projected sensitivities of the LUX-ZEPLIN experiment to new physics via low-energy electron recoils,"LUX-ZEPLIN (LZ) is a dark matter detector expected to obtain world-leading sensitivity to weakly interacting massive particles (WIMPs) interacting via nuclear recoils with a ~7-tonne xenon target mass. This manuscript presents sensitivity projections to several low-energy signals of the complementary electron recoil signal type: 1) an effective neutrino magnetic moment and 2) an effective neutrino millicharge, both for pp-chain solar neutrinos, 3) an axion flux generated by the Sun, 4) axion-like particles forming the galactic dark matter, 5) hidden photons, 6) mirror dark matter, and 7) leptophilic dark matter. World-leading sensitivities are expected in each case, a result of the large 5.6t 1000d exposure and low expected rate of electron recoil backgrounds in the $<$100keV energy regime. A consistent signal generation, background model and profile-likelihood analysis framework is used throughout.",2021-02-23,https://www.semanticscholar.org/paper/16174d4ab330427e8100d1b551359c46121c875d,Physical Review D
2235,Neutrophil function in inflammation and inflammatory diseases.,"In inflammatory conditions such as RA, the neutrophil has tended to be dismissed as a short-lived, terminally differentiated, irrelevant bystander cell. However, this is clearly not the case. A better understanding of the complex heterogeneous pathways and processes that constitute RA, in parallel with a more sophisticated knowledge of neutrophil biology has identified many potential roles for these cells in the persistence of inflammation and progression of joint damage, which should not be underestimated. Not only are neutrophils found in high numbers within the rheumatoid joint, both in synovial tissue and in joint fluid, they have a huge potential to directly inflict damage to tissue, bone and cartilage via the secretion of proteases and toxic oxygen metabolites, as well as driving inflammation through antigen presentation and secretion of cytokines, chemokines, prostaglandins and leucotrienes. Drugs already used to treat RA down-regulate many neutrophil functions, including migration to the joint, degranulation and production of inflammatory mediators, and these cells should be considered as important targets for the development of new therapies in the future.",2010-09-01,https://www.semanticscholar.org/paper/f7a35e22338cbf7d4e37140df4ea26aba878eb62,Rheumatology
1180,Search for a resonance decaying into WZ boson pairs in pp collisions.,"We present the first search for an electrically charged resonance W' decaying to a WZ boson pair using 4.1 fb(-1) of integrated luminosity collected with the D0 detector at the Fermilab Tevatron pp collider. The WZ pairs are reconstructed through their decays into three charged leptons (l=e, mu). A total of 9 data events is observed in good agreement with the background prediction. We set 95% C.L. limits on the W'WZ coupling and on the W' production cross section multiplied by the branching fractions. We also exclude W' masses between 188 and 520 GeV within a simple extension of the standard model and set the most restrictive limits to date on low-scale technicolor models.",2009-12-03,https://www.semanticscholar.org/paper/c28d43fc66b8c81880cfea1a805ac0d843cdc9db,Physical Review Letters
2817,Lack of Galectin-3 Disturbs Mesenteric Lymph Node Homeostasis and B Cell Niches in the Course of Schistosoma mansoni Infection,"Galectin-3 is a β-galactoside-binding protein that has been shown to regulate pathophysiological processes, including cellular activation, differentiation and apoptosis. Recently, we showed that galectin-3 acts as a potent inhibitor of B cell differentiation into plasma cells. Here, we have investigated whether galectin-3 interferes with the lymphoid organization of B cell compartments in mesenteric lymph nodes (MLNs) during chronic schistosomiasis, using WT and galectin-3-/- mice. Schistosoma mansoni synthesizes GalNAcβ1-4(Fucα1-3)GlcNAc(Lac-DiNAc) structures (N-acetylgalactosamine β1-4 N-acetylglucosamine), which are known to interact with galectin-3 and elicit an intense humoral response. Antigens derived from the eggs and adult worms are continuously drained to MLNs and induce a polyclonal B cell activation. In the present work, we observed that chronically-infected galectin-3-/- mice exhibited a significant reduced amount of macrophages and B lymphocytes followed by drastic histological changes in B lymphocyte and plasma cell niches in the MLNs. The lack of galectin-3 favored an increase in the lymphoid follicle number, but made follicular cells more susceptible to apoptotic stimuli. There were an excessive quantity of apoptotic bodies, higher number of annexin V+/PI- cells, and reduced clearance of follicular apoptotic cells in the course of schistosomiasis. Here, we observed that galectin-3 was expressed in non-lymphoid follicular cells and its absence was associated with severe damage to tissue architecture. Thus, we convey new information on the role of galectin-3 in regulation of histological events associated with B lymphocyte and plasma cell niches, apoptosis, phagocytosis and cell cycle properties in the MLNs of mice challenged with S.mansoni.",2011-05-06,https://www.semanticscholar.org/paper/134b8458734b3c96ebdf116f85f499222bced7b7,PLoS ONE
2295,Stimulation of Intracellular Ca2+ Levels in Human Neutrophils by Soluble Immune Complexes,"Soluble immune complexes bind to unprimed neutrophils and generate intracellular Ca2+transients but fail to activate the NADPH oxidase. Following priming of the neutrophils with either tumor necrosis factor α or granulocyte-macrophage colony-stimulating factor, stimulation of the cells with the soluble immune complexes leads to an enhanced Ca2+ signal and significant secretion of reactive oxidants. The enhanced Ca2+ signal observed in primed neutrophils results from the influx of Ca2+ from the external environment and is partly sensitive to tyrosine kinase inhibitors. This is in contrast to the Ca2+ signal observed in unprimed neutrophils, which arises from the mobilization of intracellular stores. When the surface expression of FcγRIIIb on primed neutrophils was decreased either through incubation with Pronase or phosphoinositide-specific phospholipase C, the extra enhanced Ca2+ mobilization seen in primed cells was significantly lowered, while the initial rise in intracellular Ca2+ was unaffected. Depletion of FcγRIIIb had no significant effect on the Ca2+ transients in unprimed neutrophils. Cross-linking FcγRII, but not FcγRIIIb, induced increases in intracellular Ca2+ in unprimed neutrophils, while cross-linking either of these receptors increased Ca2+ levels in primed neutrophils. The FcγRII-dependent intracellular Ca2+ rise in primed cells was unaffected by incubation in Ca2+-free medium, whereas the FcγRIIIb-dependent transient was significantly decreased when Ca2+ influx was prevented in Ca2+-free medium supplemented with EGTA. Cross-linking either FcγRII or FcγRIIIb in primed or unprimed cells failed to stimulate substantial levels of inositol 1,4,5-trisphosphate production. These results indicate that following stimulation of primed neutrophils with soluble immune complexes the enhanced Ca2+ mobilization observed is the result of a functional activation of the glycosylphosphatidylinositol-linked FcγRIIIb.",1997-07-18,https://www.semanticscholar.org/paper/2b09547913a5f27e2c24b8888db636fad775c082,Journal of Biological Chemistry
2778,"Intensity-dependent gamma electrical stimulation regulates microglial activation, reduces beta-amyloid load, and facilitates memory in a mouse model of Alzheimer’s disease",,2023-07-28,https://www.semanticscholar.org/paper/5eeba8728650e8a9443e5532e5ed1489ada8e0e4,Cell & Bioscience
2682,A standard reference model for intelligent multimedia presentation systems,,1997-12-01,https://www.semanticscholar.org/paper/16143a7302880cdd423131b00167ede701dd75a8,Comput. Stand. Interfaces
971,Optic Disc-guided Optical Coherence Tomography Interpretation for Diagnosis of Early-glaucoma: Selecting the Optimal Parameters,,2019-06-01,https://www.semanticscholar.org/paper/4cc9d080731b41608a328173082945fdc6c3a9f5,Journal of the Korean Glaucoma Society
3527,Fast approximation algorithms for multicommodity flow problems,"All previously known algorithms for solving the multicommodity flow problem with capacities are based on linear programming. The best of these algorithms uses a fast matrix multiplication algorithm and takes O(k3.5n3m0.5 log(nDU)) time for the multicommodity flow problem with integer demands and at least O(k2.5n2m0.5 log(n��1DU)) time to find an approximate solution, where k is the number of commodities, n and m denote the number of nodes and edges in the network, D is the largest demand, and U is the largest edge capacity. As a consequence, even multicommodity flow problems with just a few commodities are believed to be much harder than single-commodity maximum-flow or minimum-cost flow problems. In this paper, we describe the first polynomial-time combinatorial algorithms for approximately solving the multicommodity flow problem. The running time of our randomized algorithm is (up to log factors) the same as the time needed to solve k single-commodity flow problems, thus giving the surprising result that approximately computing a k-commodity maximum-flow is not much harder than computing about k single-commodity maximum-flows in isolation. In fact, we prove that a (simple) k-commodity flow problem can be approximately solved by approximately solving O(k log2n) single-commodity minimum-cost flow problems. Our k-commodity algorithm runs in O (knm log4n) time with high probability. We also describe a deterministic algorithm that uses an O(k)-factor more time. Given any multicommodity flow problem as input, both algorithms are guaranteed to provide a feasible solution to a modified flow problem in which all capacities are increased by a (1 + �)-factor, or to provide a proof that there is no feasible solution to the original problem. We also describe faster approximation algorithms for multicommodity flow problems with a special structure, such as those that arise in ""sparsest cut"" problems and uniform concurrent flow problems.",1991-01-03,https://www.semanticscholar.org/paper/32307dd1f82f26b0ee13fb4e417a98e86d18bb62,Symposium on the Theory of Computing
695,Center-Embedding and Constituency in the Brain and a New Characterization of Context-Free Languages,"A computational system implemented exclusively through the spiking of neurons was re-cently shown capable of syntax, that is, of carrying out the dependency parsing of simple English sentences. We address two of the most important questions left open by that work: constituency (the identification of key parts of the sentence such as the verb phrase) and the processing of dependent sentences, especially center-embedded ones. We show that these two aspects of language can also be implemented by neurons and synapses in a way that is com-patible with what is known, or widely believed, about the structure and function of the language organ 1 . Surprisingly, the way we implement center embedding points to a new characterization of context-free languages.",2022-06-27,https://www.semanticscholar.org/paper/9bae9312babddd5707e38f81fcba6579fae6ad82,NALOMA
2470,Virtual Replicas for Remote Assistance in Virtual and Augmented Reality,"In many complex tasks, a remote subject-matter expert may need to assist a local user to guide actions on objects in the local user's environment. However, effective spatial referencing and action demonstration in a remote physical environment can be challenging. We introduce two approaches that use Virtual Reality (VR) or Augmented Reality (AR) for the remote expert, and AR for the local user, each wearing a stereo head-worn display. Both approaches allow the expert to create and manipulate virtual replicas of physical objects in the local environment to refer to parts of those physical objects and to indicate actions on them. This can be especially useful for parts that are occluded or difficult to access. In one approach, the expert points in 3D to portions of virtual replicas to annotate them. In another approach, the expert demonstrates actions in 3D by manipulating virtual replicas, supported by constraints and annotations. We performed a user study of a 6DOF alignment task, a key operation in many physical task domains, comparing both approaches to an approach in which the expert uses a 2D tablet-based drawing system similar to ones developed for prior work on remote assistance. The study showed the 3D demonstration approach to be faster than the others. In addition, the 3D pointing approach was faster than the 2D tablet in the case of a highly trained expert.",2015-11-05,https://www.semanticscholar.org/paper/90a67bc508e86c6cf20b14955820d6fa0be5614b,ACM Symposium on User Interface Software and Technology
1766,The Discrete Infinite Logistic Normal Distribution for Mixed-Membership Modeling,"We present the discrete innite logistic normal distribution (DILN), a Bayesian nonparametric prior for mixed membership models. DILN generalizes the hierarchical Dirichlet process (HDP) to model correlation structure between the weights of the atoms at the group level. We derive a representation of DILN as a normalized collection of gamma-distributed random variables and study its statistical properties. We derive a variational inference algorithm for approximate posterior inference. We apply DILN to topic modeling of documents and study its empirical performance on four corpora, comparing performance with the HDP and the correlated topic model (CTM). To compute with large-scale data, we develop a stochastic variational inference algorithm for DILN and compare with similar algorithms for HDP and latent Dirichlet allocation (LDA) on a collection of 350; 000 articles from Nature.",2011-03-24,https://www.semanticscholar.org/paper/593cb435702d2e6e7ed8c84dec1c3154bcc11df9,International Conference on Artificial Intelligence and Statistics
721,On Complexity as Bounded Rationality1,,,https://www.semanticscholar.org/paper/cbaeb3cf17ad447b368fcdf9d1fbec6a321b23fc,Symposium on the Theory of Computing
933,Testing the Universal Instance Assumption,,1980-02-12,https://www.semanticscholar.org/paper/a25d4cf6ad249170199281569388c785212bd7ff,Information Processing Letters
2314,Interleukin-1 expression by neutrophils in rheumatoid arthritis.,"OBJECTIVE--To determine if neutrophils from blood and synovial fluid of patients with rheumatoid arthritis and other joint arthropathies express interleukin-1 beta mRNA. METHODS--RNA was isolated from neutrophils from patient and control blood, and synovial fluid of patients, probed in northern blots, and quantified by densitometry. It was also isolated and analysed from control blood neutrophils after incubation in vitro with granulocyte macrophage colony stimulating factor (GM-CSF). RESULTS--Neutrophils from the synovial fluid of patients with rheumatoid arthritis contained low levels of mRNA for interleukin-1 beta--between 0.1 and 2% of those observed during stimulation of control neutrophils with GM-CSF for one hour. Higher levels (4-40% of the maximal GM-CSF values) were observed in blood neutrophils from patients with rheumatoid arthritis. CONCLUSIONS--Neutrophils contribute to the cytokine network in rheumatoid arthritis. In some circumstances, activation of transcription may occur within the circulation of these patients.",1995-11-01,https://www.semanticscholar.org/paper/949adcc803a7c3a9c15ba40b07cba056b376fa8c,Annals of the Rheumatic Diseases
709,Fixed Point Computation Problems and Facets of Complexity (Invited Talk),"Many problems from a wide variety of areas can be formulated mathematically as the problem of computing a fixed point of a suitable given multivariate function. Examples include a variety of problems from game theory, economics, optimization, stochastic analysis, verification, and others. In some problems there is a unique fixed point (for example if the function is a contraction); in others there may be multiple fixed points and any one of them is an acceptable solution; while in other cases the desired object is a specific fixed point (for example the least fixed point or greatest fixed point of a monotone function). In this talk we will discuss several types of fixed point computation problems, their complexity, and some of the common themes that have emerged: classes of problems for which there are efficient algorithms, and other classes for which there seem to be serious obstacles. 2012 ACM Subject Classification Theory of computation → Complexity theory and logic",,https://www.semanticscholar.org/paper/feb7d5c39ce504385b820b9739f0905e9a929e02,"International Colloquium on Automata, Languages and Programming"
559,Algorithmic aspects of multiversion concurrency control,"Multiversion schedulers are now a widely accepted method for enhancing the performance of the concurrency control component of a database. In this paper we introduce a new notion of multiversion serializability (MVSR) based on conflicts (MVCSR), and discuss its relation with the well known single version conflict serializability (CSR). On-line schedulable (OLS) subsets of (MVSR) were defined in Papadimitriou and Kanellakis, ACM Trans. Database Systems 9, No. 1 (1984). We prove there that it is NP-complete to decide whether a set of schedules is OLS. We next introduce the concept of maximal OLS sets, and show that no efficient scheduler can be designed that recognizes maximal subsets of the MVSR or MVCSR schedules.",1985-03-25,https://www.semanticscholar.org/paper/7bcb7dbd2ac8e5ea52d37f909e189116b6d2e5a1,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems
3753,Following Gaze Across Views,"Following the gaze of people inside videos is an important signal for understanding people and their actions. In this paper, we present an approach for following gaze across views by predicting where a particular person is looking throughout a scene. We collect VideoGaze, a new dataset which we use as a benchmark to both train and evaluate models. Given one view with a person in it and a second view of the scene, our model estimates a density for gaze location in the second view. A key aspect of our approach is an end-to-end model that solves the following sub-problems: saliency, gaze pose, and geometric relationships between views. Although our model is supervised only with gaze, we show that the model learns to solve these subproblems automatically without supervision. Experiments suggest that our approach follows gaze better than standard baselines and produces plausible results for everyday situations.",2016-12-09,https://www.semanticscholar.org/paper/1145d3d5c96157a9b19c1bedb090a5157537ad97,arXiv.org
3236,Consistent individual variation across interaction networks indicates social personalities in lemurs,,2017-12-01,https://www.semanticscholar.org/paper/1ff498e835e28a105e8afe741f03aa55e8a2c984,Animal Behaviour
2598,Personalized Digital Television,,,https://www.semanticscholar.org/paper/3f8f98c12a335c25c7a0ce434efca47490018aed,Human-Computer Interaction Series
825,The complexity of probabilistic verification,"We determine the complexity of testing whether a finite state, sequential or concurrent probabilistic program satisfies its specification expressed in linear-time temporal logic. For sequential programs, we present an algorithm that runs in time linear in the program and exponential in the specification, and also show that the problem is in PSPACE, matching the known lower bound. For concurrent programs, we show that the problem can be solved in time polynomial in the program and doubly exponential in the specification, and prove that it is complete for double exponential time. We also address these questions for specifications described by ω-automata or formulas in extended temporal logic.",1995-07-01,https://www.semanticscholar.org/paper/561a79c9309d9e0338c06e4939056bb931d080ea,JACM
661,Intracoronary stenting as an adjunct to angioplasty in acute myocardial infarction.,,1991-11-01,https://www.semanticscholar.org/paper/ea332dcd5369b8b97cead38b98a3075352cc560b,The Journal of invasive cardiology
1544,A Proxy Variable View of Shared Confounding,"Causal inference from observational data can be biased by unobserved confounders. Confounders—the variables that affect both the treatments and the outcome—induce spurious non-causal correlations between the two. Without additional conditions, unobserved confounders generally make causal quantities hard to identify. In this paper, we focus on the setting where there are many treatments with shared confounding, and we study under what conditions is causal identification possible. The key observation is that we can view subsets of treatments as proxies of the unobserved confounder and identify the intervention distributions of the rest. Moreover, while existing identification formulas for proxy variables involve solving integral equations, we show that one can circumvent the need for such solutions by directly modeling the data. Finally, we extend these results to an expanded class of causal graphs, those with other confounders and selection variables.",,https://www.semanticscholar.org/paper/87f8173265cfe3b077e53d09fd3598f14f2667e4,International Conference on Machine Learning
2194,Human neutrophils in auto-immunity.,,2016-04-01,https://www.semanticscholar.org/paper/84651a2aa6278cbf0b7b6151d7ac5d2e1133ae4b,Seminars in Immunology
2700,Managing networks through a virtual world,"As network services become more sophisticated to handle multimedia exchange, demands for real-time network control are increasing. With 2D workstations, managers can observe network activities only at a distance. Direct observation requires a 3D interface combined with a structured control architecture. >",1995-06-01,https://www.semanticscholar.org/paper/4b9a15dd00a738b6425cf0de6b45d07b802fad6f,IEEE Parallel Distributed Technol. Syst. Appl.
937,Locking policies: Safety and freedom from deadlock,"A database consists of ellliflc.\' yvhich reLlte to each other in certain ways, i,e., they satisfy cerltlin cOllsistency constraints. Many tinles, when a user updates the database, he nlay have to update tcnlporarily these constraints in orde r tC) eventuaII y t I'an s1'0 I' 111 the database in to a new, consis ten t stat C . For this I'eas 0 n, at 0 nl ic act ion s by the sa nlC user arc grou ped toget her into un its of consistency called transactiolls. In practice. a transaction nlay be either an interactive session, or the execution of a user update progranl. When, however, nlan y' transactions access and update the SanlC database cOI1curTently, there rnust he sonle kind of coordination to ensure that the resulting sequence of interleaved atonlic actions (or schedule) is correct. This TlleanS that all transactions have a consistent view of the data. and furthernlore the database is Icft at the end in sonle consistent state, This required coordination is achic\cd via the COIlcurrency cOlltrol,nechalllsfn ()f the database. ('onsiderahle research effort has heen devoted recently to the theoretical aspects or the design of such a systenl !ECiLTl. SLR, SK, KS, Pa, PBR, KPI. The theory of databasc concurrency control bears a superficial silllilarity to the () pe ra ting systenl Sinspi I'ed con cLI rrency 1he 0 I'Y [K [vI, (' [) 1. The difference is lhtl{ in operating systeIllS \\le have cooperating, Ill0nitoring. dnd 1110n itored. processes, and the goal is to prevent had cooperation or Tllanagenlent (e.g. indetcrnlinacy. deadlocks) In databases, we have a population of' users that arc una\\'are of each other's pres-",1979-10-01,https://www.semanticscholar.org/paper/05f213d51dc7ea3fadbb4794256ec24d774b5533,20th Annual Symposium on Foundations of Computer Science (sfcs 1979)
2929,Distinct Classes of Complex Structural Variation Uncovered across Thousands of Cancer Genome Graphs,,2020-10-01,https://www.semanticscholar.org/paper/69c0c194ac64239a6149fbe07e45671c35ea61ba,Cell
2886,"Evidence for IgG autoantibodies to galectin-3, a beta-galactoside-binding lectin (Mac-2, epsilon binding protein, or carbohydrate binding protein 35) in human serum.","Galectin-3 is a beta-galactoside-binding animal lectin formerly called epsilon protein, Mac-2, carbohydrate binding protein 35, CBH 30, L-29, or L34. The possible occurrence of autoantibodies to galectin-3 was investigated because crosslinking of galectins bound to IgE or Fc epsilon RI might produce mediator release from mast cells or basophils. Unexpectedly, a control serum from an individual free of current allergic symptoms was found to have a significantly elevated level of IgG anti-galectin-3 by ELISA employing galectin-3-coated wells incubated with test serum followed by HRPO-conjugated goat anti-human IgG. The reaction was not inhibitable by lactose, suggesting that it is not a result of binding of IgG by galectin-3 through lectin-carbohydrate interactions. The antibody activity was specifically adsorbed by galectin-3 and protein A-conjugated Sepharose and was associated primarily with subclass IgG1. The presence of the antibodies was confirmed by immunoblotting showing binding of IgG to the 30-kD galectin-3 band. The relevant epitopes were in the galectin-3 N-terminal domain. The propositus was subsequently found to have adenocarcinoma of the colon, and titers of IgG anti-galectin-3 were found to be sharply elevated after hemicolectomy. Similar antibody titers have not been found in family members, but small numbers of normal persons and patients with malignant neoplasms have been found to have evidence of IgG anti-galectin-3 antibodies at lower titers than the propositus. The pathogenesis of this autoimmune reaction is unclear, though there is a trend for it to occur in older persons.",,https://www.semanticscholar.org/paper/f978c88e82407e29f98f40659a07f3adaf43c1a3,Journal of Clinical Immunology
101,Combining Strategies for Extracting Relations from Text Collections,"Text documents often contain valuable structured data that is hidden in regular English sentences. This data is best exploited if available as a relational table that we could use for answering precise queries or for running data mining tasks. Our Snowball system extracts these relations from document collections starting with only a handful of user-provided example tuples. Based on these tuples, Snowball generates patterns that are used, in turn, to find more tuples. In this paper we introduce a new pattern and tuple generation scheme for Snowball, with different strengths and weaknesses than those of our original system. We also show preliminary results on how we can combine the two versions of Snowball to extract tuples more accurately.",,https://www.semanticscholar.org/paper/98a39e8a2d31d0a6adeccb86effb990067d6ab85,ACM SIGMOD Workshop on Research Issues in Data Mining and Knowledge Discovery
1591,Matrix Factorization,,,https://www.semanticscholar.org/paper/188933428ec7e2776c6e83407a5a4768c4a0ef62,Encyclopedia of Social Network Analysis and Mining. 2nd Ed.
151,"Design, architecture and control of a mobile site-modeling robot","A distributed, modular, heterogeneous architecture is presented that illustrates an approach to solving and integrating common tasks in mobile robotics, such as path planning, localization, sensor fusion, environmental modeling, and motion control. Experimental results are shown for an autonomous navigation task to confirm the applicability of our approach.",2000-04-24,https://www.semanticscholar.org/paper/9bd959bac105c8a25cb50776cc25e6090754f035,Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No.00CH37065)
822,Searching a Fixed Graph,,1996-07-08,https://www.semanticscholar.org/paper/dfdb3be95b742a6a44fcae89b5f771441d12349f,"International Colloquium on Automata, Languages and Programming"
1908,A Hybrid Forecasting Framework with Neural Network and Time-Series Method for Intermittent Demand in Semiconductor Supply Chain,,2018-08-26,https://www.semanticscholar.org/paper/75b4df2b12f2b547c414498d668c5ab9aef30c5c,Advances in Production Management Systems
3404,Extending Search Phases in the Micali-Vazirani Algorithm,"The Micali-Vazirani algorithm is an augmenting path algorithm that offers the best theoretical runtime of O(n0.5m) for solving the maximum cardinality matching problem for non-bipartite graphs. This paper builds upon the algorithm by focusing on the bottleneck caused by its search phase structure and proposes a new implementation that improves efficiency by extending the search phases in order to find more augmenting paths. Experiments on different types of randomly generated and real world graphs demonstrate this new implementation’s effectiveness and limitations. 1998 ACM Subject Classification G.2.2 Graph Theory, F.2.2 Nonnumerical Algorithms and Problems",,https://www.semanticscholar.org/paper/e6caff814bba9949fce6a48c76e3b158a8ddafbb,The Sea
3777,Inverting and Visualizing Features for Object Detection,"Abstract : This paper presents methods to visualize feature spaces commonly used in object detection. The tools in this paper allow a human to put on feature space glasses and see the visual world as a computer might see it. We found that these glasses allow us to gain insight into the behavior of computer vision systems. We show a variety of experiments with our visualizations, such as examining the linear separability of recognition in HOG space, generating high scoring super objects for an object detector, and diagnosing false positives. We pose the visualization problem as one of feature inversion, i.e. recovering the natural image that generated a feature descriptor. We describe four algorithms to tackle this task, with different trade-offs in speed accuracy, and scalability. Our most successful algorithm uses ideas from sparse coding to learn a pair of dictionaries that enable regression between HOG features and natural images, and can invert features at interactive rates. We believe these visualizations are useful tools to add to an object detector researcher's toolbox, and code is available.",2012-12-10,https://www.semanticscholar.org/paper/dc4ea16406f985b7a045cf0fa6254c8b12accf9d,arXiv.org
306,Nash Equilibria: Where We Stand,,2007-10-08,https://www.semanticscholar.org/paper/46f72c3c054632256428d41ad240517b5107d336,Embedded Systems and Applications
3752,Generating the Future with Adversarial Transformers,"We learn models to generate the immediate future in video. This problem has two main challenges. Firstly, since the future is uncertain, models should be multi-modal, which can be difficult to learn. Secondly, since the future is similar to the past, models store low-level details, which complicates learning of high-level semantics. We propose a framework to tackle both of these challenges. We present a model that generates the future by transforming pixels in the past. Our approach explicitly disentangles the models memory from the prediction, which helps the model learn desirable invariances. Experiments suggest that this model can generate short videos of plausible futures. We believe predictive models have many applications in robotics, health-care, and video understanding.",2017-07-01,https://www.semanticscholar.org/paper/6d2892f82a89bfc81f9924adb8bd070fe007adf7,Computer Vision and Pattern Recognition
1372,Measurement of the Lambda0b lifetime in the decay lambda0b--> J/psiLambda0 with the D0 detector.,"We present measurements of the Lambda(0)(b) lifetime in the exclusive decay channel Lambda(0)(b)--> J/psiLambda(0), with J/psi--> mu(+)mu(-) and Lambda(0)--> ppi(-), the B0 lifetime in the decay B0-->J/psiK(0)(S) with J/psi--> mu(+)mu(-) and K(0)(S)-->pi(+)pi(-), and the ratio of these lifetimes. The analysis is based on approximately 250 pb(-1) of data recorded with the D0 detector in pp collisions at sqrt[s] = 1.96 TeV. The Lambda(0)(b) lifetime is determined to be tau(Lambda(0)(b)) = 1.22(+0.22)(-0.18)(stat) +/- 0.04(syst) ps, the B0 lifetime tau(B0) = 1.40(+0.11)(-0.10)(stat) +/- 0.03(syst) ps, and the ratio tau(Lambda(0)(b))/tau(B0) = 0.87(+0.17)(-0.14)(stat) +/- 0.03(syst). In contrast with previous measurements using semileptonic decays, this is the first determination of the Lambda(0)(b) lifetime based on a fully reconstructed decay channel.",2004-10-19,https://www.semanticscholar.org/paper/736be2a3eec762cbccf2d9dbf70f4275a8567087,Physical Review Letters
1694,Variational Tempering,"Variational inference (VI) combined with data subsampling enables approximate posterior inference over large data sets, but suffers from poor local optima. We first formulate a deterministic annealing approach for the generic class of conditionally conjugate exponential family models. This approach uses a decreasing temperature parameter which deterministically deforms the objective during the course of the optimization. A well-known drawback to this annealing approach is the choice of the cooling schedule. We therefore introduce variational tempering, a variational algorithm that introduces a temperature latent variable to the model. In contrast to related work in the Markov chain Monte Carlo literature, this algorithm results in adaptive annealing schedules. Lastly, we develop local variational tempering, which assigns a latent temperature to each data point; this allows for dynamic annealing that varies across data. Compared to the traditional VI, all proposed approaches find improved predictive likelihoods on held-out data.",2014-11-07,https://www.semanticscholar.org/paper/209e1d36f36b8e7db3147b0e424874e54df9012e,International Conference on Artificial Intelligence and Statistics
185,Long Term Memory and the Densest K-Subgraph Problem,"In a recent experiment, a cell in the human medial temporal lobe (MTL) encoding one sensory stimulus starts to also respond to a second stimulus following a combined experience associating the two. We develop a theoretical model predicting that an assembly of cells with exceptionally high synaptic intraconnectivity can emerge, in response to a particular sensory experience, to encode and abstract that experience. We also show that two such assemblies are modified to increase their intersection after a sensory event that associates the two corresponding stimuli. The main technical tools employed are random graph theory, and Bernoulli approximations. Assembly creation must overcome a computational challenge akin to the Densest K-Subgraph problem, namely selecting, from a large population of randomly and sparsely interconnected cells, a subset with exceptionally high density of interconnections. We identify three mechanisms that help achieve this feat in our model: (1) a simple two-stage randomized algorithm, and (2) the ""triangle completion bias"" in synaptic connectivity and a ""birthday paradox"", while (3) the strength of these connections is enhanced through Hebbian plasticity.",,https://www.semanticscholar.org/paper/d4d28f5e9907b531d5b3046dceb88b6025424700,Information Technology Convergence and Services
2922,Evaluation of fracture toughness and residual stress in AISI 316L electron beam welds,"Weld residual stress and fracture behaviour of 316L electron beam weldments, which are of particular interest in power generation industry, were investigated in this work. Two butt-weld joints were manufactured in stainless steel 316L plates of 6 mm and 25.4 mm thicknesses. Three complementary methods were used to measure the three orthogonal components of the residual stress in the weld coupons and fracture tests were conducted on single edge notched bending specimens extracted from different regions of the welds and parent metals. The residual stress measurements showed a maximum value of 450 MPa in longitudinal direction, while it was less than 150 MPa in the other two orthogonal directions, revealing that in our material, and with the chosen weld parameters, the residual stresses were biaxial. The fracture resistance of the weldment and parent material was similar, with material microstructure differences being more significant than the measured residual stresses. The study suggests that 316L electron beam weldments are not susceptible to fracture failure due to their high ductility and ability to relieve residual stresses through gross plasticity. Electron beam welding may therefore be suggested as a reliable manufacturing technology for safety critical 316L components.",2021-04-12,https://www.semanticscholar.org/paper/fe8d5a709bb55881ad0ce67040658ef133daa59a,Fatigue & Fracture of Engineering Materials & Structures
2558,Urban Computing and Mobile Devices,"In this issue's Works in Progress department, we have 12 urban computing and mobile device entries that span a wide range of computing and social areas. The first entry examines how an urban environment could operate as a large-scale, real-time control system. One project focuses on annotating public spaces and sharing the tags with others. Two projects tie together social networking in cyberspace with local urban communities. Two projects examine computing and social interactions in physical spaces. Two entries explore how to combine synthetic and physical views of urban environments. Four entries investigate how we explore urban spaces, interact with technology in those spaces, and create shared community histories.",2007-07-01,https://www.semanticscholar.org/paper/9216aa5bc9e3f20d190e7c7c9fc9e6c3b591b39a,IEEE pervasive computing
1579,Equal Opportunity and Affirmative Action via Counterfactual Predictions,"Machine learning (ML) can automate decision-making by learning to predict decisions from historical data. However, these predictors may inherit discriminatory policies from past decisions and reproduce unfair decisions. In this paper, we propose two algorithms that adjust fitted ML predictors to make them fair. We focus on two legal notions of fairness: (a) providing equal opportunity (EO) to individuals regardless of sensitive attributes and (b) repairing historical disadvantages through affirmative action (AA). More technically, we produce fair EO and AA predictors by positing a causal model and considering counterfactual decisions. We prove that the resulting predictors are theoretically optimal in predictive performance while satisfying fairness. We evaluate the algorithms, and the trade-offs between accuracy and fairness, on datasets about admissions, income, credit and recidivism.",2019-05-26,https://www.semanticscholar.org/paper/be3d91b978c5691910270ed0de132cf8dc1be62b,arXiv.org
1680,Objective Variables for Probabilistic Revenue Maximization in Second-Price Auctions with Reserve,"Many online companies sell advertisement space in second-price auctions with reserve. In this paper, we develop a probabilistic method to learn a profitable strategy to set the reserve price. We use historical auction data with features to fit a predictor of the best reserve price. This problem is delicate - the structure of the auction is such that a reserve price set too high is much worse than a reserve price set too low. To address this we develop objective variables, an approach for combining probabilistic modeling with optimal decision-making. Objective variables are ""hallucinated observations"" that transform the revenue maximization task into a regularized maximum likelihood estimation problem, which we solve with the EM algorithm. This framework enables a variety of prediction mechanisms to set the reserve price. As examples, we study objective variable methods with regression, kernelized regression, and neural networks on simulated and real data. Our methods outperform previous approaches both in terms of scalability and profit.",2015-06-24,https://www.semanticscholar.org/paper/59c8ca723801e441840fdde577f8e33b995d45a4,The Web Conference
1460,Test of spin dependence in charm-quark fragmentation to D*,"We have measured the polarization of {ital D}{sup *}, the energy dependence of the polarization, and the spin-density matrix of {ital D}{sup *} in {ital e}{sup +}{ital e{minus}} annihilation at a center-of-mass energy of 29 GeV using the Time Projection Chamber detector at the SLAC storage ring PEP. In 147 pb{sup {minus}1} of data we see no strong evidence for polarization, alignment, or final-state interactions in this fragmentation process.",,https://www.semanticscholar.org/paper/11375838762daec290b9c8c70c71fcf5501241d0,"Physical Review D, Particles and fields"
1946,Multi-objective multi-population biased random-key genetic algorithm for the 3-D container loading problem,,2015-11-01,https://www.semanticscholar.org/paper/8bfafe5ede5bf4758bb9fabfecd943b85a5f5663,Computers & industrial engineering
2759,Specifying composite illustrations with communicative goals,"IBIS (Intent-Based Illustration System) generates illustrations automatically, guided by communicative goals. Communicative goals specify that particular properties of objects, such as their color, size, or location are to be conveyed in the illustration. IBIS is intended to be part of an interactive multimedia explanation generation system. It has access to a knowledge base that contains a collection of objects, including information about their geometric properties, material, and location. As the goals are interpreted by a rule-based control component, the system generates a precise definition of the final illustration. If IBIS determines that a set of goals cannot be satisfied in a single picture, then it attempts to create a composite illustration that has multiple viewports. For example, a composite illustration may contain a nested inset illustration showing an object in greater detail than is possible in the parent picture. Each component illustration is defined by its placement, size, viewing specification, lighting specification, and list of objects to be displayed and their graphical style.",1989-11-13,https://www.semanticscholar.org/paper/d972ba590cd8ed600860c7cac308f065701fdcdb,ACM Symposium on User Interface Software and Technology
923,The Complexity of Testing Whether a Graph is a Superconcentrator,,,https://www.semanticscholar.org/paper/840694d1906eb3615479e13e6afa28313d10118a,Information Processing Letters
187,Stathis Zachos at 70!,,2017-05-24,https://www.semanticscholar.org/paper/034e89b491fabac25b233a7d19109cb7f1672a2a,International/Italian Conference on Algorithms and Complexity
455,The Comparative Linguistics of Knowledge Representation,"We develop a methodology for comparing knowledge representation formalisms in terms of their ""representational succinctness,"" that is, their ability to express knowledge situations relatively efficiently. We use this framework for comparing many important formalisms for knowledge base representation: propositional logic, default logic, circumscription, and model preference defaults; and, at a lower level, Horn formulas, characteristic models, decision trees, disjunctive normal form, and conjunctive normal form. We also show that adding new variables improves the effective expressibility of certain knowledge representation formalisms.",1995-08-20,https://www.semanticscholar.org/paper/9fad50544027f4b7b6fd87fa145b6ae5cb0c310c,International Joint Conference on Artificial Intelligence
1790,Exploiting Covariate Similarity in Sparse Regression via the Pairwise Elastic Net,"A new approach to regression regularization called the Pairwise Elastic Net is proposed. Like the Elastic Net, it simultaneously performs automatic variable selection and continuous shrinkage. In addition, the Pairwise Elastic Net encourages the grouping of strongly correlated predictors based on a pairwise similarity measure. We give examples of how the approach can be used to achieve the objectives of Ridge regression, the Lasso, the Elastic Net, and Group Lasso. Finally, we present a coordinate descent algorithm to solve the Pairwise Elastic Net.",2010-03-31,https://www.semanticscholar.org/paper/d82d35c96fb2aa5980b34a6312c7caf3b772ec7c,International Conference on Artificial Intelligence and Statistics
3748,The Sound of Pixels,,2018-04-09,https://www.semanticscholar.org/paper/fe018f22600d07cbd0452a070e03708886470015,European Conference on Computer Vision
3415,A Fast Distributed Algorithm for α-Fair Packing Problems,"Over the past two decades, fair resource allocation problems received considerable attention in a variety of application areas. While polynomial time distributed algorithms have been designed for max-min fair resource allocation, the design of distributed algorithms with convergence guarantees for the more general �−fair allocations received little attention. In this paper, we study weighted �-fair packing problems, that is, the problems of maximizing the objective functions P j wjx 1 � j /(1 − �) when � 6 1 and P j wj lnxj when � = 1 over linear constraints Ax ≤ b, x ≥ 0, where wj are positive weights and A and b are non-negative. We consider the distributed computation model that was used for packing linear programs and network utility maximization problems. Under this model, we provide a distributed algorithm for general �. The algorithm uses simple local update rules and is stateless (namely, it allows asynchronous updates, is self-stabilizing, and allows incremental and local adjustments). It converges to approximate solutions in running times that have an inverse polynomial dependence on the approximation parameter "". The convergence time has polylogarithmic dependence on the problem size for � 6 1, and a nearly-linear dependence on the number of variables for � = 1. These are the best convergence times known for these problems.",2015-02-11,https://www.semanticscholar.org/paper/f3efee70865ee39fe2becd82c76eecf1881cbcc7,arXiv.org
649,Angiographic follow-up and clinical experience with the flexible Tantalum Cordis stent.,"The Cordis stent is a flexible, highly radioopaque intracoronary stent engineered from a single Tantalum filament folded into a sinusoidal helical coil. It is premounted on a semicompliant balloon expandable stent delivery system. From September 1995-March 1996, 147 Cordis stents were deployed in 105 patients (aged 58+/-12 yr, 71% male). Clinical indications for stenting were unstable angina in 59 (55%), stable angina in 41 (38%), and acute myocardial infarction in 7 (7%). The target vessel was the right coronary artery in 45%, the left anterior descending in 31%, and the circumflex artery in 22%. One stent was deployed in a vein graft, and one stent was deployed in a left internal mammary artery graft. Stent deployment was achieved in all but one patient. Acute in-stent thrombosis occurred in 3 patients (2.9%). Two of these patients required urgent coronary artery bypass surgery. Subacute stent thrombosis occurred in 2 patients (1.9%). Minimum lumen diameter increased from 0.70+/-0.41 mm to 3.50+/-0.60 mm following stent placement. All patients received aspirin. Eighty-one patients (77%) received ticlopidine, and 4 patients (4%) received warfarin therapy. The mean hospital stay was 3.4+/-2.3 days. Six-month follow-up angiography was performed on 50 out of 55 eligible patients at one of the two institutions involved in this study. Computer-assisted quantitative coronary angiography defined a restenosis rate of 26%. Repeat revascularization was required in 8 patients (14.5%) at 6-mo follow-up. The Tantalum Cordis intracoronary stent is an effective and safe means of treating coronary lesions, even in patients with unstable ischemic syndromes. Acute and subacute rates of in-stent thrombosis were acceptable, and the long-term angiographic restenosis rates and need for repeat revascularization were favorable.",1998-02-01,https://www.semanticscholar.org/paper/3614865f3590afa28337d2cd304f74c37359a3aa,Catheterization and Cardiovascular Diagnosis
3609,Untangling the balancing and searching of balanced binary search trees,"A balanced binary search tree can be characterized by two orthogonal issues: its search strategy and its balancing strategy. In this paper, we show how to decouple search and balancing strategies so that they can be expressed independently of each other, communicating only by basic operations such as rotations. Different balancing strategies, such as red–black trees and splay trees, and different search applications, such as key search and rank search, can be combined arbitrarily. As a new result, we show how optimal string search can be expressed as a search application on any balanced binary tree.",2003-11-10,https://www.semanticscholar.org/paper/a2be43ce0ad84d77cfc88d535a61f765e9442f0e,"Software, Practice & Experience"
2591,Introduction to computer graphics,"Computer graphics is an exciting field of endeavor, but it is often difficult for a newcomer to get started. This course is that opportunity! The topics being presented will address many areas within computer graphics and treat each from the point of view of ""why-do-I-care"" and ""how-to."" Those who take this course will emerge well-prepared to take on further study, including the taking of other SIGGRAPH courses. Attendees will also be ready to take on the vendor show and better appreciate the Electronic Theatre. We hope you enjoy reading and using these notes as much as we enjoyed preparing them.",2004-08-08,https://www.semanticscholar.org/paper/07de518de3b47427a38f5be2f15f538a0a22686f,International Conference on Computer Graphics and Interactive Techniques
2805,Deletion of galectin-3 exacerbates microglial activation and accelerates disease progression and demise in a SOD1G93A mouse model of amyotrophic lateral sclerosis,"Galectins are pleiotropic carbohydrate‐binding lectins involved in inflammation, growth/differentiation, and tissue remodeling. The functional role of galectins in amyotrophic lateral sclerosis (ALS) is unknown. Expression studies revealed increases in galectin‐1 mRNA and protein in spinal cords from SOD1 G93A mice, and in galectin‐3 and ‐9 mRNAs and proteins in spinal cords of both SOD1 G93A mice and sporadic ALS patients. As the increase in galectin‐3 appeared in early presymptomatic stages and increased progressively through to end stage of disease in the mouse, it was selected for additional study, where it was found to be mainly expressed by microglia. Galectin‐3 antagonists are not selective and do not readily cross the blood–brain barrier; therefore, we generated SOD1 G93A/Gal‐3 −/− transgenic mice to evaluate galectin‐3 deletion in a widely used mouse model of ALS. Disease progression, neurological symptoms, survival, and inflammation were assessed to determine the effect of galectin‐3 deletion on the SOD1 G93A disease phenotype. Galectin‐3 deletion did not change disease onset, but resulted in more rapid progression through functionally defined disease stages, more severely impaired neurological symptoms at all stages of disease, and expiration, on average, 25 days earlier than SOD1 G93A/Gal‐3 +/+ cohorts. In addition, microglial staining, as well as TNF‐α, and oxidative injury were increased in SOD1 G93A/Gal‐3 −/− mice compared with SOD1 G93A/Gal‐3 +/+ cohorts. These data support an important functional role for microglial galectin‐3 in neuroinflammation during chronic neurodegenerative disease. We suggest that elevations in galectin‐3 by microglia as disease progresses may represent a protective, anti‐inflammatory innate immune response to chronic motor neuron degeneration.",2012-07-23,https://www.semanticscholar.org/paper/5c9d734398875d223ff9b904cb5fd3a388df5fd1,Brain and Behavior
1754,How They Vote: Issue-Adjusted Models of Legislative Behavior,"We develop a probabilistic model of legislative data that uses the text of the bills to uncover lawmakers' positions on specific political issues. Our model can be used to explore how a lawmaker's voting patterns deviate from what is expected and how that deviation depends on what is being voted on. We derive approximate posterior inference algorithms based on variational methods. Across 12 years of legislative data, we demonstrate both improvement in heldout predictive performance and the model's utility in interpreting an inherently multi-dimensional space.",2012-12-03,https://www.semanticscholar.org/paper/9805ddd8b0a92764b6df18b0408e8ef78d505e15,Neural Information Processing Systems
2021,UNISON analysis to model and reduce step-and-scan overlay errors for semiconductor manufacturing,,2011-06-01,https://www.semanticscholar.org/paper/f2e1693592371e7b0c66689d9a7dcb8c111a957a,Journal of Intelligent Manufacturing
2572,Interaction Techniques for Exploring Historic Sites through Situated Media,"We present a set of augmented reality and virtual reality interaction techniques that enable mobile users to visualize and interact virtually with representations of past events. These approaches use historic photographic imagery registered with real and virtual 3D objects to depict events in situ, and to provide interactive timelines. We demonstrate our techniques through examples developed for an important landmark, the Cathedral of St. John the Divine.",2006-03-25,https://www.semanticscholar.org/paper/5ce18bcb93be61b600faa79cacb533b0c33c1ae5,IEEE Symposium on 3D User Interfaces
2210,The protective effect of GM-CSF on serum-induced neutrophil apoptosis in juvenile systemic lupus erythematosus patients,,,https://www.semanticscholar.org/paper/a4c621a2ec807d12a2948c704b92a987f5dfae4e,Clinical Rheumatology
858,Fundamental discrepancies between average-case analyses under discrete and continuous distributions: a bin packing case study,"We consider the average case behavior of onedmensional bin paekmg algorithms in the case where bins have unit capacity and item sizes are chosen according to the ‘ ‘dficrete uniform” distribution U~; k), 1 s j < k, where each item size in the set {llk,21k,..., ji k) has probability 1/j of beiig chosen. Note that for fixed j,k the distributions U{?nj;mk]’ approach the continuous distribution U(O, jlk] as m A W, where in U(O, jl k] the item sizes are chosen uniformly horn the half-open interval (O,jik]. In this paper, we show that average case behavior can differ substantially under the two types of distributions. We show that for all j, k, j < k-1, there exist on-line algorithms that have constant expected waste under U~; k], whereas no on-line algorithm can have less than C2(n1’2) waste under U(O, U] for any u s 1. Conmariwise, although the First Fit Decreasing (off-line) algorithm has constant expected waste under U(O, u] for all u < 1/2, there are many combinations j,k with j < k/2 such that First Fit Decreasing has t3(tI) expected waste under U(j;k). The constant of proportionality is maxtilzed for j = 6 and k = 13, in which case the expected waste k nl 624.",1991-01-03,https://www.semanticscholar.org/paper/48097d98a358c85aa9b2885e79df00d45c9b4648,Symposium on the Theory of Computing
3516,Task Scheduling in Networks (Extended Abstract),,1994-07-06,https://www.semanticscholar.org/paper/a4486673a4615971d066b9a3871dd498e9f09921,Scandinavian Workshop on Algorithm Theory
1267,Measurement of the lifetime of the Bc+/- meson in the semileptonic decay channel.,"Using approximately 1.3 fb(-1) of data collected by the D0 detector between 2002 and 2006, we measure the lifetime of the Bc+/- meson in the Bc-/+-->J/psimicro+/-+X final state. A simultaneous unbinned likelihood fit to the J/psi+micro invariant mass and lifetime distributions yields a signal of 881+/-80(stat) candidates and a lifetime measurement of tau(Bc+/-)=0.448(-0.036)(+0.038)(stat)+/-0.032(syst) ps.",2008-05-16,https://www.semanticscholar.org/paper/ef62b32f10832870d6b65c9e92ae9d8bd4e0c72a,Physical Review Letters
2786,Intracranial alternating current stimulation facilitates neurogenesis in a mouse model of Alzheimer’s disease,,2020-07-23,https://www.semanticscholar.org/paper/9f8d2b8f28a17c052b8f1bc7c407261c917ac429,Alzheimer's Research & Therapy
1197,Measurement of the polarization of the upsilon(1S) and upsilon(2S) states in pp collisions at square root[s]=1.96 TeV.,"We present a study of the polarization of the Upsilon(1S) and Upsilon(2S) states using a 1.3 fb;{-1} data sample collected by the D0 experiment in 2002-2006 during run II of the Fermilab Tevatron Collider. We measure the polarization parameter alpha=(sigma_{T}-2sigma_{L})/(sigma_{T}+2sigma_{L}), where sigma_{T} and sigma_{L} are the transversely and longitudinally polarized components of the production cross section, as a function of the transverse momentum (p_{T};{Upsilon}) for the Upsilon(1S) and Upsilon(2S). Significant p_{T};{Upsilon}-dependent longitudinal polarization is observed for the Upsilon(1S). A comparison with theoretical models is presented.",2008-04-01,https://www.semanticscholar.org/paper/07ee235b5776fc14e47a861001b27c47e10a8ea4,Physical Review Letters
3596,Concepts: linguistic support for generic programming in C++,"Generic programming has emerged as an important technique for the development of highly reusable and efficient software libraries. In C++, generic programming is enabled by the flexibility of templates, the C++ type parametrization mechanism. However, the power of templates comes with a price: generic (template) libraries can be more difficult to use and develop than non-template libraries and their misuse results in notoriously confusing error messages. As currently defined in C++98, templates are unconstrained, and type-checking of templates is performed late in the compilation process, i.e., after the use of a template has been combined with its definition. To improve the support for generic programming in C++, we introduce concepts to express the syntactic and semantic behavior of types and to constrain the type parameters in a C++ template. Using concepts, type-checking of template definitions is separated from their uses, thereby making templates easier to use and easier to compile. These improvements are achieved without limiting the flexibility of templates or decreasing their performance - in fact their expressive power is increased. This paper describes the language extensions supporting concepts, their use in the expression of the C++ Standard Template Library, and their implementation in the ConceptGCC compiler. Concepts are candidates for inclusion in the upcoming revision of the ISO C++ standard, C++0x.",2006-10-16,https://www.semanticscholar.org/paper/43913a70c2226c853b1e0a45bcb87c7b711d7212,"Conference on Object-Oriented Programming Systems, Languages, and Applications"
2144,Directly lower bounding the information capacity for channels with I.I.D.deletions and duplications,"In this paper, we directly lower bound the information capacity for channels with independent identically distributed (i.i.d.) deletions and duplications. Our approach differs from previous work in that we focus on the information capacity using ideas from renewal theory, rather than focusing on the transmission capacity by analyzing the error probability of some randomly generated code using a combinatorial argument. Of course, the transmission and information capacities are equal, but our change of perspective allows for a much simpler analysis that gives more general theoretical results. We then apply these results to the binary deletion channel to improve existing lower bounds on its capacity.",,https://www.semanticscholar.org/paper/98b83fad457ed9d4b0227948c78d1dcf3b84dfde,IEEE Transactions on Information Theory
219,Strategic Classification,"Machine learning relies on the assumption that unseen test instances of a classification problem follow the same distribution as observed training data. However, this principle can break down when machine learning is used to make important decisions about the welfare (employment, education, health) of strategic individuals. Knowing information about the classifier, such individuals may manipulate their attributes in order to obtain a better classification outcome. As a result of this behavior -- often referred to as gaming -- the performance of the classifier may deteriorate sharply. Indeed, gaming is a well-known obstacle for using machine learning methods in practice; in financial policy-making, the problem is widely known as Goodhart's law. In this paper, we formalize the problem, and pursue algorithms for learning classifiers that are robust to gaming. We model classification as a sequential game between a player named ""Jury"" and a player named ""Contestant."" Jury designs a classifier, and Contestant receives an input to the classifier drawn from a distribution. Before being classified, Contestant may change his input based on Jury's classifier. However, Contestant incurs a cost for these changes according to a cost function. Jury's goal is to achieve high classification accuracy with respect to Contestant's original input and some underlying target classification function, assuming Contestant plays best response. Contestant's goal is to achieve a favorable classification outcome while taking into account the cost of achieving it. For a natural class of ""separable"" cost functions, and certain generalizations, we obtain computationally efficient learning algorithms which are near optimal, achieving a classification error that is arbitrarily close to the theoretical minimum. Surprisingly, our algorithms are efficient even on concept classes that are computationally hard to learn. For general cost functions, designing an approximately optimal strategy-proof classifier, for inverse-polynomial approximation, is NP-hard.",2015-06-23,https://www.semanticscholar.org/paper/81fd20c2b903d979075e0c6a59258b0a84213095,Information Technology Convergence and Services
3122,Using certes to infer client response time at the web server,"As businesses continue to grow their World Wide Web presence, it is becoming increasingly vital for them to have quantitative measures of the mean client perceived response times of their web services. We present Certes (CliEnt Response Time Estimated by the Server), an online server-based mechanism that allows web servers to estimate mean client perceived response time, as if measured at the client. Certes is based on a model of TCP that quantifies the effect that connection drops have on mean client perceived response time by using three simple server-side measurements: connection drop rate, connection accept rate and connection completion rate. The mechanism does not require modifications to HTTP servers or web pages, does not rely on probing or third party sampling, and does not require client-side modifications or scripting. Certes can be used to estimate response times for any web content, not just HTML. We have implemented Certes and compared its response time estimates with those obtained with detailed client instrumentation. Our results demonstrate that Certes provides accurate server-based estimates of mean client response times in HTTP 1.0/1.1 environments, even with rapidly changing workloads. Certes runs online in constant time with very low overhead. It can be used at websites and server farms to verify compliance with service level objectives.",2004-02-01,https://www.semanticscholar.org/paper/5afd1c7147cb182d9841cb67ffd9e572206f837d,TOCS
2660,Situated documentaries: embedding multimedia presentations in the real world,"We describe an experimental wearable augmented reality system that enables users to experience hypermedia presentations that are integrated with the actual outdoor locations to which they are relevant. Our mobile prototype uses a tracked see-through head-worn display to overlay 3D graphics, imagery, and sound on top of the real world, and presents additional, coordinated material on a hand-held pen computer. We have used these facilities to create several situated documentaries that tell the stories of events that took place on our campus. We describe the software and hardware that underly our prototype system and explain the user interface that we have developed for it.",1999-10-18,https://www.semanticscholar.org/paper/54443021bdc5f11587ecf4406b53835e53a030c9,Digest of Papers. Third International Symposium on Wearable Computers
124,"Fully Adaptive Minimal Deadlock-Free Packet Routing in Hypercubes, Meshes, and other Networks: Algorithms and Simulations","This paper deals with the problem of packet-switched routing in parallel machines. Several new routing algorithms for different interconnection networks are presented. While the new techniques apply to a wide variety of networks, routing algorithms will be shown for the hypercube, the two-dimensional mesh, and the shuffle-exchange. Although the new techniques are designed for packet routing, they can be used alternatively for virtual cut-through routing models. The techniques presented for hypercubes and meshes are fully-adaptive and minimal. A fully-adaptive and minimal routing is one in which all possible minimal paths between a source and a destination are of potential use at the time a message is injected into the network. Minimal paths followed by messages ultimately depend on the local congestion encountered in each node of the network. All of the new techniques are completely free of deadlock situations. >",1994-03-01,https://www.semanticscholar.org/paper/405b70a06edf58884990df609a622bd245f816d6,IEEE Trans. Parallel Distributed Syst.
864,Testing finite state machines,"We present simple randomized algorithms for the fault detection problem: Given a specification in the form of a deterministic finite state machine A and an implementation machine B, determine whether B is equal to A. If A has n states and p inputs, then in randomized polynomial time we can construct with high probability a checking sequence of length O(pn4 log n), i.e., a sequence that detects all faulty machines with at most n states. Better bounds can be obtained in certain cases. The techniques generalize to partially specified finite state machines.",1991-01-03,https://www.semanticscholar.org/paper/9d60a1796e9feeeb0c6ecf8e5ed7ce79c31809b4,Symposium on the Theory of Computing
3737,Metric Learning for Adversarial Robustness,"Deep networks are well-known to be fragile to adversarial attacks. We conduct an empirical analysis of deep representations under the state-of-the-art attack method called PGD, and find that the attack causes the internal representation to shift closer to the ``false'' class. Motivated by this observation, we propose to regularize the representation space under attack with metric learning to produce more robust classifiers. By carefully sampling examples for metric learning, our learned representation not only increases robustness, but also detects previously unseen adversarial samples. Quantitative experiments show improvement of robustness accuracy by up to 4% and detection efficiency by up to 6% according to Area Under Curve score over prior work. The code of our work is available at https://github.com/columbia/Metric_Learning_Adversarial_Robustness.",2019-09-01,https://www.semanticscholar.org/paper/e1dea4c733ee7c98aaa42972452f545821b5d3b5,Neural Information Processing Systems
1796,Markov Topic Models,"We develop Markov topic models (MTMs), a novel family of generative probabilistic models that can learn topics simultaneously from multiple corpora, such as papers from different conferences. We apply Gaussian (Markov) random fields to model the correlations of different corpora. MTMs capture both the internal topic structure within each corpus and the relationships between topics across the corpora. We derive an efficient estimation procedure with variational expectation-maximization. We study the performance of our models on a corpus of abstracts from six different computer science conferences. Our analysis reveals qualitative discoveries that are not possible with traditional topic models, and improved quantitative performance over the state of the art.",2009-04-15,https://www.semanticscholar.org/paper/1e7d12d0782b700af534ad56f888903d6d80431b,International Conference on Artificial Intelligence and Statistics
3561,Design and evaluation of C++ open multi-methods,,2010-07-01,https://www.semanticscholar.org/paper/50a1234bd6992539e231a7809bb802ff6424b20d,Science of Computer Programming
3030,POSIX Has Become Outdated,,,https://www.semanticscholar.org/paper/8984bf438d496a6861098bae613aa838a83aff0a,Login: The Usenix Magazine
3054,"Finding Concurrency Errors in Sequential Code - OS-level, In-vivo Model Checking of Process Races","While thread races have drawn huge attention from the research community, little has been done for process races, where multiple--possibly sequential--processes access a shared resource, such as a file, without proper synchronization. We present a preliminary study of real process races and show that they are numerous, dangerous, and difficult to detect. To address this problem, we present the design of RACEPRO, an in-vivo model checking system for automatically detecting process races in deployed systems, along with preliminary results from a RACEPRO prototype. To the best of our knowledge, we are the first to study real process races, and RACEPRO is the first system to detect them.",2011-05-09,https://www.semanticscholar.org/paper/c4b0d2557ecf70a53a7255a66dd7b31599dcd73c,USENIX Workshop on Hot Topics in Operating Systems
347,Selfish caching in distributed systems: a game-theoretic analysis,"We analyze replication of resources by server nodes that act selfishly, using a game-theoretic approach. We refer to this as the selfish caching problem. In our model, nodes incur either cost for replicating resources or cost for access to a remote replica. We show the existence of pure strategy Nash equilibria and investigate the price of anarchy, which is the relative cost of the lack of coordination. The price of anarchy can be high due to undersupply problems, but with certain network topologies it has better bounds. With a payment scheme the game can always implement the social optimum in the best case by giving servers incentive to replicate.",2004-07-25,https://www.semanticscholar.org/paper/a913c03850b031c7b9d5b423a2f1309ff248da1e,ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing
3683,Affective Faces for Goal-Driven Dyadic Communication,"We introduce a video framework for modeling the association between verbal and non-verbal communication during dyadic conversation. Given the input speech of a speaker, our approach retrieves a video of a listener, who has facial expressions that would be socially appropriate given the context. Our approach further allows the listener to be conditioned on their own goals, personalities, or backgrounds. Our approach models conversations through a composition of large language models and vision-language models, creating internal representations that are interpretable and controllable. To study multimodal communication, we propose a new video dataset of unscripted conversations covering diverse topics and demographics. Experiments and visualizations show our approach is able to output listeners that are significantly more socially appropriate than baselines. However, many challenges remain, and we release our dataset publicly to spur further progress. See our website for video results, data, and code: https://realtalk.cs.columbia.edu.",2023-01-26,https://www.semanticscholar.org/paper/35f5483fa6c1816739b604a5bb57719fadd79249,arXiv.org
352,Mythematics: storytelling in the teaching of computer science and mathematics,"• Illustrating a key concept: there are many ways to introduce and illustrate exponential growth -other than the dry mathematical expression with n up high, or the graph that pierces its upper right margin. Perhaps in terms of the physical world, via chain reactions and the initial growth of bacteria and embrya; or by recalling the Malthusian argument about populations and resources; or by discussing Moore's Law and the way it has been driving the world. Or, even better, by recounting the tale of the wise man who asked the grateful khaliph to simply place for him one grain of rice at the first square of the chessboard, two at the second, four at the third, and so forth.",2003-09-01,https://www.semanticscholar.org/paper/1ce11b93a9f9be555c27b050a3c5356a44be6048,Annual Conference on Innovation and Technology in Computer Science Education
2832,Role of galectin-3 in macrophage response to Listeria monocytogenes infection (133.34),"
 Galectin-3 is a β-galactoside-binding animal lectin expressed in various immune cell types including macrophages. It has been implicated as a positive regulator of macrophage phagocytosis and as a negative regulator of LPS-mediated inflammation. However, its role in macrophage antibacterial function has not been fully elucidated. The aim of this study was to investigate the role of galectin-3 in macrophage antibacterial function against Listeria monocytogenes (LM) infection by studying bone marrow-derived macrophages (BMM) from galectin-3-deficient (gal3-/-) and wild-type (gal3+/+) mice. After in vitro infection, gal3-/- BMM contained significantly fewer viable intracellular bacteria than gal3+/+ BMM. Consistently, immunofluorescent staining of infected BMM revealed a significantly lower percentage of cytosolic LM escaped from phagosomes in gal3-/- BMM compared to gal3+/+ BMM. Higher levels of reactive nitrogen intermediate nitric oxide (NO) but lower levels of inflammatory cytokine IL-1β were detected in LM-infected gal3-/- BMM compared to gal3+/+ cells. In addition, we noted elevated levels of NAD(P)H in gal3-/- BMM which may contribute to higher amounts of NO in gal3-/- BMM. We conclude that galectin-3 plays a regulatory role in macrophage antibacterial function by interfering with the production of NO, thereby facilitating the survival and replication of intracellular bacteria.",2009-04-01,https://www.semanticscholar.org/paper/972f323f0545fd93072011a81b64810f5b0cc372,Journal of Immunology
3029,"POSIX abstractions in modern operating systems: the old, the new, and the missing","The POSIX standard, developed 25 years ago, comprises a set of operating system (OS) abstractions that aid application portability across UNIX-based OSes. While OSes and applications have evolved tremendously over the last 25 years, POSIX, and the basic set of abstractions it provides, has remained largely unchanged. Little has been done to measure how and to what extent traditional POSIX abstractions are being used in modern OSes, and whether new abstractions are taking form, dethroning traditional ones. We explore these questions through a study of POSIX usage in modern desktop and mobile OSes: Android, OS X, and Ubuntu. Our results show that new abstractions are taking form, replacing several prominent traditional abstractions in POSIX. While the changes are driven by common needs and are conceptually similar across the three OSes, they are not converging on any new standard, increasing fragmentation.",2016-04-18,https://www.semanticscholar.org/paper/33f757edceab3d51979d145b56349f39e99fc2dc,European Conference on Computer Systems
3643,Run Time Type Identification for C++,,,https://www.semanticscholar.org/paper/a0e0a184e21c9252500aebf2c4f002b0c69a0e75,C++ Conference
1711,Decomposing spatiotemporal brain patterns into topographic latent sources,,2014-09-01,https://www.semanticscholar.org/paper/a5b50170e7270d20c03fed29602f1fccf4552965,NeuroImage
1981,An empirical study of design-of-experiment data mining for yield-loss diagnosis for semiconductor manufacturing,,2013-05-28,https://www.semanticscholar.org/paper/af13de380d4496c89d52f3d0a03a06e5289ef4c4,Journal of Intelligent Manufacturing
704,The Complexity of Finding S-factors in Regular Graphs,"A graph G has an S-factor if there exists a spanning subgraph F of G such that for all v ∈ V : degF (v) ∈ S. The simplest example of such factor is a 1-factor, which corresponds to a perfect matching in a graph. In this paper we study the computational complexity of finding S-factors in regular graphs. Our techniques combine some classical as well as recent tools from graph theory. 2012 ACM Subject Classification Mathematics of computing → Matchings and factors; Theory of computation → Problems, reductions and completeness",,https://www.semanticscholar.org/paper/7d38c50398a8d94af33c3cbd8a7fe4edd9aba1a4,Electron. Colloquium Comput. Complex.
3345,Animal behaviour: The serpent's seductive scent,,1985-12-01,https://www.semanticscholar.org/paper/0a0508e25a53dc060fd727e220f59b209dc0bed8,Nature
1817,Content-Based Musical Similarity Computation using the Hierarchical Dirichlet Process,"We develop a method for discovering the latent structure in MFCC feature data using the Hierarchical Dirichlet Process (HDP). Based on this structure, we compute timbral similarity between recorded songs. The HDP is a nonparametric Bayesian model. Like the Gaussian Mixture Model (GMM), it represents each song as a mixture of some number of multivariate Gaussian distributions However, the number of mixture components is not fixed in the HDP, but is determined as part of the posterior inference process. Moreover, in the HDP the same set of Gaussians is used to model all songs, with only the mixture weights varying from song to song. We compute the similarity of songs based on these weights, which is faster than previous approaches that compare single Gaussian distributions directly. Experimental results on a genre-based retrieval task illustrate that our HDPbased method is both faster and produces better retrieval quality than such previous approaches.",,https://www.semanticscholar.org/paper/e82307b16df5046d166a53f55212dfc70b469f8f,International Society for Music Information Retrieval Conference
307,The Computation of Equilibria,,2007-12-12,https://www.semanticscholar.org/paper/6d1002c294e86e92c0ef6fb47ac242ba73a59a2d,Workshop on Internet and Network Economics
3460,Group Ratio Round-Robin: O(1) Proportional Share Scheduling for Uniprocessor and Multiprocessor Systems,"We present Group Ratio Round-Robin (GR3), the first proportional share scheduler that combines accurate proportional fairness scheduling behavior with O(1) scheduling overhead on both uniprocessor and multiprocessor systems. GR3 uses a simple grouping strategy to organize clients into groups of similar processor allocations which can be more easily scheduled. Using this strategy, GR3 combines the benefits of low overhead round-robin execution with a novel ratio-based scheduling algorithm. GR3 introduces a novel frontlog mechanism and weight readjustment algorithm to operate effectively on multiprocessors. GR3 provides fairness within a constant factor of the ideal generalized processor sharing model for client weights with a fixed upper bound and preserves its fairness properties on multiprocessor systems. We have implemented GR3 in Linux and measured its performance. Our experimental results show that GR3 provides much lower scheduling overhead and much better scheduling accuracy than other schedulers commonly used in research and practice.",2005-04-10,https://www.semanticscholar.org/paper/f260606d743bcf066eaf7a3c99c41fc10d39a822,"USENIX Annual Technical Conference, General Track"
3219,Revealing life‐history traits by contrasting genetic estimations with predictions of effective population size,"Effective population size, a central concept in conservation biology, is now routinely estimated from genetic surveys and can also be theoretically predicted from demographic, life‐history, and mating‐system data. By evaluating the consistency of theoretical predictions with empirically estimated effective size, insights can be gained regarding life‐history characteristics and the relative impact of different life‐history traits on genetic drift. These insights can be used to design and inform management strategies aimed at increasing effective population size. We demonstrated this approach by addressing the conservation of a reintroduced population of Asiatic wild ass (Equus hemionus). We estimated the variance effective size (Nev) from genetic data ( N ev =24.3 ) and formulated predictions for the impacts on Nev of demography, polygyny, female variance in lifetime reproductive success (RS), and heritability of female RS. By contrasting the genetic estimation with theoretical predictions, we found that polygyny was the strongest factor affecting genetic drift because only when accounting for polygyny were predictions consistent with the genetically measured Nev. The comparison of effective‐size estimation and predictions indicated that 10.6% of the males mated per generation when heritability of female RS was unaccounted for (polygyny responsible for 81% decrease in Nev) and 19.5% mated when female RS was accounted for (polygyny responsible for 67% decrease in Nev). Heritability of female RS also affected Nev; hf2=0.91 (heritability responsible for 41% decrease in Nev). The low effective size is of concern, and we suggest that management actions focus on factors identified as strongly affecting Nev , namely, increasing the availability of artificial water sources to increase number of dominant males contributing to the gene pool. This approach, evaluating life‐history hypotheses in light of their impact on effective population size, and contrasting predictions with genetic measurements, is a general, applicable strategy that can be used to inform conservation practice.",2018-08-01,https://www.semanticscholar.org/paper/168dbe3fb90c9d47fb94f361791aec2db8e744b1,Conservation Biology
1333,Production of WZ events in pp collisions at square root(s) = 1.96 TeV and limits on anomalous WWZ couplings.,"We present results from a search for WZ production with subsequent decay to l nu l' l' (l and l' = e or mu) using 0.30 fb(-1) of data collected by the D0 experiment between 2002 and 2004 at the Fermilab Tevatron. Three events with WZ decay characteristics are observed. With an estimated background of 0.71 +/- 0.08 events, we measure the WZ production cross section to be 4.5(-2.6)(+3.8) pb, with a 95% C.L. upper limit of 13.3 pb. The 95% C.L. limits for anomalous WWZ couplings are found to be -2.0 < delta kappaZ < 2.4 for form factor scale lambda = 1 TeV, and -0.48 < lambdaZ < 0.48 and -0.49 < delta g(1)(Z) < 0.66 for lambda = 1.5 TeV.",,https://www.semanticscholar.org/paper/157f94c7eeadbda7752a6557f808f3e82413fd4c,Physical Review Letters
2509,Creating hybrid user interfaces with a 2D multi-touch tabletop and a 3D see-through head-worn display,"How can multiple different display and interaction devices be used together to create an effective augmented reality environment? We explore the design of several prototype hybrid user interfaces that combine a 2D multi-touch tabletop display with a 3D head-tracked video-see-through display. We describe a simple modeling application and an urban visualization tool in which the information presented on the head-worn display supplements the information displayed on the tabletop, using a variety of approaches to track the head-worn display relative to the tabletop. In all cases, our goal is to allow users who can see only the tabletop to interact effectively with users wearing head-worn displays.",2011-10-26,https://www.semanticscholar.org/paper/a765f6b0bbaf57832023d69c7272642db7ec4e02,2011 10th IEEE International Symposium on Mixed and Augmented Reality
664,Improving Model Training via Self-learned Label Representations,"Modern neural network architectures have shown remarkable success in several large-scale classification and prediction tasks. Part of the success of these architectures is their flexibility to transform the data from the raw input representations (e.g. pixels for vision tasks, or text for natural language processing tasks) to one-hot output encoding. While much of the work has focused on studying how the input gets transformed to the one-hot encoding, very little work has examined the effectiveness of these one-hot labels. In this work, we demonstrate that more sophisticated label representations are better for classification than the usual one-hot encoding. We propose Learning with Adaptive Labels (LwAL) algorithm, which simultaneously learns the label representation while training for the classification task. These learned labels can significantly cut down on the training time (usually by more than 50%) while often achieving better test accuracies. Our algorithm introduces negligible additional parameters and has a minimal computational overhead. Along with improved training times, our learned labels are semantically meaningful and can reveal hierarchical relationships that may be present in the data.",2022-09-09,https://www.semanticscholar.org/paper/bf2755f9014befbc3c02262f6770ab7605aac86d,arXiv.org
2680,Efficiently planning coherent visual discourse,,1998-03-01,https://www.semanticscholar.org/paper/ec8ef0ac755725140abef7d2feece61fd4e00afa,Knowledge-Based Systems
2568,Visualizing and navigating complex situated hypermedia in augmented and virtual reality,"We present a set of techniques that enable mobile users to visualize and navigate complex hypermedia structures embedded in the real world, through augmented reality or virtual reality. Situating hypermedia in the 3D physical environment makes it possible to represent information about users' surroundings in context. However, it requires addressing a new set of problems beyond those of visualizing hypermedia on a 2D display: Nodes and links can potentially be distributed across large distances, and may be occluded by other objects, both real and virtual. Our techniques address these issues by enabling mobile users to select and manipulate portions of the hypermedia structure by tilting, lifting and shifting them, to view more clearly links and nodes that would otherwise be occluded or ambiguously connected.",2006-10-22,https://www.semanticscholar.org/paper/36ed21db5484ff40cd64597a9909cb234789f023,2006 IEEE/ACM International Symposium on Mixed and Augmented Reality
3140,The design and implementation of Zap: a system for migrating computing environments,"We have created Zap, a novel system for transparent migration of legacy and networked applications. Zap provides a thin virtualization layer on top of the operating system that introduces pods, which are groups of processes that are provided a consistent, virtualized view of the system. This decouples processes in pods from dependencies to the host operating system and other processes on the system. By integrating Zap virtualization with a checkpoint-restart mechanism, Zap can migrate a pod of processes as a unit among machines running independent operating systems without leaving behind any residual state after migration. We have implemented a Zap prototype in Linux that supports transparent migration of unmodified applications without any kernel modifications. We demonstrate that our Linux Zap prototype can provide general-purpose process migration functionality with low overhead. Our experimental results for migrating pods used for running a standard user's X windows desktop computing environment and for running an Apache web server show that these kinds of pods can be migrated with subsecond checkpoint and restart latencies.",,https://www.semanticscholar.org/paper/4bed5060dcf909d851a8ba1876b8aba4c9d8819b,USENIX Symposium on Operating Systems Design and Implementation
2631,A menu interface for wearable computing,"We present a menu interface designed primarily for head-worn displays that have a small field-of-view. To support interaction with a hierarchical menu, we logically divide an absolute positioning device into finger-operated strip segments, which we use as one-dimensional scrolling devices. Our menu system is intended to make user interaction faster by not requiring constant visual feedback. This is preferable for interaction in which the visual user interface elements occupy only a small portion of the eye's entire field-of-view and in which navigating in menus with a pointer would be awkward and time-consuming. With our approach, it is even possible for the user to use peripheral vision for interaction, since there is no need to precisely position a small pointer on the screen. Thus, the user can maintain eye contact with others or keep his or her focus of attention on the environment while using a wearable device.",2002-10-07,https://www.semanticscholar.org/paper/5454925c64247189aae73eb94333ea19e15b12b8,"Proceedings. Sixth International Symposium on Wearable Computers,"
2633,Augmented reality: a new way of seeing.,,2002-04-01,https://www.semanticscholar.org/paper/6fb18b3265a9697e8f88a80b8ab2890d5e253f56,Scientific American
3556,"A Principled, Complete, and Efficient Representation of C++",,2011-12-07,https://www.semanticscholar.org/paper/5b4802e07945dd6b370a8408f4f607b8168038f5,Mathematics and Computer Science
712,The complexity of optimal multidimensional pricing for a unit-demand buyer,,2018-07-01,https://www.semanticscholar.org/paper/9ce476d5a1fa6b85b041d826f949acc2286f5c09,Games Econ. Behav.
2861,Impaired retinal angiogenesis in diabetes: role of advanced glycation end products and galectin-3.,"Suppression of angiogenesis during diabetes is a recognized phenomenon but is less appreciated within the context of diabetic retinopathy. The current study has investigated regulation of retinal angiogenesis by diabetic serum and determined if advanced glycation end products (AGEs) could modulate this response, possibly via AGE-receptor interactions. A novel in vitro model of retinal angiogenesis was developed and the ability of diabetic sera to regulate this process was quantified. AGE-modified serum albumin was prepared according to a range of protocols, and these were also analyzed along with neutralization of the AGE receptors galectin-3 and RAGE. Retinal ischemia and neovascularization were also studied in a murine model of oxygen-induced proliferative retinopathy (OIR) in wild-type and galectin-3 knockout mice (gal3(-/-)) after perfusion of preformed AGEs. Serum from nondiabetic patients showed significantly more angiogenic potential than diabetic serum (P < 0.0001) and within the diabetic group, poor glycemic control resulted in more AGEs but less angiogenic potential than tight control (P < 0.01). AGE-modified albumin caused a dose-dependent inhibition of angiogenesis (P < 0.001), and AGE receptor neutralization significantly reversed the AGE-mediated suppression of angiogenesis (P < 0.01). AGE-treated wild-type mice showed a significant increase in inner retinal ischemia and a reduction in neovascularization compared with non-AGE controls (P < 0.001). However, ablation of galectin-3 abolished the AGE-mediated increase in retinal ischemia and restored the neovascular response to that seen in controls. The data suggest a significant suppression of angiogenesis by the retinal microvasculature during diabetes and implicate AGEs and AGE-receptor interactions in its causation.",2005-03-01,https://www.semanticscholar.org/paper/10a39a1c3c2236902bf5a3f0674474fee5143f2f,Diabetes
1674,Modeling User Exposure in Recommendation,"Collaborative filtering analyzes user preferences for items (e.g., books, movies, restaurants, academic papers) by exploiting the similarity patterns across users. In implicit feedback settings, all the items, including the ones that a user did not consume, are taken into consideration. But this assumption does not accord with the common sense understanding that users have a limited scope and awareness of items. For example, a user might not have heard of a certain paper, or might live too far away from a restaurant to experience it. In the language of causal analysis (Imbens & Rubin, 2015), the assignment mechanism (i.e., the items that a user is exposed to) is a latent variable that may change for various user/item combinations. In this paper, we propose a new probabilistic approach that directly incorporates user exposure to items into collaborative filtering. The exposure is modeled as a latent variable and the model infers its value from data. In doing so, we recover one of the most successful state-of-the-art approaches as a special case of our model (Hu et al. 2008), and provide a plug-in method for conditioning exposure on various forms of exposure covariates (e.g., topics in text, venue locations). We show that our scalable inference algorithm outperforms existing benchmarks in four different domains both with and without exposure covariates.",2015-10-23,https://www.semanticscholar.org/paper/3b93f37e5af2f6f66af33720dc8d0e4de7dc4e65,The Web Conference
977,Positional Change of Optic Nerve Head Vasculature during Axial Elongation as Evidence of Lamina Cribrosa Shifting: Boramae Myopia Cohort Study Report 2.,,2018-03-12,https://www.semanticscholar.org/paper/2f1d12a4d30b73cede31891c8de6697386351445,"Ophthalmology (Rochester, Minn.)"
354,On a network creation game,"We introduce a novel game that models the creation of Internet-like networks by selfish node-agents without central design or coordination. Nodes pay for the links that they establish, and benefit from short paths to all destinations. We study the Nash equilibria of this game, and prove results suggesting that the ""price of anarchy"" [4] in this context (the relative cost of the lack of coordination) may be modest. Several interesting: extensions are suggested.",2003-07-13,https://www.semanticscholar.org/paper/2cf44c2edd3e0da932ca3e94cbe385998fdecbeb,ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing
2796,Analysis of the intracellular role of galectins in cell growth and apoptosis.,,,https://www.semanticscholar.org/paper/8962f119f52770d58cb6059d6192ae40de658a66,Methods in molecular biology
2063,Data mining for yield enhancement in semiconductor manufacturing and an empirical study,,2007-07-01,https://www.semanticscholar.org/paper/219748927fa306d6a8d667869ac1c90e86ec68db,Expert systems with applications
2275,Synovial fluid neutrophils transcribe and express class II major histocompatibility complex molecules in rheumatoid arthritis.,"OBJECTIVE
To investigate a potential interaction between neutrophils and T cells in rheumatoid arthritis (RA), by defining the optimal conditions for induction of class II major histocompatibility complex (MHC) expression on peripheral blood neutrophils in vitro and investigating the capacity for neutrophils to express class II MHC molecules in RA.


METHODS
Surface expression of class II MHC and costimulatory molecules by peripheral blood and synovial fluid (SF) neutrophils obtained from healthy controls and patients with RA was measured by flow cytometry and fluorescence microscopy. Intracellular class II MHC protein and messenger RNA (mRNA) were detected by Western blotting and Northern blotting, respectively.


RESULTS
Freshly isolated peripheral blood neutrophils from controls did not express surface class II MHC; expression was induced by culture with appropriate cytokines. Freshly isolated peripheral blood neutrophils from patients with RA expressed mRNA, but there was no surface expression of class II MHC. Freshly isolated SF neutrophils from patients with RA contained high levels of class II MHC mRNA, did not express surface class II MHC, but did have large intracellular amounts of this protein as detected by Western blotting. After culture for 20 hours in vitro, SF neutrophils from RA patients expressed large amounts of surface class II MHC but very low levels of costimulatory molecules CD80 and CD86. Fluorescence microscopy localized surface class II MHC to discrete areas on the neutrophil. Class II MHC-expressing neutrophils stimulated T cell proliferation.


CONCLUSION
Peripheral blood neutrophils from patients with RA but not healthy controls express class II MHC mRNA. SF neutrophils in RA synthesize and express large amounts of class II MHC but not costimulatory molecules. This might underlie a novel interaction with T cells that is important in terms of disease pathology.",2003-10-01,https://www.semanticscholar.org/paper/dd8c82f8f0dba5dda5a1b98b97a8450387d07bfd,Arthritis & Rheumatism
546,On the Complexity of Circulations,,1986-03-01,https://www.semanticscholar.org/paper/1260f69b4f88e2229f5da07a21a7c48896ac7fef,J. Algorithms
2290,Regulation of neutrophil FcγRIIIb (CD16) surface expression following delayed apoptosis in response to GM‐CSF and sodium butyrate,"When neutrophils undergo apoptosis, they lose expression of the surface receptor CD16 (FcγRIIIb). Thus levels of surface CD16 are good indicators of apoptotic or non‐apoptotic neutrophils. Shedding of CD16 occurs via the activity of a metalloproteinase that cleaves the receptor from the plasma membrane. Granulocyte‐macrophage colony‐stimulating factor (GM‐CSF) and sodium butyrate both stimulate neutrophil gene expression, protect these cells from apoptosis, and maintain expression of surface CD16. In this report we have investigated whether these agents maintain surface expression of CD16 via (1) decreased shedding (2) increased mobilization of the internal pool of pre‐formed CD16, or (3) via de novo biosynthesis of new receptor molecules. Although GM‐CSF and sodium butyrate both preserved surface expression of CD16, GM‐CSF actually accelerated the rate of shedding of this receptor. Maintenance of surface levels was achieved by substantial mobilization of the internal pool of CD16. Sodium butyrate, on the other hand, maintained surface expression without extensive store depletion via a mechanism that appeared to involve a decreased rate of shedding. In these experiments we could find no evidence for de novo biosynthesis of CD16 stimulated by either GM‐CSF or sodium butyrate. These experiments indicate that multiple mechanisms exist for the maintenance of surface CD16 during rescue of neutrophils from apoptosis by different agents. J. Leukoc. Biol. 65: 875–882; 1999.",1999-06-01,https://www.semanticscholar.org/paper/53588395da6c61f88d9e169e17a0549af241cfbf,Journal of Leukocyte Biology
3594,An Early-Reply Based Framework: Reliable Concurrency that Is Verifiable,"Despite its widespread use, concurrent programming is still plagued by reliability problems, such as race conditions and deadlock, not found in sequential programs. We present a concurrency framework to help developers avoid these error conditions, and make it possible to verify their absence through static analysis.",2007-11-14,https://www.semanticscholar.org/paper/cdaccb20f518fce93dc417fe13af130e1f48813f,IEEE International Symposium on High-Assurance Systems Engineering
735,The complexity of non-monotone markets,"We introduce the notion of non-monotone utilities, which covers a wide variety of utility functions in economic theory. We show that it is PPAD-hard to compute an approximate Arrow-Debreu market equilibrium in markets with linear and non-monotone utilities. Building on this result, we settle the long-standing open problem regarding the computation of an approximate Arrow-Debreu market equilibrium in markets with CES utilities, by proving that it is PPAD-complete when the Constant Elasticity of Substitution parameter, ρ, is any constant less than -1.",2012-11-20,https://www.semanticscholar.org/paper/7a1ec5c9cbff3b233f276e358c1f371011a66ca0,Symposium on the Theory of Computing
1815,Syntactic Topic Models,"We develop the syntactic topic model (STM), a nonparametric Bayesian model of parsed documents. The STM generates words that are both thematically and syntactically constrained, which combines the semantic insights of topic models with the syntactic information available from parse trees. Each word of a sentence is generated by a distribution that combines document-specific topic weights and parse-tree-specific syntactic transitions. Words are assumed to be generated in an order that respects the parse tree. We derive an approximate posterior inference method based on variational methods for hierarchical Dirichlet processes, and we report qualitative and quantitative results on both synthetic data and hand-parsed documents.",2008-12-08,https://www.semanticscholar.org/paper/457628a1c232bb48acc2db8440571e289cc80e15,Neural Information Processing Systems
1670,The $χ$-Divergence for Approximate Inference,"Variational inference enables Bayesian analysis for complex probabilistic models with massive data sets. It works by positing a family of distributions and finding the member in the family that is closest to the posterior. While successful, variational methods can run into pathologies; for example, they typically underestimate posterior uncertainty. We propose CHI-VI, a complementary algorithm to traditional variational inference with KL($q$ || $p$) and an alternative algorithm to EP. CHI-VI is a black box algorithm that minimizes the $\chi$-divergence from the posterior to the family of approximating distributions. In EP, only local minimization of the KL($p$ || $q$) objective is possible. In contrast, CHI-VI optimizes a well-defined global objective. It directly minimizes an upper bound to the model evidence that equivalently minimizes the $\chi$-divergence. In experiments, we illustrate the utility of the upper bound for sandwich estimating the model evidence. We also compare several probabilistic models and a Cox process for basketball data. We find CHI-VI often yields better classification error rates and better posterior uncertainty.",2016-11-01,https://www.semanticscholar.org/paper/feb8736dbeb1b6d4049511bd461b2fef37b64a69,arXiv.org
534,Some computational aspects of circumscription,"The effects of circumscribing first-order formulas are explored from a computational standpoint. First, extending work of V. Lifschitz, it is Shown that the circumscription of any existential first-order formula is equivalent to a first-order formula. After this, it is established that a set of universal Horn clauses has a first-order circumscription if and only if it is bounded (when considered as a logic program); thus it is undecidable to tell whether such formulas have first-order circumscription. Finally, it is shown that there arefirst-order formulas whode circumscription has a coNP-complete model-checking problem.",1988-08-21,https://www.semanticscholar.org/paper/fe36f876880c23a429608a70fdd08433f4d8d116,JACM
746,Computational Aspects of Equilibria,,2009-10-13,https://www.semanticscholar.org/paper/6c1bb7d143b71b61ee6abf49583fddb25e9b8818,Algorithmic Game Theory
681,CitiSense: improving geospatial environmental assessment of air quality using a wireless personal exposure monitoring system,"Environmental exposures are a critical component in the development of chronic conditions such as asthma and cancer. Yet, medical and public health practitioners typically must depend on sparse regional measurements of the environment that provide macro-scale summaries. Recent projects have begun to measure an individual's exposure to these factors, often utilizing body-worn sensors and mobile phones to visualize the data. Such data, collected from many individuals and analyzed across an entire geographic region, holds the potential to revolutionize the practice of public health.
 We present CitiSense, a participatory air quality sensing system that bridges the gap between personal sensing and regional measurement to provide micro-level detail at a regional scale. In a user study of 16 commuters using CitiSense, measurements were found to vary significantly from those provided by official regional pollution monitoring stations. Moreover, applying geostatistical kriging techniques to our data allows CitiSense to infer a regional map that contains considerably greater detail than official regional summaries. These results suggest that the cumulative impact of many individuals using personal sensing devices may have an important role to play in the future of environmental measurement for public health.",2012-10-23,https://www.semanticscholar.org/paper/75eb613a3cbaa25d5fb2f7afd084d07d20af103e,Wireless Health
741,On the Complexity of Nash Equilibria and Other Fixed Points (Extended Abstract),"We reexamine, what it means to compute Nash equilibria and, more, generally, what it means to compute a fixed point of a given Brouwer function, and we investigate the complexity of the associated problems. Specifically, we study the complexity of the following problem: given a finite game, Gamma, with 3 or more players, and given epsiv > 0, compute a vector x' (a mixed strategy profile) that is within distance e (say in t^) of some (exact) Nash equilibrium. We show that approximation of an (actual) Nash equilibrium for games with 3 players, even to within any non-trivial constant additive factor epsiv < 1/2 in just one desired coordinate, is at least as hard as the long standing square-root sum problem, as well as more general arithmetic circuit decision problems, and thus that even placing the approximation problem in NP would-resolve a major open problem in the complexity of numerical computation. Furthermore, we show that the (exact or approximate) computation of Nash equilibria for 3 or more players is complete for the class of search problems, which we call FIXP, that can be cast as fixed point computation problems for functions represented by algebraic circuits (straight line programs) over basis {+, *, -, /, max, min}, with rational constants. We show that the linear fragment of FIXP equals PPAD. Many problems in game theory, economics, and probability theory, can be cast as fixed point problems for such algebraic functions. We discuss several important such problems: computing the value of Shapley's stochastic games, and the simpler games of Condon, extinction probabilities of branching processes, termination probabilities of stochastic context-free grammars, and of Recursive Markov Chains. We show that for some of them, the approximation, or even exact computation, problem can be placed-in PPAD, while for others, they are at least as hard as the square-root sum and arithmetic circuit decision problems.",2010-03-01,https://www.semanticscholar.org/paper/130eaa0fcdf2c5f27edfb531067fb21602b7dbf4,IEEE Annual Symposium on Foundations of Computer Science
344,The complexity of pure Nash equilibria,"We investigate from the computational viewpoint multi-player games that are guaranteed to have pure Nash equilibria. We focus on congestion games, and show that a pure Nash equilibrium can be computed in polynomial time in the symmetric network case, while the problem is PLS-complete in general. We discuss implications to non-atomic congestion games, and we explore the scope of the potential function method for proving existence of pure Nash equilibria.",2004-06-13,https://www.semanticscholar.org/paper/71573900a6f2b934c5fd5eaf0e57958dc53527f6,Symposium on the Theory of Computing
2380,Oxygen-radical production during inflammation may be limited by oxygen concentration.,The relationship between oxygen-radical production by rat polymorphonuclear leucocytes and O2 concentration was established by the measurement of luminol-dependent chemiluminescence at defined O2 concentrations. The O2 concentration that gave 50% of the maximum stimulated oxygen-radical production was 31 +/- 9 microM for non-opsonized latex beads and 22 +/- 9 microM for chemotactic peptide. The O2 concentration in rheumatoid synovial fluid was approx. 30 microM. It is therefore proposed that radical production at an inflammatory site may be limited by O2 concentration.,1984-02-01,https://www.semanticscholar.org/paper/ac5d1d220dfd1176e337724b78cf3335167ddfa6,Biochemical Journal
2145,Delay with network coding and feedback,"We consider the problem of minimizing delay when broadcasting over erasure channels with feedback. A sender wishes to communicate the same set of µ messages to several receivers over separate erasure channels. The sender can broadcast a single message or a combination (encoding) of messages at each timestep. Receivers provide feedback as to whether the transmission was received. If at some time step a receiver cannot identify a new message, delay is incurred. Our notion of delay is motivated by real-time applications that request progressively refined input, such as the successive refinement of an image encoded using multiple description coding. Our setup is novel because it combines coding techniques with feedback information to the end of minimizing delay. It allows Θ(µ) benefits as compared to previous approaches for offline algorithms, while feedback allows online algorithms to achieve smaller delay than online algorithms without feedback. Our main complexity results are that the offline minimization problem is NP-hard when the sender only schedules single messages and that the general problem remains NP-hard even when coding is allowed. However we show that coding does offer delay and complexity gains over scheduling. We also discuss online heuristics and evaluate their performance through simulations.",2009-06-28,https://www.semanticscholar.org/paper/7150cbbbef826cc9c753a44c947a689c8237fe06,2009 IEEE International Symposium on Information Theory
130,Adaptive deadlock- and livelock-free routing with all minimal paths in Torus networks,"This paper consists of two parts. In the first part, two new algorithms for deadlock- and livelock-free wormhole routing in the torus network are presented. The first algorithm, called Channels, is for the n-dimensional torus network. This technique is fully-adaptive minimal, that is, all paths with a minimal number of hops from source to destination are available for routing, and needs only five virtual channels per bidirectional link, the lowest channel requirement known in the literature for fully-adaptive minimal worm-hole routing. In addition, this result also yields the lowest buffer requirement known in the literature for packet-switched fully-adaptive minimal routing. The second algorithm, called 4-Classes, is for the bidimensional torus network. This technique is fully-adaptive minimal and requires only eight virtual channels per bidirectional link. Also, it allows for a highly parallel implementation of its associated routing node. In the second part of this paper, four worm-hole routing techniques for the two-dimensional torus are experimentally evaluated using a dynamic message injection model and different traffic patterns and message lengths. >",1992-06-01,https://www.semanticscholar.org/paper/278dd6415f07db4d6ec13096642e4f1cf5189f58,ACM Symposium on Parallelism in Algorithms and Architectures
2179,Relationships between blood leukocyte mitochondrial DNA copy number and inflammatory cytokines in knee osteoarthritis,"Osteoarthritis (OA) is a degenerative articular disorder manifested by cartilage destruction, subchondral sclerosis, osteophytes, and synovitis, resulting in chronic joint pain and physical disability in the elderly. The purpose of this study was to investigate mitochondrial DNA copy number (mtDNACN) and inflammatory cytokines in primary knee OA patients and healthy volunteers. A total of 204 knee OA patients and 169 age-matched healthy volunteers were recruited. Their relative blood leukocyte mtDNACN was assessed by quantitative real-time polymerase chain reaction (qRT-PCR), and ten inflammatory cytokines in their plasma were detected by multiplex immunoassay. Blood leukocyte mtDNACN in the OA group was significantly lower than that in the control group. Leukocyte mtDNACN in the control group was negatively correlated with their age (r=−0.380, P<0.0001), whereas mtDNACN in the OA group was positively correlated with their age (r=0.198, P<0.001). Plasma interleukin-4 (IL-4) and IL-6 were significantly higher in the knee OA group than in the control group. The plasma IL-6 level was positively correlated with blood leukocyte mtDNACN in the OA group (r=0.547, P=0.0014). IL-5 showed as a major factor (coefficient 0.69) in the second dimension of principle components analysis (PCA)-transformed data and was significantly higher in the OA group (P<0.001) as well as negatively correlated with mtDNACN (r=−0.577, P<0.001). These findings suggest that elevation of plasma IL-4 and IL-6 and a relative reduction in mtDNACN might be effective biomarkers for knee OA. IL-5 is a plausible factor responsible for decreasing blood leukocyte mtDNACN in knee OA patients.",2019-12-21,https://www.semanticscholar.org/paper/3e66b05f0cd7e4a7fecf6413d91fa005e609b2cd,Journal of Zhejiang University SCIENCE B
3190,A new classification of mammalian uni-male multi-female groups based on the fundamental principles governing inter- and intrasexual relationships,,2021-11-01,https://www.semanticscholar.org/paper/4a1be45eacc753291d3153ac04b1d0772da623a1,Behavioral Ecology and Sociobiology
3416,Max-min Fair Rate Allocation and Routing in Energy Harvesting Networks: Algorithmic Analysis,,2014-06-14,https://www.semanticscholar.org/paper/994215e7ebabbb7613b62738fff28c342fb8df14,ACM Interational Symposium on Mobile Ad Hoc Networking and Computing
2263,Oscillations in NF-kappaB signaling control the dynamics of gene expression.,"Signaling by the transcription factor nuclear factor kappa B (NF-kappaB) involves its release from inhibitor kappa B (IkappaB) in the cytosol, followed by translocation into the nucleus. NF-kappaB regulation of IkappaBalpha transcription represents a delayed negative feedback loop that drives oscillations in NF-kappaB translocation. Single-cell time-lapse imaging and computational modeling of NF-kappaB (RelA) localization showed asynchronous oscillations following cell stimulation that decreased in frequency with increased IkappaBalpha transcription. Transcription of target genes depended on oscillation persistence, involving cycles of RelA phosphorylation and dephosphorylation. The functional consequences of NF-kappaB signaling may thus depend on number, period, and amplitude of oscillations.",,https://www.semanticscholar.org/paper/7973a9988e4a3cc3381a0d56b53adaa74b5952d4,Science
1363,Search for supersymmetry with gauge-mediated breaking in diphoton events at D0.,"We report the results of a search for supersymmetry (SUSY) with gauge-mediated breaking in the missing transverse energy distribution of inclusive diphoton events using 263 pb(-1) of data collected by the D0 experiment at the Fermilab Tevatron Collider in 2002-2004. No excess is observed above the background expected from standard model processes, and lower limits on the masses of the lightest neutralino and chargino of about 108 and 195 GeV, respectively, are set at the 95% confidence level. These are the most stringent limits to date for models with gauge-mediated SUSY breaking with a short-lived neutralino as the next-to-lightest SUSY particle.",2004-08-30,https://www.semanticscholar.org/paper/0729fb5586404cf7f666dba612cea89b317f9d4b,Physical Review Letters
2891,Using epigenomics to understand cellular responses to environmental influences in diseases,"It is a generally accepted model that environmental influences can exert their effects, at least in part, by changing the molecular regulators of transcription that are described as epigenetic. As there is biochemical evidence that some epigenetic regulators of transcription can maintain their states long term and through cell division, an epigenetic model encompasses the idea of maintenance of the effect of an exposure long after it is no longer present. The evidence supporting this model is mostly from the observation of alterations of molecular regulators of transcription following exposures. With the understanding that the interpretation of these associations is more complex than originally recognised, this model may be oversimplistic; therefore, adopting novel perspectives and experimental approaches when examining how environmental exposures are linked to phenotypes may prove worthwhile. In this review, we have chosen to use the example of nonalcoholic fatty liver disease (NAFLD), a common, complex human disease with strong environmental and genetic influences. We describe how epigenomic approaches combined with emerging functional genetic and single-cell genomic techniques are poised to generate new insights into the pathogenesis of environmentally influenced human disease phenotypes exemplified by NAFLD.",2023-01-01,https://www.semanticscholar.org/paper/27513e9e7c3b7ba2222da2689b39de05fe799f62,PLoS Genetics
2068,Using Rough Set Theory to Recruit and Retain High-Potential Talents for Semiconductor Manufacturing,"To recruit and retain high-potential talent is critical for semiconductor companies to maintain competitive advantages in a modern knowledge-based economy. Conventional personnel selection methodologies focusing on static work and job analysis will no longer be appropriate for knowledge workers in high-tech industries. This paper aims to develop an effective data mining approach based on Rough Set Theory to explore and analyze human resource data for personnel selection and human capital enhancement. An empirical study was conducted in a leading semiconductor company in Taiwan to estimate the validity of the proposed approach for predicting work behaviors including performance and resignation. The results showed that latent knowledge can be discovered as a basis to derive specific recruitment and human resource management strategies. In particular, 29 rules have been adopted as references for recruiting the right talent. This paper concludes with discussions of empirical findings and future research directions.",2007-11-12,https://www.semanticscholar.org/paper/7ebdcc84a4e7371d7513debcc7d49276b04240cf,IEEE transactions on semiconductor manufacturing
1004,Low Limit for Effective Signal Strength in the Stratus OCT in Imperative Low Signal Strength Cases,"Purpose To determine the lowest limit of signal strength that is still effective for accurate analysis of optic coherence tomography (OCT) values, we investigated the reproducibility of OCT scans by signal strength (SS). Methods A total of 668 subjects were scanned for measurements of retinal nerve fiber layer (RNFL) thickness using the Stratus OCT twice on the same day. The variability of overall RNFL thickness parameters obtained at different SS was analyzed and compared by repeated-measures of ANOVA and Spearman's correlation coefficient. Values of the intraclass correlation coefficient (ICC) and variability (standard deviation) of RNFL thickness were obtained. The false positive ratio was analyzed. Results When SS was 3, the variability of RNFL thickness was significantly different (low ICC, high variability) in comparison to when SS was 4 or greater. Significant negative correlations were observed between variability in RNFL thickness and signal strength. The difference of variability of average RNFL thickness between SS 4 (4.94 µm) and SS 6 (4.41 µm) was 0.53 µm. Conclusions Clinically, the difference of variability of average RNFL thickness between SS 4 and SS 6 was quite small. High SS is important, however, when signal strength is low due to uncorrectable factors in patients in need of OCT for glaucoma and retinal disease. Our results suggest that SS 4 is the lowest acceptable limit of signal strength for obtaining reproducible scanning images.",2012-05-22,https://www.semanticscholar.org/paper/4e85363a86af1d5b41804faa2ed4693790c68d3b,Korean Journal of Ophthalmology
373,Learning the Internet,,2002-07-08,https://www.semanticscholar.org/paper/8379729ba4fc0c07834bf9834d73d16175b27f66,Annual Conference Computational Learning Theory
202,From Nash Equilibria to Chain Recurrent Sets: Solution Concepts and Topology,"Nash's universal existence theorem for his notion of equilibria was essentially an ingenious application of fixed point theorems, the most sophisticated result in his era's topology --- in fact, recent algorithmic work has established that Nash equilibria are in fact computationally equivalent to fixed points. Here, we shift focus to universal non-equilibrium solution concepts that arise from an important theorem in the topology of dynamical systems that was unavailable to Nash. This approach takes as input both a game and a learning dynamic, defined over mixed strategies. Nash equilibria are guaranteed to be fixed points of such dynamics; however, the system behavior is captured by a more general object that is known in dynamical systems theory as chain recurrent set. Informally, once we focus on this solution concept, every game behaves like a potential game with the dynamic converging to these states. We characterize this solution for simple benchmark games under replicator dynamics, arguably the best known evolutionary dynamic in game theory. For potential games it coincides with the notion of equilibrium; however, in simple zero sum games, it can cover the whole state space. We discuss numerous novel computational as well as structural, combinatorial questions that chain recurrence raises.",2016-01-14,https://www.semanticscholar.org/paper/77512cd92ad8a22e756835c74c51c7853ab2cd0e,Information Technology Convergence and Services
1588,Exponential Families,"Surprisingly many of the distributions we use in statistics for random variables X taking value in some space X (often R or N0 but sometimes R n, Z, or some other space), indexed by a parameter θ from some parameter set Θ, can be written in exponential family form, with pdf or pmf f(x | θ) = exp [η(θ)t(x) −B(θ)] h(x) for some statistic t : X → R, natural parameter η : Θ → R, and functions B : Θ → R and h : X → R+. The likelihood function for a random sample of size n from the exponential family is",2018-11-19,https://www.semanticscholar.org/paper/077cacdc9754c7dc2c7fa072ee57417cb7d61218,Graduate Studies in Mathematics
1802,Distance dependent Chinese restaurant processes,"We develop the distance dependent Chinese restaurant process (CRP), a flexible class of distributions over partitions that allows for non-exchangeability. This class can be used to model dependencies between data in infinite clustering models, including dependencies across time or space. We examine the properties of the distance dependent CRP, discuss its connections to Bayesian nonparametric mixture models, and derive a Gibbs sampler for both observed and mixture settings. We study its performance with time-dependent models and three text corpora. We show that relaxing the assumption of exchangeability with distance dependent CRPs can provide a better fit to sequential data. We also show its alternative formulation of the traditional CRP leads to a faster-mixing Gibbs sampling algorithm than the one based on the original formulation.",2009-10-06,https://www.semanticscholar.org/paper/72cc610bcdaf4166839eeff04776adcea225439f,International Conference on Machine Learning
1827,A Topic Model for Word Sense Disambiguation,"We develop latent Dirichlet allocation with WORDNET (LDAWN), an unsupervised probabilistic topic model that includes word sense as a hidden variable. We develop a probabilistic posterior inference algorithm for simultaneously disambiguating a corpus and learning the domains in which to consider each word. Using the WORDNET hierarchy, we embed the construction of Abney and Light (1999) in the topic model and show that automatically learned domains improve WSD accuracy compared to alternative contexts.",2007-06-01,https://www.semanticscholar.org/paper/a91760aca33559a6c7703c0fccf3289e1c4dd729,Conference on Empirical Methods in Natural Language Processing
1833,PU-BCD: Exponential Family Models for the Coarse- and Fine-Grained All-Words Tasks,,,https://www.semanticscholar.org/paper/dc6107c5507d252e10a28ae68405c7f64bf8c089,International Workshop on Semantic Evaluation
3541,Open pattern matching for C++,"Pattern matching is an abstraction mechanism that can greatly simplify source code. We present functional-style pattern matching for C++ implemented as a library, called Mach71. All the patterns are user-definable, can be stored in variables, passed among functions, and allow the use of class hierarchies. As an example, we implement common patterns used in functional languages.
 Our approach to pattern matching is based on compile-time composition of pattern objects through concepts. This is superior (in terms of performance and expressiveness) to approaches based on run-time composition of polymorphic pattern objects. In particular, our solution allows mapping functional code based on pattern matching directly into C++ and produces code that is only a few percent slower than hand-optimized C++ code.
 The library uses an efficient type switch construct, further extending it to multiple scrutinees and general patterns. We compare the performance of pattern matching to that of double dispatch and open multi-methods in C++.",2014-03-05,https://www.semanticscholar.org/paper/391c58a1a7510f6c975d29349705b1058bb05efc,International Conference on Generative Programming: Concepts and Experiences
3106,Experiences teaching operating systems using virtual platforms and linux,"Operating system courses teach students much more when they provide hands-on kernel-level project experience with a real operating system. However, enabling a large class of students to do kernel development can be difficult. To address this problem, we created a virtual kernel development environment in which operating systems can be developed, debugged, and rebooted in a shared computer facility without affecting other users. Using virtual machines and remote display technology, our virtual kernel development laboratory enables even distance learning students at remote locations to participate in kernel development projects with on-campus students. We have successfully deployed and used our virtual kernel development environment together with the open-source Linux kernel to provide kernel-level project experiences for over nine hundred students in the introductory operating system course at Columbia University.",,https://www.semanticscholar.org/paper/369699cb9e83032b6de03deb02a1029dee4ae030,Technical Symposium on Computer Science Education
2304,"Neutrophil apoptosis is delayed by the diadenosine polyphosphates, Ap5A and Ap6A: synergism with granulocyte‐macrophage colony‐stimulating factor","In addition to ATP, platelets and other cell types can secrete high quantities of diadenosine polyphosphates Ap3A, Ap4A, Ap5A and Ap6A. There is increasing evidence to show that these molecules can function as novel modulators of cell function. For this report we have measured the effects of the diadenosine polyphosphates Ap5A and Ap6A on neutrophil apoptosis. These molecules can themselves delay neutrophil apoptosis (as assessed by morphology, function, CD16 expression and chromatin integrity), and are as effective on a molar basis as ATP, Ap3A and Ap4A. Moreover, these dinucleotides act synergistically with granulocyte‐macrophage colony‐stimulating factor (GM‐CSF) to delay neutrophil apoptosis. Thus, diadenosine polyphosphates may act, in concert with cytokines, as novel modulators of neutrophil function and survival in certain types of inflammatory conditions.",1996-12-01,https://www.semanticscholar.org/paper/3f42dafdab68b3ef914e6474235d2d89635a37d3,British Journal of Haematology
382,"Game theory, algorithms, and the Internet","Among the many characteristics of the Internet (huge and growing, available and unstructured, dynamic and chaotic), perhaps the most novel, distinguishing, and intellectually challenging one is that, unlike previous computational artifacts and systems, the Internet is built, operated, and used by a dazzling diversity of economic interests, in various degrees of collaboration and competition with each other. Consequently, it can be argued that the mathematical arsenal necessary for attaining an algorithmic and conceptual understanding of the Internet must include some kind of fusion between mathematical economics (especially game theory and its inverse problem, mechanism design) and algorithmic thinking. In this talk I shall survey recent formalisms and results aiming in this general direction, and discuss the research agenda that appears to be emerging.",2001-01-09,https://www.semanticscholar.org/paper/2a7d35c80cd6ba315c9bf42c459d87aa9eb89ce3,ACM-SIAM Symposium on Discrete Algorithms
820,On nested depth first search,We show in this paper that the algorithm for solving the model checking problem with a nested depth-first search can interfere with algorithms that support partial order reduction. We introduce a revised version of the algorithm that guarantees compatibility. The change also improves the performance of the nested depth-first search algorithm when partial order reduction is not used.,,https://www.semanticscholar.org/paper/c35f37a7980d4523d6de2f7ffbb4797405d1bc93,The Spin Verification System
1699,Introduction to Mixed Membership Models and Methods,1.1 Historical Developments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 1.2 A General Formulation for Mixed Membership Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 1.3 Advantages of Mixed Membership Models in Applied Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 1.4 Theoretical Issues with Mixed Membership Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 1.4.1 General Issues Inherent to Mixtures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10,2014-11-06,https://www.semanticscholar.org/paper/6cec1861b1717d3ec1f558d48d40ae23e5d42b1e,Handbook of Mixed Membership Models and Their Applications
404,Worst-case Equilibria,,1999-03-04,https://www.semanticscholar.org/paper/3ae866f55cebbffe18d07c27d7d5fd1b7dea6e14,Symposium on Theoretical Aspects of Computer Science
2820,"Ablation of a galectin preferentially expressed in adipocytes increases lipolysis, reduces adiposity, and improves insulin sensitivity in mice","The breakdown of triglycerides, or lipolysis, is a tightly controlled process that regulates fat mobilization in accord with an animal's energy needs. It is well established that lipolysis is stimulated by hormones that signal energy demand and is suppressed by the antilipolytic hormone insulin. However, much still remains to be learned about regulation of lipolysis by intracellular signaling pathways in adipocytes. Here we show that galectin-12, a member of a β-galactoside–binding lectin family preferentially expressed by adipocytes, functions as an intrinsic negative regulator of lipolysis. Galectin-12 is primarily localized on lipid droplets and regulates lipolytic protein kinase A signaling by acting upstream of phosphodiesterase activity to control cAMP levels. Ablation of galectin-12 in mice results in increased adipocyte mitochondrial respiration, reduced adiposity, and ameliorated insulin resistance/glucose intolerance. This study identifies unique properties of this intracellular galectin that is localized to an organelle and performs a critical function in lipid metabolism. These findings add to the significant functions exhibited by intracellular galectins, and have important therapeutic implications for human metabolic disorders.",2011-10-03,https://www.semanticscholar.org/paper/6d12b82abbcaf131b48078946682aadd4a57ccf3,Proceedings of the National Academy of Sciences of the United States of America
2987,Nonparametric Bayesian Sparse Factor Models with application to Gene Expression modelling,"A nonparametric Bayesian extension of Factor Analysis (FA) is proposed where observed data $\mathbf{Y}$ is modeled as a linear superposition, $\mathbf{G}$, of a potentially infinite number of hidden factors, $\mathbf{X}$. The Indian Buffet Process (IBP) is used as a prior on $\mathbf{G}$ to incorporate sparsity and to allow the number of latent features to be inferred. The model's utility for modeling gene expression data is investigated using randomly generated data sets based on a known sparse connectivity matrix for E. Coli, and on three biological data sets of increasing complexity.",2010-11-29,https://www.semanticscholar.org/paper/effe59b2ac139fcef5cc5f7470f323a09fb1cd6f,Annals of Applied Statistics
2328,Phospholipase D-dependent and-independent activation of the neutrophil NADPH oxidase,,1994-04-01,https://www.semanticscholar.org/paper/983bcb6372b2ae1c1e7efe036fb507fb490d3e44,Bioscience Reports
2926,A human lung tumor microenvironment interactome identifies clinically relevant cell-type cross-talk,,2020-05-07,https://www.semanticscholar.org/paper/2d8a5c8740384b4f8f10620597e7e859133249eb,Genome Biology
1795,Connections between the lines: augmenting social networks with text,"Network data is ubiquitous, encoding collections of relationships between entities such as people, places, genes, or corporations. While many resources for networks of interesting entities are emerging, most of these can only annotate connections in a limited fashion. Although relationships between entities are rich, it is impractical to manually devise complete characterizations of these relationships for every pair of entities on large, real-world corpora.
 In this paper we present a novel probabilistic topic model to analyze text corpora and infer descriptions of its entities and of relationships between those entities. We develop variational methods for performing approximate inference on our model and demonstrate that our model can be practically deployed on large corpora such as Wikipedia. We show qualitatively and quantitatively that our model can construct and annotate graphs of relationships and make useful predictions.",2009-06-28,https://www.semanticscholar.org/paper/161cc1dc14a4c30f16b35fac1868f4b9b9ad7f1d,Knowledge Discovery and Data Mining
1554,Invariant Representation Learning for Treatment Effect Estimation,"The defining challenge for causal inference from observational data is the presence of `confounders', covariates that affect both treatment assignment and the outcome. To address this challenge, practitioners collect and adjust for the covariates, hoping that they adequately correct for confounding. However, including every observed covariate in the adjustment runs the risk of including `bad controls', variables that \emph{induce} bias when they are conditioned on. The problem is that we do not always know which variables in the covariate set are safe to adjust for and which are not. To address this problem, we develop Nearly Invariant Causal Estimation (NICE). NICE uses invariant risk minimization (IRM) [Arj19] to learn a representation of the covariates that, under some assumptions, strips out bad controls but preserves sufficient information to adjust for confounding. Adjusting for the learned representation, rather than the covariates themselves, avoids the induced bias and provides valid causal inferences. NICE is appropriate in the following setting. i) We observe data from multiple environments that share a common causal mechanism for the outcome, but that differ in other ways. ii) In each environment, the collected covariates are a superset of the causal parents of the outcome, and contain sufficient information for causal identification. iii) But the covariates also may contain bad controls, and it is unknown which covariates are safe to adjust for and which ones induce bias. We evaluate NICE on both synthetic and semi-synthetic data. When the covariates contain unknown collider variables and other bad controls, NICE performs better than existing methods that adjust for all the covariates.",2020-11-24,https://www.semanticscholar.org/paper/44071a76a00a37caea959068800f5be1b0192f1c,Conference on Uncertainty in Artificial Intelligence
2127,An object-oriented analysis and design method for shop floor control systems,"An object-oriented analysis and design method is proposed here for the modelling of shop floor control systems. The major idea is to convert the static structure and the dynamic behaviour of a controlled system into the functional specification of a controlling system by using some modelling techniques. Then the shop floor control logic is translated into the language of the G2 expert system. In this way, it is easier to achieve the computer-integrated manufacturing (CIM) at the shop floor level. The proposed method is different from the procedure-oriented method, which is based on the functional decomposition of a system, and has the potential to be applied to more complex systems. During the modelling process, its modularity and reusability can dramatically reduce the complexity in system analysis and provides more flexibility for system development. The design of the shop floor control system of a mould-filling shop is used to illustrate the proposed method.",1998-01-01,https://www.semanticscholar.org/paper/0e44471231885dbc9df0c806b692bfbc038f8c2f,International journal of computer integrated manufacturing (Print)
1739,Modeling Overlapping Communities with Node Popularities,"We develop a probabilistic approach for accurate network modeling using node popularities within the framework of the mixed-membership stochastic block-model (MMSB). Our model integrates two basic properties of nodes in social networks: homophily and preferential connection to popular nodes. We develop a scalable algorithm for posterior inference, based on a novel nonconjugate variant of stochastic variational inference. We evaluate the link prediction accuracy of our algorithm on nine real-world networks with up to 60,000 nodes, and on simulated networks with degree distributions that follow a power law. We demonstrate that the AMP predicts significantly better than the MMSB.",2013-12-05,https://www.semanticscholar.org/paper/e2df74941a0a3f11f6126b009e4c154f97600d13,Neural Information Processing Systems
2574,Prototyping retractable string-based interaction techniques for dual-display mobile devices,"Accessing information on mobile and wearable devices often requires the user's visual attention, and the precise operation of virtual or physical widgets. However, these interactions may sometimes be too time-consuming and socially inappropriate. To address this, we introduce a novel input/output device that is based on the manipulation of a retractable string in a polar coordinate frame. Depending on how the user pulls the string from its enclosure--to a particular length, at a particular angle--various system features may be directly accessed. Furthermore, we present our concept for a 1D pixel array, embedded in the string that may be used as a secondary 1D display. Since it is possible to unwind the display itself and trigger functionality with a single pull, information may be accessed and presented quickly, and perceived at a glance. We present scenarios for how the string input/output device may be used in conjunc-tion with the mobile device's primary 2D display and describe our augmented reality proof-of-concept prototype.",2006-04-22,https://www.semanticscholar.org/paper/895131b2e558ba3a2824ca422dc0afa20dd37ea9,International Conference on Human Factors in Computing Systems
1906,An Integrated Approach for IC Design R&D Portfolio Decision and Project Scheduling and a Case Study,"Research and development (R&D) projects are crucial for semiconductor companies to maintain growth, profitability, and competitiveness. Integrated circuit (IC) design is capital intensive and continuously migrates to new technologies to meet various market demands. Moreover, the scheduling of selected R&D projects that enables technology roadmap involving complicated interrelationships, while competing for similar resources. Focusing on realistic needs, this paper aims to propose an integrated approach for selecting IC design projects for R&D portfolios and scheduling the selected projects simultaneously. In particular, a hybrid autotuning multiobjective genetic algorithm was developed to solve large sized problem instances. An empirical study was conducted at a leading IC design service company in Taiwan to test the validity of the proposed approach. The proposed algorithm was compared with conventional approaches for both convergence and diversity. The results have shown the practical viability of this approach in efficiently and effectively generating near-optimal portfolio alternatives for portfolio selection. The approach also enables the scheduling of the selected projects to achieve R&D portfolio objectives. The developed solution was fully implemented and adopted by the company.",2018-01-12,https://www.semanticscholar.org/paper/6a9664cbd7d3e41c746ae1dc2fb0925489bbb2c0,IEEE transactions on semiconductor manufacturing
456,Optimal Information Delivery,,1995-12-04,https://www.semanticscholar.org/paper/a1b40a67999e055bb2e625730e9d23d1235f8b1d,International Symposium on Algorithms and Computation
173,Reductions in PPP,,2019-05-01,https://www.semanticscholar.org/paper/9040aef172a9a0af63a835b73f9c8fe5033f6929,Information Processing Letters
3268,Individual recognition through olfactory–auditory matching in lemurs,"Individual recognition can be facilitated by creating representations of familiar individuals, whereby information from signals in multiple sensory modalities become linked. Many vertebrate species use auditory–visual matching to recognize familiar conspecifics and heterospecifics, but we currently do not know whether representations of familiar individuals incorporate information from other modalities. Ring-tailed lemurs (Lemur catta) are highly visual, but also communicate via scents and vocalizations. To investigate the role of olfactory signals in multisensory recognition, we tested whether lemurs can recognize familiar individuals through matching scents and vocalizations. We presented lemurs with female scents that were paired with the contact call either of the female whose scent was presented or of another familiar female from the same social group. When the scent and the vocalization came from the same individual versus from different individuals, females showed greater interest in the scents, and males showed greater interest in both the scents and the vocalizations, suggesting that lemurs can recognize familiar females via olfactory–auditory matching. Because identity signals in lemur scents and vocalizations are produced by different effectors and often encountered at different times (uncoupled in space and time), this matching suggests lemurs form multisensory representations through a newly recognized sensory integration underlying individual recognition.",2014-06-07,https://www.semanticscholar.org/paper/128ee4887ea51f263852fc803fb4b29e23b7a02a,Proceedings of the Royal Society B: Biological Sciences
863,Linear approximation of shortest superstrings,"We consider the following problem: given a collection of strings s1,…, sm, find the shortest string s such that each si appears as a substring (a consecutive block) of s. Although this problem is known to be NP-hard, a simple greedy procedure appears to do quite well and is routinely used in DNA sequencing and data compression practice, namely: repeatedly merge the pair of (distinct) strings with maximum overlap until only one string remains. Let n denote the length of the optimal superstring. A common conjecture states that the above greedy procedure produces a superstring of length O(n) (in fact, 2n), yet the only previous nontrivial bound known for any polynomial-time algorithm is a recent O(n log n) result.We show that the greedy algorithm does in fact achieve a constant factor approximation, proving an upper bound of 4n. Furthermore, we present a simple modified version of the greedy algorithm that we show produces a superstring of length at most 3n. We also show the superstring problem to be MAXSNP-hard, which implies that a polynomial-time approximation scheme for this problem is unlikely.",1991-01-03,https://www.semanticscholar.org/paper/94dc290720959e2f8f4314e1c119db19775038e5,Symposium on the Theory of Computing
2292,"Stimulation of primed neutrophils by soluble immune complexes: priming leads to enhanced intracellular Ca2+ elevations, activation of phospholipase D, and activation of the NADPH oxidase.","Soluble immune complexes activate a rapid burst of reactive oxidant secretion from neutrophils that have previously been primed with GM-CSF. Binding of these complexes to the cell surface of unprimed neutrophils results in the generation of intracellular Ca2+ transients, but the NADPH oxidase fails to become activated. No phospholipase D activity was observed following the addition of soluble immune complexes to unprimed cells. Upon priming with GM-CSF, the intracellular Ca2+ signal generated following soluble complex binding was greatly extended and phospholipase D was activated: there was also increased phosphorylation of proteins on tyrosine residues and the NADPH oxidase was activated. When Ca2+ influx was prevented, this phospholipase D activity was not observed. This primed oxidase activity was completely inhibited by erbstatin. Treatment of unprimed neutrophils with pervanadate (to inhibit protein tyrosine phosphatases) mimicked the effects of priming in that pervanadate-treated neutrophils secreted reactive oxidants in response to soluble immune complexes. The data indicate that during priming a new signaling pathway is activated that involves Ca2+ influx, phosphorylation on tyrosine residues, phospholipase D activity, and NADPH oxidase activation.",1998-06-29,https://www.semanticscholar.org/paper/65b5dba8816b57993072c210fd7f968056050b39,Biochemical and Biophysical Research Communications - BBRC
591,Hamilton Paths in Grid Graphs,"A grid graph is a node-induced finite subgraph of the infinite grid. It is rectangular if its set of nodes is the product of two intervals. Given a rectangular grid graph and two of its nodes, we give necessary and sufficient conditions for the graph to have a Hamilton path between these two nodes. In contrast, the Hamilton path (and circuit) problem for general grid graphs is shown to be NP-complete. This provides a new, relatively simple, proof of the result that the Euclidean traveling salesman problem is NP-complete.",1982-11-01,https://www.semanticscholar.org/paper/4d2abfc483c9ffa4187d43269a53ec37e64e1c5d,SIAM journal on computing (Print)
1848,Statistical modeling of biomedical corpora: mining the Caenorhabditis Genetic Center Bibliography for genes related to life span,,2006-05-08,https://www.semanticscholar.org/paper/afd354506626d4b9ee30de86239a82d586f5800c,BMC Bioinformatics
1865,Latent Dirichlet Allocation,,2001-01-03,https://www.semanticscholar.org/paper/f198043a866e9187925a8d8db9a55e3bfdd47f2c,Journal of machine learning research
445,Topological queries in spatial databases,"Handling spatial information is required by many database applications, and each poses different requirements on query languages. In many cases the precise size of the regions is important, while in other applications we may only be interested in the TOPOLOGICAL relations- hips between regions — intuitively, those that pertain to adjacency and connectivity properties of the regions, and are therefore invariant under homeomorphisms. Such differences in scope and emphasis are crucial, as they affect the data model, the query language, and performance. This talk focuses on queries targeted towards topological information for two- dimensional spatial databases, where regions are specified by polynomial inequalities with integer coeficients. We focus on two main aspects: (i) languages for expressing topological queries, and (ii) the representation of topological information. In regard to (i), we study several languages geared towards topological queries, building upon well-known topologi- cal relationships between pairs of planar regions proposed by Egenhofer. In regard to (ii), we show that the topological information in a spatial database can be precisely summarized by a finite relational database which can be viewed as a topological annotation to the raw spatial data. All topological queries can be answered using this annotation, called to- pological invariant. This yields a potentially more economical evaluation strategy for such queries, since the topological invariant is generally much smaller than the raw data. We examine in detail the problem of transla- ting topological queries against the spatial database into queries against the topological invariant. The languages considered are first-order on the spatial database side, and fixpoint and first-order on the topological in- variant side. In particular, it is shown that fixpoint expresses precisely the PTIME queries on topological invariants. This suggests that topolo- gical invariants are particularly well-behaved with respect to descriptive complexity. (Based on joint work with C.H.Papadimitriou, D. Suciu and L. Segoufin.)",1996-06-03,https://www.semanticscholar.org/paper/1de9a034e0511eb2800c5947309f32106ab73da5,Journal of computer and system sciences (Print)
73,Querying text databases for efficient information extraction,"A wealth of information is hidden within unstructured text. This information is often best exploited in structured or relational form, which is suited for sophisticated query processing, for integration with relational databases, and for data mining. Current information extraction techniques extract relations from a text database by examining every document in the database, or use filters to select promising documents for extraction. The exhaustive scanning approach is not practical or even feasible for large databases, and the current filtering techniques require human involvement to maintain and to adapt to new databases and domains. We develop an automatic query-based technique to retrieve documents useful for the extraction of user-defined relations from large text databases, which can be adapted to new domains, databases, or target relations with minimal human effort. We report a thorough experimental evaluation over a large newspaper archive that shows that we significantly improve the efficiency of the extraction process by focusing only on promising documents.",2003-03-05,https://www.semanticscholar.org/paper/dda99e75f4c54db13bb891060c63cb796cb44466,Proceedings / International Conference on Data Engineering
1943,An Algorithm of Multi-Subpopulation Parameters With Hybrid Estimation of Distribution for Semiconductor Scheduling With Constrained Waiting Time,"Scheduling for wafer fabrication of advanced technology nodes entails complicated constraints such as limited waiting times. Focusing on real settings, this paper aims to develop a novel genetic algorithm of multi-subpopulation parameters with hybrid estimation of distribution (MSPHEDA) to solve the present problem effectively and efficiently. To estimate the validity of this approach, ten scenarios were simulated on the basis of empirical data as the basis to compare the performance of MSPHEDA and other heuristic methods for minimizing makespan and reducing the total exceeded limited waiting time. The results have shown practical viability of the proposed approach.",2015-06-01,https://www.semanticscholar.org/paper/13a80ee9203dc54be0473e07957a2da82f116ac1,IEEE transactions on semiconductor manufacturing
3238,Animal Population Censusing at Scale with Citizen Science and Photographic Identification,"Population censusing is critical to monitoring the health of an animal population. A census results in a population size estimate, which is a fundamental metric for deciding the demographic and conservation status of a species. Current methods for producing a population census are expensive, demanding, and may be invasive, leading to the use of overly-small sample sizes. In response, we propose to use volunteer citizen scientists to collect large numbers of photographs taken over large geographic areas, and to use computer vision algorithms to semi-automatically identify and count individual animals. Our data collection and processing are distributed, non-invasive, and require no specialized hardware and no scientific training. Our method also engages the community directly in conservation. We analyze the results of two population censusing events, the Great Zebra and Giraffe Count (2015) and the Great Grevy’s Rally (2016), where combined we processed over 50,000 photographs taken with more than 200 different cameras and over 300 on-the-ground volunteers.",,https://www.semanticscholar.org/paper/46a934b8c1b2ee73d4f855e9b16a646779157ba8,AAAI Spring Symposia
2925,Molecular Choreography of Acute Exercise,"Exercise testing is routinely used in clinical practice to assess fitness - a strong predictor of survival - as well as causes of exercise limitations. While these studies often focus on cardiopulmonary response and selected molecular pathways, the dynamic system-wide molecular response to exercise has not been fully characterized. We performed a longitudinal multi-omic profiling of plasma and peripheral blood mononuclear cells including transcriptome, immunome, proteome, metabolome and lipidome in 36 well-characterized volunteers before and after a controlled bout of acute exercise (2, 15, 30 min and 1 hour in recovery). Integrative analysis revealed an orchestrated choreography of biological processes across key tissues. Most of these processes were dampened in insulin resistant participants. Finally, we discovered biological pathways involved in exercise capacity and developed prediction models revealing potential resting blood-based biomarkers of fitness.",2020-01-14,https://www.semanticscholar.org/paper/177d5b1da1097f0f91368610b486eb98a2da337f,Cell
2604,Visual end user configuration of hybrid user interfaces,"Hybrid user interfaces are a promising paradigm for human-computer interaction, employing a range of displays and devices. However, most experimental hybrid user interfaces use a relatively rigid configuation. Our demo explores the possibilities of end users configuring the setup of a hybrid user interface, using novel interaction techniques and visualizations, based on a shared augmented reality.",2004-10-15,https://www.semanticscholar.org/paper/a64a05dd8f3854b5d5d2c3331ff49da0b17a99dc,ACM SIGMM workshop on Experiential Telepresence
3038,A measurement study of google play,"Although millions of users download and use third-party Android applications from the Google Play store, little information is known on an aggregated level about these applications. We have built PlayDrone, the first scalable Google Play store crawler, and used it to index and analyze over 1,100,000 applications in the Google Play store on a daily basis, the largest such index of Android applications. PlayDrone leverages various hacking techniques to circumvent Google's roadblocks for indexing Google Play store content, and makes proprietary application sources available, including source code for over 880,000 free applications. We demonstrate the usefulness of PlayDrone in decompiling and analyzing application content by exploring four previously unaddressed issues: the characterization of Google Play application content at large scale and its evolution over time, library usage in applications and its impact on application portability, duplicative application content in Google Play, and the ineffectiveness of OAuth and related service authentication mechanisms resulting in malicious users being able to easily gain unauthorized access to user data and resources on Amazon Web Services and Facebook.",2014-06-16,https://www.semanticscholar.org/paper/a58b4e6528a9c2698edd45ac1e16e017b82143b8,Measurement and Modeling of Computer Systems
2540,Lets go out: Research in outdoor mixed and augmented reality,,2009-10-19,https://www.semanticscholar.org/paper/fc0c54d72ba200268c9769469f178f134bba376b,2009 8th IEEE International Symposium on Mixed and Augmented Reality
1377,Search for doubly charged higgs boson pair production in the decay to mu(+)mu(+)mu(-)mu(-) in pp collisions at sqrt[s]=1.96 TeV.,"A search for pair production of doubly charged Higgs bosons in the process pp -->H(++)H(--) -->mu(+)mu(+)mu(-)mu(-) is performed with the D0 run II detector at the Fermilab Tevatron. The analysis is based on a sample of inclusive dimuon data collected at an energy of sqrt[s]=1.96 TeV, corresponding to an integrated luminosity of 113 pb(-1). In the absence of a signal, 95% confidence level mass limits of M(H(+/-+/-)(L))>118.4 GeV/c(2) and M(H(+/-+/-)(R))>98.2 GeV/c(2) are set for left-handed and right-handed doubly charged Higgs bosons, respectively, assuming 100% branching into muon pairs.",,https://www.semanticscholar.org/paper/c5a38c8ca241e3be8363d65309b712d9def4cc67,Physical Review Letters
2957,Impact of the X Chromosome and sex on regulatory variation,"The X chromosome, with its unique mode of inheritance, contributes to differences between the sexes at a molecular level, including sex-specific gene expression and sex-specific impact of genetic variation. We have conducted an analysis of the impact of both sex and the X chromosome on patterns of gene expression identified through transcriptome sequencing of whole blood from 922 individuals. We identified that genes on the X chromosome are more likely to have sex-specific expression compared to the autosomal genes. Furthermore, we identified a depletion of regulatory variants on the X chromosome, especially among genes under high selective constraint. In contrast, we discovered an enrichment of sex-specific regulatory variants on the X chromosome. To resolve the molecular mechanisms underlying such effects, we generated and connected sex-specific chromatin accessibility to sex-specific expression and regulatory variation. As sex-specific regulatory variants can inform sex differences in genetic disease prevalence, we have integrated our data with genome-wide association study data for multiple immune traits and to identify traits with significant sex biases. Together, our study provides genome-wide insight into how the X chromosome and sex shape human gene regulation and disease.",2015-08-07,https://www.semanticscholar.org/paper/5f97571deb7668228837070ea09f5c25e036a182,bioRxiv
2996,Effective Performance Issue Diagnosis with Value-Assisted Cost Profiling,"Diagnosing performance issues is often difficult, especially when they occur only during some program executions. Profilers can help with performance debugging, but are ineffective when the most costly functions are not the root causes of performance issues. To address this problem, we introduce a new profiling methodology, value-assisted cost profiling, and a tool vProf. Our insight is that capturing the values of variables can greatly help diagnose performance issues. vProf continuously records values while profiling normal and buggy program executions. It identifies anomalies in the values and the functions where they occur to pinpoint the real root causes of performance issues. Using a set of 15 real-world performance bugs in four widely used applications, we show that vProf is effective at diagnosing all of the issues while other state-of-the-art tools diagnose only a few of them. We further use vProf to diagnose longstanding performance issues in these applications that have been unresolved for over four years.",2023-05-08,https://www.semanticscholar.org/paper/4831c52655050e37dd61c98b2c984b1060eafe9c,European Conference on Computer Systems
3759,Visualizing Object Detection Features,,2016-03-01,https://www.semanticscholar.org/paper/d274a0c1c383bc2440f9d90cab61df4ff934efdb,International Journal of Computer Vision
1916,Tool allocation to smooth work-in-process for cycle time reduction and an empirical study,,2018-09-01,https://www.semanticscholar.org/paper/c1d12a9116c695d61f8027e313a0d5ec33ae38e1,Annals of Operations Research
206,Sex as an algorithm,Looking at the mysteries of evolution from a computer science point of view yields some unexpected insights.,2016-10-28,https://www.semanticscholar.org/paper/8f1501d68983e86ed0ad77ee1b052a981ebd4781,Communications of the ACM
2056,Modeling strategic semiconductor assembly outsourcing decisions based on empirical settings,,2008-01-12,https://www.semanticscholar.org/paper/b561da237a72782cc38c534a6a6fa699bf3cc1ac,OR Spectr.
599,The complexity of distributed concurrency control,"We present a formal framework for distributed databases, and we study the complexity of the concurrency control problem in this framework. Our transactions are partially ordered sets, of actions, as opposed to the straight-line programs of the centralized case. The concurrency control algorithm, or scheduler, is itself a distributed program. Three notions of performance of the scheduler are studied and interrelated: (i) its parallelism, (ii) the computational complexity of the problems it needs to solve, and (iii) the cost of communication between the various parts of the scheduler. We show that the number of messages necessary and sufficient to support a given level of parallelism is equal to the minmax value of a combinatorial game. We show that this game is PSPACE-complete. It follows that, unless NP=PSPACE, a scheduler cannot simultaneously minimize communication and be computationally efficient. This result, we argue, captures the quantum jump in complexity of the transition from centralized to distributed concurrency control problems.",1981-10-28,https://www.semanticscholar.org/paper/0de37de50ce23ae2851bc05ee1fb1be27aeb42bd,22nd Annual Symposium on Foundations of Computer Science (sfcs 1981)
3639,A History of C++: 1979-1991,,,https://www.semanticscholar.org/paper/14d1f59cbd20a813f4f6145844a9ebab2c6c3c98,HOPL Preprints
2363,Gamma interferon enhances the killing of Staphylococcus aureus by human neutrophils.,"The effect of purified human interferon-gamma on the responsiveness of human neutrophils was investigated. Pre-incubation of neutrophils with 100 U interferon ml-1 for 10 min at 37 degrees C resulted in a 2.5-fold increase in N-formylmethionyl-leucyl-phenylalanine-stimulated reactive oxygen metabolite generation (as assayed by luminol-dependent chemiluminescence). Pre-treatment of neutrophils with interferon also potentiated their ability to kill Staphylococcus aureus, and thus it is proposed that this lymphokine may also enhance neutrophil function in vivo under certain pathological conditions.",,https://www.semanticscholar.org/paper/8a85d8d36040f60995cc2b6bdc3f330a02f28990,Journal of General Microbiology
2301,The diadenosine polyphosphates Ap3A and Ap4A and adenosine triphosphate interact with granulocyte-macrophage colony-stimulating factor to delay neutrophil apoptosis: implications for neutrophil: platelet interactions during inflammation.,"Incubation of neutrophils with cytokines such as granulocyte macrophage colony-stimulating factor (GM-CSF) delays their loss of function and changes in cellular morphology that are characteristic of apoptosis. Adenosine triphosphate (ATP) and the diadenosine polyphosphates Ap4A and AP3A were almost as effective as GM-CSF in delaying neutrophil apoptosis. The nucleotides could thus preserve cellular morphology, protect against chromatin fragmentation, and preserve functions such as NADPH oxidase activity and expression of CD16. Moreover, addition of ATP, AP3A and AP4A together with GM-CSF resulted in more pronounced protection from apoptosis than was observed during incubation with either the cytokine or the nucleotides alone. Because ATP, Ap3A, and AP4A may be secreted from activated platelets, these observations suggest that platelet-derived products, perhaps acting in combination with endothelial-derived or immune cell-derived cytokines, can regulate neutrophil function during certain types of inflammation.",1996-04-15,https://www.semanticscholar.org/paper/1c8c9e9966a94daf09743551405d8bed90a53315,Blood
1696,Statistical Models,,,https://www.semanticscholar.org/paper/33ad5a55670f346716e974221f03330dac6f6c9f,Encyclopedia of Social Network Analysis and Mining
2083,Magnetic resonance microsystems for life science applications,"Nuclear magnetic (MR) resonance spectroscopy and imaging technique are powerful methods available for determining molecular structures and non-invasive 3D imaging. In the effort of developing a nanoMRI microsystem, the authors have designed, fabricated, assembled and did preliminary characterization of the nanoMRI probe. A multilayer high aspect ratio metal process has been developed for this project. NanoMRI probes are designed through multi-physics finite element 3D analysis, integrated using the high aspect ratio process, assembled, and the RF coils are matched and tuned to a 500MHz system. Due to the large magnetic field gradients and fast switching gradient coils, the high mass-sensitivity and additional orthogonal RF signal channels, special MR pulse sequencies (Lauterbur et al., 1992) can be developed for imaging and molecular structural analysis.",2005-06-05,https://www.semanticscholar.org/paper/168c40ac4697ed55820d1047a27ca8f1478d72e7,"The 13th International Conference on Solid-State Sensors, Actuators and Microsystems, 2005. Digest of Technical Papers. TRANSDUCERS '05."
3712,Discrete Representations Strengthen Vision Transformer Robustness,"Vision Transformer (ViT) is emerging as the state-of-the-art architecture for image recognition. While recent studies suggest that ViTs are more robust than their convolutional counterparts, our experiments find that ViTs trained on ImageNet are overly reliant on local textures and fail to make adequate use of shape information. ViTs thus have difficulties generalizing to out-of-distribution, real-world data. To address this deficiency, we present a simple and effective architecture modification to ViT's input layer by adding discrete tokens produced by a vector-quantized encoder. Different from the standard continuous pixel tokens, discrete tokens are invariant under small perturbations and contain less information individually, which promote ViTs to learn global information that is invariant. Experimental results demonstrate that adding discrete representation on four architecture variants strengthens ViT robustness by up to 12% across seven ImageNet robustness benchmarks while maintaining the performance on ImageNet.",2021-11-20,https://www.semanticscholar.org/paper/601ab36b6f077ff57472f4a0cf2e061dd05b9b85,International Conference on Learning Representations
506,The weighted region problem: finding shortest paths through a weighted planar subdivision,"The problem of determining shortest paths through a weighted planar polygonal subdivision with <italic>n</italic> vertices is considered. Distances are measured according to a weighted Euclidean metric: The length of a path is defined to be the weighted sum of (Euclidean) lengths of the subpaths within each region. An algorithm that constructs a (restricted) “shortest path map” with respect to a given source point is presented. The output is a partitioning of each edge of the subdivion into intervals of ε-optimality, allowing an ε-optimal path to be traced from the source to any query point along any edge. The algorithm runs in worst-case time <italic>O</italic>(<italic>ES</italic>) and requires <italic>O</italic>(<italic>E</italic>) space, where  <italic>E</italic> is  the number of “events” in our algorithm and <italic>S</italic> is the time it takes to run a numerical search procedure. In the worst case, <italic>E</italic> is bounded above by <italic>O</italic>(<italic>n</italic><supscrpt>4</supscrpt>) (and we give an &OHgr;(<italic>n</><supscrpt>4</supscrpt>) lower bound), but it is likeky that <italic>E</italic> will be much smaller in practice. We also show that <italic>S</italic> is bounded by <italic>O</italic>(<italic>n</italic><supscrpt>4</supscrpt><italic>L</italic>), where <italic>L</italic> is the precision of the problem instance (including the number of bits in the user-specified tolerance ε). Again, the value of <italic>S</italic> should be smaller in practice. The algorithm applies the “continuous  Dijkstra” paradigm  and exploits the fact that shortest paths obey Snell's Law of Refraction at region boundaries, a   local optimaly property of shortest paths that is well known from the analogous optics model. The algorithm generalizes to the multi-source case to compute Voronoi diagrams.",1991-01-03,https://www.semanticscholar.org/paper/d54b409eefbb5db295f0ee2e817caf006e6d804b,JACM
802,A convex relaxation for the asymmetric TSP,"1 The problem The Traveling Salesman Problem (TSP) has been widely studied. Given a graph with lengths on its edges, the problem is to find a tour of minimum length that visits every vertex once. One usually makes the assumption that the edge lengths satisfy the triangle inequality, in which case the problem is equivalent to requiring the tour to visit every vertex at least (instead of exactly) once. With the further assumption that the edge lengths are symmetric (in other words, the graph is undirected), it is well-known that the optimum can be approximated to within a factor of 3/2. Without the latter assumption, i.e., when the graph is directed, the best-known approximation is a factor of log n ,[l]. Unfortunately, this algorithm can be nearly logn off from the optimum even when the edge lengths are all 1, and the underlying graph is known to be Hamiltonian. Here we study a linear programming relaxation for the problem. Our main result is that one can find a fractional solution to the relaxation that is very sparse ( with < 3n edges). We also show that in the special case when the underlying graph is Hamiltonian with edge lengths 1 and the (inand out-) degrees of every vertex are each at most 2, a solution to the relaxation can be rounded to an integral solution (a tour) whose length is at most twice the optimum. Note that even this special case of the problem is NP-hard! (and the previous algorithm is 0( $$&) off).",,https://www.semanticscholar.org/paper/55fd55189f6caa9d764a5dfdf091be11e0892578,ACM-SIAM Symposium on Discrete Algorithms
511,The Geometry of Grasping,"A form closure (or complete restraint) of a solid object is a finite set of wrenches (force-moment combinations) applied on the object, with the property that any other wrench acting on the object can be balanced by a positive combination of the original ones. Intuitively, form closure is a way of defining the notion of a ""firm grip"" of the object (when friction is not taken into account). It has been pointed out by Reuleaux (1875) and Somoff (1897), and more recently by Lakshmin arayana (1978), that the form closure of a two-dimensional object requires at least four wrenches, and that form closure of a three-dimensional object requires at least seven wrenches. It was also conjectured that these numbers can be achieved by wrenches realizable as forces normal to the surface of the object (such wrenches are called fingers). In this paper we prove this conjecture. In particular we show that form closure of any two-dimensional bounded object with piecewise smooth boundary (except a circle) can be achieved by four fingers. For three dimensions, we prove that form closure of any bounded object with piecewise smooth boundary can be achieved with 12 fingers if and only if the object does not have a rotational symmetry (in which case, of course, form closure is not achievable, since no moment along the axis of symmetry can be opposed). We also show that, under very general conditions, form closure of three-dimensional objects without rotational symmetries can be achieved with seven fingers. Finally, we show that when Coulomb friction is taken into account, under the most relaxed assumptions three fingers are necessary and sufficient in two dimensions, and four fingers in three dimensions.",1990-01-02,https://www.semanticscholar.org/paper/3a1815d82f04c211f997b05890d18b7482f1432d,Int. J. Robotics Res.
3735,Relational Action Forecasting,"This paper focuses on multi-person action forecasting in videos. More precisely, given a history of H previous frames, the goal is to detect actors and to predict their future actions for the next T frames. Our approach jointly models temporal and spatial interactions among different actors by constructing a recurrent graph, using actor proposals obtained with Faster R-CNN as nodes. Our method learns to select a subset of discriminative relations without requiring explicit supervision, thus enabling us to tackle challenging visual data. We refer to our model as Discriminative Relational Recurrent Network (DRRN). Evaluation of action prediction on AVA demonstrates the effectiveness of our proposed method compared to simpler baselines. Furthermore, we significantly improve performance on the task of early action classification on J-HMDB, from the previous SOTA of 48% to 60%.",2019-04-08,https://www.semanticscholar.org/paper/6edfe8350da54cd563158b0d7d0c664f16cb91a8,Computer Vision and Pattern Recognition
875,Memory-efficient algorithms for the verification of temporal properties,,1990-06-18,https://www.semanticscholar.org/paper/e3a34614264c3bd9cdccc4c6ea163ff09934a019,Formal Methods Syst. Des.
2837,Galectin‐3 regulates T‐cell functions,"Summary:  Galectin‐3 is absent in resting CD4+ and CD8+ T cells but is inducible by various stimuli. These include viral transactivating factors, T‐cell receptor (TCR) ligation, and calcium ionophores. In addition, galectin‐3 is constitutively expressed in human regulatory T cells and CD4+ memory T cells. Galectin‐3 exerts extracellular functions because of its lectin activity and recognition of cell surface and extracellular matrix glycans. These include cell activation, adhesion, induction of apoptosis, and formation of lattices with cell surface glycoprotein receptors. Formation of lattices can result in restriction of receptor mobility and cause attenuation of receptor functions. Consistent with the presence of galectin‐3 in intracellular locations, several functions have been described for this protein inside T cells. These include inhibition of apoptosis, promotion of cell growth, and regulation of TCR signal transduction. Studies of cell surface glycosylation have led to convergence of glycobiology and galectin biology and provided new clues on how galectin‐3 may participate in the regulation of cell surface receptor activities. The rapid expansion of the field of galectin research has positioned galectin‐3 as a key regulator in T‐cell functions.",2009-07-01,https://www.semanticscholar.org/paper/ef99c0482d3e4c7c8cc9e3b9f2a9d05430adc4b1,Immunological Reviews
3703,Revealing Occlusions with 4D Neural Fields,"For computer vision systems to operate in dynamic situations, they need to be able to represent and reason about object permanence. We introduce a framework for learning to estimate 4D visual representations from monocular RGB-D video, which is able to persist objects, even once they become obstructed by occlusions. Unlike traditional video representations, we encode point clouds into a continuous representation, which permits the model to attend across the spatiotemporal context to resolve occlusions. On two large video datasets that we release along with this paper, our experiments show that the representation is able to successfully reveal occlusions for several tasks, without any architectural changes. Visualizations show that the attention mechanism automatically learns to follow occluded objects. Since our approach can be trained end-to-end and is easily adaptable, we believe it will be useful for handling occlusions in many video understanding tasks. Data, code, and models are available at occ1usions. cs. co1umbia. edu.",2022-04-22,https://www.semanticscholar.org/paper/b4a2c2ae8bcefb389a7f9aeab38b90d6f4583fa5,Computer Vision and Pattern Recognition
1942,A novel approach to hedge and compensate the critical dimension variation of the developed-and-etched circuit patterns for yield enhancement in semiconductor manufacturing,,,https://www.semanticscholar.org/paper/06415b548501d440c37fec2db9aefabd77d570ee,Computers & Operations Research
753,Succinct approximate convex pareto curves,"We study the succinct approximation of convex Pareto curves of multiobjective optimization problems. We propose the concept of ε-convex Pareto (ε-CP) set as the appropriate one for the convex setting, and observe that it can offer arbitrarily more compact representations than ε-Pareto sets in this context. We characterize when an ε-CP can be constructed in polynomial time in terms of an efficient routine Comb for optimizing (exactly or approximately) monotone linear combinations of the objectives. We investigate the problem of computing minimum size ε-convex Pareto sets, both for discrete (combinatorial) and continuous (convex) problems, and present general algorithms using a Comb routine. For bi-objective problems, we show that if we have an exact Comb optimization routine, then we can compute the minimum ε-CP for continuous problems (this applies for example to bi-objective Linear Programming and Markov Decision Processes), and factor 2 approximation to the minimum ε-CP for discrete problems (this applies for example to bi-objective versions of polynomial-time solvable combinatorial problems such as Shortest Paths, Spanning Tree, etc.). If we have an approximate Comb routine, then we can compute factor 3 and 6 approximations respectively to the minimum ε-CP for continuous and discrete bi-objective problems. We consider also the case of three and more objectives and present some upper and lower bounds.",2008-01-20,https://www.semanticscholar.org/paper/c2870f43813bbff5d3c7d1276ece79816bc5ebe7,ACM-SIAM Symposium on Discrete Algorithms
2783,Intracranial alternating current stimulation facilitates neurogenesis in a mouse model of Alzheimer’s disease,,2020-07-23,https://www.semanticscholar.org/paper/0c57759037fae41ed0d09a57f95c1d503666c9bd,Alzheimer's Research & Therapy
3387,Log Diameter Rounds Algorithms for 2-Vertex and 2-Edge Connectivity,"Many modern parallel systems, such as MapReduce, Hadoop and Spark, can be modeled well by the MPC model. The MPC model captures well coarse-grained computation on large data --- data is distributed to processors, each of which has a sublinear (in the input data) amount of memory and we alternate between rounds of computation and rounds of communication, where each machine can communicate an amount of data as large as the size of its memory. This model is stronger than the classical PRAM model, and it is an intriguing question to design algorithms whose running time is smaller than in the PRAM model. 
In this paper, we study two fundamental problems, $2$-edge connectivity and $2$-vertex connectivity (biconnectivity). PRAM algorithms which run in $O(\log n)$ time have been known for many years. We give algorithms using roughly log diameter rounds in the MPC model. Our main results are, for an $n$-vertex, $m$-edge graph of diameter $D$ and bi-diameter $D'$, 1) a $O(\log D\log\log_{m/n} n)$ parallel time $2$-edge connectivity algorithm, 2) a $O(\log D\log^2\log_{m/n}n+\log D'\log\log_{m/n}n)$ parallel time biconnectivity algorithm, where the bi-diameter $D'$ is the largest cycle length over all the vertex pairs in the same biconnected component. Our results are fully scalable, meaning that the memory per processor can be $O(n^{\delta})$ for arbitrary constant $\delta>0$, and the total memory used is linear in the problem size. Our $2$-edge connectivity algorithm achieves the same parallel time as the connectivity algorithm of Andoni et al. (FOCS 2018). We also show an $\Omega(\log D')$ conditional lower bound for the biconnectivity problem.",2019-05-02,https://www.semanticscholar.org/paper/09adeb7e2464f6960782352ccaeb1d0bd8ad6982,"International Colloquium on Automata, Languages and Programming"
3599,Specifying C++ concepts,"C++ templates are key to the design of current successful mainstream libraries and systems. They are the basis of programming techniques in diverse areas ranging from conventional general-purpose programming to software for safety-critical embedded systems. Current work on improving templates focuses on the notion of concepts (a type system for templates), which promises significantly improved error diagnostics and increased expressive power such as concept-based overloading and function template partial specialization. This paper presents C++ templates with an emphasis on problems related to separate compilation. We consider the problem of how to express concepts in a precise way that is simple enough to be usable by ordinary programmers. In doing so, we expose a few weakness of the current specification of the C++ standard library and suggest a far more precise and complete specification. We also present a systematic way of translating our proposed concept definitions, based on use-patterns rather than function signatures, into constraint sets that can serve as convenient basis for concept checking in a compiler.",2006-01-11,https://www.semanticscholar.org/paper/fbf25ee400933d680d9ff3fb7823fd10d2d135f0,ACM-SIGACT Symposium on Principles of Programming Languages
764,Recursive Concurrent Stochastic Games,"We study Recursive Concurrent Stochastic Games (RCSGs), extending our recent analysis of recursive simple stochastic games [14, 15] to a concurrent setting where the two players choose moves simultaneously and independently at each state. For multi-exit games, our earlier work already showed undecidability for basic questions like termination, thus we focus on the important case of single-exit RCSGs (1-RCSGs) 
 
We first characterize the value of a 1-RCSG termination game as the least fixed point solution of a system of nonlinear minimax functional equations, and use it to show PSPACE decidability for the quantitative termination problem. We then give a strategy improvement technique, which we use to show that player 1 (maximizer) has e-optimal randomized Stackless & Memoryless (r-SM) strategies, while player 2 (minimizer) has optimal r-SM strategies. Thus, such games are r-SM-determined. These results mirror and generalize in a strong sense the randomized memoryless determinacy results for finite stochastic games, and extend the classic Hoffman-Karp [19] strategy improvement approach from the finite to an infinite state setting. The proofs in our infinite-state setting are very different however 
 
We show that our upper bounds, even for qualitative termination, can not be improved without a major breakthrough, by giving two reductions: first a P-time reduction from the long-standing square-root sum problem to the quantitative termination decision problem for finite concurrent stochastic games, and then a P-time reduction from the latter problem to the qualitative termination problem for 1-RCSGs",2006-07-10,https://www.semanticscholar.org/paper/aed9b30a7110548d71186b8c5c0ab86bb3c6a311,Log. Methods Comput. Sci.
2067,Real option analysis for capacity investment planning for semiconductor manufacturing,"This study aims to propose a real option analysis to evaluate capital investment decisions for capacity expansion under demand uncertainty. Comparing to conventional analysis, this approach can provide a decision framework to incorporate management flexibility and thus provide a better measurement of optional value of capacity investment from potential benefits to avoid capacity shortage and losing growth opportunity. In particular, a binomial model with risk neutral method was employed to illustrate the expansion of the uncertainty event tree and the assessment of option value for supporting top managers' decision flexibility in light of dynamic decision contexts.",2007-10-01,https://www.semanticscholar.org/paper/7b4b8aa6515c87f098a44334cadfe7d9eb2519a9,International Symposium on Semiconductor Manufacturing
996,The Effect of Silica Nanoparticles on Human Corneal Epithelial Cells,,2016-10-01,https://www.semanticscholar.org/paper/ece35efa751d3f4bc972dd7c3b0ed9499639d9e4,Scientific Reports
3530,A Parallel Algorithm for Eliminating Cycles in Undirected Graphs,,1990-05-28,https://www.semanticscholar.org/paper/18ce185cff1d04deb38229aa9b33653704f1de19,Information Processing Letters
3702,Causal Transportability for Visual Recognition,"Visual representations underlie object recognition tasks, but they often contain both robust and non-robust features. Our main observation is that image classifiers may perform poorly on out-of-distribution samples because spurious correlations between non-robust features and labels can be changed in a new environment. By analyzing procedures for out-of-distribution generalization with a causal graph, we show that standard classifiers fail because the association between images and labels is not transportable across settings. However, we then show that the causal effect, which severs all sources of confounding, remains invariant across domains. This motivates us to develop an algorithm to estimate the causal effect for image classification, which is transportable (i.e., invariant) across source and target environments. Without observing additional variables, we show that we can derive an estimand for the causal effect under empirical assumptions using representations in deep models as proxies. Theoretical analysis, empirical results, and visualizations show that our approach captures causal invariances and improves overall generalization.",2022-04-26,https://www.semanticscholar.org/paper/b400b066929e8070842b33b450fe69698c5ed826,Computer Vision and Pattern Recognition
7,Evaluating Twitter for Foodborne Illness Outbreak Detection in New York City,"Objective To incorporate data from Twitter into the New York City Department of Health and Mental Hygiene foodborne illness surveillance system and evaluate its utility and impact on foodborne illness complaint and outbreak detection. Introduction An estimated one in six Americans experience illness from the consumption of contaminated food (foodborne illness) annually; most are neither diagnosed nor reported to health departments 1 . Eating food prepared outside of the home is an established risk factor for foodborne illness 2 . New York City (NYC) has approximately 24,000 restaurants and >8.5 million residents, of whom 78% report eating food prepared outside of the home at least once per week 3 . Residents and visitors can report incidents of restaurant-associated foodborne illness to a citywide non-emergency information service, 311. In 2012, the NYC Department of Health and Mental Hygiene (DOHMH) began collaborating with Columbia University to improve the detection of restaurant-associated foodborne illness complaints using a machine learning algorithm and a daily feed of Yelp reviews to identify reports of foodborne illness 4 . Annually, DOHMH manages over 4,000 restaurant-associated foodborne illness reports received via 311 and identified on Yelp which lead to the detection of about 30 outbreaks associated with a restaurant in NYC. Given the small number of foodborne illness outbreaks identified, it is probable that many restaurant-associated foodborne illness incidents remain unreported. DOHMH sought to incorporate and evaluate an additional data source, Twitter, to enhance foodborne illness complaint and outbreak detection efforts in NYC. Methods DOHMH epidemiologists continue to collaborate with computer scientists at Columbia University who developed a text mining algorithm that identifies tweets indicating foodborne illness. Twitter data are received via a targeted application program interface query that searches for foodborne illness key words and uses metadata to select for tweets with a possible NYC location. Each tweet is assigned a sick score between 0–1; those meeting a threshold value of 0.5 are manually reviewed by an epidemiologist, and a survey link is tweeted to users who have tweeted about foodborne illness, requesting more information regarding the date and time of the foodborne illness event, restaurant details, and user contact information. Survey data are used to validate complaints and are incorporated in a daily analysis using all sources of complaint data to identify restaurants with multiple foodborne illness complaints within a 30-day period. This system was launched on November 29, 2016. Results During November 29, 2016–September 27, 2017, 12,015 tweets qualified for review (39/day on average); 2,288 (19.0%) indicated foodborne illness in NYC, and 1,778 (14.8%) were tweeted a survey link (510 foodborne illness tweets were either deleted by the Twitter user or were tweets from a user who was already sent a survey for the same foodborne illness incident). The survey tweets resulted in 92 likes, 12 retweets, 65 replies, 232 profile views and 348 survey link clicks. Of the 1,778 surveys sent, 27 were completed (response rate 1.5%), of which 20 (74.7%) confirmed foodborne illness associated with a NYC restaurant; none had been reported via 311/Yelp. Of those, 11 (55%) provided a phone number, of which 10 (90.9%) completed phone interviews. The completed surveys contributed to the identification of two restaurants with multiple foodborne illness complaints within a 30-day period. Conclusions The utility of Twitter for foodborne illness outbreak detection continues to be evaluated. While the survey response rate has been low, the identification of new complaints not otherwise reported to 311 and Yelp suggests this will be a useful tool. Future plans include using feedback data collected by DOHMH epidemiologist review to increase the sensitivity and specificity of the text mining algorithm and improve the location detection for Twitter users. In addition, we plan to implement enhancements to the survey and create a web page to promote survey responses. Furthermore, we intend to share this system with other health departments so that they might incorporate Twitter in their outbreak detection and public health surveillance activities. References 1. Scallan E, Griffin PM, Angulo FJ, Tauxe RV, Hoekstra RM. Foodborne illness acquired in the United States--unspecified agents. Emerg Infect Dis. 2011 Jan;17(1):16-22. 2. Jones TF, Angulo FJ. Eating in restaurants: a risk factor for foodborne disease? Clin Infect Dis. 2006 Nov 15;43(10):1324-8. 3. New York City Health and Nutrition Examination Survey, 2013-2014 [Internet]. New York: New York City Department of Health and Mental Hygiene and The City University of New York; 2017 [cited 2017 Aug 28]. Available from: http://nychanes.org/data/ 4. Harrison C, Jorder M, Stern H, Stavinsky F, Reddy V, Hanson H, Waechter H, Lowe L, Gravano L, Balter S; Centers for Disease Control and Prevention (CDC).. Using online reviews by restaurant patrons to identify unreported cases of foodborne illness - New York City, 2012-2013. MMWR Morb Mortal Wkly Rep. 2014 May 23;63(20):441-5.",2018-05-22,https://www.semanticscholar.org/paper/5da218d8f5303e1a93a17842b6762c6ea45fd7e8,Online Journal of Public Health Informatics
2548,"Session details: Scratching, tapping, rubbing and rolling",,2008-10-19,https://www.semanticscholar.org/paper/b74d9c5d49fd55405c6f391b9f880bd090112a2e,Proceedings of the 21st annual ACM symposium on User interface software and technology
584,Proceedings of the fifteenth annual ACM symposium on Theory of computing,,1983-12-01,https://www.semanticscholar.org/paper/d55b456551771bdb5a4b2ddc942ecd1c7b6c162e,Symposium on the Theory of Computing
667,"A Neural Network Solves and Generates Mathematics Problems by Program Synthesis: Calculus, Differential Equations, Linear Algebra, and More",,,https://www.semanticscholar.org/paper/6806b0d1e425639a42da4763f170569b86448c87,arXiv.org
41,Optimizing SQL Queries over Text Databases,"Text documents often embed data that is structured in nature, and we can expose this structured data using information extraction technology. By processing a text database with information extraction systems, we can materialize a variety of structured ""relations,"" over which we can then issue regular SQL queries. A key challenge to process SQL queries in this text-based scenario is efficiency: information extraction is time-consuming, so query processing strategies should minimize the number of documents that they process. Another key challenge is result quality: in the traditional relational world, all correct execution strategies for a SQL query produce the same (correct) result; in contrast, a SQL query execution over a text database might produce answers that are not fully accurate or complete, for a number of reasons. To address these challenges, we study a family of select-project-join SQL queries over text databases, and characterize query processing strategies on their efficiency and - critically - on their result quality as well. We optimize the execution of SQL queries over text databases in a principled, cost-based manner, incorporating this tradeoff between efficiency and result quality in a user-specific fashion. Our large-scale experiments- over real data sets and multiple information extraction systems - show that our SQL query processing approach consistently picks appropriate execution strategies for the desired balance between efficiency and result quality.",2008-04-07,https://www.semanticscholar.org/paper/c81a0b8e4bb79cd72dc22bcf966a58504a04e979,IEEE International Conference on Data Engineering
3668,The UNIX system: Data abstraction in C,"C++ is a superset of the C programming language; it is fully implemented and has been used for nontrivial projects. There are now more than one hundred C++ installations. This paper describes the facilities for data abstraction provided in C++. These include Simula-like classes providing (optional) data hiding, (optional) guaranteed initialization of data structures, (optional) implicit type conversion for user-defined types, and (optional) dynamic typing; mechanisms for overloading function names and operators; and mechanisms for user-controlled memory management. It is shown how a new data type, like complex numbers, can be implemented, and how an “object-based” graphics package can be structured. A program using these data abstraction facilities is at least as efficient as an equivalent program not using them, and the compiler is faster than older C compilers.",1984-10-01,https://www.semanticscholar.org/paper/51b06d367a1f343157ace7b2abec69e7fb42bdbe,AT&T Bell Laboratories Technical Journal
2739,A Nose Gesture Interface Device: Extending Virtual Realities,"This paper reports on the1 development of a nose-machine interface device that provides real-time gesture, position, smell and facial expression information. The DATANOSE™2—Data AtomaTa CORNUCOPIA pNeumatic Olfactory 1/O-deviSE Tactile Manipulation (Olsen, 1986; Myers, 1991)—allows novice users without any formal nose training to perform complex interactive tasks.",1991-11-11,https://www.semanticscholar.org/paper/5a397cf9ca943a529a0c75fc0ce020265ec41478,Presence: Teleoperators & Virtual Environments
2723,Towards Coordinated Temporal Multimedia Presentations,,1993-01-02,https://www.semanticscholar.org/paper/d404cf581be53ba316587ce470ca50f3c5b26df1,AAAI Workshop on Intelligent Multimedia Interfaces
2516,Comparing steering-based travel techniques for search tasks in a CAVE,"We present a novel bimanual body-directed travel technique, PenguFly (PF), and compare it with two standard travel-by-pointing techniques by conducting a between-subject experiment in a CAVE. In PF, the positions of the user's head and hands are projected onto the ground, and travel direction and speed are computed based on direction and magnitude of the vector from the midpoint of the projected hand positions to the projected head position. The two baseline conditions both use a single hand to control the direction, with speed controlled discretely by button pushes with the same hand in one case, and continuously by the distance between the hands in the other case. Users were asked to travel through a simple virtual world and collect virtual coins within a set time. We found no significant differences between travel conditions for reported presence or usability, but a significant increase in nausea with PF. Total travel distance was significantly higher for the baseline condition with discrete speed selection, whereas travel accuracy in terms of coin-to-distance ratio was higher with PF.",2011-03-19,https://www.semanticscholar.org/paper/f805897ba10f8fdb69a9f8d57a305b189e3be02c,IEEE Conference on Virtual Reality and 3D User Interfaces
3684,Sin3DM: Learning a Diffusion Model from a Single 3D Textured Shape,"Synthesizing novel 3D models that resemble the input example has long been pursued by researchers and artists in computer graphics. In this paper, we present Sin3DM, a diffusion model that learns the internal patch distribution from a single 3D textured shape and generates high-quality variations with fine geometry and texture details. Training a diffusion model directly in 3D would induce large memory and computational cost. Therefore, we first compress the input into a lower-dimensional latent space and then train a diffusion model on it. Specifically, we encode the input 3D textured shape into triplane feature maps that represent the signed distance and texture fields of the input. The denoising network of our diffusion model has a limited receptive field to avoid overfitting, and uses triplane-aware 2D convolution blocks to improve the result quality. Aside from randomly generating new samples, our model also facilitates applications such as retargeting, outpainting and local editing. Through extensive qualitative and quantitative evaluation, we show that our model can generate 3D shapes of various types with better quality than prior methods.",2023-05-24,https://www.semanticscholar.org/paper/4cb4303e79acef4c276a87615641f94cd77a3e4a,arXiv.org
3171,Discovering Novel Biological Traits From Images Using Phylogeny-Guided Neural Networks,"Discovering evolutionary traits that are heritable across species on the tree of life (also referred to as a phylogenetic tree) is of great interest to biologists to understand how organisms diversify and evolve. However, the measurement of traits is often a subjective and labor-intensive process, making trait discovery a highly label-scarce problem. We present a novel approach for discovering evolutionary traits directly from images without relying on trait labels. Our proposed approach, Phylo-NN, encodes the image of an organism into a sequence of quantized feature vectors -or codes- where different segments of the sequence capture evolutionary signals at varying ancestry levels in the phylogeny. We demonstrate the effectiveness of our approach in producing biologically meaningful results in a number of downstream tasks including species image generation and species-to-species image translation, using fish species as a target example",2023-06-05,https://www.semanticscholar.org/paper/f40e9ee118fa4c1d6ea7ba1aad56c250e2902030,Knowledge Discovery and Data Mining
887,The Maximum k-Colorable Subgraph Problem for Chordal Graphs,,1987-01-30,https://www.semanticscholar.org/paper/174747140fbd56a777161f37131f8a120d355da1,Information Processing Letters
2641,View management for virtual and augmented reality,"We describe a view-management component for interactive 3D user interfaces. By view management, we mean maintaining visual constraints on the projections of objects on the view plane, such as locating related objects near each other, or preventing objects from occluding each other. Our view-management component accomplishes this by modifying selected object properties, including position, size, and transparency, which are tagged to indicate their constraints. For example, some objects may have geometric properties that are determined entirely by a physical simulation and which cannot be modified, while other objects may be annotations whose position and size are flexible.We introduce algorithms that use upright rectangular extents to represent on the view plane a dynamic and efficient approximation of the occupied space containing the projections of visible portions of 3D objects, as well as the unoccupied space in which objects can be placed to avoid occlusion. Layout decisions from previous frames are taken into account to reduce visual discontinuities. We present augmented reality and virtual reality examples to which we have applied our approach, including a dynamically labeled and annotated environment.",2001-11-11,https://www.semanticscholar.org/paper/11035becfa4b6d71dac0d647abf78d201bf05417,ACM Symposium on User Interface Software and Technology
3472,Minimizing Makespan for the Lazy Bureaucrat Problem,,2002-07-03,https://www.semanticscholar.org/paper/fb4369ee8600d2792028bd2a37e7bcf5b813503b,Scandinavian Workshop on Algorithm Theory
2697,Language-level support for exploratory programming of distributed virtual environments,"We describe COTERIE, a toolkit that provides languagelevel support for building distributed virtual environments. COTERIE is based on the distributed data-object paradigm for distributed shared memory. Any data object in COTERIE can be declared to be a Shared Object that is replicated fully in any process that is interested in it. These Shared Objects support asynchronous data propagation with atomic serializable updates, and asynchronous notification of updates. COTERIE is built in Modula-3 and uses existing Modula-3 packages that support an integrated interpreted language, multithreading, and 3D animation. Unlike other VE toolkits, COTERIE is based on a set of general-purpose parallel and distributed language concepts designed with the needs of virtual environments in mind. We summarize the requirements that we identified for COTERIE, describe its implementation, compare it with other toolkits, and provide examples that show COTERIE’s advantages.",1996-11-01,https://www.semanticscholar.org/paper/cc6a82c166bf7098eb7fc9152c4f485065c34582,ACM Symposium on User Interface Software and Technology
2810,The involvement of the spleen during chronic phase of Schistosoma mansoni infection in galectin-3-/- mice.,"Schistosoma mansoni synthesizes glycoconjugates which interact with galectin-3, eliciting an intense humoral immune response. Moreover, it was demonstrated that galectin-3 regulates B cell differentiation into plasma cells. Splenomegaly is a hallmark event characterized by polyclonal B cell activation and enhancement of antibody production. Here, we investigated whether galectin-3 interferes with spleen organization and B cell compartment during chronic schistosomiasis, using wild type (WT) and galectin-3-/- mice. In chronically-infected galectin-3-/- mice the histological architecture of the spleen, including white and red pulps, was disturbed with heterogeneous lymphoid follicles, an increased number of plasma cells (CD19-B220-/lowCD138+) and a reduced number of macrophages (CD19-B220-Mac-1+CD138-) and B lymphocytes (CD19+B220+/highCD138-), compared with the WT infected mice. In the absence of galectin-3 there was an increase of annexin-V+PI- cells and a major presence of apoptotic cells in spleen compared with WT infected mice. In spleen of WT infected mice galectin-3 was largely expressed in lymphoid follicles and extrafollicular sites. Thus, we propose that galectin-3 plays a role in splenic architecture, controlling distinct events such as apoptosis, macrophage activity, B cell differentiation and plasmacytogenesis in the course of S. mansoni infection.",2012-08-01,https://www.semanticscholar.org/paper/c0eae18e21823bc305c980dc4a86f0a8f47876a1,Histology and Histopathology
247,Efficiency-Revenue Trade-Offs in Auctions,,2012-05-14,https://www.semanticscholar.org/paper/dbf88d2b5aea256ca93d697c0ef762d4605c6d89,"International Colloquium on Automata, Languages and Programming"
2489,The Development of Mobile Augmented Reality,,,https://www.semanticscholar.org/paper/031a79ddfc901299466d1b5708524cc680355606,Expanding the Frontiers of Visual Analytics and Visualization
3455,An optimal online algorithm for packet scheduling with agreeable deadlines,"An important issue in IP-based QoS networks is the effective management of packets at the router level. Specifically, if the arriving packets cannot all be stored in a buffer, or if the packets have deadlines by which they must be delivered, the router needs to identify the packets that should be dropped. In recent work, Kesselman et al. [6] propose a model, called <i>buffer management with bounded delay</i>, which can be thought of as an online scheduling problem on a single machine: packets arrive at a network switch and are stored in a buffer of size <i>B.</i> Each packet has a positive weight and a deadline, with the weight representing the value of transmitting the packet by its deadline. At each integer time step, exactly one packet can be transmitted, and the objective is to maximize the total weight of the transmitted packets. If <i>B</i> = ∞, this is the online version of the scheduling problem 1| <i>p<inf>j</inf></i> = 1, <i>r<inf>j</inf></i>, <i>d<inf>j</inf></i> |Σ <i>w<inf>j</inf> U<inf>j</inf></i>. (We assume that <i>r<inf>j</inf></i> and <i>d<inf>j</inf></i> are integers.)",2005-01-23,https://www.semanticscholar.org/paper/85a75cd582cb662b275b35908315dff17ecb4ecf,ACM-SIAM Symposium on Discrete Algorithms
3483,Better Rounding Algorithms for a Geometric Embedding Relaxation of Minimum Multiway Cut,,,https://www.semanticscholar.org/paper/2b492fc0343e07af3b7cc22fa705f3fc9eeac531,Symposium on the Theory of Computing
1732,Black Box Variational Inference,"Variational inference has become a widely used method to approximate posteriors in complex latent variables models. However, deriving a variational inference algorithm generally requires significant model-specific analysis, and these efforts can hinder and deter us from quickly developing and exploring a variety of models for a problem at hand. In this paper, we present a ""black box"" variational inference algorithm, one that can be quickly applied to many models with little additional derivation. Our method is based on a stochastic optimization of the variational objective where the noisy gradient is computed from Monte Carlo samples from the variational distribution. We develop a number of methods to reduce the variance of the gradient, always maintaining the criterion that we want to avoid difficult model-based derivations. We evaluate our method against the corresponding black box sampling based methods. We find that our method reaches better predictive likelihoods much faster than sampling methods. Finally, we demonstrate that Black Box Variational Inference lets us easily explore a wide space of models by quickly constructing and evaluating several models of longitudinal healthcare data.",2013-12-31,https://www.semanticscholar.org/paper/6a667700100e228cb30a5d884258a0db921603fe,International Conference on Artificial Intelligence and Statistics
1331,Search for techniparticles in e+jets events at D0.,"We search for the technicolor process pp-->rhoT/omegaT-->WpiT in events containing one electron and two jets, in data corresponding to an integrated luminosity of 390 pb(-1), recorded by the D0 experiment at the Fermilab Tevatron. Technicolor predicts that technipions pi(T) decay dominantly into bb, bc, or bc, depending on their charge. In these events b and c quarks are identified by their secondary decay vertices within jets. Two analysis methods based on topological variables are presented. Since no excess above the standard model prediction was found, the result is presented as an exclusion in the pi(T) vs rho(T) mass plane for a given set of model parameters.",2006-12-08,https://www.semanticscholar.org/paper/efa78cab88c0b094e1fec31a4a5cd0ad2d4d3a93,Physical Review Letters
1519,An Invariant Learning Characterization of Controlled Text Generation,"Controlled generation refers to the problem of creating text that contains stylistic or semantic attributes of interest. Many approaches reduce this problem to training a predictor of the desired attribute. For example, researchers hoping to deploy a large language model to produce non-toxic content may use a toxicity classifier to filter generated text. In practice, the generated text to classify, which is determined by user prompts, may come from a wide range of distributions.In this paper, we show that the performance of controlled generation may be poor if the distributions of text in response to user prompts differ from the distribution the predictor was trained on. To address this problem, we cast controlled generation under distribution shift as an invariant learning problem: the most effective predictor should be invariant across multiple text environments. We then discuss a natural solution that arises from this characterization and propose heuristics for selecting natural environments.We study this characterization and the proposed method empirically using both synthetic and real data. Experiments demonstrate both the challenge of distribution shift in controlled generation and the potential of invariance methods in this setting.",2023-05-31,https://www.semanticscholar.org/paper/d95b441c2838888d7ac1af73b5f9c800f22fad3a,Annual Meeting of the Association for Computational Linguistics
3726,Globetrotter: Unsupervised Multilingual Translation from Visual Alignment,"Multi-language machine translation without parallel corpora is challenging because there is no explicit supervision between languages. Existing unsupervised methods typically rely on topological properties of the language representations. We introduce a framework that instead uses the visual modality to align multiple languages, using images as the bridge between them. We estimate the cross-modal alignment between language and images, and use this estimate to guide the learning of cross-lingual representations. Our language representations are trained jointly in one model with a single stage. Experiments with fifty-two languages show that our method outperforms baselines on unsupervised word-level and sentence-level translation using retrieval.",2020-12-08,https://www.semanticscholar.org/paper/f1085830042fdc07961f4bf4a7e6ff60cd534fd9,arXiv.org
3280,"Initiators, Leaders, and Recruitment Mechanisms in the Collective Movements of Damselfish","Explaining how individual behavior and social interactions give rise to group-level outcomes and affect issues such as leadership is fundamental to the understanding of collective behavior. Here we examined individual and collective behavioral dynamics in groups of humbug damselfish both before and during a collective movement. During the predeparture phase, group activity increased until the collective movement occurred. Although such movements were precipitated by one individual, the success or failure of any attempt to instigate a collective movement was not solely dependent on this initiator’s behavior but on the behavior of the group as a whole. Specifically, groups were more active and less cohesive before a successful initiation attempt than before a failed attempt. Individuals who made the most attempts to initiate a collective movement during each trial were ultimately most likely to lead the collective movement. Leadership was not related to dominance but was consistent between trials. The probability of fish recruiting to a group movement initiative was an approximately linear function of the number of fish already recruited. Overall, these results are consistent with nonselective local mimetism, with the decision to leave based on a group’s, rather than any particular individual’s, readiness to leave.",2013-04-12,https://www.semanticscholar.org/paper/7527d195acceb3b56fccedecae491dc290416ea9,American Naturalist
2843,Targeted disruption of the galectin-3 gene results in decreased susceptibility to NNK-induced lung tumorigenesis: an oligonucleotide microarray study,,2008-01-17,https://www.semanticscholar.org/paper/919e2d9d923884b364f5557b3e04140c42d32f5b,Journal of Cancer Research and Clinical Oncology
88,Snowball: a prototype system for extracting relations from large text collections,"Text documents often hide valuable structured data. For example, a collection of newspaper articles might contain information on the location of the headquarters of a number of organizations. If we need to nd the location of the headquarters of, say, Microsoft, we could try and use traditional information-retrieval techniques for nding documents that contain the answer to our query. Alternatively, we could answer such a query more precisely if we somehow had available a table listing all the organization-location pairs that are mentioned in our document collection. One could view the extraction process as automatically building a materialized view over the unstructured text data. In this demo we present an interactive prototype of our Snowball system for extracting relations from collections of plain-text documents with minimal human participation. Our method builds on the DIPRE idea introduced by Brin [3]. Our system and techniques were presented in detail in [2] and [1].",2001-05-01,https://www.semanticscholar.org/paper/5d26d446c7e809bd4e1ee6e696a17764521618c7,ACM SIGMOD Conference
2053,Economic efficiency analysis of wafer fabrication facilities,"Semiconductor industry is capital intensive and competitive, and thus efficiently utilizing resources to provide products and services is essential for maintaining competitive advantages. Knowing whether the resource is properly utilized is the foundation for future improvements and/or decision making. This study investigates the economic efficiency of fabrication facility (fab) operations. We develop a two-stage overall efficiency model, which clearly defines and explains the ¿real¿ performance of fab production operations and the non-production issues. The model provides an overall performance index while considering different aspects. A single performance index can be used to evaluate and rank the performance for period review. Factors affecting performance can be identified. Furthermore, according to a real case, an ex post relative efficiency analysis is conducted and the initial results are reported. The case study can help providing diagnosis for inefficient production facilities and identifying best practices of efficient production units.",2008-12-01,https://www.semanticscholar.org/paper/7d33fb68842a826097a604d7092081e2a207fbc3,Online World Conference on Soft Computing in Industrial Applications
3716,Adversarial Attacks are Reversible with Natural Supervision,"We find that images contain intrinsic structure that enables the reversal of many adversarial attacks. Attack vectors cause not only image classifiers to fail, but also collaterally disrupt incidental structure in the image. We demonstrate that modifying the attacked image to restore the natural structure will reverse many types of attacks, providing a defense. Experiments demonstrate significantly improved robustness for several state-of-the-art models across the CIFAR-10, CIFAR-100, SVHN, and ImageNet datasets. Our results show that our defense is still effective even if the attacker is aware of the defense mechanism. Since our defense is deployed during inference instead of training, it is compatible with pre-trained networks as well as most other defenses. Our results suggest deep networks are vulnerable to adversarial examples partly because their representations do not enforce the natural structure of images.",2021-03-26,https://www.semanticscholar.org/paper/d59fb7b76578e725d3179aa236ba8a26c5e7b844,IEEE International Conference on Computer Vision
3240,Physiology modulates social flexibility and collective behaviour in equids and other large ungulates,"Though morphologically very similar, equids across the extant species occupy ecological niches that are surprisingly non-overlapping. Occupancy of these distinct niches appears related to subtle physiological and behavioural adaptations which, in turn, correspond to significant differences in the social behaviours and emergent social systems characterizing the different species. Although instances of intraspecific behavioural variation in equids demonstrate that the same body plan can support a range of social structures, each of these morphologically similar species generally shows robust fidelity to its evolved social system. The pattern suggests a subtle relationship between physiological phenotypes and behavioural flexibility. While environmental conditions can vary widely within relatively short temporal or spatial scales, physiological changes and changes to the behaviours that regulate physiological processes, are constrained to longer cycles of adaptation. Physiology is then the limiting variable in the interaction between ecological variation and behavioural and socio-structural flexibility. Behavioural and socio-structural flexibility, in turn, will generate important feedbacks that will govern physiological function, thus creating a coupled web of interactions that can lead to changes in individual and collective behaviour. Longitudinal studies of equid and other large-bodied ungulate populations under environmental stress, such as those discussed here, may offer the best opportunities for researchers to examine, in real time, the interplay between individual behavioural plasticity, socio-structural flexibility, and the physiological and genetic changes that together produce adaptive change. This article is part of the themed issue ‘Physiological determinants of social behaviour in animals’.",2017-08-19,https://www.semanticscholar.org/paper/85340fba23be41562d8ffa24840666a11c901bd2,Philosophical Transactions of the Royal Society B: Biological Sciences
1128,Search for Higgs boson production in dilepton and missing energy final states with 5.4 fb(-1) of pp collisions at square root(s) = 1.96 TeV.,"A search for the standard model Higgs boson is presented using events with two charged leptons and large missing transverse energy selected from 5.4 fb(-1) of integrated luminosity in pp collisions at square root(s) = 1.96 TeV collected with the D0 detector at the Fermilab Tevatron collider. No significant excess of events above background predictions is found, and observed (expected) upper limits at 95% confidence level on the rate of Higgs boson production are derived that are a factor of 1.55 (1.36) above the predicted standard model cross section at m(H) = 165 GeV.",,https://www.semanticscholar.org/paper/4d48d4444a497c9f70c9d5e66de19d94cd68e975,Physical Review Letters
3519,Long tours and short superstrings,"This paper considers weight-maximizing variants of the classical symmetric and asymmetric traveling-salesman problems. Like their weight-minimizing counterparts, these variants are MAX SNP-hard. We present the first nontrivial approximation algorithms for these problems. Our algorithm for directed graphs finds a tour whose weight is at least 38/63/spl ap/0.603 times the weight of a maximum-weight tour, and our algorithm for undirected graphs finds a tour whose weight is at least 5/7/spl ap/0.714 times optimal. These bounds compare favorably with the 1/2 and 2/3 bounds that can be obtained for undirected and directed graphs, respectively, by simply deleting the minimum-weight edge from each cycle of a maximum-weight cycle cover. Our algorithm for directed graphs can be used to improve several recent approximation results for the shortest-superstring problem.<<ETX>>",1994-11-20,https://www.semanticscholar.org/paper/d5cbeb2321ba9b9d3fa2d8a5b2484bab9408200b,Proceedings 35th Annual Symposium on Foundations of Computer Science
2071,A novel timetabling algorithm for a furnace process for semiconductor fabrication with constrained waiting and frequency-based setups,,2007-05-23,https://www.semanticscholar.org/paper/99c24785b64c6e94ee4fb8f89fb3b11662cb862a,OR Spectr.
2377,The tetrahymena rRNA intron self-splices in E. coli: In vivo evidence for the importance of key base-paired regions of RNA for RNA enzyme function,,1985-02-01,https://www.semanticscholar.org/paper/586cf250606a9203062c02e2a22c48eae954884f,Cell
2752,Worlds within worlds: metaphors for exploring n-dimensional virtual worlds,"n-Vision is a testbed for exploring n-dimensional worlds containing functions of an arbitrary number of variables. Although our interaction devices and display hardware are inherently 3D, we demonstrate how they can be used to support interaction with these higher-dimensional objects. We introduce a new interaction metaphor developed for the system, which we call “worlds within worlds”: nested heterogeneous coordinate systems that allow the user to view and manipulate functions. Objects in our world may be explored with a set of tools. We describe an example n-Vision application in “financial visualization,” where the functions are models of financial instruments. n-Vision’s software architecture supports a hierarchy of arbitrarily transformed, nested boxes that defines an interactive space within which information is displayed and input obtained. Our design, modeled in part after the hierarchical 2D windows of the X Window System, is intended to provide an environment that is well suited to the use of true 3D input and stereo display devices. Boxes are associated with event handlers that support 3D motion, enter, and leave events, and provide recognition of finger gestures. CR",1990-08-01,https://www.semanticscholar.org/paper/e08c8b4e2d3d2f11ff2302cd4ca9f6fed71e228f,ACM Symposium on User Interface Software and Technology
2775,Interactive Multimedia Explanation for Equipment Maintenance and Repair,,,https://www.semanticscholar.org/paper/aa3147683126d5f033fa727f90b764ff86fba43d,Human Language Technology - The Baltic Perspectiv
317,Reducibility among equilibrium problems,"We address the fundamental question of whether the Nash equilibria of a game can be computed in polynomial time. We describe certain efficient reductions between this problem for normal form games with a fixed number of players and graphical games with fixed degree. Our main result is that the problem of solving a game for any constant number of players, is reducible to solving a 4-player game.",2006-05-21,https://www.semanticscholar.org/paper/46064f88324ec2504692389e38dabf900906c2c5,Symposium on the Theory of Computing
1028,A physical parameter-based skidding model for the snakeboard,"The physical violation of a nonholonomic system's idealized constraints in the form of skidding has recently elucidated interest in new models in order to directly incorporate the phenomenon into the system dynamics. However, such models either are too simple to capture physical attributes or have otherwise been tested only on systems with simple behaviors, such as the rolling disk. In this work, we present a novel skidding model, based on physical parameters, for a snakeboard system, which is simultaneously rich in behavior but simple in design. This model extends the system's configuration space and associates traction forces to a skidding angle using an experimentally verified observation from the literature. We validate our model in simulation and discuss its advantages over the Rayleigh dissipation function skidding model. We also show that the model accurately predicts a physical system's behavior in experimentation with tuned parameters and standard controllers.",2016-12-01,https://www.semanticscholar.org/paper/12f79d003d4dd6b2db848ce71b233d9d8ff943cd,IEEE Conference on Decision and Control
2315,Potentiation of the respiratory burst of human neutrophils by cycloheximide: Regulation of reactive oxidant production by a protein(s) with rapid turnover,,1995-04-01,https://www.semanticscholar.org/paper/f56875016f71e381cd31d4fd31b823cd6b3717a7,Inflammation Research
2629,An annotated situation-awareness aid for augmented reality,"We present a situation-awareness aid for augmented reality systems based on an annotated ""world in miniature."" Our aid is designed to provide users with an overview of their environment that allows them to select and inquire about the objects that it contains. Two key capabilities are discussed that are intended to address the needs of mobile users. The aid's position, scale, and orientation are controlled by a novel approach that allows the user to inspect the aid without the need for manual interaction. As the user alternates their attention between the physical world and virtual aid, popup annotations associated with selected objects can move freely between the objects' representations in the two models.",2002-10-27,https://www.semanticscholar.org/paper/2e4eb85e1173798f96ebc6ddce4b3e500ba9829e,ACM Symposium on User Interface Software and Technology
2640,Unsolved problems in mobile computer graphics and interaction,"With the number of mobile devices exceeding PCs, research is required in many areas with respect to graphics and interaction. There are problems with interaction, streaming, graphics algorithms, bandwidth with current and future devices. This panel examines the state of the art from both an industrial and research point of view, and provides directions for future work in this area.",2002-07-21,https://www.semanticscholar.org/paper/f88c3da92a676be25d182d2990cc2f39e494d5ac,International Conference on Computer Graphics and Interactive Techniques
475,The power of reflective relational machines,"A model of database programming with reflection, called reflective relational machine, is introduced and studied. The reflection consists here of dynamic generation of queries in a host programming language. The main results characterize the power of the machine in terms of known complexity classes. In particular, the polynomial-time restriction of the machine is shown to express PSPACE, and to correspond precisely to uniform circuits of polynomial depth and exponential size. This provides an alternative, logic-based formulation of the uniform circuit model, more convenient for problems naturally formulated in logic terms. Since time in the polynomially-bounded machine coincides with time in the uniform circuit model, this also shows that reflection allows for more ""intense"" parallelism, which is not attainable otherwise (unless P=PSPACE). Other results concern the power of the reflective relational machine subject to restrictions on the number of variables used.<<ETX>>",1994-07-04,https://www.semanticscholar.org/paper/c6de23a1189d30848e8d94c39fac12c42dc6ad25,Proceedings Ninth Annual IEEE Symposium on Logic in Computer Science
1526,A Bayesian model of dose-response for cancer drug studies,"Exploratory cancer drug studies test multiple tumor cell lines against multiple candidate drugs. The goal in each paired (cell line, drug) experiment is to map out the dose-response curve of the cell line as the dose level of the drug increases. We propose Bayesian tensor ﬁltering (BTF), a hierarchical Bayesian model for dose-response modeling in multisample, mul-titreatment cancer drug studies. BTF uses low-dimensional embeddings to share statistical strength between similar drugs and similar cell lines. Structured shrinkage priors in BTF encourage smoothness in the dose-response curves while remaining adaptive to sharp jumps when the data call for it. We focus on a pair of cancer drug studies exhibiting a particular pathol-ogy in their experimental design, leading us to a nonconjugate monotone mixture-of-gammas likelihood. To perform posterior inference, we develop a variant of the elliptical slice sampling algorithm for sampling from linearly-constrained multivariate normal priors with nonconjugate likelihoods. In benchmarks, BTF outperforms state-of-the-art methods for covariance regression and dynamic Poisson matrix factorization. On the two cancer drug studies, BTF outperforms the current standard approach in biology and reveals potential new biomarkers of drug sensitivity in cancer. Code is available at https://github.com/tansey/functionalmf. a Bayesian trend ﬁltering prior (Faulkner and Minin on top of a linear dynamical system with Pólya-gamma augmentation and Windle for binomial observations. and develop Poisson-gamma dynamical systems (PGDS), a dynamic matrix factorization model speciﬁcally for Poisson-distributed observations; we compare BTF with a tensor extension of PGDS in Section 6.",2022-06-01,https://www.semanticscholar.org/paper/79d4464f1b0244e0527f10c290767152067ea5ca,Annals of Applied Statistics
2501,Quick viewpoint switching for manipulating virtual objects in hand-held augmented reality using stored snapshots,"Magic-lens style augmented reality applications allow users to control camera pose easily by manipulating a portable hand-held device and provide immediate visual feedback. However, strategic vantage points must often be revisited repeatedly, adding time and error and taxing memory. We describe a new approach that allows users to take snapshots of augmented scenes that can be virtually revisited at later times. The system stores still images of scenes along with camera poses, so that augmentations remain dynamic and interactive. Users can manipulate virtual objects while viewing snapshots, instead of moving to real-world views. We present a study comparing performance in snapshot and live mode conditions in a task in which a virtual object must be aligned with two pairs of physical objects. Proper alignment requires sequentially visiting two viewpoints. Participants completed the alignment task significantly faster and more accurately using snapshots than when using the live mode. Moreover, participants preferred manipulating virtual objects using snapshots to the live mode.",2012-11-05,https://www.semanticscholar.org/paper/d965f4444695383e12ecdcb319fffcca7145bd01,International Symposium on Mixed and Augmented Reality
3161,SVR4UNIX Scheduler Unacceptable for Multimedia Applications,,1993-11-03,https://www.semanticscholar.org/paper/c3f01f7e2704192afe78a7e8ae54f431e6c09722,International Workshop on Network and Operating System Support for Digital Audio and Video
3002,Cloud Computing Security: Foundations and Research Directions,,,https://www.semanticscholar.org/paper/86a301c4fcd85ae6e003c34087d25df812b9da0a,Found. Trends Priv. Secur.
3020,AnDrone: Virtual Drone Computing in the Cloud,"With the continued proliferation of drones, unmanned aerial vehicles, additional uses for them are growing and the demand for their services is on the rise. We present AnDrone, a drone-as-a-service solution that makes drones accessible in the cloud. AnDrone pairs a cloud service with the first drone virtualization architecture. This enables a physical drone to run multiple virtual drones simultaneously in an isolated and secure manner at little additional cost, as computational costs are cheap compared to the operational and energy costs of putting a drone in the air. AnDrone virtualizes drones using a novel Linux container architecture. Android Things virtual drone containers provide a familiar user and development environment that can run existing Android apps. A real-time Linux flight controller container supports existing drone flight software and provides virtual drones with geofenced flight control. A device container transparently multiplexes access from virtual drones to a full range of drone hardware devices, including cameras and other sensors. Upon flight completion, virtual drones and their data can be uploaded to the cloud for offline access. We have implemented an AnDrone prototype based on Raspberry Pi 3 drone hardware. We demonstrate that it incurs minimal runtime performance and energy overhead, supports real-time virtual drone flight control, and runs untrusted third-party software in virtual drones in a secure manner while ensuring that the service provider maintains control of the drone hardware.",2019-03-25,https://www.semanticscholar.org/paper/736801135400eceded045b64b356fa038fa4a89c,European Conference on Computer Systems
658,Nonsurgical closure of femoral pseudoaneurysms complicating cardiac catheterization and percutaneous transluminal coronary angioplasty.,,1992-09-01,https://www.semanticscholar.org/paper/792eb127077cee8e547f17676a23db269b809da7,Journal of the American College of Cardiology
2267,Regulation of neutrophil apoptosis.,,2003-05-21,https://www.semanticscholar.org/paper/214ef2528bc1a3ffffac6a2c17e4c484309d8ce5,Chemical Immunology
3558,General constant expressions for system programming languages,"Most mainstream system programming languages provide support for builtin types, and extension mechanisms through userdefined types. They also come with a notion of constant expressions whereby some expressions (such as array bounds) can be evaluated at compile time. However, they require constant expressions to be written in an impoverished language with minimal support from the type system; this is tedious and error-prone. This paper presents a framework for generalizing the notion of constant expressions in modern system programming languages. It extends compile time evaluation to functions and variables of user-defined types, thereby including formerly ad hoc notions of Read Only Memory (ROM) objects into a general and type safe framework. It allows a programmer to specify that an operation must be evaluated at compile time. Furthermore, it provides more direct support for key meta programming and generative programming techniques. The framework is formalized as an extension of underlying type system with a binding time analysis. It was designed to meet real-world requirements. In particular, key design decisions relate to balancing expressive power to implementability in industrial compilers and teachability. It has been implemented for C++ in the GNU Compiler Collection, and is part of the next ISO C++ standard.",2010-03-22,https://www.semanticscholar.org/paper/048aa35d1ca049b612c71aa6a702b4e954b1077c,ACM Symposium on Applied Computing
486,On the Greedy Algorithm for Satisfiability,,1992-08-10,https://www.semanticscholar.org/paper/08addbc01d79d517d44a43bc6cd955d8ef64559e,Information Processing Letters
783,Adaptive Model Checking,"We consider the case where inconsistencies are present between a system and its corresponding model, used for automatic verification. Such inconsistencies can be the result of modeling errors or recent modifications of the system. Despite such discrepancies we can still attempt to perform automatic verification. In fact, as we show, we can sometimes exploit the verification results to assist in automatically learning the required updates to the model. In a related previous work, we have suggested the idea of black box checking, where verification starts without any model, and the model is obtained while repeated verification attempts are performed. Under the current assumptions, an existing inaccurate (but not completely obsolete) model is used to expedite the updates. We use techniques from black box testing and machine learning. We present an implementation of the proposed methodology called AMC (for Adaptive Model Checking). We discuss some experimental results, comparing various tactics of updating a model while trying to perform model checking.",2002-04-08,https://www.semanticscholar.org/paper/09fd5633e393d214a7fa7c2a3adfa354e2179f82,Logic Journal of the IGPL
916,Freedom from Deadlock of Safe Locking Policies,"The usual method for preserving the consistency of a database when accessed (read and updated) concurrently by several transactions, is by locking the transactions according to some locking policy; a locking policy that guarantees the preservation of consistency of the database is called safe. Furthermore, if no deadlocks can arise the policy is called deadlock-free. In this paper we are concerned with the freedom from deadlock of safe locking policies. We show that a simple extension of the DAG policy of [Y] is the most general safe and deadlock-free policy for a pair of transactions. We prove however, that it is NP-complete to test whether a set of transactions is not deadlock-free even for the simplest kind of transactions, those that are two-phase locked [E]. We show that for the natural class of safe locking policies, the L-policies, studied in [Y], freedom from deadlock is determined only by the order in which entities are accessed by the transactions and not by the way in which safety is ensured. A...",1982-05-01,https://www.semanticscholar.org/paper/a55c37c432b6550867bbbcf96e0360ba9a9fcd86,SIAM journal on computing (Print)
1308,Model-independent measurement of the W-boson helicity in top-quark decays at D0.,"We present the first model-independent measurement of the helicity of W bosons produced in top quark decays, based on a 1 fb(-1) sample of candidate tt events in the dilepton and lepton plus jets channels collected by the D0 detector at the Fermilab Tevatron pp Collider. We reconstruct the angle theta(*) between the momenta of the down-type fermion and the top quark in the W boson rest frame for each top quark decay. A fit of the resulting costheta(*) distribution finds that the fraction of longitudinal W bosons f(0)=0.425+/-0.166(stat)+/-0.102(syst) and the fraction of right-handed W bosons f(+)=0.119+/-0.090(stat)+/-0.053(syst), which is consistent at the 30% C.L. with the standard model.",2006-09-25,https://www.semanticscholar.org/paper/37657f98230b68262164dee86e6d7c8fb01ae6e1,Physical Review Letters
293,"The complexity of game dynamics: BGP oscillations, sink equilibria, and beyond","We settle the complexity of a well-known problem in networking by establishing that it is PSPACE-complete to tell whether a system of path preferences in the BGP protocol [25] can lead to oscillatory behavior; one key insight is that the BGP oscillation question is in fact one about Nash dynamics. We also show that the concept of sink equilibria proposed recently in [11] is PSPACE-complete to analyze and approximate for graphical games. Finally, we propose a new equilibrium concept inspired by game dynamics, unit recall equilibria, which we show to be close to universal (exists with high probability in a random game) and algorithmically promising. We also give a relaxation thereof, called componentwise unit recall equilibria, which we show to be both tractable and universal (guaranteed to exist in every game).",2008-01-20,https://www.semanticscholar.org/paper/b284c43884f57935e61e189a18ffe432a2a1ef9e,ACM-SIAM Symposium on Discrete Algorithms
904,Permuting Elements Within Columns of a Matrix in Order to Minimize Maximum Row Sum,"We study the problem of permuting the elements within columns of a given m × n matrix A so as to minimize its maximum row sum (sum of the elements in a row). We introduce and analyze an approximation algorithm of greedy type for this NP-complete problem. We prove that our algorithm produces matrices with maximum row sums no more than 3/2 − 1/2m times greater than those found by an optimization rule. Moreover, examples are presented which achieve this relative performance. Thus, our algorithm represents a substantial improvement in that all earlier algorithms have a worst-case performance that is asymptotically twice that of an optimization rule. We verify that our algorithm requires at most O(m2n) time, which is a modest increase over the earlier algorithms requiring Θ(mn log n) time in the worst-case.",1984-08-01,https://www.semanticscholar.org/paper/dbed55666e673619435ab8bc32b3588dd0305456,Mathematics of Operations Research
824,"Proceedings of the Fourteenth ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems, May 22-25, 1995, San Jose, California, USA",,1995-05-22,https://www.semanticscholar.org/paper/39cc47184bd378c934bf021dc181c032514a9e94,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems
1987,Overall Wafer Effectiveness (OWE): A novel industry standard for semiconductor ecosystem as a whole,,2013-05-01,https://www.semanticscholar.org/paper/e0f724525c5b3d0bc00038f2e60f3570ecb7280f,Computers & industrial engineering
2626,User Interfaces for Mobile Augmented Reality Systems,"In this dissertation, we present typical components of, useful services associated with, and user interactions possible with mobile augmented reality systems, based on a comprehensive series of hardware and software infrastructures and application prototypes we developed. We define a practical taxonomy of user interface components for such systems and establish methodology for adaptive mobile augmented reality interfaces that dynamically rearrange themselves in response to changes in user context. 
The research contributions to the state-of-the-art in augmented reality begin with the author's participation in the design of the “Columbia Touring Machine” in 1997, the first example of an outdoor mobile augmented reality system, and his lead in developing later prototypes. We develop a series of hardware and software infrastructures for prototyping mobile augmented reality applications that allow multiple users to participate in collaborative tasks taking place indoors and outdoors. 
We present exploratory user interfaces for many different applications and user scenarios, including the Situated Documentaries application framework for experiencing spatially distributed hypermedia presentations. Based on these explorations, we develop a taxonomic categorization of mobile augmented reality interface components and their properties. Virtual and real world objects alike are considered part of the interface. We tag each component with information about its purpose, its intrinsic properties, its relationship to other objects, and its capabilities and flexibility with regard to various manipulations. 
Mobile augmented reality has until now faced a significant challenge: the complexity of the augmented views rapidly increases when many virtual objects fight for screen space to annotate physical entities in the dynamic views of multiple fast-paced roaming users. Responding to this, we develop user interface management techniques for mobile augmented reality. A rule-based reasoning architecture uses the taxonomic data classification mentioned above to automatically rearrange augmented reality views in dynamic situations; for example to remove clutter in the augmented view of the world or to react to infrastructural context changes, such as variations in tracking accuracy.",,https://www.semanticscholar.org/paper/e801b5711f4727ddf42176820cc2e3e31243a20c,"International Conference on Vision, Video and Graphics"
2638,Information at a Glance,"What if we could visualize and interact with information directly in the context of our surroundings? Our research group is exploring how augmented reality (AR) could someday make this possible. AR integrates a complementary virtual world with the physical world-for example, by using head-tracked see-through head-worn displays to overlay graphics on what we see. Instead of looking back and forth between the real world and a PDA, we look directly at the real world and the virtual information overlaid on it. At the heart of this approach is context-aware computing, computing systems that are sensitive to the context in which they operate, ranging from human relationships to physical location. For example, information might be tied to specific locations within a global, Earth-centered, coordinate system. How can we design effective mobile AR user interfaces? We've been trying to answer this question in part by developing experimental AR research prototypes. In AR, as in work on information visualization using desktop technologies, the amount of information available can far exceed what a system can legibly display at a given time, necessitating information filtering. Julier et al. (2000) have developed information filtering techniques for AR that depend on the user's goals, object importance, and proximity. We assume that a system can accomplish information filtering of this sort and that our system is displaying everything it should.",2002-07-01,https://www.semanticscholar.org/paper/e1e3adef54e596a766db14eba657eda388c89437,IEEE Computer Graphics and Applications
945,Equivalence among Relational Expressions with the Union and Difference Operation,"A generalization of tableaux as a method for representing queries in relational databases, called sets of tableaux, is proposed. Every relational expression with the operators select, project, join and union can be represented by a set of tableaux. This paper studies the equivalence problem for sets of tableaux. It is shown that the theory of tableaux is easily extended to sets of tableaux, but the equivalence problem for sets of tableaux (as well as the containment problem for single tableaux) is NP-complete even in very restricted cases. Polynomial time algorithms for testing equivalence of sets of tableaux (and containment of tableaux) in three special cases are presented. Sets of tableaux are further generalized to sets of elementary differences in order to include also the difference operator. The equivalence problem for sets of elementary differences is investigated.",1978-09-13,https://www.semanticscholar.org/paper/2d77349cfe804f4709d0d71e8438067d4f57c3d2,Very Large Data Bases Conference
2715,Knowledge-based augmented reality,,1993-07-01,https://www.semanticscholar.org/paper/4580129bc3105521dc1e2ffd3a3cfc151728b158,CACM
10,Sampling strategies for information extraction over the deep web,,2017-03-01,https://www.semanticscholar.org/paper/b358851d198a0fe1797bdcae52cf625c10b68986,Information Processing & Management
2530,Poster: Shake menus: Towards activation and placement techniques for prop-based 3D graphical menus,"Shake menus are a novel method for activating, displaying, and selecting options presented relative to a tangible object or manipulator in a 3D user interface. They provide ready-to-hand interaction, including facile selection and placement of objects. We present the technique, several alternative methods for presenting shake menus (world-referenced, display-referenced, and object-referenced), and an evaluation of menu placement.",2009-03-14,https://www.semanticscholar.org/paper/2df9639cea73c98390f6c8dd152cb4888cbd835b,IEEE Symposium on 3D User Interfaces
1251,Search for anomalous top-quark couplings with the D0 detector.,Anomalous Wtb couplings modify the angular correlations of the top-quark decay products and change the single top-quark production cross section. We present limits on anomalous top-quark couplings by combining information from W boson helicity measurements in top-quark decays and anomalous coupling searches in the single top-quark final state. We set limits on right-handed vector couplings as well as left-handed and right-handed tensor couplings based on about 1 fb(-1) of data collected by the D0 experiment.,2008-12-31,https://www.semanticscholar.org/paper/bce163495cde2142a8122e0d071353b81c0a98e7,Physical Review Letters
2093,Developing statistical models in an early warning system and its empirical study,"When a new equipment or process is released, it is critical to ensure it behave as expected and stay in normal condition. The study proposes a research framework in which a statistical model is constructed for newly released equipment and process monitoring. An empirical study is conducted in a DRAM fabrication facility for validation. Based on the model, a best set of sample test items which discriminates the newly released equipment is selected and a group of normal equipments is obtained. Thus, the alarm signals can be triggered in an early warning system.",2004-09-09,https://www.semanticscholar.org/paper/0afbdfbb2b8668a6c832ecad0226fd2768813b28,2004 Semiconductor Manufacturing Technology Workshop Proceedings (IEEE Cat. No.04EX846)
419,How to learn an unknown environment. I: the rectilinear case,"We consider the problem faced by a robot that must explore and learn an unknown room with obstacles in it. We seek algorithms that achieve a bounded ratio of the worst-case distance traversed in order to see all visible points of the environment (thus creating a map), divided by the optimum distance needed to verify the map, if we had it in the beginning. The situation is complicated by the fact that the latter off-line problem (the problem of optimally verifying a map) is NP-hard. Although we show that there is no such “competitive” algorithm for general obstacle courses, we give a competitive algorithm for the case of a polygonal room with a bounded number of obstacles in it. We restrict ourselves to the rectilinear case, where each side of the obstacles and the room is parallel to one of the coordinates, and the robot must also move either parallel or perpendicular to the sides. (In a subsequent paper, we will discuss the extension to polygons of general shapes.)
We also discuss the off-line problem for simple rectilinear polygons and find an optimal solution (in the L1 metric) in polynomial time, in the case where the entry and the exit are different points.",1998-03-01,https://www.semanticscholar.org/paper/4b408ec4ab1828d7b9ed9a5ccccd32e998671a72,JACM
3412,A Fast Distributed Stateless Algorithm for alpha-Fair Packing Problems,"Over the past two decades, fair resource allocation problems have received considerable attention in a variety of application areas. However, little progress has been made in the design of distributed algorithms with convergence guarantees for general and commonly used $\alpha$-fair allocations. In this paper, we study weighted $\alpha$-fair packing problems, that is, the problems of maximizing the objective functions (i) $\sum_j w_j x_j^{1-\alpha}/(1-\alpha)$ when $\alpha > 0$, $\alpha \neq 1$ and (ii) $\sum_j w_j \ln x_j$ when $\alpha = 1$, over linear constraints $Ax \leq b$, $x\geq 0$, where $w_j$ are positive weights and $A$ and $b$ are non-negative. We consider the distributed computation model that was used for packing linear programs and network utility maximization problems. Under this model, we provide a distributed algorithm for general $\alpha$ that converges to an $\varepsilon-$approximate solution in time (number of distributed iterations) that has an inverse polynomial dependence on the approximation parameter $\varepsilon$ and poly-logarithmic dependence on the problem size. This is the first distributed algorithm for weighted $\alpha-$fair packing with poly-logarithmic convergence in the input size. The algorithm uses simple local update rules and is stateless (namely, it allows asynchronous updates, is self-stabilizing, and allows incremental and local adjustments). We also obtain a number of structural results that characterize $\alpha-$fair allocations as the value of $\alpha$ is varied. These results deepen our understanding of fairness guarantees in $\alpha-$fair packing allocations, and also provide insight into the behavior of $\alpha-$fair allocations in the asymptotic cases $\alpha\rightarrow 0$, $\alpha \rightarrow 1$, and $\alpha \rightarrow \infty$.",2015-02-11,https://www.semanticscholar.org/paper/8d507b7f9bd9880134cdad1809cca22ada7a3faa,"International Colloquium on Automata, Languages and Programming"
3261,Concordance on zebra stripes is not black and white: response to comment by Caro & Stankowich (2015),"We agree that the results of Larison et al . [1] and Caro et al . [2] are largely congruent—however, we remain divided on their interpretation. Both papers assessed a number of variables for an association with striping. Larison et al . [1] found temperature, specifically isothermality and the coldest temperature of the coldest quarter, to be the primary predictor of the degree of striping in plains zebra, with other climate and habitat variables playing very minor roles. Caro et al . [2] purport to have found a strong correlation between tabanid abundance and the degree of striping across equids. Caro and Stankowich would like readers to believe that both sets of data strongly support the notion that the evolution of striping has been driven by tabanid flies. However, we believe both sets …",2015-09-01,https://www.semanticscholar.org/paper/9262bff3704afd6dca1e4023e0a1d3f3e028913a,Royal Society Open Science
1826,A Computational Approach to Style in American Poetry,"We develop a quantitative method to assess the style of American poems and to visualize a collection of poems in relation to one another. Qualitative poetry criticism helped guide our development of metrics that analyze various orthographic, syntactic, and phonemic features. These features are used to discover comprehensive stylistic information from a poem's multi-layered latent structure, and to compute distances between poems in this space. Visualizations provide ready access to the analytical components. We demonstrate our method on several collections of poetry, showing that it better delineates poetry style than the traditional word-occurrence features that are used in typical text analysis algorithms. Our method has potential applications to academic research of texts, to research of the intuitive personal response to poetry, and to making recommendations to readers based on their favorite poems.",2007-10-28,https://www.semanticscholar.org/paper/9c514afc65342e71adc84ed9b46fda3a54a3d127,Industrial Conference on Data Mining
2411,Mitigation of VR Sickness During Locomotion With a Motion-Based Dynamic Vision Modulator,"In virtual reality, VR sickness resulting from continuous locomotion via controllers or joysticks is still a significant problem. In this article, we present a set of algorithms to mitigate VR sickness that dynamically modulate the user's field of view by modifying the contrast of the periphery based on movement, color, and depth. In contrast with previous work, this vision modulator is a shader that is triggered by specific motions known to cause VR sickness, such as acceleration, strafing, and linear velocity. Moreover, the algorithm is governed by delta velocity, delta angle, and average color of the view. We ran two experiments with different washout periods to investigate the effectiveness of dynamic modulation on the symptoms of VR sickness, in which we compared this approach against a baseline and pitch-black field-of-view restrictors. Our first experiment made use of a just-noticeable-sickness design, which can be useful for building experiments with a short washout period.",2022-06-10,https://www.semanticscholar.org/paper/1cc6aee12e6fdd1cc30b9684212d5cc7b71e3563,IEEE Transactions on Visualization and Computer Graphics
740,How good is the Chord algorithm?,"The Chord algorithm is a popular, simple method for the succinct approximation of curves, which is widely used, under different names, in a variety of areas, such as, multiobjective and parametric optimization, computational geometry, and graphics. We analyze the performance of the chord algorithm, as compared to the optimal approximation that achieves a desired accuracy with the minimum number of points. We prove sharp upper and lower bounds, both in the worst case and average case setting.",2010-01-17,https://www.semanticscholar.org/paper/02feacac034ce8109deaeb6625704e070a1a4caf,ACM-SIAM Symposium on Discrete Algorithms
2430,ICthroughVR: Illuminating Cataracts through Virtual Reality,"Vision impairments, such as cataracts, affect the way many people interact with their environment, yet are rarely considered by architects and lighting designers because of a lack of design tools. To address this, we present a method to simulate vision impairments, in particular cataracts, graphically in virtual reality (VR), using eye tracking for gaze-dependent effects. We also conduct a VR user study to investigate the effects of lighting on visual perception for users with cataracts. In contrast to existing approaches, which mostly provide only simplified simulations and are primarily targeted at educational or demonstrative purposes, we account for the user's vision and the hardware constraints of the VR headset. This makes it possible to calibrate our cataract simulation to the same level of degraded vision for all participants. Our study results show that we are able to calibrate the vision of all our participants to a similar level of impairment, that maximum recognition distances for escape route signs with simulated cataracts are significantly smaller than without, and that luminaires visible in the field of view are perceived as especially disturbing due to the glare effects they create. In addition, the results show that our realistic simulation increases the understanding of how people with cataracts see and could therefore also be informative for health care personnel or relatives of cataract patients.",2019-03-23,https://www.semanticscholar.org/paper/fef502774ed48df19536f5d9e239d1ee365fd4a9,IEEE Conference on Virtual Reality and 3D User Interfaces
1664,"Edward: A library for probabilistic modeling, inference, and criticism","Probabilistic modeling is a powerful approach for analyzing empirical information. We describe Edward, a library for probabilistic modeling. Edward's design reflects an iterative process pioneered by George Box: build a model of a phenomenon, make inferences about the model given data, and criticize the model's fit to the data. Edward supports a broad class of probabilistic models, efficient algorithms for inference, and many techniques for model criticism. The library builds on top of TensorFlow to support distributed training and hardware such as GPUs. Edward enables the development of complex probabilistic models and their algorithms at a massive scale.",2016-10-31,https://www.semanticscholar.org/paper/d51fefa58ceafe9bebddf03f2379842068dae3bc,arXiv.org
1666,Posterior Dispersion Indices,"Probabilistic modeling is cyclical: we specify a model, infer its posterior, and evaluate its performance. Evaluation drives the cycle, as we revise our model based on how it performs. This requires a metric. Traditionally, predictive accuracy prevails. Yet, predictive accuracy does not tell the whole story. We propose to evaluate a model through posterior dispersion. The idea is to analyze how each datapoint fares in relation to posterior uncertainty around the hidden structure. We propose a family of posterior dispersion indices (PDI) that capture this idea. A PDI identifies rich patterns of model mismatch in three real data examples: voting preferences, supermarket shopping, and population genetics.",2016-05-24,https://www.semanticscholar.org/paper/d952453fc406a13ddac04c8d408a556c37f7baa1,arXiv.org
471,On the k-server conjecture,,1994-05-23,https://www.semanticscholar.org/paper/8137870c0007aefddb92366c2eadcd8dff582567,Symposium on the Theory of Computing
2882,"Human T lymphotropic virus-I infection of human T lymphocytes induces expression of the beta-galactoside-binding lectin, galectin-3.","Animal lectins play important roles in a variety of biological processes via their recognition of glycoconjugates. Galectin-3 is a beta-galactoside-binding lectin previously designated as epsilon BP (IgE-binding protein), CBP35, Mac-2, L-29, and L-34, and its expression has been associated with various physiological and pathological processes, including cell growth, tumor transformation, and metastasis. Galectin-3 is widely distributed in various tissues and cell types and is expressed in many leukocytes, with the notable exception of B and T lymphocytes. We now report that galectin-3 is abundantly expressed in a number of human T lymphotropic virus (HTLV)-I-infected human T cell lines, including F6T, HUT 102, K3T, MT-2, and SLB-I, but is not expressed in non-HTLV-I-infected T cell lines such as Jurkat, CEM, and MOLT-4. In addition, the galectin-3 level was markedly increased in human thymocytes after infection with HTLV-I as compared with uninfected thymocytes. The up-regulation of galectin-3 expression appeared to correlate well with HTLV-I gene expression, as undetectable or very low levels of galectin-3 were found in the S1T and ATL-1K cell lines, which are nonproductively infected with HTLV-I. In co-transfection experiments, the galectin-3 promoter was significantly up-regulated by expression vectors encoding the 40-kd Tax protein, a potent transactivator in HTLV-I. Analysis of various Tax mutants suggested that galectin-3 promoter induction is dependent on activation of the cyclic-AMP-responsive element binding protein/activation transcription factor family of transcription factors and, to a lesser extent, nuclear factor-kappa B/Rel induction. Transfection of human promonocytic U-937 cells with an HTLV-I Tax expression vector induced galectin-3 expression in this cell line. Functionally, galectin-3 was shown to activate interleukin-2 production in Jurkat T cells. Together, these findings raise the possibility that HTLV-I Tax production induces the transcription and subsequent synthesis and secretion of galectin-3, which in turn may further activate these T cells and contribute to the altered properties of cell growth found in adult T cell leukemia induced by HTLV-I.",1996-05-01,https://www.semanticscholar.org/paper/a9f9ab94664b80312e5414019c9f1dfc18103fb5,American Journal of Pathology
784,Closed Partition Lattice and Machine Decomposition,"Finite-state machines are widely used to model systems in diverse areas. Often, the modeling machines can be decomposed into smaller component machines and this decomposition can facilitate the system design, implementation and analysis. J. Hartmanis and R.E. Stearns (1966) developed an elegant algebraic theory for machine decomposition that is based on the closed-partition lattice of a machine. In this paper, we study the computation of the closed-partition lattice of finite-state machines for the application to their decomposition. We present efficient algorithms for constructing the closed-partition lattice and for machine decomposition.",2002-02-01,https://www.semanticscholar.org/paper/1ba76d961bda5cc5db8ae2b9142135f24c576da7,IEEE Trans. Computers
3218,Contact Calls Facilitate Group Contraction in Free-Ranging Goats (Capra aegagrus hircus),"Many social animal species produce vocalizations believed to facilitate group contraction when one or more group members have become distant. However, the mechanisms underlying this function remain unclear for many species. We examined this question with data on a semi-free ranging group of 16 adult domesticated goats (Capra aegagrus hircus) inhabiting Tsaobis Nature Park, Namibia. All goats wore dataloggers consisting of a GPS and audio recorder for 5-6 hours per day for 10 days, providing continuous data on their geolocations and vocal communication. We found that callers were farther from the group centroid than expected by chance and that call production was associated with the cessation of group expansion and subsequent group contraction. We did not find strong evidence for antiphonal call exchange between distant and core group members. Rather, we found that (i) call production by distant group members is associated with a significant reduction of group movement away from the caller, and (ii) call production by core group members is associated with greater, though not significantly greater, group movement towards the caller. These findings suggest that calls may be used by distant, and potentially core, group members to facilitate the contraction of group spread. Results from our study clarify the mechanisms through which social animals can regulate collective movement behavior and the specific role that vocalizations play in this process.",2019-03-19,https://www.semanticscholar.org/paper/f2162c7cf0a6130d5809d92533264c72bf83faaa,Frontiers in Ecology and Evolution
283,Linked decompositions of networks and the power of choice in Polya urns,"A linked decomposition of a graph with n nodes is a set of subgraphs covering the n nodes such that all pairs of subgraphs intersect; we seek linked decompositions such that all subgraphs have about √n vertices, logarithmic diameter, and each vertex of the graph belongs to either one or two subgraphs. A linked decomposition enables many control and management functions to be implemented locally, such as resource sharing, maintenance of distributed directory structures, deadlock-free routing, failure recovery and load balancing, without requiring any node to maintain information about the state of the network outside the subgraphs to which it belongs. Linked decompositions also enable efficient routing, schemes with small routing tables, which we describe in Section 5. Our main contribution is to show that ""Internet-like graphs"" (e.g. the preferential attachment model proposed by Barabasi et al. [10] and other similar models) have linked decompositions with the parameters described above with high probability; moreover, our experiments show that the Internet topology itself can be so decomposed. Our proof proceeds by analyzing a novel process, which we call Polya urns with the power of choice, which may be of great independent interest. In this new process, we start with n nonempty bins containing O(n) balls total, and each arriving ball is placed in the least loaded of m bins, drawn independently at random with probability proportional to load. Our analysis shows that in our new process, with high probability the bin loads become roughly balanced some time before O(n2+ε) further balls have arrived and stay roughly balanced, regardless of how the initial O(n) balls were distributed, where ε > 0 can be arbitrarily small, provided m is large enough.",2008-01-20,https://www.semanticscholar.org/paper/16e70181c78909e744c132566279b012b1285f8b,ACM-SIAM Symposium on Discrete Algorithms
3663,C++ Programming Language,"From the Publisher: 
Written by Bjarne Stroustrup, the creator of C, this is the world's most trusted and widely read book on C. 
 
For this special hardcover edition, two new appendixes on locales and standard library exception safety have been added. The result is complete, authoritative coverage of the C language, its standard library, and key design techniques. Based on the ANSI/ISO C standard, The C Programming Language provides current and comprehensive coverage of all C language features and standard library components. 
 
For example: 
abstract classes as interfaces class hierarchies for object-oriented programming templates as the basis for type-safe generic software exceptions for regular error handling namespaces for modularity in large-scale software run-time type identification for loosely coupled systems the C subset of C for C compatibility and system-level work standard containers and algorithms standard strings, I/O streams, and numerics C compatibility, internationalization, and exception safety 
Bjarne Stroustrup makes C even more accessible to those new to the language, while adding advanced information and techniques that even expert C programmers will find invaluable.",,https://www.semanticscholar.org/paper/c04e29b09f67158e7c4405ddad18108a1ddecbd4,IEEE Software
873,Markov Decision Processes and Regular Events (Extended Abstract),,1990-07-16,https://www.semanticscholar.org/paper/adc3b3836ce5fba21550a04c8159fbc4f0f52c7f,"International Colloquium on Automata, Languages and Programming"
3563,Support for the Evolution of C++ Generic Functions,,2010-10-12,https://www.semanticscholar.org/paper/a005b52922ebb34ed6edf7f44a6305cddf50ddc6,Software Language Engineering
227,Simultaneous bayesian auctions and computational complexity,"Bayesian equilibria of simultaneous auctions for individual items have been explored recently [Christodoulou et al. 2008; Bhawalkar and Roughgarden 2011; Hassidim et al. 2011; Feldman et al. 2013] as an alternative to the well-known complexity issues plaguing combinatorial auctions with incomplete information, and some strong positive results have been shown about their performance. We point out some very serious complexity obstacles to this approach: Computing a Bayesian equilibrium in such auctions is hard for PP --- a complexity class between the polynomial hierarchy and PSPACE --- and even finding an approximate such equilibrium is as hard as NP, for some small approximation ratio (additive or multiplicative); therefore, the assumption that such equilibria will be arrived at by rational agents is quite problematic. In fact, even recognizing a Bayesian Nash equilibrium is intractable. Furthermore, these results hold even if bidder valuations are quite benign: Only one bidder valuation in our construction is unit demand or monotone submodular, while all others are additive. We also explore the possibility of favorable price of anarchy results for no-regret dynamics of the Bayesian simultaneous auctions game, and identify complexity obstacles there as well.",2014-06-01,https://www.semanticscholar.org/paper/525477ddf1b6787b816e800d80060871aa776a79,ACM Conference on Economics and Computation
3224,Mutualistic acacia ants exhibit reduced aggression and more frequent off‐tree movements near termite mounds,"In many ant–plant mutualisms, ants establish colonies in hollow thorns, leaf pouches, or other specialized structures on their host plants, which they then defend from herbivores. Resource heterogeneity could affect the maintenance of these mutualisms if it leads to one or both partners altering their investment in the interaction. Such a phenomenon may be especially pertinent to the Acacia–ant mutualism found in East African savannas, where termite mounds have a profound effect on the spatial structuring of resources used by both plants and ants. Here, we examined whether the proximity to termite mounds of Acacia drepanolobium trees is associated with variation in the behavior of one of their ant associates, Crematogaster nigriceps. We found that ant colonies near termite mounds had decreased aggressive responses to simulated herbivory as well as increased off‐tree movement. We hypothesize that these changes are the result of resident ant colonies near termite mounds shifting investment from defense of their host plant to foraging for nearby resources.",2018-06-13,https://www.semanticscholar.org/paper/43ace10ea9281ee469e62aa0ddc37fa28187ee6c,Biotropica
2676,IMPROVISE: Automated Generation of Animated Graphics for Coordinated Multimedia Presentations,,1998-01-28,https://www.semanticscholar.org/paper/782114bf4b816bc98da4c511a4babe5971e43743,Cooperative Multimodal Communication
2849,Galectin-3 translocates to the immunological synapse and inhibits cytokine production in CD4+ T cells activated by engagement of TCR (88.12),"
 Galectin-3 is induced in CD4+ T cells upon activation by mitogens or TCR engagement, but its function in this cell type has not been determined. We have compared the cytokine responses of gal3−/− and gal3+/+ CD4+ T cells upon activation induced by TCR engagement and found that gal3−/− CD4+ T cells secreted more IFNγ and IL-4 compared to gal3+/+ cells. We also found that serum levels of IFNγ were significantly higher in gal3−/− than gal3+/+ mice after exposure to Staphylococcus enterotoxin B. In addition, gal3−/− CD4+ T cells exhibited higher levels of tyrosine-phosphorylated proteins, as well as enhanced phosphorylation of early signaling molecules, including LAT and Zap70, upon anti-CD3 stimulation. These results suggest that endogenous galectin-3 suppresses T cell responses. Immunofluorescence analysis revealed that galectin-3 was recruited to the immunological synapse in CD4+ T cells after TCR engagement. Our preliminary data suggest that galectin-3 expression in T cells correlates with an enhanced down-regulation of TCR when the cells are activated by engagement of the receptor, in a manner similar to that described for other intracellular proteins that negatively regulate T cell activation. We conclude that galectin-3 is an inhibitory regulator of T cell activation that down-regulates signal transduction, and functions by recruitment to the immunological synapse following activation by TCR engagement.",2007-04-01,https://www.semanticscholar.org/paper/3a0135278b776eaa835dfc8ef449165d9f6bec21,Journal of Immunology
3265,Coping with transition: offspring risk and maternal behavioural changes at the end of the hiding phase,,2015-11-01,https://www.semanticscholar.org/paper/fcd51857b0b426b2f6a7bd995d8dce0730f4916d,Animal Behaviour
3254,DNA metabarcoding illuminates dietary niche partitioning by African large herbivores,"Significance Theory holds that sympatric large mammalian herbivores (LMH) must partition food resources to coexist, and traditional frameworks categorize LMH along a spectrum from grass-eating grazers to non–grass-eating browsers. Yet it has never been clear how finely LMH partition the enormous species diversity subsumed within these two broad plant types. By sequencing plant DNA from LMH fecal samples, we analyzed the diets of an LMH assemblage in Kenya. Diet composition was similar within species and strongly divergent across species, irrespective of feeding guild: Grazers ate similar total amounts of grass but different suites of grass species. These results suggest that species-specific plant traits may be key to understanding the dietary differences thought to underpin LMH diversity. Niche partitioning facilitates species coexistence in a world of limited resources, thereby enriching biodiversity. For decades, biologists have sought to understand how diverse assemblages of large mammalian herbivores (LMH) partition food resources. Several complementary mechanisms have been identified, including differential consumption of grasses versus nongrasses and spatiotemporal stratification in use of different parts of the same plant. However, the extent to which LMH partition food-plant species is largely unknown because comprehensive species-level identification is prohibitively difficult with traditional methods. We used DNA metabarcoding to quantify diet breadth, composition, and overlap for seven abundant LMH species (six wild, one domestic) in semiarid African savanna. These species ranged from almost-exclusive grazers to almost-exclusive browsers: Grass consumption inferred from mean sequence relative read abundance (RRA) ranged from >99% (plains zebra) to <1% (dik-dik). Grass RRA was highly correlated with isotopic estimates of % grass consumption, indicating that RRA conveys reliable quantitative information about consumption. Dietary overlap was greatest between species that were similar in body size and proportional grass consumption. Nonetheless, diet composition differed between all species—even pairs of grazers matched in size, digestive physiology, and location—and dietary similarity was sometimes greater across grazing and browsing guilds than within them. Such taxonomically fine-grained diet partitioning suggests that coarse trophic categorizations may generate misleading conclusions about competition and coexistence in LMH assemblages, and that LMH diversity may be more tightly linked to plant diversity than is currently recognized.",2015-06-01,https://www.semanticscholar.org/paper/486f8b467aec592d848e6e3795c0f0e53dcbc2d4,Proceedings of the National Academy of Sciences of the United States of America
2230,"Changes in expression of membrane TNF, NF-κB activation and neutrophil apoptosis during active and resolved inflammation","Background Tumour necrosis factor (TNF) is central to the pathophysiological process of rheumatoid arthritis (RA), whether as soluble cytokine or membrane-expressed pro-TNF (mTNF). Objectives To determine whether neutrophils, which can express TNF, are activated in the blood of patients with RA compared with healthy controls. To investigate, by focusing on mTNF expression, if the functions of RA neutrophils change in response to therapeutic TNF inhibition. Methods TNF was measured by flow cytometry and qPCR in neutrophils from 20 patients with RA before and after the start of TNF inhibitor therapy. Apoptosis was measured by morphology, and western blotting of pro- and antiapoptotic proteins in cell lysates. Nuclear factor κB (NF-κB) activation was determined by western blotting of phosphorylated NF-κB (p65). Results Before treatment RA neutrophils exhibited increased TNF mRNA expression, elevated mTNF levels and NF-κB activity compared with controls. They also underwent delayed apoptosis as shown by altered expression of anti- and proapoptotic proteins, such as Mcl-1 and caspases. Neutrophil TNF expression returned to baseline levels during successful treatment with anti-TNF biological agents, and there was a close correlation between clinical disease improvement and changes in neutrophil function. Conclusions Neutrophils express elevated levels of TNF in RA and the transcription factor, NF-κB, a target of TNF, is activated. This mechanism could lead to a self-sustained inflammatory process. These data point to an important role of neutrophils in the abnormal TNF signalling pathways activated in RA and provide new evidence that neutrophils actively contribute to altered cytokine signalling in inflammatory diseases.",2010-11-24,https://www.semanticscholar.org/paper/2d7f6bec8bc34bd53c554821060916e5a0a7dcba,Annals of the Rheumatic Diseases
2300,GTPgammaS-stimulated phospholipase D activation in human neutrophils occurs by protein kinase C-dependent and -independent pathways but not tyrosine kinases.,"Addition of GTPgammaS to saponin-permeabilised human neutrophils activated both the NADPH oxidase and phospholipase D (PLD). This PLD activation was hardly affected by staurosporine or Ro31-8220 (at concentrations which inhibited PMA stimulated PLD activity), indicating that it was largely independent of protein kinase C (PKC). This GTPgammaS stimulated PLD activity was enhanced by 1 mM ATP, but this ATP-enhanced activity was blocked by inhibitors of PKC. Addition of GTPgammaS resulted in very low levels of phosphorylation on tyrosine residues, but higher levels of phosphorylation on serine/threonine residues. Addition of pervanadate hydroperoxides stimulated phosphorylation on tyrosine residues and activated PLD which was blocked by addition of inhibitors of tyrosine kinases. Thus, GTPgammaS can stimulate PKC-dependent and -independent pathways of PLD activation. Whilst phosphorylation on tyrosine residues can result in activation of PLD, this is regulated independently of activation via G-proteins.",,https://www.semanticscholar.org/paper/145ba8b37a4c68962b22d72ffcd5887d20b95ba3,Biochemical and Biophysical Research Communications - BBRC
3363,SEXUAL SELECTION IN TOADS: THE ROLES OF FEMALE CHOICE AND MALE BODY SIZE,"Darwin (1871, p. 568) wrote ""sexual selection depends on the advantage which certain individuals have over others of the same sex and species solely in respect to reproduction."" This selection may act in several ways. Two extremes are either the individuals of one sex, usually males (but see Trivers, 1972), compete amongst themselves, with the winners acquiring the most mates; or the members of one sex, usually females, discriminate among members of the other sex choosing to mate with the most ""attractive"" individuals. The first extreme of sexual selection, competition among males, has been documented by field studies on baboons (De Vore, 1965), dungflies (Parker, 1970), elephant seals (Le Boeuff, 1974), lizards (Trivers, 1972, 1976), prairie chickens (Robel, 1966), and sage grouse (Scott, 1942). For example, Le Boeuff (1974) found that less than one third of the males in a breeding aggregation accounted for all of the matings and that copulation frequency was correlated with social rank. Competition among males may be incited in turn by the behavior of females (Cox and Le Boeuff, 1977). The second extreme of sexual selection, female choice among alternate mates, has not been studied as vigorously, and some biologists (Huxley, 1938; Lack, 1968) have doubted its importance to evolution. However, Trivers (1972) has theoretically shown the adaptive advantage that a female can gain by choosing the best of alternative mates and O'Donald (1972, 1973) with computer simulations has dem-",1978-06-01,https://www.semanticscholar.org/paper/3a84a580356b48a465465518b2decfdcb53f579c,Evolution; international journal of organic evolution
3200,Both Prey and Predator Features Determine Predation Risk and Survival of Schooling Prey,"Predation is one of the main evolutionary drivers of social grouping. While it is well appreciated that predation risk is likely not shared equally among individuals within groups, its detailed quantification has remained difficult due to the speed of attacks and the highly-dynamic nature of collective prey response. Here, using high-resolution tracking of solitary predators (Northern pike) hunting schooling fish (golden shiners), we not only provide detailed insights into predator decision-making but show which key spatial and kinematic features of predator and prey influence individual’s risk to be targeted and survive attacks. Pike tended to stealthily approach the largest groups, and were often already inside the school when launching their attack, making prey in this frontal “strike zone” the most vulnerable to be targeted. From the prey’s perspective, those fish in central locations, but relatively far from, and less aligned with, neighbours, were most likely to be targeted. While the majority of attacks (70%) were successful, targeted individuals that did manage to avoid capture exhibited a higher maximum acceleration response just before the attack and were further away from the pike‘s head. Our results highlight the crucial interplay between predators’ attack strategy and response of prey in determining predation risk in mobile animal groups.",2021-12-14,https://www.semanticscholar.org/paper/e7dae451255c279aef1f0118ce70579f165ad1c6,bioRxiv
2177,The clinical significance of fungi in atopic dermatitis,"Atopic dermatitis (AD) is one of the most common chronic inflammatory skin diseases and is caused by multiple factors including genetic factors, skin barrier defects, host immune responses, allergen sensitivity, environmental effects, and infections. Commonly, bacterial and viral infections are present in the eczematous lesions of AD patients and clearly aggravate the symptoms. However, studies of fungal infections in AD are limited in spite of the fact that there are reports showing that Malassezia, Candida, and some dermatophytes can affect the symptoms of AD. Moreover, certain fungal infections are sometimes overlooked and need to be considered particularly in AD patients with treatment failure as clinical features of those fungal infections could mimic eczematous lesions in AD. Here, we review the epidemiology, pathogenesis, clinical manifestations, and overlooked features of fungal infections associated with the symptoms of AD including the diagnosis and effectiveness of fungal treatments in AD patients.",2020-05-22,https://www.semanticscholar.org/paper/e04457891260a48b9918a0a9cc60b3cf6d1f8bad,International Journal of Dermatology
2260,Regulation of neutrophil apoptosis by Mcl-1.,"Neutrophils rapidly undergo spontaneous apoptosis, but this process can be considerably delayed by exposure to a variety of agents such as pro-inflammatory cytokines. The anti-apoptotic protein of the Bcl-2 family, Mcl-1, plays a key role in the regulation of neutrophil apoptosis. The protein has some unusual properties compared with other family members, including an extremely high turnover rate. Many factors, such as cytokines and local oxygen concentrations, can regulate cellular levels of Mcl-1 via transcription and post-transcriptional modification, control the survival time of neutrophils within tissues and thereby influence the inflammatory response.",2004-06-01,https://www.semanticscholar.org/paper/0144e1339b5701c70e07bd9d63385c893b0fb60d,Biochemical Society Transactions
1996,Advanced decision and intelligence technologies for manufacturing and logistics,,2012-12-01,https://www.semanticscholar.org/paper/2855b8de620e32fa39ecd42df222a667ab6a4538,Journal of Intelligent Manufacturing
1311,Search for resonant second generation slepton production at the Fermilab Tevatron.,"We present a search for supersymmetry in the R-parity violating resonant production and decay of smuons and muon sneutrinos in the channels mu-->chi(1)(0)mu, mu-->chi(2,3,4)(0)mu, and nu(mu)-->chi(1,2)(+/-)mu. We analyzed 0.38 fb(-1) of integrated luminosity collected between April 2002 and August 2004 with the D0 detector at the Fermilab Tevatron Collider. The observed number of events is in agreement with the standard model expectation, and we calculate 95% C.L. limits on the slepton production cross section times branching fraction to gaugino plus muon, as a function of slepton and gaugino masses. In the framework of minimal supergravity, we set limits on the coupling parameter lambda(211)('), extending significantly previous results obtained in Run I of the Tevatron and at the CERN LEP collider.",2006-05-03,https://www.semanticscholar.org/paper/3ddae2c1269439d3ad98ec88c08e23c1abd95cae,Physical Review Letters
3177,Both prey and predator features predict the individual predation risk and survival of schooling prey,"Predation is one of the main evolutionary drivers of social grouping. While it is well appreciated that predation risk is likely not shared equally among individuals within groups, its detailed quantification has remained difficult due to the speed of attacks and the highly dynamic nature of collective prey response. Here, using high-resolution tracking of solitary predators (Northern pike) hunting schooling fish (golden shiners), we not only provide insights into predator decision-making, but show which key spatial and kinematic features of predator and prey predict the risk of individuals to be targeted and to survive attacks. We found that pike tended to stealthily approach the largest groups, and were often already inside the school when launching their attack, making prey in this frontal ‘strike zone’ the most vulnerable to be targeted. From the prey’s perspective, those fish in central locations, but relatively far from, and less aligned with, neighbours, were most likely to be targeted. While the majority of attacks were successful (70%), targeted individuals that did manage to avoid being captured exhibited a higher maximum acceleration response just before the attack and were further away from the pike‘s head. Our results highlight the crucial interplay between predators’ attack strategy and response of prey underlying the predation risk within mobile animal groups.",2022-07-19,https://www.semanticscholar.org/paper/62e0f7a5c2f48d394471fa1dc81e59a724c96107,eLife
750,"Automata, Probability, and Recursion",,2008-07-21,https://www.semanticscholar.org/paper/13140970ded9750d41733996dea1a879e8b55210,International Conference on Implementation and Application of Automata
2396,Carbon monoxide- and oxygen-reacting haemoproteins in the mitochondrial fraction from the soil amoeba Acanthamoeba castellanii. Studies at subzero temperatures.,"1. Mitochondria-enriched fractions of the soil amoeba Acanthamoeba castellanii contained four haemoproteins that in their reduced forms reacted with CO to give photodissociable CO complexes; these were cytochromes a 3, a 614, b- and c-type cytochromes. 2. Non-photodissociable oxygen-containing compounds were formed at temperatures between -130 and -150 degrees C after photodissociation of CO in the presence of 200 microM-O2, 3. Electron transport, indicated by the oxidation of cytochromes a + a3 and cytochrome c, did not occur until the temperature was raised to -80 degrees C.",1981-11-15,https://www.semanticscholar.org/paper/daac4b6de967447485c182114ab3de15c43e8ad1,Biochemical Journal
178,From Battlefields to Elections: Winning Strategies of Blotto and Auditing Games,"Mixed strategies are often evaluated based on the expected payoff that they guarantee. This is not always desirable. In this paper, we consider games for which maximizing the expected payoff deviates from the actual goal of the players. To address this issue, we introduce the notion of a (u, p)-maxmin strategy which ensures receiving a minimum utility of u with probability at least p. We then give approximation algorithms for the problem of finding a (u, p)-maxmin strategy for these games. The first game that we consider is Colonel Blotto, a well-studied game that was introduced in 1921. In the Colonel Blotto game, two colonels divide their troops among a set of battlefields. Each battlefield is won by the colonel that puts more troops in it. The payoff of each colonel is the weighted number of battlefields that she wins. We show that maximizing the expected payoff of a player does not necessarily maximize her winning probability for certain applications of Colonel Blotto. For example, in presidential elections, the players' goal is to maximize the probability of winning more than half of the votes, rather than maximizing the expected number of votes that they get. We give an exact algorithm for a natural variant of continuous version of this game. More generally, we provide constant and logarithmic approximation algorithms for finding (u, p)-maxmin strategies. We also introduce a security game version of Colonel Blotto which we call auditing game. It is played between two players, a defender and an attacker. The goal of the defender is to prevent the attacker from changing the outcome of an instance of Colonel Blotto. Again, maximizing the expected payoff of the defender is not necessarily optimal. Therefore we give a constant approximation for (u, p)-maxmin strategies.",2018-01-07,https://www.semanticscholar.org/paper/0dab34a0bb73b2b966a7edfddfce414646fe6986,ACM-SIAM Symposium on Discrete Algorithms
1249,Measurement of the tt[over] production cross section in pp[over] collisions at sqrt(s)=1.96 TeV.,"We measure the tt[over] production cross section in pp[over] collisions at sqrt(s)=1.96 TeV in the lepton + jets channel. Two complementary methods discriminate between signal and background: b tagging and a kinematic likelihood discriminant. Based on 0.9 fb(-1) of data collected by the D0 detector at the Fermilab Tevatron Collider, we measure sigma(tt[over])=7.62+/-0.85 pb, assuming the current world average m(t)=172.6 GeV. We compare our cross section measurement with theory predictions to determine a value for the top-quark mass of 170+/-7 GeV.",,https://www.semanticscholar.org/paper/b5a4157d9eb86d8f249e46adbcac662096eba2d0,Physical Review Letters
3086,Transparent Checkpoint-Restart of Multiple Processes on Commodity Operating Systems,"The ability to checkpoint a running application and restart it later can provide many useful benefits including fault recovery, advanced resources sharing, dynamic load balancing and improved service availability. However, applications often involve multiple processes which have dependencies through the operating system. We present a transparent mechanism for commodity operating systems that can checkpoint multiple processes in a consistent state so that they can be restarted correctly at a later time. We introduce an efficient algorithm for recording process relationships and correctly saving and restoring shared state in a manner that leverages existing operating system kernel functionality. We have implemented our system as a loadable kernel module and user-space utilities in Linux. We demonstrate its ability on real-world applications to provide transparent checkpoint-restart functionality without modifying, recompiling, or relinking applications, libraries, or the operating system kernel. Our results show checkpoint and restart times 3 to 55 times faster than OpenVZ and 5 to 1100 times faster than Xen.",2007-06-17,https://www.semanticscholar.org/paper/3ec5430347717ff2f02f435cdcf951f35cd1479a,USENIX Annual Technical Conference
2850,Role of galectin-3 in prion infections of the CNS.,,2007-08-03,https://www.semanticscholar.org/paper/57d41d27a733abba2ebfb616d2c33d03b5133551,Biochemical and Biophysical Research Communications - BBRC
2286,In vivo localisation and stability of human Mcl‐1 using green fluorescent protein (GFP) fusion proteins,,2000-07-28,https://www.semanticscholar.org/paper/6e228b663d66c0d20c96f599ecacbbc3afcc7bf0,FEBS Letters
639,Some complexity results for the Traveling Salesman Problem,"It is shown that, unless P&equil;NP, local search algorithms for the Traveling Salesman Problem having polynomial time complexity per iteration will generate solutions arbitrarily far from the optimal. The Traveling Salesman Problem is also shown to be NP-Complete even if its instances are restricted to be realizable by a set of points on the Euclidean plane.",1976-05-03,https://www.semanticscholar.org/paper/b71714a5a99290fb9f9b1f9406c60be6cc32de8a,Symposium on the Theory of Computing
1557,Text-Based Ideal Points,"Ideal point models analyze lawmakers’ votes to quantify their political positions, or ideal points. But votes are not the only way to express a political position. Lawmakers also give speeches, release press statements, and post tweets. In this paper, we introduce the text-based ideal point model (TBIP), an unsupervised probabilistic topic model that analyzes texts to quantify the political positions of its authors. We demonstrate the TBIP with two types of politicized text data: U.S. Senate speeches and senator tweets. Though the model does not analyze their votes or political affiliations, the TBIP separates lawmakers by party, learns interpretable politicized topics, and infers ideal points close to the classical vote-based ideal points. One benefit of analyzing texts, as opposed to votes, is that the TBIP can estimate ideal points of anyone who authors political texts, including non-voting actors. To this end, we use it to study tweets from the 2020 Democratic presidential candidates. Using only the texts of their tweets, it identifies them along an interpretable progressive-to-moderate spectrum.",2020-05-08,https://www.semanticscholar.org/paper/49f9d8505e8d27041cc57cd3d06c2741f21120c1,Annual Meeting of the Association for Computational Linguistics
1897,A UNISON framework for knowledge management of university-industry collaboration and an illustration,,2019-03-01,https://www.semanticscholar.org/paper/aa81a441b01bbe3225bc1d4f217c3229c0f3d597,Computers & industrial engineering
2973,An Infinite Latent Attribute Model for Network Data,"Latent variable models for network data extract a summary of the relational structure underlying an observed network. The simplest possible models subdivide nodes of the network into clusters; the probability of a link between any two nodes then depends only on their cluster assignment. Currently available models can be classified by whether clusters are disjoint or are allowed to overlap. These models can explain a ""flat"" clustering structure. Hierarchical Bayesian models provide a natural approach to capture more complex dependencies. We propose a model in which objects are characterised by a latent feature vector. Each feature is itself partitioned into disjoint groups (subclusters), corresponding to a second layer of hierarchy. In experimental comparisons, the model achieves significantly improved predictive performance on social and biological link prediction tasks. The results indicate that models with a single layer hierarchy over-simplify real networks.",2012-06-26,https://www.semanticscholar.org/paper/425c51b95476e5ffa39978ae9bf76b729b9db782,International Conference on Machine Learning
1572,Hierarchical recurrent state space models reveal discrete and continuous dynamics of neural activity in C. elegans,"Modern recording techniques enable large-scale measurements of neural activity in a variety of model organisms. The dynamics of neural activity shed light on how organisms process sensory information and generate motor behavior. Here, we study these dynamics using optical recordings of neural activity in the nematode C. elegans. To understand these data, we develop state space models that decompose neural time-series into segments with simple, linear dynamics. We incorporate these models into a hierarchical framework that combines partial recordings from many worms to learn shared structure, while still allowing for individual variability. This framework reveals latent states of population neural activity, along with the discrete behavioral states that govern dynamics in this state space. We find stochastic transition patterns between discrete states and see that transition probabilities are determined by both current brain activity and sensory cues. Our methods automatically recover transition times that closely match manual labels of different behaviors, such as forward crawling, reversals, and turns. Finally, the resulting model can simulate neural data, faithfully capturing salient patterns of whole brain dynamics seen in real data.",2019-04-29,https://www.semanticscholar.org/paper/8160f14cb60f8decfaf18c1e0f1597f872f7779d,bioRxiv
657,Intracoronary Stenting for Acute and Threatened Closure Complicating Percutaneous Transluminal Coronary Angioplasty,"BackgroundAcute closure remains a significant limitation of percutaneous transluminal coronary angioplasty (PTCA) and underlies the majority of ischemic complications. This study details the clinical and angiographic characteristics of a series of patients receiving an intracoronary stent device to manage acute and threatened closure and presents the early clinical results. Methods and ResultsFrom October 1989 through June 1991, 115 patients undergoing PTCA received intracoronary stents to treat acute or threatened closure in 119 vessels. Sixty-three percent had multivessel coronary disease, 33 (29%) had undergone prior coronary artery bypass grafting (CABG), and 52 (45%) had had previous PTCA. Using the American College of Cardiology/American Heart Association (ACC/AHA) classification, 15% of lesions were class A, 55% were class B, and 30% were class C. Eight patients were referred with severe coronary dissection and unstable angina after PTCA at other institutions. Acute closure was defined as occlusion of the vessel with TIMI (Thrombolysis in Myocardial Infarction) 0 or 1 flow immediately before stent placement. Threatened closure required two or more of the following criteria: 1) a residual stenosis greater than 50%, 2) TIMI grade 2 flow, 3) angiographic dissection comprising extraluminal dye extravasation and/or a length of greater than 15 mm, 4) evidence of clinical ischemia (either typical angina or ECG changes). Twelve vessels (10%) met the criteria for acute closure, and 87 vessels (73%) satisfied the criteria for threatened closure. Twenty vessels (17%) failed to meet two criteria. Stenting produced optimal angiographic results in 111 vessels (93%), with mean diameter stenosis (±1 SD) reduced from 83±12% before to 18±29% after stenting. Overall, in-hospital mortality was 1.7% and CABG was required in 4.2%; Q wave myocardial infarction (MI) occurred in 7% and non-Q wave MI in 91%. Stent thrombosis occurred in nine patients (7.6%). For the 108 patients who presented to the catheterization laboratory without evolving MI, Q wave MI occurred in 4% and non-Q wave MI occurred in 7%. Angiographic follow-up has been performed in 81 eligible patients (76%), and 34 patients (41%) had a lesion of .50%. ConclusionsThis stent may be a useful adjunct to balloon dilatation in acute or threatened closure. Randomized studies comparing this stent with alternative technologies are required.",1992-03-01,https://www.semanticscholar.org/paper/159f520e94781eb63dd1d15eb52b3b6a55dd0ec7,Circulation
473,Motion Planning on a Graph (Extended Abstract),"We are given a connected. undirected graph G on n vertices. There is a mobile robot on one of the vertices; this vertex is labeled s. Each of several other vertices contains a single movable obsracle. The robot and the obstacles may only reside at vertices, although they may be moved across edges. A vertex may never contain more than one object (robot/obstacle). In one step, we may move either the robot or one of the obstacles from its current position to a vacant vertex adjacent to U. Our goal is to move the robot to a designated vertex t using the smallest number of steps possible. The problem is a simple abstraction of a robot motion planning problem. with the geometry replaced by the adjacencies in the graph. Wc point out its connections to robot motion planning. We study its complexity. giving exact and approximate algorithms for several cases.",,https://www.semanticscholar.org/paper/b0acd8d6d928657549da06735164c478afff34ae,IEEE Annual Symposium on Foundations of Computer Science
2340,Stimulation of neutrophils by insoluble immunoglobulin aggregates from synovial fluid of patients with rheumatoid arthritis,"Abstract. Insoluble immunoglobulin aggregates present in the synovial fluid of patients with rheumatoid arthritis have been examined for their ability to activate reactive oxidant and granule enzyme secretion from bloodstream neutrophils. These insoluble complexes activated luminol chemiluminescence, but did not activate O2‐, H2O2 or granule enzyme secretion and did not activate lucigenin chemiluminescence, which also measures reactive oxidant secretion. Hence, the luminol chemiluminescence detected after activation by insoluble immunoglobulin aggregates must be due to intracellularly generated reactive oxidants, i.e. produced within phagolysosomes. Because reactive oxidant and granule enzyme secretion has occurred within rheumatoid joints, other mechanisms of neutro‐phil activation must exist.",1992-05-01,https://www.semanticscholar.org/paper/54e8621b2c5037d06f3b8e05b0a93ed553d97ecf,European Journal of Clinical Investigation
3176,An observation of attempted infanticide and female–female cooperation in wild plains zebras (Equus quagga),"
 Male infanticide has been reported in a wide range of taxa as a strategy for redirecting maternal investment and increasing a male’s chance of siring future offspring. Plains zebras (Equus quagga) possess many of the social organization and life history traits found to favour infanticide. However, most documented cases are from captive animals, while it has not been detected in studies of free-ranging populations. Here, we report an apparent infanticide attempt in which the historical associations of all participants were known. In addition, we report the first instance of non-kin female–female cooperative defence against male aggression in this species. We discuss why this behaviour may not have been observed by other longitudinal studies. We then explore how intraspecific and inter-individual variation may factor into its relative rarity, how the reproductive biology of plains zebras relates to this behaviour, and how female–female cooperation between non-kin can operate as an effective counterstrategy.",2022-06-15,https://www.semanticscholar.org/paper/450f34922b318a2af2ece9951cb1281275725038,Behaviour
690,A Concentration Theorem for Projections,"X in R^D has mean zero and finite second moments. We show that there is a precise sense in which almost all linear projections of X into R^d (for d < D) look like a scale-mixture of spherical Gaussians -- specifically, a mixture of distributions N(0, sigma^2 I_d) where the weight of the particular sigma component is P (| X |^2 = sigma^2 D). The extent of this effect depends upon the ratio of d to D, and upon a particular coefficient of eccentricity of X's distribution. We explore this result in a variety of experiments.",2006-07-13,https://www.semanticscholar.org/paper/4526320dd1be5d52789953faa04419f2c620862f,Conference on Uncertainty in Artificial Intelligence
1901,AI and Big Data Analytics for Wafer Fab Energy Saving and Chiller Optimization to Empower Intelligent Manufacturing,"The chiller machine is one of the most electricity-consuming parts of factory facilities in high-tech industries such as semiconductor and TFT-LCD manufacturing. To reduce variability and optimize the chiller allocation, researchers have come up with various solutions, but few of them can indeed be widely adopted due to differences of factory layout, machine types, data collections, etc. This study proposes a solution that integrates big data analytics and machine learning techniques to automatically provide recommendations of chiller optimization for energy saving. The optimal chiller adjustment is defined as the condition that the required cooling load for a wafer fab is satisfied while and the electricity consumption is minimized. In the meantime, those adjustment alternatives considering chiller healthy status to obviate inappropriate combinations. Hence, engineers only need to judge the rationality of these recommendations to adjust chillers so that can guarantee operation effectiveness and efficiency as well as empower intelligent manufacturing. An empirical study was conducted in a leading semiconductor company in Taiwan to demonstrate the validity of the proposed approach.",2018-09-01,https://www.semanticscholar.org/paper/3ef3ba2ae62f0c834ec2f63d555d96fcae9d4768,2018 e-Manufacturing & Design Collaboration Symposium (eMDC)
109,The Stanford InfoBus and Its Service Layers: Augmenting the Internet with High-Level Information Management Protocols,,,https://www.semanticscholar.org/paper/4e54e204e30f8143b48e62a3822607df87ca53e9,The MeDoc Approach
1808,Relational Topic Models for Document Networks,"We develop the relational topic model (RTM), a model of documents and the links between them. For each pair of documents, the RTM models their link as a binary random variable that is conditioned on their contents. The model can be used to summarize a network of documents, predict links between them, and predict words within them. We derive efficient inference and learning algorithms based on variational methods and evaluate the predictive performance of the RTM for large networks of scientific abstracts and web documents.",2009-04-15,https://www.semanticscholar.org/paper/9f68d27df3a4c4be8636f376cb15f77e55a2f496,International Conference on Artificial Intelligence and Statistics
333,... The Interaction Between Algorithms and Game Theory,,2005-05-10,https://www.semanticscholar.org/paper/78fcc69adea888eea0e82626d83ce37d60c671e6,Workshop on Engineering Applications
538,Exponential lower bounds for finding Brouwer fixed points,,1987-10-12,https://www.semanticscholar.org/paper/a25e30be4d0aa5f11d284acbc3fd3374e0e3ce1a,28th Annual Symposium on Foundations of Computer Science (sfcs 1987)
662,A Dataset for Learning University STEM Courses at Scale and Generating Questions at a Human Level,"We present a new dataset for learning to solve, explain, and generate university-level STEM questions from 27 courses across a dozen departments in seven universities. We scale up previous approaches to questions from courses in the departments of Mechanical Engineering, Materials Science and Engineering, Chemistry, Electrical Engineering, Computer Science, Physics, Earth Atmospheric and Planetary Sciences, Economics, Mathematics, Biological Engineering, Data Systems, and Society, and Statistics. We visualize similarities and differences between questions across courses. We demonstrate that a large foundation model is able to generate questions that are as appropriate and at the same difficulty level as human-written questions.",2023-06-26,https://www.semanticscholar.org/paper/58d225fc5c13db9328c573209b51a2a7bf1340c8,AAAI Conference on Artificial Intelligence
960,A Case Report of Hemodialysis Graft-induced Increased Intraocular Pressure.,"Dear Editor, Hemodialysis is performed in the patients with end-stage renal disease, and arteriovenous fistula (AVF) is regarded as the best type of hemodialysis access [1]. However, various complications, such as thrombosis, stenosis and infection have been reported [2]. In the field of ophthalmology, rare cases of increased intraocular pressure (IIOP) related to hemodialytic condition have been reported [2]. In this study, we report the case of a patient who presented with IIOP, severe headache, and prolonged choroidal detachment due to abnormal venous drainage. A 50-year-old Korean man, with end-stage renal disease and in a hemodialysis program for 8 years, presented with IIOP in his left eye. His right eye was blinded due to a history of surgery for retinal detachment, and he was undergoing treatment for chronic open-angle glaucoma in his left eye. Visual acuity was 20 / 25, IOP was 18 mmHg, and spherical equivalent was -0.375 diopter. Ocular examination revealed regular pseudophakia, axial length of 23.13 mm and central corneal thickness of 507 μm. Fundus exam showed a pale optic disc and a diffuse superior thinning of the retinal nerve fiber layer. He underwent trabeculectomy due to IIOP even with the maximum tolerated medical therapy. After surgery, the IOP decreased to 9 mmHg, but choroidal detachment was developed. Nine days after surgery, he presented with severe headache and ocular pain. The IOP increased to 25 mmHg, and functional filtration bleb formed by trabeculectomy was not well elevated. In addition, although the IOP was elevated, no significant improvement in choroidal detachment was observed (Fig. 1A, 1B). Afterwards, the status of choroidal detachment tended to wax and wane, although the IOP was maintained relatively high. He underwent brain work-up, and result showed f low signs on left internal jugular vein (IJV) with a significant dilatation in the arterial phase (Fig. 1C). Due to the suspicion that the IIOP may be related to an abnormality in the hemodialytic vessels, he underwent fistulography to check the arteriovenous f low. Surprisingly, fistulography confirmed that the venous flow of the cephalic vein in the AVF circuit was directly joined to the IJV, and not to the subclavian vein, inducing venous reflux to the brain (Fig. 1D). He underwent a left cephalic vein ligation and new AVF formation in the right side. After surgery, the IOP decreased to 16 mmHg on the first day and 6 mmHg on the second day, and the patient’s subjective symptoms also improved. After two weeks, IOP was stable and the visual acuity was improved to 20 / 40. In addition, a remarkable improvement of choroidal detachment over time was observed, which completely resolved after two months. He was disKorean J Ophthalmol 2021;35(4):328-329 https://doi.org/10.3341/kjo.2021.0031",2021-06-21,https://www.semanticscholar.org/paper/fd870011a2a488fb3c6563e5031d3b09d7bcf7ef,Korean Journal of Ophthalmology
3026,Binary compatible graphics support in Android for running iOS apps,"Mobile apps make extensive use of GPUs on smartphones and tablets to access Web content. To support pervasive Web content, we introduce three key OS techniques for binary graphics compatibility necessary to build a real-world system to run iOS and Android apps together on the same smartphone or tablet. First diplomat usage patterns manage resources to bridge proprietary iOS and Android graphics implementations. Second, thread impersonation allows a single thread-specific context to be shared amongst multiple threads using multiple iOS and Android personas. Third, dynamic library replication allows multiple, independent instances of the same library to be loaded in a single process to support iOS apps on Android while using multiple graphics API versions at the same time. We use these techniques to build a system prototype, and demonstrate that it runs widely-used iOS apps, including apps such as Safari that use the popular GPU-accelerated WebKit framework, using a Google Nexus tablet running Android.",2017-12-11,https://www.semanticscholar.org/paper/3416a5227b1e1956acfacba812081c2686367471,International Middleware Conference
1651,Detecting and Characterizing Events,"Significant events are characterized by interactions between entities (such as countries, organizations, or individuals) that deviate from typical interaction patterns. Analysts, including historians, political scientists, and journalists, commonly read large quantities of text to construct an accurate picture of when and where an event happened, who was involved, and in what ways. In this paper, we present the Capsule model for analyzing documents to detect and characterize events of potential significance. Specifically, we develop a model based on topic modeling that distinguishes between topics that describe “business as usual” and topics that deviate from these patterns. To demonstrate this model, we analyze a corpus of over two million U.S. State Department cables from the 1970s. We provide an open-source implementation of an inference algorithm for the model and a pipeline for exploring its results.",2016-11-01,https://www.semanticscholar.org/paper/56b874006900ade34feb39f1317ebbe6c0b0e1d2,Conference on Empirical Methods in Natural Language Processing
537,The weighted region problem,"<italic>We present an algorithm for determining the shortest path between a source and a destination through a planar subdivision in which each region has an associated weight. Distances are measured according to a weighted Euclidean metric: Each region of the subdivision has associated with it a weight, and the weighted distance between two points in a convex region is the product of the corresponding weight and the Euclidean distance between them. Our algorithm runs in time &Ogr;</italic>(<italic>n</italic><supscrpt>7</supscrpt> <italic>L</italic>) <italic>and requires &Ogr;</italic>(<italic>n</italic><supscrpt>3</supscrpt>) <italic>space, where n is the number of edges of the subdivision, and L is the precision of the problem instance (including the number of bits in a user-specified tolerance ∈, which is the percentage the solution is allowed to differ from an optimal solution). The algorithm uses the fact that shortest paths obey Snell's Law of Refraction at region boundaries, a local optimality property of shortest paths that is well-known from the analogous optics model.</italic>",1987-10-01,https://www.semanticscholar.org/paper/3206d6f93bebaca6f41dee4a9aea3e9f5e4732fb,SCG '87
2037,Foreword,,,https://www.semanticscholar.org/paper/da89180216e106fc747718a9792c7396306185b1,Computers & industrial engineering
286,The Search for Equilibrium Concepts,,2008-04-30,https://www.semanticscholar.org/paper/4cfee2549f408f62e675a29e514a4650bfdc3fe3,Algorithmic Game Theory
2277,Cultivation of an ovine strain of Ehrlichia phagocytophila in tick cell cultures.,"Ehrlichia phagocytophila (previously known as Cytoecetes phagocytophila) which causes tick-borne fever (TBF) in sheep and pasture fever in cattle in the UK and mainland Europe is transmitted by the temperate hard tick Ixodes ricinus. The disease in sheep is characterized by fever, leucopenia and immunosuppression. Studies on the pathogenesis and other aspects of the disease have been hampered because the organism has not been cultivated in continuous or primary cell culture systems. This paper describes the first successful cultivation of a European isolate of E. phagocytophila in two continuous cell lines, IDE8 and ISE6, derived from the temperate hard tick Ixodes scapularis. Once adapted to tick cell cultures the organism was serially sub-cultured in new cells by transferring small portions of infected cell suspension every 2 to 3 weeks. The identity of the organism was confirmed by polymerase chain reaction (PCR), with primers specific to the granulocytic ehrlichiae. Sequence analysis of the PCR products amplified from infected tick cells were shown to be identical with those amplified from the blood of sheep infected with the same strain of E. phagocytophila. A susceptible sheep inoculated with a third passage of the tick cell-adapted E. phagocytophila reacted with fever and rickettsiaemia 5 days later, thus satisfying Koch's postulates.",2002-10-01,https://www.semanticscholar.org/paper/1bcbef85536d10b2a400684dc4c1aadcd04889c0,Journal of Comparative Pathology
3230,Moving in the Anthropocene: Global reductions in terrestrial mammalian movements,"Restrictions on roaming Until the past century or so, the movement of wild animals was relatively unrestricted, and their travels contributed substantially to ecological processes. As humans have increasingly altered natural habitats, natural animal movements have been restricted. Tucker et al. examined GPS locations for more than 50 species. In general, animal movements were shorter in areas with high human impact, likely owing to changed behaviors and physical limitations. Besides affecting the species themselves, such changes could have wider effects by limiting the movement of nutrients and altering ecological interactions. Science, this issue p. 466 Human alterations of the landscape shorten the distances traveled by individual animals. Animal movement is fundamental for ecosystem functioning and species survival, yet the effects of the anthropogenic footprint on animal movements have not been estimated across species. Using a unique GPS-tracking database of 803 individuals across 57 species, we found that movements of mammals in areas with a comparatively high human footprint were on average one-half to one-third the extent of their movements in areas with a low human footprint. We attribute this reduction to behavioral changes of individual animals and to the exclusion of species with long-range movements from areas with higher human impact. Global loss of vagility alters a key ecological trait of animals that affects not only population persistence but also ecosystem processes such as predator-prey interactions, nutrient cycling, and disease transmission.",2018-01-26,https://www.semanticscholar.org/paper/b61897cfc5e3ae33ed81b3f961417347626125dc,Science
694,"Extremal combinatorics, iterated pigeonhole arguments, and generalizations of PPP","We study the complexity of computational problems arising from existence theorems in extremal combinatorics. For some of these problems, a solution is guaranteed to exist based on an iterated application of the Pigeonhole Principle. This results in the definition of a new complexity class within TFNP, which we call PLC (for""polynomial long choice""). PLC includes all of PPP, as well as numerous previously unclassified total problems, including search problems related to Ramsey's theorem, the Sunflower theorem, the Erd\H{o}s-Ko-Rado lemma, and K\""onig's lemma. Whether the first two of these four problems are PLC-complete is an important open question which we pursue; in contrast, we show that the latter two are PPP-complete. Finally, we reframe PPP as an optimization problem, and define a hierarchy of such problems related to Tur\'an's theorem.",2022-09-15,https://www.semanticscholar.org/paper/2fa43041e7d7c8badb18902ae06ccd8bd2cd8058,Information Technology Convergence and Services
3392,Scheduling When You Do Not Know the Number of Machines,"Often in a scheduling problem, there is uncertainty about the jobs to be processed. The issue of uncertainty regarding the machines has been much less studied. In this article, we study a scheduling environment in which jobs first need to be grouped into some sets before the number of machines is known, and then the sets need to be scheduled on machines without being separated. To evaluate algorithms in such an environment, we introduce the idea of an α-robust algorithm, one that is guaranteed to return a schedule on any number m of machines that is within an α factor of the optimal schedule on m machine, where the optimum is not subject to the restriction that the sets cannot be separated. Under such environment, we give a (5\3+ε)-robust algorithm for scheduling on parallel machines to minimize makespan and show a lower bound 4\3. For the special case when the jobs are infinitesimal, we give a 1.233-robust algorithm with an asymptotic lower bound of 1.207. We also study a case of fair allocation, where the objective is to minimize the difference between the maximum and minimum machine load.",2019-11-15,https://www.semanticscholar.org/paper/dba7ac9f2c38409f746e80c791a192911b60d737,ACM Trans. Algorithms
353,Geographic routing without location information,"For many years, scalable routing for wireless communication systems was a compelling but elusive goal. Recently, several routing algorithms that exploit geographic information (e.g. GPSR) have been proposed to achieve this goal. These algorithms refer to nodes by their location, not address, and use those coordinates to route greedily, when possible, towards the destination. However, there are many situations where location information is not available at the nodes, and so geographic methods cannot be used. In this paper we define a scalable coordinate-based routing algorithm that does not rely on location information, and thus can be used in a wide variety of ad hoc and sensornet environments.",2003-09-14,https://www.semanticscholar.org/paper/2209d6da6d473a7720509b8500781a85e8151670,ACM/IEEE International Conference on Mobile Computing and Networking
1142,Search for axions with the CDMS experiment.,We report on the first axion search results from the Cryogenic Dark Matter Search (CDMS) experiment at the Soudan Underground Laboratory. An energy threshold of 2 keV for electron-recoil events allows a search for possible solar axion conversion into photons or local galactic axion conversion into electrons in the germanium crystal detectors. The solar axion search sets an upper limit on the Primakov coupling g(agammagamma) of 2.4x10(-9) GeV-1 at the 95% confidence level for an axion mass less than 0.1 keV/c2. This limit benefits from the first precise measurement of the absolute crystal plane orientations in this type of experiment. The galactic axion search analysis sets a world-leading experimental upper limit on the axioelectric coupling g(aee) of 1.4x10(-12) at the 90% confidence level for an axion mass of 2.5 keV/c2.,2009-10-02,https://www.semanticscholar.org/paper/082a681571a4e2da71b8b9ba008c8fd880a32ab2,Physical Review Letters
1648,Deep Survival Analysis,"The electronic health record (EHR) provides an unprecedented opportunity to build actionable tools to support physicians at the point of care. In this paper, we investigate survival analysis in the context of EHR data. We introduce deep survival analysis, a hierarchical generative approach to survival analysis. It departs from previous approaches in two primary ways: (1) all observations, including covariates, are modeled jointly conditioned on a rich latent structure; and (2) the observations are aligned by their failure time, rather than by an arbitrary time zero as in traditional survival analysis. Further, it (3) scalably handles heterogeneous (continuous and discrete) data types that occur in the EHR. We validate deep survival analysis model by stratifying patients according to risk of developing coronary heart disease (CHD). Specifically, we study a dataset of 313,000 patients corresponding to 5.5 million months of observations. When compared to the clinically validated Framingham CHD risk score, deep survival analysis is significantly superior in stratifying patients according to their risk.",2016-08-06,https://www.semanticscholar.org/paper/3b41bb3a470fbfba54283331f31256cc09f0f37e,Machine Learning in Health Care
8,Discovering foodborne illness in online restaurant reviews,"Objective
We developed a system for the discovery of foodborne illness mentioned in online Yelp restaurant reviews using text classification. The system is used by the New York City Department of Health and Mental Hygiene (DOHMH) to monitor Yelp for foodborne illness complaints.


Materials and Methods
We built classifiers for 2 tasks: (1) determining if a review indicated a person experiencing foodborne illness and (2) determining if a review indicated multiple people experiencing foodborne illness. We first developed a prototype classifier in 2012 for both tasks using a small labeled dataset. Over years of system deployment, DOHMH epidemiologists labeled 13 526 reviews selected by this classifier. We used these biased data and a sample of complementary reviews in a principled bias-adjusted training scheme to develop significantly improved classifiers. Finally, we performed an error analysis of the best resulting classifiers.


Results
We found that logistic regression trained with bias-adjusted augmented data performed best for both classification tasks, with F1-scores of 87% and 66% for tasks 1 and 2, respectively.


Discussion
Our error analysis revealed that the inability of our models to account for long phrases caused the most errors. Our bias-adjusted training scheme illustrates how to improve a classification system iteratively by exploiting available biased labeled data.


Conclusions
Our system has been instrumental in the identification of 10 outbreaks and 8523 complaints of foodborne illness associated with New York City restaurants since July 2012. Our evaluation has identified strong classifiers for both tasks, whose deployment will allow DOHMH epidemiologists to more effectively monitor Yelp for foodborne illness investigations.",2018-12-01,https://www.semanticscholar.org/paper/e5f24e69d39cb25b2c75a58213abdd427435efe3,J. Am. Medical Informatics Assoc.
2338,Activation of neutrophils by soluble and insoluble immunoglobulin aggregates from synovial fluid of patients with rheumatoid arthritis.,"OBJECTIVES--Previous work has shown that synovial fluid isolated from patients with active rheumatoid arthritis contains soluble (not sedimented by centrifugation at 11,600 g for two minutes) and insoluble (sedimented by centrifugation at 11,600 g for two minutes) immunoglobulin aggregates that are capable of activating reactive oxidant production by bloodstream neutrophils. The purpose of this study was to determine which of these types of immunoglobulin aggregates activated the secretion of reactive oxygen metabolites and granule enzymes from neutrophils. METHODS--Cell free synovial fluid (from patients with rheumatoid arthritis) was added to neutrophils isolated from blood of healthy controls that had been incubated in the presence and absence of granulocyte-macrophage colony stimulating factor (GM-CSF). Reactive oxidant production was measured by luminol chemiluminescence (which detects both intracellular and extracellular oxidant production) and by cytochrome c reduction (which measures superoxide secretion). RESULTS--The soluble aggregates only activated neutrophils that were previously primed, and activated a rapid and transient burst of reactive oxidant secretion. On the other hand, the insoluble aggregates activated primed and unprimed neutrophils with similar efficacy and most of the oxidants generated (especially in unprimed cells) were intracellular. The soluble aggregates, but not the insoluble aggregates, also activated the secretion of myeloperoxidase from neutrophils that had either been pretreated with cytochalasin B or primed with GM-CSF. CONCLUSION--It is thus proposed that these soluble immunoglobulin aggregates are responsible for activation of the release of tissue damaging granule enzymes and reactive oxidants from primed neutrophils within the rheumatoid joint.",1993-05-01,https://www.semanticscholar.org/paper/c2f927bc9b6699b6e51e7827ac55216d6f34459d,Annals of the Rheumatic Diseases
3073,GamePod: Persistent Gaming Sessions on Pocketable Storage Devices,"We present GamePod, a portable system that enables mobile users to use the same persistent, gaming environment on any available computer. No matter what computer is being used, GamePod provides a consistent gaming environment, maintaining all of a user's games, including active game state. This is achieved by leveraging rapid improvements in capacity, cost, and size of portable storage devices. GamePod provides a middleware layer that enables virtualization and checkpoint/restart functionality that decouples the gaming environment from a host machine. This enables gaming sessions to be suspended to portable storage, carried around, and resumed from the storage device on another computer. GamePod's middleware layer also isolates gaming sessions from the host, protecting the host by preventing malicious executable content from damaging the host. We have implemented a Linux GamePod prototype and demonstrate its ability to quickly suspend and resume gaming sessions, enabling a seamless gaming experience for mobile users as they move among computers.",2009-10-11,https://www.semanticscholar.org/paper/69e8214ef4a374299f3ce1abddb7aeddc79514fb,"2009 Third International Conference on Mobile Ubiquitous Computing, Systems, Services and Technologies"
3198,"Characterization of intestinal microbiota and fecal cortisol, T3, and IgA in forest musk deer (Moschus berezovskii) from birth to weaning","Abstract Analysis of the intestinal microbiota and physiological parameters in mammalian infancy can reveal health status. In this study, we used a combination of molecular and immunochemical approaches to assess fecal microbiota as well as Cortisol (Cor), Triiodothyronine (T3), and immunoglobulin A (IgA) levels of young forest musk deer (FMD), from birth to one month after weaning (7 days of age–110 days of age). During development as the diet of FMD changes from consuming milk to eating plants, the richness and diversity of intestinal microbiota of young FMD increased significantly. Cor levels remained unchanged throughout early development while significantly increased after weaning, T3 and IgA initially were derived from milk during lactation, significantly decreased after weaning. Correlation network analysis showed that the community of food‐oriented microbes were highly structured and that many genera were correlated. Overall, this study provides scientific insights into effective management strategies for the protection of FMD population.",2021-01-16,https://www.semanticscholar.org/paper/cfae7858676245aba8003626c6872a5ce0e55ed3,Integrative Zoology
2588,Session details: Smart interaction techniques 1,,2005-04-02,https://www.semanticscholar.org/paper/b77a4ce3cbe3dcd9c73596729259e32a036ca98c,International Conference on Human Factors in Computing Systems
3757,Learning Aligned Cross-Modal Representations from Weakly Aligned Data,"People can recognize scenes across many different modalities beyond natural images. In this paper, we investigate how to learn cross-modal scene representations that transfer across modalities. To study this problem, we introduce a new cross-modal scene dataset. While convolutional neural networks can categorize cross-modal scenes well, they also learn an intermediate representation not aligned across modalities, which is undesirable for crossmodal transfer applications. We present methods to regularize cross-modal convolutional neural networks so that they have a shared representation that is agnostic of the modality. Our experiments suggest that our scene representation can help transfer representations across modalities for retrieval. Moreover, our visualizations suggest that units emerge in the shared representation that tend to activate on consistent concepts independently of the modality.",2016-06-27,https://www.semanticscholar.org/paper/7e64992091458256f438fbe1bd44fffcc197b76c,Computer Vision and Pattern Recognition
1750,Nonparametric variational inference,"Variational methods are widely used for approximate posterior inference. However, their use is typically limited to families of distributions that enjoy particular conjugacy properties. To circumvent this limitation, we propose a family of variational approximations inspired by nonparametric kernel density estimation. The locations of these kernels and their bandwidth are treated as variational parameters and optimized to improve an approximate lower bound on the marginal likelihood of the data. Unlike most other variational approximations, using multiple kernels allows the approximation to capture multiple modes of the posterior. We demonstrate the efficacy of the nonparametric approximation with a hierarchical logistic regression model and a nonlinear matrix factorization model. We obtain predictive performance as good as or better than more specialized variational methods and MCMC approximations. The method is easy to apply to graphical models for which standard variational methods are difficult to derive.",2012-06-18,https://www.semanticscholar.org/paper/6ba0491f9dde8ea042ea4a49df34838b345f23c2,International Conference on Machine Learning
944,Node-and edge-deletion NP-complete problems,"If &pgr; is a graph property, the general node(edge) deletion problem can be stated as follows: Find the minimum number of nodes(edges), whose deletion results in a subgraph satisfying property &pgr;. In this paper we show that if &pgr; belongs to a rather broad class of properties (the class of properties that are hereditary on induced subgraphs) then the node-deletion problem is NP-complete, and the same is true for several restrictions of it. For the same class of properties, requiring the remaining graph to be connected does not change the NP-complete status of the problem; moreover for a certain subclass, finding any ""reasonable"" approximation is also NP-complete. Edge-deletion problems seem to be less amenable to such generalizations. We show however that for several common properties (e.g. planar, outer-planar, line-graph, transitive digraph) the edge-deletion problem is NP-complete.",1978-05-01,https://www.semanticscholar.org/paper/1855866d7433853989ed01cf3851a5d7d3c2b286,Symposium on the Theory of Computing
150,AVENUE: Automated site modeling in urban environments,This paper is an overview of the AVENUE project at Columbia University. AVENUE's main goal is to automate the site modeling process in urban environments. The first component of AVENUE is a 3-D modeling system which constructs complete 3-D geometric models with photometric texture mapping acquired from different viewpoints. The second component is a planning system that plans the Next-Best-View for acquiring a model of the site. The third component is a mobile robot we have built that contains an integrated sensor suite for automatically performing the site modeling task. We present results for modeling buildings in New York City.,2001-05-28,https://www.semanticscholar.org/paper/862967da1feb15fc9dcfd9a6a1e61f9821fa0ece,Proceedings Third International Conference on 3-D Digital Imaging and Modeling
3356,"Role assessment, reserve strategy, and acquisition of information in asymmetric animal conflicts",,1981-02-01,https://www.semanticscholar.org/paper/992341bdf241ddd725ec2b75b5b3bae1c87e733d,Animal Behaviour
819,Optimization problems from feature testing of communication protocols,"In feature testing of communication protocols, we want to construct a minimal number of tests with a desirable fault coverage. We model the protocols by extended finite state machines and reduce the test generation process to optimization problems on graphs. We study efficient algorithms and their complexity. We report experimental results on real systems, including Personal HandyPhone System, a 5ESS based ISDN wireless system, and 5ESS Intelligent Network Application Protocol.",1996-10-29,https://www.semanticscholar.org/paper/a648028dd16367cd196e5863c44e50880c841ab1,Proceedings of 1996 International Conference on Network Protocols (ICNP-96)
2523,SnapAR: Storing snapshots for quick viewpoint switching in hand-held augmented reality,"Many tasks require a user to move between various locations within an environment to get different perspectives. This can take significant time and effort, especially when the user must switch among those viewpoints repeatedly. We explore augmented reality interaction techniques that involve taking still pictures of a physical scene using a tracked hand-held magic lens and seamlessly switching between augmenting either the live view or one of the still views, without needing to physically revisit the snapshot locations. We describe our optical-marker-tracking-based implementation and how we represent and switch among snapshots. To determine the effectiveness of our techniques, we developed a test application that lets its user view physical and virtual objects from different viewpoints.",2010-11-22,https://www.semanticscholar.org/paper/bb211aa7d23581d001ef9941eee8fe0433871181,2010 IEEE International Symposium on Mixed and Augmented Reality
450,Computational Aspacts of Organization Theory (Extended Abstract),,1996-09-25,https://www.semanticscholar.org/paper/e47a8a19d30c3476887a234b085e6a12568011a3,Embedded Systems and Applications
3650,Multiple Inheritance for C++,"Multiple Inheritance is the ability of a class to have more than one base class (super class). In a language where multiple inheritance is supported a program can be structured as a set of inheritance lattices instead of (just) as a set of inheritance trees. This is widely believed to be an important structuring tool. It is also widely believed that multiple inheritance complicates a programming language significantly, is hard to implement, and is expensive to run. I will demonstrate that none of these last three conjectures are true.",,https://www.semanticscholar.org/paper/35a501c412162dba797ce75f857ce58bc55d211c,Computing Systems
2901,2: Comparison of Factor Products for Treatment of Bleeding Related to Cardiac Surgery,,2022-12-15,https://www.semanticscholar.org/paper/0b2993bc879c74afdb866f6632e4b646b34ff2d8,Critical Care Medicine
1357,Study of Zgamma events and limits on anomalous ZZgamma and Zgammagamma couplings in pp collisions at square root s = 1.96 TeV.,"We present a measurement of the Zgamma production cross section and limits on anomalous ZZgamma and Zgammagamma couplings for form-factor scales of lambda = 750 and 1000 GeV. The measurement is based on 138 (152) candidates in the eegamma (mumugamma) final state using 320(290) pb(-1) of pp(-1) collisions at square root of s = 1.96 TeV. The 95% C.L. limits on real and imaginary parts of individual anomalous couplings are /h(10,30)Z/ < 0.23, /h(20,40)Z/ < 0.020, /h(10,30)gamma/ < 0.23, and /h(20,40)gamma/ < 0.019 for lambda = 1000 GeV.",,https://www.semanticscholar.org/paper/c4f1b25de53082c78062c51a523a191f6b216a5c,Physical Review Letters
452,Multimedia Information Caching for Personalized Video-on-Demand,,1995-03-01,https://www.semanticscholar.org/paper/2d7165b2b27f4d58b6a892f5eeccdf252af104b0,Computer Communications
3027,Hardware and Software Support for Virtualization,"This book focuses on the core question of the necessary architectural support provided by hardware to efficiently run virtual machines, and of the corresponding design of the hypervisors that run them. Virtualization is still possible when the instruction set architecture lacks such support, but the hypervisor remains more complex and must rely on additional techniques. Despite the focus on architectural support in current architectures, some historical perspective is necessary to appropriately frame the problem. The first half of the book provides the historical perspective of the theoretical framework developed four decades ago by Popek and Goldberg. It also describes earlier systems that enabled virtualization despite the lack of architectural support in hardware. As is often the case, theory defines a necessary-but not sufficient-set of features, and modern architectures are the result of the combination of the theoretical framework with insights derived from practical systems. The second half of the book describes state-of-the-art support for virtualization in both x86-64 and ARM processors. This book includes an in-depth description of the CPU, memory, and I/O virtualization of these two processor architectures, as well as case studies on the Linux/KVM, VMware, and Xen hypervisors. It concludes with a performance comparison of virtualization on current-generation x86- and ARM-based systems across multiple hypervisors.",2017-02-21,https://www.semanticscholar.org/paper/79d0d56f06beb438366a503078cf0aa4b8c94396,Synthesis Lectures on Computer Architecture
915,The complexity of restricted spanning tree problems,"The complexity of the foUowmg class of problems Is investigated: Given a distance matrix, fred the shortest spanning tree that is isomorphic to a given prototype. Several classical combinatorial problems, both easy and hard, fall into this category for an appropriate choice of the family of prototypes, for example, taking the family to be the set of all paths gives the traveling salesman problem or taking the family to be the set of all 2-stars gives the weighted matching problem It is shown that the complexity of these problems depends explicitly on the rate of growth of a sLmple parameter of the family of prototypes.",1982-04-01,https://www.semanticscholar.org/paper/877e282d16d2f1aa657c8840514df781ee46abc2,JACM
2681,The Representation and Use of a Visual Lexicon for Automated Graphics Generation,"Most automated graphics generation systems employ either a constructive or a parametric graphics synthesis approach. Constructive graphics synthesis is a deductive approach that builds visual presentations from scratch by gluing together the most basic visual variables. Conversely, parametric graphics synthesis defines a set of parametrized visual models and interprets the information to be presented through instantiation of the selected model. To increase efficiency, we have combined parametric and constructive approaches in a system called IMPROVISE. In this paper, we focus on the parametric aspect of our approach. We present a comprehensive, general, and extensible formalism to represent a visual lexicon for use in automated graphics generation. A visual lexicon is a collection of parametrized primitive visual objects that serve as building blocks for constructing more complex visual presentations. We also illustrate how this representation can be effectively employed to aid the selection and instantiation of a visual lexical item in the graphics generation process. Examples are given from IMPROVISE to demonstrate the representation and use of this visual lexicon.",1997-08-23,https://www.semanticscholar.org/paper/01ba58fe5be3af5c1e8704556f2d1ab25d4d48e8,International Joint Conference on Artificial Intelligence
2234,Active Replication Of RSV In Neutrophils From Airways And Blood Of Infants With Severe Bronchiolitis,,2010-05-01,https://www.semanticscholar.org/paper/cfb7843ab401caa17048b854115d8ce74a3c51cb,Asian Test Symposium
1753,Truncation-free stochastic variational inference for Bayesian nonparametric models,"We present a truncation-free stochastic variational inference algorithm for Bayesian nonparametric models. While traditional variational inference algorithms require truncations for the model or the variational distribution, our method adapts model complexity on the fly. We studied our method with Dirichlet process mixture models and hierarchical Dirichlet process topic models on two large data sets. Our method performs better than previous stochastic variational inference algorithms.",2012-12-03,https://www.semanticscholar.org/paper/8f302be0d4f9914dc21769c8b1f1aa36bb4cb7f3,Neural Information Processing Systems
2508,Session details: 3D,,2011-10-16,https://www.semanticscholar.org/paper/7884a516ac023687fac021242c51129cf9a20da6,Proceedings of the 24th annual ACM symposium on User interface software and technology
3382,Incremental Edge Orientation in Forests,"For any forest $G = (V, E)$ it is possible to orient the edges $E$ so that no vertex in $V$ has out-degree greater than $1$. This paper considers the incremental edge-orientation problem, in which the edges $E$ arrive over time and the algorithm must maintain a low-out-degree edge orientation at all times. We give an algorithm that maintains a maximum out-degree of $3$ while flipping at most $O(\log \log n)$ edge orientations per edge insertion, with high probability in $n$. The algorithm requires worst-case time $O(\log n \log \log n)$ per insertion, and takes amortized time $O(1)$. The previous state of the art required up to $O(\log n / \log \log n)$ edge flips per insertion. We then apply our edge-orientation results to the problem of dynamic Cuckoo hashing. The problem of designing simple families $\mathcal{H}$ of hash functions that are compatible with Cuckoo hashing has received extensive attention. These families $\mathcal{H}$ are known to satisfy \emph{static guarantees}, but do not come typically with \emph{dynamic guarantees} for the running time of inserts and deletes. We show how to transform static guarantees (for $1$-associativity) into near-state-of-the-art dynamic guarantees (for $O(1)$-associativity) in a black-box fashion. Rather than relying on the family $\mathcal{H}$ to supply randomness, as in past work, we instead rely on randomness within our table-maintenance algorithm.",2021-07-05,https://www.semanticscholar.org/paper/df478052482b7248606768347c76c50326b6629f,Embedded Systems and Applications
1052,Design and characterization of a phonon-mediated cryogenic particle detector with an eV-scale threshold and 100 keV-scale dynamic range,"We present the design and characterization of a cryogenic phonon-sensitive 1-gram Si detector exploiting the Neganov-Trofimov-Luke effect to detect single-charge excitations. This device achieved 2.65(2)~eV phonon energy resolution when operated without a voltage bias across the crystal and a corresponding charge resolution of 0.03 electron-hole pairs at 100~V bias. With a continuous-readout data acquisition system and an offline optimum-filter trigger, we obtain a 9.2~eV threshold with a trigger rate of the order of 20~Hz. The detector's energy scale is calibrated up to 120~keV using an energy estimator based on the pulse area. The high performance of this device allows its application to different fields where excellent energy resolution, low threshold, and large dynamic range are required, including dark matter searches, precision measurements of coherent neutrino-nucleus scattering, and ionization yield measurements.",2020-12-23,https://www.semanticscholar.org/paper/728f3be5889c6c038095f7abeee8a71ef729bb4d,Physical Review D
2524,Rolling and shooting: two augmented reality games,"We present two fast-paced augmented reality games. One is a single-player game experienced through a head-worn display. The player manipulates a tracked board to guide a virtual ball through a dynamic maze of obstacles. Combining the 3DOF absolute orientation tracker on the head-worn display with 6DOF optical marker tracking allows the system to always account for the correct direction of gravity. The second game is a networked, two-player, first-person-shooter, in which tracked hand-held UMPCs are used to blast virtual dominoes off a table. Players' virtual locations are warped to keep them from physically interfering with each other.",2010-04-09,https://www.semanticscholar.org/paper/bb97200290ef21291dc84975fe7c0256467ea5a1,CHI Extended Abstracts
463,The complexity of optimal queueing network control,"We consider the classical problem of optimal control (routing and sequencing) of a network of queues. We prove that this problem is EXP-complete and, therefore, provably intractable. Similar results are established for restricted versions of the problem. A weaker result is also established for the restless bandit problem.<<ETX>>",1994-06-28,https://www.semanticscholar.org/paper/101418b1a67f3fe27c842c70482b6d5f1b899a7c,Proceedings of IEEE 9th Annual Conference on Structure in Complexity Theory
140,View planning and automated data acquisition for three‐dimensional modeling of complex sites,"Constructing highly detailed three‐dimensional (3‐D) models of large complex sites using range scanners can be a time‐consuming manual process. One of the main drawbacks is determining where to place the scanner to obtain complete coverage of a site. We have developed a system for automatic view planning called VuePlan. When combined with our mobile robot, AVENUE, we have a system that is capable of modeling large‐scale environments with minimal human intervention throughout both the planning and acquisition phases. The system proceeds in two distinct stages. In the initial phase, the system is given a two‐dimensional site footprint with which it plans a minimal set of sufficient and properly constrained covering views. We then use a 3‐D laser scanner to take scans at each of these views. When this planning system is combined with our mobile robot it automatically computes and executes a tour of these viewing locations and acquires them with the robot's onboard laser scanner. These initial scans serve as an approximate 3‐D model of the site. The planning software then enters a second phase in which it updates this model by using a voxel‐based occupancy procedure to plan the next best view (NBV). This NBV is acquired, and further NBVs are sequentially computed and acquired until an accurate and complete 3‐D model is obtained. A simulator tool that we developed has allowed us to test our entire view planning algorithm on simulated sites. We have also successfully used our two‐phase system to construct precise 3‐D models of real‐world sites located in New York City: Uris Hall on the campus of Columbia University and Fort Jay on Governors Island. © 2009 Wiley Periodicals, Inc.",,https://www.semanticscholar.org/paper/983d19a3b6559e487b6ed831a4631ca1a1bc1a37,J. Field Robotics
2231,Mcl‐1; the molecular regulation of protein function,,2010-07-16,https://www.semanticscholar.org/paper/8a94028104b1deb532ba9c56022072a0a672cfab,FEBS Letters
1902,Industry Applications of Computational Intelligence: Preface,,,https://www.semanticscholar.org/paper/53844058751531751a9442d670e8cd5c377781ca,International Journal of Computational Intelligence Systems
2961,An Empirical Study of Stochastic Variational Inference Algorithms for the Beta Bernoulli Process,,2015-06-01,https://www.semanticscholar.org/paper/fa2883d2074b02151df459eddd10992e35ad8c98,International Conference on Machine Learning
1746,Nested Hierarchical Dirichlet Processes,"We develop a nested hierarchical Dirichlet process (nHDP) for hierarchical topic modeling. The nHDP generalizes the nested Chinese restaurant process (nCRP) to allow each word to follow its own path to a topic node according to a per-document distribution over the paths on a shared tree. This alleviates the rigid, single-path formulation assumed by the nCRP, allowing documents to easily express complex thematic borrowings. We derive a stochastic variational inference algorithm for the model, which enables efficient inference for massive collections of text documents. We demonstrate our algorithm on 1.8 million documents from The New York Times and 2.7 million documents from Wikipedia.",2012-10-25,https://www.semanticscholar.org/paper/2ecf9a574ef868a48fadd94bd5ca45f6a051b49a,IEEE Transactions on Pattern Analysis and Machine Intelligence
3082,Proceedings of the 1st ACM workshop on Virtual machine security,"Welcome to the 1st Workshop on Virtual Machine Security (VMSec)! 
 
In these proceedings, you will find the 7 papers presented at VMSec Workshop, held on October 31st at George Mason University in conjunction with the ACM Conference on Computer & Communications Security. This workshop aims to bring together leading researchers and practitioners in the fields of virtualization and security to present the latest work on these topics. This year we had 20 submissions and we decided to accept six (6) full papers and one (1) short paper. A program committee of 11 experts reviewed and discussed the papers. Each paper received at least 3 reviews from the PC members with most of the papers having 4 reviews in total. Some of the papers were very controversial and were discussed in detail by the PC members after the end of the review process. The final decisions were made based on those discussions. 
 
VMSec is the first workshop to deal exclusively with virtual machine security. Virtualization has seen an explosion in growth in deployment, implementations, and applications. In addition, it seems that virtualization holds unique properties that make it attractive for security including isolation, compartmentalization, live state capture, and replay. In the past, virtualization has been used to study malicious software as well as to prevent malicious software infection. We believe that the large-scale use of virtualization offers new opportunities and challenges regarding security. We hope that this workshop will provoke fruitful discussions for researchers and practitioners, from both industry and academia.",2008-10-27,https://www.semanticscholar.org/paper/b533831773ccadaaf4d92fdc2328064891ef7b4a,Conference on Computer and Communications Security
2894,The Inclusion and Role of Micro Mechanical Residual Stress on Deformation of Stainless Steel Type 316l at Grain Level,,2023-05-01,https://www.semanticscholar.org/paper/cb005c5b65b1f1cb6d7bde1688673e497b6acfe9,Social Science Research Network
931,The Node-Deletion Problem for Hereditary Properties is NP-Complete,,1980-04-01,https://www.semanticscholar.org/paper/0df73f3939d60578b09d59ac0d1d7c0dbdb53c6e,Journal of computer and system sciences (Print)
164,Brain computation by assemblies of neurons,"Significance Our expanding understanding of the brain at the level of neurons and synapses, and the level of cognitive phenomena such as language, leaves a formidable gap between these two scales. Here we introduce a computational system which promises to bridge this gap: the Assembly Calculus. It encompasses operations on assemblies of neurons, such as project, associate, and merge, which appear to be implicated in cognitive phenomena, and can be shown, analytically as well as through simulations, to be plausibly realizable at the level of neurons and synapses. We demonstrate the reach of this system by proposing a brain architecture for syntactic processing in the production of language, compatible with recent experimental results. Assemblies are large populations of neurons believed to imprint memories, concepts, words, and other cognitive information. We identify a repertoire of operations on assemblies. These operations correspond to properties of assemblies observed in experiments, and can be shown, analytically and through simulations, to be realizable by generic, randomly connected populations of neurons with Hebbian plasticity and inhibition. Assemblies and their operations constitute a computational model of the brain which we call the Assembly Calculus, occupying a level of detail intermediate between the level of spiking neurons and synapses and that of the whole brain. The resulting computational system can be shown, under assumptions, to be, in principle, capable of carrying out arbitrary computations. We hypothesize that something like it may underlie higher human cognitive functions such as reasoning, planning, and language. In particular, we propose a plausible brain architecture based on assemblies for implementing the syntactic processing of language in cortex, which is consistent with recent experimental results.",2019-12-08,https://www.semanticscholar.org/paper/022210ae703b93ed29ce6aec8ffcfad6cbc084f8,Proceedings of the National Academy of Sciences of the United States of America
1270,Search for stopped gluinos from pp collisions at square root s = 1.96 TeV.,"Long-lived, heavy particles are predicted in a number of models beyond the standard model of particle physics. We present the first direct search for such particles' decays, occurring up to 100 h after their production and not synchronized with an accelerator bunch crossing. We apply the analysis to the gluino (g), predicted in split supersymmetry, which after hadronization can become charged and lose enough momentum through ionization to come to rest in dense particle detectors. Approximately 410 pb(-1) of pp collisions at square root(s) = 1.96 TeV collected with the D0 detector during Run II of the Fermilab Tevatron collider are analyzed in search of such ""stopped gluinos"" decaying into a gluon and a neutralino (chi(1)(0)). Limits are placed on the (gluino cross section) x (probability to stop) x [BR(g --> g chi(1)(0))] as a function of the gluino and chi(1)(0) masses, for gluino lifetimes from 30 micros-100 h.",2007-05-01,https://www.semanticscholar.org/paper/041312bfe9475f4539f4b49666db6013aec15ffa,Physical Review Letters
2482,Gaze locking: passive eye contact detection for human-object interaction,"Eye contact plays a crucial role in our everyday social interactions. The ability of a device to reliably detect when a person is looking at it can lead to powerful human-object interfaces. Today, most gaze-based interactive systems rely on gaze tracking technology. Unfortunately, current gaze tracking techniques require active infrared illumination, calibration, or are sensitive to distance and pose. In this work, we propose a different solution-a passive, appearance-based approach for sensing eye contact in an image. By focusing on gaze *locking* rather than gaze tracking, we exploit the special appearance of direct eye gaze, achieving a Matthews correlation coefficient (MCC) of over 0.83 at long distances (up to 18 m) and large pose variations (up to ±30° of head yaw rotation) using a very basic classifier and without calibration. To train our detector, we also created a large publicly available gaze data set: 5,880 images of 56 people over varying gaze directions and head poses. We demonstrate how our method facilitates human-object interaction, user analytics, image filtering, and gaze-triggered photography.",2013-10-08,https://www.semanticscholar.org/paper/06f02199690961ba52997cde1527e714d2b3bf8f,ACM Symposium on User Interface Software and Technology
81,Evaluating top-k queries over Web-accessible databases,"A query to a Web search engine usually consists of a list of keywords, to which the search engine responds with the best or ""top"" k pages for the query. This top-k query model is prevalent over multimedia collections in general, but also over plain relational data for certain applications. For example, consider a relation with information on available restaurants, including their location, price range for one diner, and overall food rating. A user who queries such a relation might simply specify the user's location and target price range, and expect in return the best 10 restaurants in terms of some combination-of proximity to the user, closeness of match to the target price range, and overall food rating. Processing such top-k queries efficiently is challenging for a number of reasons. One critical such reason is that, in many Web applications, the relation attributes might not be available other than through external Web-accessible form interfaces, which we will have to query repeatedly for a potentially large set of candidate objects. In this paper, we study how to process top-k queries efficiently in this setting, where the attributes for which users specify target values might be handled by external, autonomous sources with a variety of access interfaces. We present several algorithms for processing such queries, and evaluate them thoroughly using both synthetic and real Web-accessible data.",2002-02-26,https://www.semanticscholar.org/paper/c6e3d4a8fba3cd5d592d62f533ff95bde84a8f22,Proceedings / International Conference on Data Engineering
3022,AnDrone,,2019-03-25,https://www.semanticscholar.org/paper/d67144e90c7efa4a656cadd0e38cf3328e7afc2b,Proceedings of the Fourteenth EuroSys Conference 2019
2950,An Efficient Multiple-Testing Adjustment for eQTL Studies that Accounts for Linkage Disequilibrium between Variants.,,2016-01-07,https://www.semanticscholar.org/paper/72ebcb025662d71d3d67f1d3776aea2d85af1dd5,American Journal of Human Genetics
2942,Determining the genetic basis of anthracycline-cardiotoxicity by molecular response QTL mapping in induced cardiomyocytes,"Anthracycline-induced cardiotoxicity (ACT) is a key limiting factor in setting optimal chemotherapy regimes for cancer patients, with almost half of patients expected to ultimately develop congestive heart failure given high drug doses. However, the genetic basis of sensitivity to anthracyclines such as doxorubicin remains unclear. To begin addressing this, we created a panel of iPSC-derived cardiomyocytes from 45 individuals and performed RNA-seq after 24h exposure to varying levels of doxorubicin. The transcriptomic response to doxorubicin is substantial, with the majority of genes being differentially expressed across treatments of different concentrations and over 6000 genes showing evidence of differential splicing. Overall, our observations indicate that splicing fidelity decreases in the presence of doxorubicin. We detect 376 response-expression QTLs and 42 response-splicing QTLs, i.e. genetic variants that modulate the individual transcriptomic response to doxorubicin in terms of expression and splicing changes respectively. We show that inter-individual variation in transcriptional response is predictive of cell damage measured in vitro using a cardiac troponin assay, which in turn is shown to be associated with in vivo ACT risk. Finally, the molecular QTLs we detected are enriched in lower ACT GWAS p-values, further supporting the in vivo relevance of our map of genetic regulation of cellular response to anthracyclines.",2017-11-02,https://www.semanticscholar.org/paper/782ad77f88c99deb082727e7b8e6e6c10bf2ba65,bioRxiv
1941,Overall Space Effectiveness (OSE) for Enhancing Fab Space Productivity,"Due to increasing global competition, effective resource utilization is critical for maintaining competitive advantages. However, little research has been done to address space utilization of wafer fabrication facility. This paper aims to design a novel index, overall space effectiveness (OSE), to diagnose the fab layouts and identify directions for improving space usage effectiveness to enhance the overall space productivity as partial effort for total resources management. Furthermore, the proposed OSE can be extended to incorporate other critical performance indices such as cycle time, throughput, and revenue for effective management. Focusing on real settings of a leading semiconductor manufacturing company, a number of case studies were conducted and compared. The results have shown OSE gaps among various fabs and the proposed OSE can effectively derive improvement directions. This paper concludes with discussions on value propositions and future research directions.",2016-07-07,https://www.semanticscholar.org/paper/ff4867b1fe2f256e40e61e283b737aef2843b4f0,IEEE transactions on semiconductor manufacturing
2426,A Hybrid RTK GNSS and SLAM Outdoor Augmented Reality System,"In the real world, we are surrounded by potentially important data. For example, military personnel and first responders may need to understand the layout of an environment, including the locations of designated assets, specified in latitude and longitude. However, many augmented reality (AR) systems cannot associate absolute geographic coordinates with the coordinate system in which they track. We describe a simple approach for developing a wide-area outdoor wearable AR system that uses RTK GNSS position tracking to align together and georegister multiple smaller maps from an existing SLAM tracking system.",2019-03-23,https://www.semanticscholar.org/paper/220ecaea8bf2d6102530cb4acc4cfc0fb79b75b3,IEEE Conference on Virtual Reality and 3D User Interfaces
3143,The design and implementation of Zap,"We have created Zap, a novel system for transparent migration of legacy and networked applications. Zap provides a thin virtualization layer on top of the operating system that introduces pods, which are groups of processes that are provided a consistent, virtualized view of the system. This decouples processes in pods from dependencies to the host operating system and other processes on the system. By integrating Zap virtualization with a checkpoint-restart mechanism, Zap can migrate a pod of processes as a unit among machines running independent operating systems without leaving behind any residual state after migration. We have implemented a Zap prototype in Linux that supports transparent migration of unmodified applications without any kernel modifications. We demonstrate that our Linux Zap prototype can provide general-purpose process migration functionality with low overhead. Our experimental results for migrating pods used for running a standard user's X windows desktop computing environment and for running an Apache web server show that these kinds of pods can be migrated with subsecond checkpoint and restart latencies.",2002-12-09,https://www.semanticscholar.org/paper/a6b100291fe50fd8a563e0ade459081827792ef6,USENIX Symposium on Operating Systems Design and Implementation
759,Multi-Objective Model Checking of Markov Decision Processes,"We study and provide efficient algorithms for multi-objective model checking problems for Markov Decision Processes (MDPs). Given an MDP, M, and given multiple linear-time (ω-regular or LTL) properties ϕi, and probabilities ri ∈ [0, 1], i = 1, . . . , k, we ask whether there exists a strategy α for the controller such that, for all i, the probability that a trajectory of M controlled by α satisfies ϕi is at least ri. We provide an algorithm that decides whether there exists such a strategy and if so produces it, and which runs in time polynomial in the size of the MDP. Such a strategy may require the use of both randomization and memory. We also consider more general multi-objective ω-regular queries, which we motivate with an application to assume-guarantee compositional reasoning for probabilistic systems. 
 
Note that there can be trade-offs between different properties: satisfying property ϕ1 with high probability may necessitate satisfying ϕ2 with low probability. Viewing this as a multi-objective optimization problem, we want information about the ""trade-off curve"" or Pareto curve for maximizing the probabilities of different properties. We show that one can compute an approximate Pareto curve with respect to a set of ω-regular properties in time polynomial in the size of the MDP. 
 
Our quantitative upper bounds use LP methods. We also study qualitative multi-objective model checking problems, and we show that these can be analysed by purely graph-theoretic methods, even though the strategies may still require both randomization and memory.",2007-03-24,https://www.semanticscholar.org/paper/973a2cb018d44d37cbc47944fc7d4e1150b5c5d8,Log. Methods Comput. Sci.
2389,Cytochrome a620 in Tetrahymena pyriformis. Reactions with carbon monoxide and oxygen at subzero temperatures and photochemical action spectra.,"1. Mitochondria-enriched fractions of the ciliate protozoan Tetrahymena pyriformis ST contained CO-reacting cytochromes b560 and a620. 2. A non-photodissociable oxygen-containing compound of cytochrome a620 was formed in whole cell suspensions at -114 degrees C after photolysis of CO in the presence of 200 microM-O2. 3. Electron transport, indicated by the oxidation of cytochrome a620 and cytochrome c, occurred at temperatures higher than -72 degrees C. 4. Photochemical action spectra for the relief of respiratory inhibition of whole cells by CO obtained by using a liquid dye laser indicate that the only CO-reacting terminal oxidase detectable was cytochrome a620. 5. It is concluded that the alternative electron transport chains in this organism utilize non-cytochrome terminal oxidases.",1982-08-15,https://www.semanticscholar.org/paper/76c74961a7a9c0d93e7b151f4f880df0f6202c78,Biochemical Journal
3441,Better online buffer management,"As the Internet becomes more mature, there is a realization that improving the performance of routers has the potential to substantially improve Internet performance in general. Currently, most routers forward packets in a First-In-First-Out (FIFO) order. However, the diversity of applications supported by modern IP-based networks has resulted in unpredictable packet flows, and heterogeneous network traffic. Thus, it is becoming more reasonable to consider differentiating between different types of packets, and perhaps to consider allowing packets to specify a deadline by which it must be processed. These issues have made buffer management at routers a critical issue in providing effective quality of service to the various applications that use the network.
 In this paper, we study an online problem in which each packet is described by its discrete arrival time, non-negative weight and discrete deadline; arriving packets are buffered for delivery and all packets have the same processing time. The packets arrive online, and our objective is to maximize the sum of weights of those packets that are sent by their deadlines. We describe an online deterministic algorithm with a competitive ratio of 1.854, improving the best previous known competitive ratio of 1.939 (Bartal et al. STACS 2004).
 The algorithmic framework we use has several interesting features. First, we do not use a potential function. Instead, after each step we modify the adversary's buffer. Second, we introduce ""dummy packets"" to facilitate the decision making.",2007-01-07,https://www.semanticscholar.org/paper/7ad72f9647513a4693fb30c6aaf0528db999b5a7,ACM-SIAM Symposium on Discrete Algorithms
176,α-Rank: Multi-Agent Evaluation by Evolution,,2019-03-04,https://www.semanticscholar.org/paper/af49ccf5add1c4b6a9a4b2e403bbe0fb25f9925f,Scientific Reports
2297,Activation of human neutrophils by soluble immune complexes: role of Fc gamma RII and Fc gamma RIIIb in stimulation of the respiratory burst and elevation of intracellular Ca2+.,"Activation of control, unprimed neutrophils with soluble immune complexes fails to generate a respiratory burst. However, if the cells are primed with either tumor necrosis factor-alpha or granulocyte-macrophage colony-stimulating factor prior to addition of soluble immune complexes, then a rapid and transient burst of reactive oxidant secretion is observed. In unprimed neutrophils the soluble immune complexes stimulate an intracellular Ca2+ transient that arises from the mobilization of intracellular Ca2+. However, in primed cells, an ""extra"" intracellular Ca2+ signal is observed that arises from Ca2+ influx. After removal of Fc gamma RIIIb by treatment with pronase or PI-PLC, the soluble immune complexes fail to activate a respiratory burst in unprimed neutrophils and the ""extra"" Ca2+ signal is not observed. These results indicate that during priming Fc gamma RIIIb becomes functionally activated and thence its ligation leads to stimulated Ca2+ influx and the generation of intracellular signals that lead to NADPH oxidase activation. Experiments using Fab/F(ab')2 fragments to specifically crosslink either Fc gamma RII or Fc gamma RIIIb and experiments with neutrophils from an individual with Fc gamma RIIIb gene deficiency confirm this important function for Fc gamma RIIIb in neutrophil activation.",,https://www.semanticscholar.org/paper/6433344edd6fb7854933cc5270722945e1762394,Annals of the New York Academy of Sciences
135,"Fully-adaptive minimal deadlock-free packet routing in hypercubes, meshes, and other networks","This paper deals with the problem of packet-switched routing in parallel machines. Several new routing algorithms for different interconnection networks are presented. While the new techniques apply to a wide variety of networks, routing algorithms will be shown for the bypercube, the 2-dinleusional mesh, and the shuffleexchange. The techniques presented for hypercubes and meshes are fully-adaptive and minimal. A similar technique can be devised for tori. A fully-adaptive and millimal routing is one in which all possible minimal paths bet,ween a source and a destination are of potential use at the time a message is injected into the network. Minimal paths followed by messages ultimately depend on the local congestion encountered in each node of the network. In the shuffle-exchange network, the routing scheme also exhibits adaptivity but paths could be up to 3 log N long for an N node machine. The shuflleexchange algorithm is the first adaptive and deadlockfree method that requires a small (and independent of N) number of buffers and queues in the routing nodes for that network. * ESLAI, Escuela Superior Latino Americana de Informitica, CC 3193,(1000) Buenos Aires, Argentina. t Computer Research and Advanced Applications Group, IBM Argentina, Ing. E. Bntti 275, (1300) Buenos Aires, Argentina. + Computer Science Dept., IBh’1 Almadeu Research Center, San",1991-06-01,https://www.semanticscholar.org/paper/63c0e3b785d8d6c379eefa505525f8d354ceb831,ACM Symposium on Parallelism in Algorithms and Architectures
2958,Using contextual information to classify nuclei in histology images,"Nucleus classification is a central task in digital pathology. Given a tissue image, our goal is to classify detected nuclei into different types, for example nuclei of tumor cells, stroma cells, or immune cells. State-of-the-art methods achieve this by extracting different types of features such as morphology, image intensities, and texture features in the nucleus regions. Such features are input to training and classification, e.g. using a support vector machine. In this paper, we introduce additional contextual information obtained from neighboring nuclei or texture in the surrounding tissue regions to improve nucleus classification. Three different methods are presented. These methods use conditional random fields (CRF), texture features computed in image patches centered at each nucleus, and a novel method based on the bag-of-word (BoW) model. The methods are evaluated on images of tumor-burdened tissue from H&E-stained and Ki-67-stained breast samples. The experimental results show that contextual information systematically improves classification accuracy. The proposed BoW-based method performs better than the CRF-based method, and requires less computation than the texture-feature-based method.",2015-04-16,https://www.semanticscholar.org/paper/65b025e06f2fb416ff3d16122e48328c8369db86,IEEE International Symposium on Biomedical Imaging
2296,Seeing the wood for the trees: the forgotten role of neutrophils in rheumatoid arthritis.,,1997-07-01,https://www.semanticscholar.org/paper/31cc5d012e7b67761fa01fcc86c51975029f9d45,Immunology today (Amsterdam. Regular ed.)
3070,A2M: Access-Assured Mobile Desktop Computing,,2009-09-04,https://www.semanticscholar.org/paper/38a8e32cce656049487137aca0f6248f7339a643,Information Security Conference
3721,Analogical Reasoning for Visually Grounded Language Acquisition,"Children acquire language subconsciously by observing the surrounding world and listening to descriptions. They can discover the meaning of words even without explicit language knowledge, and generalize to novel compositions effortlessly. In this paper, we bring this ability to AI, by studying the task of Visually grounded Language Acquisition (VLA). We propose a multimodal transformer model augmented with a novel mechanism for analogical reasoning, which approximates novel compositions by learning semantic mapping and reasoning operations from previously seen compositions. Our proposed method, Analogical Reasoning Transformer Networks (ARTNet), is trained on raw multimedia data (video frames and transcripts), and after observing a set of compositions such as ""washing apple"" or ""cutting carrot"", it can generalize and recognize new compositions in new video frames, such as ""washing carrot"" or ""cutting apple"". To this end, ARTNet refers to relevant instances in the training data and uses their visual features and captions to establish analogies with the query image. Then it chooses the suitable verb and noun to create a new composition that describes the new image best. Extensive experiments on an instructional video dataset demonstrate that the proposed method achieves significantly better generalization capability and recognition accuracy compared to state-of-the-art transformer models.",2020-07-22,https://www.semanticscholar.org/paper/8320ea909c38a616f9daccff4e5a49cfce4d9735,arXiv.org
617,The Complexity of Coloring Circular Arcs and Chords,"The word problem for products of symmetric groups, the circular arc graph coloring problem, and the circle graph coloring problem, as well as several related problems, are proved to be $NP$-complete. For any fixed number K of colors, the problem of determining whether a given circular arc graph is K-colorable is shown to be solvable in polynomial time.",1980-06-01,https://www.semanticscholar.org/paper/b2997317ace4ee970ea960218d0ad3e8fc4ecc36,SIAM J. Algebraic Discret. Methods
2551,Pointer warping in heterogeneous multi-monitor environments,"Warping the pointer across monitor bezels has previously been demonstrated to be both significantly faster and preferred to the standard mouse behavior when interacting across displays in homogeneous multi-monitor configurations. Complementing this work, we present a user study that compares the performance of four pointer-warping strategies, including a previously untested frame-memory placement strategy, in heterogeneous multi-monitor environments, where displays vary in size, resolution, and orientation. Our results show that a new frame-memory pointer warping strategy significantly improved targeting performance (up to 30% in some cases). In addition, our study showed that, when transitioning across screens, the mismatch between the visual and the device space has a significantly bigger impact on performance than the mismatch in orientation and visual size alone. For mouse operation in a highly heterogeneous multi-monitor environment, all our participants strongly preferred using pointer warping over the regular mouse behavior.",2007-05-28,https://www.semanticscholar.org/paper/07c34b4df41357ef44058a09127ff372aaad634a,International Genetic Improvement Workshop
1079,Results from the Super Cryogenic Dark Matter Search Experiment at Soudan.,"We report the result of a blinded search for weakly interacting massive particles (WIMPs) using the majority of the SuperCDMS Soudan data set. With an exposure of 1690 kg d, a single candidate event is observed, consistent with expected backgrounds. This analysis (combined with previous Ge results) sets an upper limit on the spin-independent WIMP-nucleon cross section of 1.4×10^{-44} (1.0×10^{-44})  cm^{2} at 46  GeV/c^{2}. These results set the strongest limits for WIMP-germanium-nucleus interactions for masses >12  GeV/c^{2}.",2017-08-29,https://www.semanticscholar.org/paper/39b32d5869b417f23b7cdd250ac3207ce413c579,Physical Review Letters
3246,"Effects of holistic grazing management on milk production, weight gain, and visitation to grazing areas by livestock and wildlife in Laikipia County, Kenya",,2016-10-29,https://www.semanticscholar.org/paper/2ee68de7112bf894afac4c7d141dac23cb0d1838,Ecological Processes
867,On the complexity of local search,We prove a number of complexity results on the computational paradigm of local op-timality. Our main results are these: (a) Finding a local optimum under the Lin-Kernighan heuristic for the traveling salesman problemis PLS-complete. (b) Finding stable configurations in neural networks in the Hopfield mode/is PLS-complete. (c) We show that a host of simple unweighted local optimality problems are P-complete. (d) We introduce a general framework for establishing exponential worst-case bounds for local optimization heuristics. (e) And we show that local search problems become PSPACE-complete if we insist that the local optimum returned be attainable by local improvements from a given initial solution.,1990-04-01,https://www.semanticscholar.org/paper/058c1da6d4bf925e57d89c4708bdebe1360d62f1,Symposium on the Theory of Computing
3641,The Evolution of C++: 1985 to 1989,"The Ct+ Programming Language [Stroustrup 1986] describes Cr+ as defrned and implemented in August 1985. This paper describes the growth of the language since then and clarifies a few points in the defrnition. It is emphasized that these language modifications are extensions; Cr+ has been and will remain a stable language suitable for long term software development. The main new featurei of Cr-+ are: multiple inheritance, type-safe linkage, better resolution of overloaded functions, recuriive definition of assignment and initialization, better facilities for user-defrned memory management, abstract classes, static member functions, const member functions, protected members, overloading of operator ->, and pointers to members. These features are provided in the 2.0 release of Cr+. This paper is a revised and expanded version of a paper with a similar title. prer.iið¿ ""t the USENIX C++ Workshop in Sanla Fe, New Mexico, November 1987. @ Computing Systems,Vol. 2'No. 3'Summer 1989 191",1993-09-07,https://www.semanticscholar.org/paper/9e6edd54b7b46bc13cb38718ebbd1de01175f690,Computing Systems
106,Evaluating Top-k Selection Queries,"In many applications, users specify target values for certain attributes, without requiring exact matches to these values in return. Instead, the result to such queries is typically a rank of the \top k"" tuples that best match the given attribute values. In this paper, we study the advantages and limitations of processing a top-k query by translating it into a single range query that traditional relational DBMSs can process eciently. In particular, we study how to determine a range query to evaluate a top-k query by exploiting the statistics available to a relational DBMS, and the impact of the quality of these statistics on the retrieval eciency of the resulting scheme.",1999-09-07,https://www.semanticscholar.org/paper/d28cfa4c4d7bf6a86eb754ba6f9f4472cc209418,Very Large Data Bases Conference
2691,"Future multimedia user interfaces
",,1996-10-01,https://www.semanticscholar.org/paper/2eff111a86f0f570b01fb49ce67399dcf7626f13,Multimedia Systems
2198,Synovial fluid IL-6 concentrations associated with positive response to tocilizumab in an RA patient with failed response to anti-TNF and rituximab.,,2015-04-01,https://www.semanticscholar.org/paper/8f5b8138ff55fc48a0780c74274cc9fd85df5b1e,Rheumatology
1568,The Medical Deconfounder: Assessing Treatment Effects with Electronic Health Records,"The treatment effects of medications play a key role in guiding medical prescriptions. They are usually assessed with randomized controlled trials (RCTs), which are expensive. Recently, large-scale electronic health records (EHRs) have become available, opening up new opportunities for more cost-effective assessments. However, assessing a treatment effect from EHRs is challenging: it is biased by unobserved confounders, unmeasured variables that affect both patients' medical prescription and their outcome, e.g. the patients' social economic status. To adjust for unobserved confounders, we develop the medical deconfounder, a machine learning algorithm that unbiasedly estimates treatment effects from EHRs. The medical deconfounder first constructs a substitute confounder by modeling which medications were prescribed to each patient; this substitute confounder is guaranteed to capture all multi-medication confounders, observed or unobserved (arXiv:1805.06826). It then uses this substitute confounder to adjust for the confounding bias in the analysis. We validate the medical deconfounder on two simulated and two real medical data sets. Compared to classical approaches, the medical deconfounder produces closer-to-truth treatment effect estimates; it also identifies effective medications that are more consistent with the findings in the medical literature.",2019-10-28,https://www.semanticscholar.org/paper/10633897910bec410e04406004f14aad90cecbb8,Machine Learning in Health Care
2510,Augmented reality in the psychomotor phase of a procedural task,"Procedural tasks are common to many domains, ranging from maintenance and repair, to medicine, to the arts. We describe and evaluate a prototype augmented reality (AR) user interface designed to assist users in the relatively under-explored psychomotor phase of procedural tasks. In this phase, the user begins physical manipulations, and thus alters aspects of the underlying task environment. Our prototype tracks the user and multiple components in a typical maintenance assembly task, and provides dynamic, prescriptive, overlaid instructions on a see-through head-worn display in response to the user's ongoing activity. A user study shows participants were able to complete psychomotor aspects of the assembly task significantly faster and with significantly greater accuracy than when using 3D-graphics-based assistance presented on a stationary LCD. Qualitative questionnaire results indicate that participants overwhelmingly preferred the AR condition, and ranked it as more intuitive than the LCD condition.",2011-10-26,https://www.semanticscholar.org/paper/ab1203db90c05b7c55597c794c522c8d58fb84c1,2011 10th IEEE International Symposium on Mixed and Augmented Reality
186,Wealth Inequality and the Price of Anarchy,"Price of anarchy quantifies the degradation of social welfare in games due to the lack of a centralized authority that can enforce the optimal outcome. At its antipodes, mechanism design studies how to ameliorate these effects by incentivizing socially desirable behavior and implementing the optimal state as equilibrium. In practice, the responsiveness to such measures depends on the wealth of each individual. This leads to a natural, but largely unexplored, question. Does optimal mechanism design entrench, or maybe even exacerbate, social inequality? 
We study this question in nonatomic congestion games, arguably one of the most thoroughly studied settings from the perspectives of price of anarchy as well as mechanism design. We introduce a new model that incorporates the wealth distribution of the population and captures the income elasticity of travel time. This allows us to argue about the equality of wealth distribution both before and after employing a mechanism. We start our analysis by establishing a broad qualitative result, showing that tolls always increase inequality in symmetric congestion games under any reasonable metric of inequality, e.g., the Gini index. Next, we introduce the iniquity index, a novel measure for quantifying the magnitude of these forces towards a more unbalanced wealth distribution and show it has good normative properties (robustness to scaling of income, no-regret learning). We analyze iniquity both in theoretical settings (Pigou's network under various wealth distributions) as well as experimental ones (based on a large scale field experiment in Singapore). Finally, we provide an algorithm for computing optimal tolls for any point of the trade-off of relative importance of efficiency and equality. We conclude with a discussion of our findings in the context of theories of justice as developed in contemporary social sciences.",2018-02-01,https://www.semanticscholar.org/paper/d6ed4e428412d6dfd54c9552366c507cd1a66bd5,Symposium on Theoretical Aspects of Computer Science
2325,Interferon-gamma enhances monocyte cytotoxicity via enhanced reactive oxygen intermediate production. Absence of an effect on macrophage cytotoxicity is due to failure to enhance reactive nitrogen intermediate production.,"Interferon-gamma (IFN-gamma) enhanced the cytotoxic capability of freshly isolated human blood monocytes but failed to enhance the tumoricidal competence of monocyte-derived macrophages. Treatment of monocytes with IFN-gamma (100 U/ml) caused a significant increase (P < 0.001) in lucigenin-dependent chemiluminescence and O2- production stimulated by N-formyl-L-methionyl-L-leucyl-L-phenylalanine (FMLP) during the first few days in culture but IFN-gamma was unable to prevent the decline to negligible levels of chemiluminescence and O2- production which occurred during the later days in vitro. Culture of monocytes in the presence of IFN-gamma had no effect on phorbol 12-myristate 13-acetate (PMA)-stimulated O2- production. However, IFN-gamma decreased PMA-stimulated lucigenin-dependent chemiluminescence during the first 24 hr in vitro but then significantly enhanced (P < 0.001) chemiluminescence after 2-4 days in culture. IFN-gamma was unable to prevent the eventual decline to undetectable levels in PMA-stimulated chemiluminescence during the later days in vitro. Nitrite production by macrophages was unaffected by IFN-gamma treatment. It is concluded therefore, that IFN-gamma enhanced the cytotoxicity of freshly isolated human blood monocytes by increasing reactive oxygen intermediate generation but was unable to enhance the tumoricidal competence of macrophages as reactive nitrogen intermediate production was unaffected.",1994-04-01,https://www.semanticscholar.org/paper/884f13b1b623b66463011887f80975c2295a873c,Immunology
3287,"Aggression, grooming and group‐level cooperation in white‐faced capuchins (Cebus capucinus): insights from social networks","The form of animal social systems depends on the nature of agonistic and affiliative interactions. Social network theory provides tools for characterizing social structure that go beyond simple dyadic interactions and consider the group as a whole. We show three groups of capuchin monkeys from Barro Colorado Island, Panama, where there are strong connections between key aspects of aggression, grooming, and proximity networks, and, at least among females, those who incur risk to defend their group have particular “social personalities.” Although there is no significant correlation for any of the network measures between giving and receiving aggression, suggesting that dominance relationships do not follow a simple hierarchy, strong correlations emerge for many measures between the aggression and grooming networks. At the local, but not global, scale, receiving aggression and giving grooming are strongly linked in all groups. Proximity shows no correlation with aggression at either the local or the global scale, suggesting that individuals neither seek out nor avoid aggressors. Yet, grooming has a global but not local connection to proximity. Extensive groomers who tend to direct their efforts at other extensive groomers also spend time in close proximity to many other individuals. These results indicate the important role that prosociality plays in shaping female social relationships. We also show that females who receive the least aggression, and thus pay low costs for group living, are most likely to participate in group defense. No consistent “social personality” traits characterize the males who invest in group defense. Am. J. Primatol. 73:821–833, 2011. © 2011 Wiley‐Liss, Inc.",2011-08-01,https://www.semanticscholar.org/paper/823dd2e8f486ecb7dac716a994f49071fa7f8706,American Journal of Primatology
2291,Effects of Staphylococcal Enterotoxins on Human Neutrophil Functions and Apoptosis,"ABSTRACT Staphylococcal enterotoxins have marked effects on the properties of T cells and monocytes and have recently been reported to affect neutrophil function. In this study, we investigated the abilities of staphylococcal enterotoxins A and B and toxic shock syndrome toxin 1 to affect respiratory burst activity and to delay apoptosis in human neutrophils. When cultures containing approximately 97% neutrophils were tested, the toxins all delayed neutrophil apoptosis in a dose-dependent manner and induced the expression of FcγRI on the neutrophil cell surface. These effects on apoptosis and expression of FcγRI were largely abrogated by the addition of a neutralizing anti-gamma interferon antibody. Similarly, the effects of these toxins on phorbol ester-induced chemiluminescence were decreased after neutralization of gamma interferon. These effects on neutrophil function were mimicked by the addition of conditioned medium from peripheral blood mononuclear cells incubated with the toxins, and again, neutralizing anti-gamma interferon antibodies largely negated the effects. However, when highly purified neutrophils prepared by immunodepletion of T cells and major histocompatibility complex class II-expressing cells were analyzed, the toxins were without effect on apoptosis and FcγRI expression, but granulocyte-macrophage colony-stimulating factor and gamma interferon could still delay apoptosis. These data indicate that these toxins have no direct effect on neutrophil apoptosis but can act indirectly via the production of T-cell-derived and monocyte-derived cytokines. It is noteworthy that such effects are detected in neutrophil suspensions containing only 3% contamination with T cells and other mononuclear cells.",1999-05-01,https://www.semanticscholar.org/paper/564c2ff9dc62f708fdc158a2427915ba834ed307,Infection and Immunity
3128,Reducing Storage Management Costs via Informed User-Based Policies,"Storage consumption continues to grow rapidly, especially with the popularity of multimedia files. Storage hardware costs represent a small fraction of overall management costs, which include frequent maintenance and backups. Our key approach to reducing total storage management costs is to reduce actual storage consumption. We achieve this in two ways. First, we classify files into categories of importance. Based on these categories, files can be backed up with various frequencies, or even not at all. Second, the system may also reclaim space based on a file’s importance (e.g., transparently compress old files). Our system provides a rich set of policies. We allow users to tailor their disk usage policies, offloading some of the management burdens from the system and its administrators. We have implemented the system and evaluated it. Performance overheads under normal use are negligible. We report space savings on modern systems ranging from 25% to 76%, which result in extending storage lifetimes by 72%.",,https://www.semanticscholar.org/paper/faac81887d6d7db83eda5e43b24c45d9261ef6a9,IEEE Conference on Mass Storage Systems and Technologies
523,Corrigendum: The Complexity of Cubical Graphs,,,https://www.semanticscholar.org/paper/0c24781a67680874a3d0cdd51e0328acbef30aed,Information and Computation
635,On the Complexity of Local Search for the Traveling Salesman Problem,"It is shown that, unless $P = NP$, local search algorithms for the traveling salesman problem having polynomial time complexity per iteration will generate solutions arbitrarily far from the optimal.",1977-03-01,https://www.semanticscholar.org/paper/051d2d7cea8d57eca321d6536ece2b69700a49c3,SIAM journal on computing (Print)
1604,Readmission prediction via deep contextual embedding of clinical concepts,"Objective Hospital readmission costs a lot of money every year. Many hospital readmissions are avoidable, and excessive hospital readmissions could also be harmful to the patients. Accurate prediction of hospital readmission can effectively help reduce the readmission risk. However, the complex relationship between readmission and potential risk factors makes readmission prediction a difficult task. The main goal of this paper is to explore deep learning models to distill such complex relationships and make accurate predictions. Materials and methods We propose CONTENT, a deep model that predicts hospital readmissions via learning interpretable patient representations by capturing both local and global contexts from patient Electronic Health Records (EHR) through a hybrid Topic Recurrent Neural Network (TopicRNN) model. The experiment was conducted using the EHR of a real world Congestive Heart Failure (CHF) cohort of 5,393 patients. Results The proposed model outperforms state-of-the-art methods in readmission prediction (e.g. 0.6103 ± 0.0130 vs. second best 0.5998 ± 0.0124 in terms of ROC-AUC). The derived patient representations were further utilized for patient phenotyping. The learned phenotypes provide more precise understanding of readmission risks. Discussion Embedding both local and global context in patient representation not only improves prediction performance, but also brings interpretable insights of understanding readmission risks for heterogeneous chronic clinical conditions. Conclusion This is the first of its kind model that integrates the power of both conventional deep neural network and the probabilistic generative models for highly interpretable deep patient representation learning. Experimental results and case studies demonstrate the improved performance and interpretability of the model.",2018-04-09,https://www.semanticscholar.org/paper/888ba07b575eda30a26edc69ef846c3a387f8394,PLoS ONE
2863,Galectin-3 and Regulation of Cell Function,"The galectin family is phylogenically conserved in metazoans, presently consisting of 14 identified members in mammals, and galectin-3 is one of the most abundant, widely distributed, and well-studied members. It is present intracellularly in nuclear and cytoplasmic compartments, but is also secreted through an unconventional mechanism that involves vesicles and exosomes and, consequently, present outside the cell. While galectin-3 shares many features with other galectins, including cellular compartmentalization and functions, it appears to be unique in its structural constituency in the N-terminal domain, which is rich in proline, glycine, and alanine. It is through this domain that galectin-3 is able to form oligomers, and this may be one feature that functionally differentiates it from other galectins. Galectin-3 has been associated with several intracellular and extracellular functions, and recent investigations have uncovered proteins through which this lectin mediates its activities. The availability of targeted mutant mice deficient in galectin- 3 and other proteins has also contributed to our appreciation of the breadth of processes in which this protein is involved. Among cell functions attributable to galectin-3 are some that are associated with neoplastic transformation and invasiveness, but the most extensively documented ones are those related to the immune and inflammatory responses.",2005-03-01,https://www.semanticscholar.org/paper/c04bf72164c8a3a9160b9d152364981e54c0a453,Transfusion Medicine and Hemotherapy
196,Zero-Sum Polymatrix Games: A Generalization of Minmax,"We show that in zero-sum polymatrix games, a multiplayer generalization of two-person zero-sum games, Nash equilibria can be found efficiently with linear programming. We also show that the set of coarse correlated equilibria collapses to the set of Nash equilibria. In contrast, other important properties of two-person zero-sum games are not preserved: Nash equilibrium payoffs need not be unique, and Nash equilibrium strategies need not be exchangeable or max-min.",2016-01-27,https://www.semanticscholar.org/paper/45edb86a7ee18a0e7063bf24a3524501d72541cb,Mathematics of Operations Research
2481,The future is here: augmented and virtual reality (full text not available),,2014-07-27,https://www.semanticscholar.org/paper/aa182c7d95a877ca5ff9d3506f3b1283de718293,International Conference on Computer Graphics and Interactive Techniques
2450,Engaging hospitalized patients in clinical care: Study protocol for a pragmatic randomized controlled trial.,,2016-03-01,https://www.semanticscholar.org/paper/1bc99006152c2ea96b98de23a6926cf96e5bf3e5,Contemporary Clinical Trials
3623,The Real Stroustrup Interview,"argued that "" a programming language is really a very tiny part of the world, and as such, it ought not be taken too seriously. Keep a sense of proportion and most importantly keep a sense of humor. Among major programming languages, C++ is the richest source of puns and jokes. That is no accident. "" For the past few months, a hoax interview between Stroustrup and Computer has been making the rounds in cyberspace. While we regret the incident , it offers us a welcome opportunity to have the father of C++ share his insights on Standard C++ and software development in general. We can also attest to his continued sense of proportion and humor—he suggests that the fictitious interview would have been a much funnier parody had he written it himself. STANDARD C++ Computer: ISO approved the Standard C++ in November 1997, and you published a third edition of your The C++ Programming Language (Addison Wes-ley, 1997). How has C++ evolved over the past few years and what does the ISO standard mean for the C++ community? Stroustrup: It is great to finally have a complete, detailed, and stable definition of C++. This will be a great help to the C++ community in myriad direct and not-so-direct ways. Obviously, we will get better implementations as compiler providers start shifting attention from catching up with the standards committee to quality-of-implementation issues. This is already happening. Standards-conforming implementations will prove a boon to tools and library suppliers by providing a larger common platform to build for. The standard gives the programmer an opportunity to be more adventurous with new techniques. Programming styles that used to be unrealistic in production code are becoming realistic propositions. Thus, more flexible, general , faster, and more maintainable code can be written. Naturally, we should keep cool and not indulge in orgies of "" advanced "" techniques. There are still no miracles, and the best code is still the code that most directly matches a sound design. However , now is the time to experiment and see which techniques will suit particular people, organizations, and projects. Much of The C++ Programming Language is devoted to these techniques and the trade-offs they represent. The most visible aspects of what makes this progress feasible are the "" new "" major language facilities—tem-plates, exceptions, runtime type information , and namespaces—and the new standard library. The minor improvements to …",1998-06-01,https://www.semanticscholar.org/paper/9717316c4089f6220cb28ee9f2b7d46961bc6e6f,Computer
3723,Video Representations of Goals Emerge from Watching Failure,"We introduce a video representation learning framework that models the latent goals behind observable human action. Motivated by how children learn to reason about goals and intentions by experiencing failure, we leverage unconstrained video of unintentional action to learn without direct supervision. Our approach models videos as contextual trajectories that represent both low-level motion and high-level action features. Experiments and visualizations show the model is able to predict underlying goals, detect when action switches from intentional to unintentional, and automatically correct unintentional action. Although the model is trained with minimal supervision, it is competitive with highly-supervised baselines, underscoring the role of failure examples for learning goal-oriented video representations. The project website is available at this https URL",2020-06-28,https://www.semanticscholar.org/paper/9a6dd28c5449ddcad7b079c0dca6ea6a518c3eb0,arXiv.org
2185,High macrophage activities are associated with advanced periductal fibrosis in chronic Opisthorchis viverrini infection,"Liver fluke infection caused by Opisthorchis viverrini induces several hepatobiliary conditions including advanced periductal fibrosis (APF) and cholangiocarcinoma (CCA), but >25% of the infected population develops APF and 1% develop CCA. The innate immune response is the first line of defence, and macrophages are critical regulators of fibrosis. We hypothesized that macrophages from infected individuals have different capacities to either promote or suppress periductal fibrosis. We compared phagocytic activities of macrophages of healthy individuals and O viverrini‐infected individuals ± APF, and found that macrophages from infected individuals with APF ingested significantly higher numbers of beads compared with healthy controls and O viverrini‐infected individuals without APF. To further investigate proteolytic activity, we monitored real‐time phagosomal proteolysis of beads conjugated to DQ‐BODIPY‐BSA using live cell imaging. We show that macrophages from O viverrini‐infected individuals with APF also have elevated phagosomal proteolysis activity, which is consistent with their increased phagocytic activity. Additionally, stimulated ROS production by blood monocytes was higher in individuals with APF compared with healthy controls and infected individuals without APF. These results suggest that during O viverrini infection, macrophages with high phagocytic and proteolytic activities together with elevated ROS production are the phenotypes that can promote tissue damage, which results in periductal fibrosis.",2018-12-03,https://www.semanticscholar.org/paper/51f7427f9015e08e2b2766c3830457d1f46bae47,Parasite immunology (Print)
2848,Kinetics of mobilization and differentiation of lymphohematopoietic cells during experimental murine schistosomiasis in galectin‐3−/− mice,"Galectin‐3 (gal‐3), a β‐galactoside‐binding animal lectin, plays a role in cell‐cell and cell‐extracellular matrix interactions. Extracellular gal‐3 modulates cell migration and adhesion in several physiological and pathological processes. Gal‐3 is highly expressed in activated macrophages. Schistosoma mansoni eggs display a large amount of gal‐3 ligands on their surface and elicit a well‐characterized, macrophage‐dependent, granulomatous, inflammatory reaction. Here, we have investigated the acute and chronic phases of S. mansoni infection in wild‐type and gal‐3−/− mice. In the absence of gal‐3, chronic‐phase granulomas were smaller in diameter, displaying thinner collagen fibers with a loose orientation. Schistosoma‐infected gal‐3−/− mice had remarkable changes in the monocyte/macrophage, eosinophil, and B lymphocyte subpopulations as compared with the infected wild‐type mice. We observed a reduction of macrophage number, an increase in eosinophil absolute number, and a decrease in B lymphocyte subpopulation (B220+/high cells) in the periphery during the evolution of the disease in gal‐3−/− mice. B lymphopenia was followed by an increase of plasma cell number in bone marrow, spleen, and mesenteric lymph nodes of the infected gal‐3−/− mice. The plasma IgG and IgE levels also increased in these mice. Gal‐3 plays a role in the organization, collagen distribution, and mobilization of inflammatory cells to chronic‐phase granulomas, niches for extramedullary myelopoiesis, besides interfering with monocyte‐to‐macrophage and B cell‐to‐plasma cell differentiation.",2007-08-01,https://www.semanticscholar.org/paper/34bcb9acea990f5c50fda2281fd96064c0ae1717,Journal of Leukocyte Biology
848,Timing Verification by Successive Approximation,"We present an algorithm for verifying that a model M with timing constraints satisfies a given temporal property T. The model M is given as a parallel composition of ?-automata Pi, where each automaton Pi is constrained by bounds on delays. The property T is given as an ?-automaton as well, and the verification problem is posed as a language inclusion question L(M) ? L(T). In constructing the composition M of the constrained automata Pi, one needs to rule out the behaviors that are inconsistent with the delay bounds, and this step is (provably) computationally expensive. We propose an iterative solution which involves generating successive approximations Mj to M, with containment L(M) ? L(Mj) and monotone convergence L(Mj) ? L(M) within a bounded number of steps. As the succession progresses, the approximations Mj become more complex. At any step of the iteration one may get a proof or a counter-example to the original language inclusion question. The described algorithm is implemented into the verifier Cospan. We illustrate the benefits of our strategy through some examples.",1992-06-29,https://www.semanticscholar.org/paper/3a799b970714d67225955861d690a24dd6e0c3b6,Information and Computation
108,Database research at Columbia University,"Columbia University has a number of projects that touch on database systems issues. In this report, we describe the Columbia Fast Query Project (Section 2), the JAM project (Section 3), the CARDGIS project (Section 4), the Columbia Internet Information Searching Project (Section 5), the Columbia Content-Based Visual Query project (Section 6), and projects associated with Columbia's Programming Systems Laboratory (Section 7).",1998-09-01,https://www.semanticscholar.org/paper/0d862a75271c152537ec41e24cbf5b0cd645a41e,SGMD
1214,Search for pair production of doubly charged Higgs bosons in the H++H- - -->mu+ mu+ mu- mu- final state.,"We report the results of a search for pair production of doubly charged Higgs bosons via pp over-->H++H - - X-->mu+ mu+ mu- mu- X at sqrt s=1.96 TeV. We use a data set corresponding to an integrated luminosity of 1.1 fb(-1) collected from 2002 to 2006 by the D0 detector at the Fermilab Tevatron Collider. In the absence of an excess above the standard model background, lower mass limits of M(H L +/- +/-) >150 GeV/c2 and M(H R+/- +/-) >127 GeV/c2 at 95% C.L. are set, respectively, for left-handed and right-handed doubly charged Higgs bosons assuming a 100% branching ratio into muons.",,https://www.semanticscholar.org/paper/4a79fcfd9742c97378bb207e75a85e5e6bef6e6b,Physical Review Letters
976,Effect of Nitric Oxide on Acanthamoeba castellanii.,"Purpose
Acanthamoeba keratitis is a well-known intractable corneal infectious disease. We investigated the anti-Acanthamoeba effect of exogenous nitric oxide (NO).


Methods
Acanthamoeba castellanii was axenically cultured and exposed to various concentrations of NO donors, such as sodium nitrite, sodium nitroprusside (SNP), and NO-releasing silica nanoparticles (coated in branched polyethylene imine, size:100 nm), for 1 to 7 days (sodium nitrite and SNP: 0, 0.1, 1, 10, 100, and 1000 μM; silica nanoparticles: 0, 6.25, 12.5, 25, 50, and 100 μg/mL). Human corneal epithelial cells (HCECs) were cultured and exposed to sodium nitrite, SNP (0, 0.1, 1, 10, 100, and 1000 μM), and silica nanoparticles for 1, 2, and 3 days.


Results
Sodium nitrite and SNP showed a dose-dependent inhibitory effect on A. castellanii viability. A more prominent inhibitory effect was observed with SNP (less than 10% of organisms survived at 7-day culture with 1000 μM) compared with sodium nitrite. However, more cytotoxicity on HCEC was observed with SNP. NO-releasing silica nanoparticles were successfully internalized into the amoebic cytoplasm and accumulated in large vacuoles. Although blank silica nanoparticles had no inhibitory effect on A. castellanii viability, NO-releasing silica nanoparticles showed a dose-dependent amoebicidal effect. Furthermore, no cystic transformation of A. castellanii was observed under a phase contrast microscope or transmission electron microscope after exogenous NO treatment.


Conclusions
Our results demonstrated the anti-Acanthamoeba effect of exogenous NO. This finding suggests that NO-releasing drug platforms, including nano-carriers, can be a promising therapeutic strategy for Acanthamoeba keratitis.",2018-07-02,https://www.semanticscholar.org/paper/1f192626400dd46c6eee42dc9710bfec3aa079fa,Investigative Ophthalmology and Visual Science
2875,Targeted disruption of the galectin-3 gene results in attenuated peritoneal inflammatory responses.,,2000-03-01,https://www.semanticscholar.org/paper/35d5a034133755ab85672a3c1e995113c743ae80,American Journal of Pathology
2914,Stress Triaxiality and Lode Angle Parameter Characterization of Flat Metal Specimen with Inclined Notch,"The stress state has an important effect on the deformation and failure of metals. While the stress states of the axisymmetric notched bars specimens are studied in the literature, the studies on the flat metal specimen with inclined notch are very limited and the stress state is not clearly characterized in them. In this paper, digital image correlation and finite element simulations are used to study the distribution of strain and stress state, that is stress triaxiality and Lode angle parameter. Flat specimen with inclined notch was tested to extract the full field strain evolution and calculate stress state parameters at three locations: specimen centre, notch root and failure starting point. It is found that compared with the centre point and the notch root, the failure initiation point can better characterize the influence of the notch angle on the strain evolution. Conversely, the centre point can more clearly characterize the effect of the notch angle on stress state, since the stress states at the failure point and the notch root change greatly during the plastic deformation. Then the calculated stress state parameters of the flat metal specimen with inclined notch at the centre point are used in Wierzbicki stress state diagram to establish a relationship between failure mode and stress state.",2021-10-13,https://www.semanticscholar.org/paper/2c1f6f9d12713e14f5f5df4d47101e7f3fe284c6,Metals
1907,Semiconductor manufacturing,,2018-10-03,https://www.semanticscholar.org/paper/6c27ce9a5aaae8954ed3bf404f2ca90dd08441ed,Electrical Engineering Handbook
832,On the hardness of approximating minimization problems,"We prove results indicating that it is hard to compute efficiently good approximate solutions to the Graph Coloring, Set Covering and other related minimization problems. Specifically, there is an e > 0 such that Graph Coloring cannot be approximated with ratio n e unless P = NP. Set Covering cannot be approximated with ratio c log n for any c < 1/4 unless NP is contained in DTIME(n poly log n ). Similar results follow for related problems such as Clique Cover, Fractional Chromatic Number, Dominating Set, and others",1994-09-01,https://www.semanticscholar.org/paper/67762ce2c7a85dbf55cdf86fedee2610229d91eb,JACM
866,On Datalog vs. polynomial time (extended abstract),We show that certain monotonic polynomial time queries are not expressible in variants of Datalog. The proof techniques include lower bounds for monotone circuit size and a “Pumping Lemma” for Datalog queries.,1991-04-01,https://www.semanticscholar.org/paper/ee93243481ea145580f5b10d61bbd4998fa41260,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems
104,Characterizing Web Resources for Improved Search,"As an important initial step to exploit such dimensions for web search, we have focused on geographical relevance. Web sites containing information on restaurants or apartment rentals, for instance, are relevant primarily to web users in geographical proximity to these locations. In contrast, an on-line newspaper may be relevant to users across the United States. We have studied how to mine the web and automatically estimate the geographical scope of web resources by using web hyperlinks and the actual content of web pages. For example, we can map every web page to a location based on where its hosting site resides. Then, we can consider the location of all the pages that point to, say, the Stanford Daily home page. By examining the distribution of these pointers, we can conclude that the Stanford Daily is of interest mainly to residents of the Stanford area, while The Wall Street Journal is of nation-wide interest. Similar conclusions can be drawn for other resources by analyzing the geographical locations that are mentioned in their pages.",,https://www.semanticscholar.org/paper/db8e062f6cba4e914460018de2291d03f1231839,DELOS Workshops / Conferences
912,A Theory of Safe Locking Policies in Database Systems,"When several transacuons access (read and update) the same database concurrently, there must be some kind of coordmauon to ensure that all transacuons receive a consistent view of the data Such coordination is usually achieved by locking the transactions according to some locking policy A locking policy that guarantees the preservation of consistency of the database is called safe Necessary and sufficient conditions are found for a locking pohcy to be safe, but it is shown that in general it is NPcomplete to test for these conditions. However, when the database has a given structure, a simple set of rules which is sufficient for safety and, moreover, necessary for a wide class of natural locking pohcles is developed Categories and SubJect Descriptors DA I [Operating Systems] Process Management--concurrency, H 2 2 [Database Management] Physical Design--deadlock avoMance General Terms. Theory AddlUonal",1982-07-01,https://www.semanticscholar.org/paper/42ee83fad69c823fb7d462014fe00b4a7e10ed6d,JACM
114,The Stanford Digital Library metadata architecture,,1997-04-15,https://www.semanticscholar.org/paper/182357ed2ad1bfe8806ccc0a63c366e55819a70d,International Journal on Digital Libraries
2408,The cytochromes of Acanthamoeba castellanii.,"1. Low-temperature difference spectra of gradient-purified mitochondria of Acanthamoeba castellanii reveal the presence of cytochromes b-555, b-562 and c-549, with a-type cytochromes having a broad asymmetrical maximum at 602 nm; these components were also observed in specta of whole cells. 2. The a-type cytochromes are unusual in that they have split Soret absorption maxima (at 442 and 449 nm) and an uncharacteristic CO difference spectrum. 3. CO difference spectra of whole cells and 'microsomal' membranes show large amounts of cytochrome P-420 compared with cytochrome P-450. 4. Difference spectra in the presence of cyanide indicate the presence of an a-type cytochrome and two cyanide-reacting components, one of which may be cytochrome a3. 5. Whole-cell respiration in a N2/O2 (19:1) atmosphere was decreased by 50%, suggesting the presence of a low-affinity oxidase. This lowered respiration is inhibited by 50% by CO, and the inhibition is partially light-reversible; photochemical action spectra suggest that cytochrome a3 contributes to this release of inhibition. Other CO-reacting oxidases are also present. 6. The results are discussed with the view that cytochrome a3 is present in A. castellanii, but its identification in CO difference spectra is obscured by other component(s).",1977-10-15,https://www.semanticscholar.org/paper/6bd6c4026b438168bce807fb8f74e8f348f5be17,Biochemical Journal
1038,Nonlinear dimensionality reduction for kinematic cartography with an application toward robotic locomotion,"Planning robot motions often requires a notion of the “distance” between configurations or the “length” of a trajectory connecting them in the configuration space. If these quantities are defined so as to correspond to the effort required to change configurations, then they would likely differ from the Euclidean distance or arclength in the system's configuration parameters, distorting the visual representation of the relative costs of executing the motions. This problem is fundamentally similar to that of producing map projections with minimal distortion in cartography. A separate problem is that of nonlinear dimensionality reduction (NLDR), which, given a set of data, projects it into a lower-dimensional space while seeking to retain the geometric relationship between data points. In this paper, we show that NLDR can be applied to the kinematic cartography problem, allowing us to generate system parameterizations in which distance and arclength correspond to the effort of motion.",2014-09-01,https://www.semanticscholar.org/paper/b6af3377a3f58507c6f489581b8624ac02f99821,2014 IEEE/RSJ International Conference on Intelligent Robots and Systems
1356,Search for supersymmetry via associated production of charginos and neutralinos in final states with three leptons.,"A search for associated production of charginos and neutralinos is performed using data recorded with the D0 detector at a pp center-of-mass energy of 1.96 TeV at the Fermilab Tevatron Collider. This analysis considers final states with missing transverse energy and three charged leptons, of which at least two are electrons or muons. No evidence for supersymmetry is found in a data set corresponding to an integrated luminosity of 320 pb-1. Limits on the product of the production cross section and leptonic branching fraction are set. For the minimal supergravity model, a chargino lower mass limit of 117 GeV at the 95% C.L. is derived in regions of parameter space with enhanced leptonic branching fractions.",2005-04-18,https://www.semanticscholar.org/paper/adbe9c8beed480277d170cdf2807b7fb85ec174b,Physical Review Letters
335,Computing correlated equilibria in multi-player games,"We develop a polynomial-time algorithm for finding correlated equilibria (a well-studied notion of rationality due to Aumann that generalizes the Nash equilibrium) in a broad class of succinctly representable multiplayer games, encompassing essentially all known kinds, including all graphical games, polymatrix games, congestion games, scheduling games, local effect games, as well as several generalizations. Our algorithm is based on a variant of the existence proof due to Hart and Schmeidler [11], and employs linear programming duality, the ellipsoid algorithm, Markov chain steady state computations, as well as application-specific methods for computing multivariate expectations.",2005-05-22,https://www.semanticscholar.org/paper/a1143ee30e652360234a1bb37c5d48ab46d34a05,Symposium on the Theory of Computing
437,Emerging opportunities for theoretical computer science,"The principles underlying this report can be summarized as follows:1. A strong theoretical foundation is vital to computer science.2. Theory can be enriched by practice.3. Practice can be enriched by theory.4. If we are guided by (2) and (3), the value, impact, and funding of theory will be enhanced.In order to achieve a greater synergy between theory and application, and to sustain and expand on the remarkable successes of Theory of Computing (TOC), we consider it essential to increase the impact of theory on key application areas. This requires additional financial resources in support of theory, and closer interaction between theoreticians and researchers in other areas of computer science and in other disciplines.The report does not make a detailed assessment of the overall state of theoretical computer science or fully chronicle the achievements of this field. Instead, it has the specific objective of recommending ways to harness these remarkable achievements for the solution of challenging problems emerging from new developments such as the information superhighway.Section 1 describes the events leading up to this report and delineates the report's objectives. Section 2 establishes the context for the report. It traces the history of TOC, describes the impact that TOC has achieved in the areas of core theory and fundamental algorithms, points out the differences between these areas and application-oriented theory, and calls for an intensified effort to bring the methods of TOC to bear on applications. It then goes on to define the four main categories into which our recommen- dations fall: building bridges between theory and applications, algorithm engineering, communication, and education. Section 3 discusses some specific opportunities for stimulating interactions between TOC and applied areas. Section 4 proposes an applied research initiative, Information Access in a Globally Distributed Environment, which identifies an exciting current technological area that we believe presents challenging opportunities for excellent theoretical work. Section 5 proposes a second applied research initiative, The Algorithmic Stockroom, that would exploit and extend the body of theoretical knowledge in the field of algorithms. Section 6 proposes a broadening in graduate education with two purposes in mind: to better prepare theoreticians to interact creatively with practitioners, and to provide future practitioners with the background they will need to benefit from this exchange.",1997-09-01,https://www.semanticscholar.org/paper/893b6f77e6243c7c85df3c6dc6f595228cf958d0,Sigact News
1051,Light Dark Matter Search with a High-Resolution Athermal Phonon Detector Operated above Ground.,"We present limits on spin-independent dark matter-nucleon interactions using a 10.6 g Si athermal phonon detector with a baseline energy resolution of σ_{E}=3.86±0.04(stat)_{-0.00}^{+0.19}(syst)  eV. This exclusion analysis sets the most stringent dark matter-nucleon scattering cross-section limits achieved by a cryogenic detector for dark matter particle masses from 93 to 140  MeV/c^{2}, with a raw exposure of 9.9 g d acquired at an above-ground facility. This work illustrates the scientific potential of detectors with athermal phonon sensors with eV-scale energy resolution for future dark matter searches.",2020-07-21,https://www.semanticscholar.org/paper/51ed2acc72a065eff0d6a81cf28443032dcb08ee,Physical Review Letters
2560,IEEE Symposium on 3D User Interfaces 2007,The following topics are dealt with: virtual reality; 3D movement; sequences & gestures; devices; mixed & augmented reality; 3D selection; forces; and 3D navigation & entertainment,,https://www.semanticscholar.org/paper/a997d172358730c82ef89283e4d42007066a7055,IEEE Symposium on 3D User Interfaces
2352,Inhibition of myeloperoxidase by salicylhydroxamic acid.,"Salicylhydroxamic acid inhibited the luminol-dependent chemiluminescence of human neutrophils stimulated by phorbol 12-myristate 13-acetate or the chemotactic peptide N-formylmethionyl-leucyl-phenylalanine (fMet-Leu-Phe). This compound had no inhibitory effect on the kinetics of O2.- generation or O2 uptake during the respiratory burst, but inhibited both the peroxidative activity of purified myeloperoxidase and the chemiluminescence generated by a cell-free myeloperoxidase/H2O2 system. The concentration of salicylhydroxamic acid necessary for complete inhibition of myeloperoxidase activity was 30-50 microM (I50 values of 3-5 microM) compared with the non-specific inhibitor NaN3, which exhibited maximal inhibition at 100-200 microM (I50 values of 30-50 microM). Whereas taurine inhibited the luminol chemiluminescence of an H2O2/HOC1 system by HOC1 scavenging, this compound had little effect on myeloperoxidase/H2O2-dependent luminol chemiluminescence; in contrast, 10 microM-salicylhydroxamic acid did not quench HOC1 significantly but greatly diminished myeloperoxidase/H2O2-dependent luminol chemiluminescence, indicating that its effects on myeloperoxidase chemiluminescence were largely due to peroxidase inhibition rather than non-specific HOC1 scavenging. Salicylhydroxamic acid prevented the formation of myeloperoxidase Compound II, but only at low H2O2 concentrations, suggesting that it may compete for the H2O2-binding site on the enzyme. These data suggest that salicylhydroxamic acid may be used as a potent inhibitor to delineate the function of myeloperoxidase in neutrophil-mediated inflammatory events.",1989-03-15,https://www.semanticscholar.org/paper/15d4ba3c7f1d0b738a97a74c8174a26771ef9da0,Biochemical Journal
981,Position of Central Retinal Vascular Trunk and Preferential Location of Glaucomatous Damage in Myopic Normal-Tension Glaucoma.,,2018-07-01,https://www.semanticscholar.org/paper/ac8301eba9fb51668161612d088274e0c181a597,Ophthalmology Glaucoma
2988,Modeling skin and ageing phenotypes using latent variable models in Infer.NET,"We demonstrate and compare three unsupervised Bayesian latent variable models implemented in Infer.NET [2] for biomedical data modeling of 42 skin and aging phenotypes measured on the 12,000 female twins in the Twins UK study [7]. We address various data modeling problems include high missingness, heterogeneous data, and repeat observations. We compare the proposed models in terms of their performance at predicting disease labels and symptoms from available explanatory variables, concluding that factor analysis type models have the strongest statistical performance in this setting. We show that such models can be combined with regression components for improved interpretability. This work is being performed in collaboration with the Department of Twin Research and Genetic Epidemiology (DTR) at King’s College London. The DTR manages the largest UK adult twin registry of around 12,000 female monozygotic and dizygotic twins, established in 1992 [7]. The data has characteristics common to many biomedical applications, each of which we our able to address using our modeling framework. 1. High missingness. Many variables have up to 80% missing, and the level of overlap between phenotypes varies considerably. This level of missingness motivates Bayesian methods which are able to naturally deal with missingness, rather than attempting crude imputation procedures. 2. Heterogeneous data. The data contains continuous, categorical (including binary), ordinal and count data. We show in simulation experiments that using appropriate likelihood functions for each of these data types improves statistical power. 3. Multiple observations. Often the same underlying phenotype is recorded as multiple measurements, and the measurements may not be consistent. Allowing the model to combine these measurements into a single phenotype aids interpretability, improves statistical power and helps deal with the missingness problem. 4. High dimensional. The Twins UK database contains over 6000 phenotype and exposure variables, measured at multiple time points. Modern healthcare records are of the same nature. For a subset of 800 individuals we have 10,000 gene expression measurements in three different tissues, and the genotype of 600k Single Nucleotide Polymorphisms (SNPs). Our modeling framework allows these issues to be straightforwardly and rigorously addressed, and provides an efficient inference platform using Variational Message Passing under the Infer.NET framework. Although the models we use all provide some form of dimensionality reduction, which is essential for the high dimensional nature of the data, we currently only analysis around 40 phenotypes of particular relevance to skin and aging. Scaling these models to handle the full dataset, including gene expression and genotype data, is ongoing research. An attribute of the data that we have not fully explored how to model at this stage is that it is time series data. Most individuals in the study group have made multiple visits to be medically",,https://www.semanticscholar.org/paper/fcf131bb41fbf01fcb3d745d78aabede2033187c,NIPS 2010
406,Novel Computational Approaches to Information Retrieval and Data Mining (Abstract),,1999-01-10,https://www.semanticscholar.org/paper/5d051d08e56b18c0a88d630f7cdaa5f60c72043e,International Conference on Database Theory
2581,Cross-dimensional gestural interaction techniques for hybrid immersive environments,"We present a set of cross-dimensional interaction techniques for a hybrid user interface that integrates existing 2D and 3D visualization and interaction devices. Our approach is built around one-and two-handed gestures that support the seamless transition of data between co-located 2D and 3D contexts. Our testbed environment combines a 2D multi-user, multi-touch, projection surface with 3D head-tracked, see-through, head-worn displays and 3D tracked gloves to form a multi-display augmented reality. We address some of the ways in which we can interact with private data in a collaborative, heterogeneous workspace. We also report on a pilot usability study to evaluate the effectiveness and ease of use of the cross-dimensional interactions.",2005-03-12,https://www.semanticscholar.org/paper/31b63946a90c1f61eab0013fe8da18b1af944350,"IEEE Proceedings. VR 2005. Virtual Reality, 2005."
215,Can Almost Everybody be Almost Happy?,"We conjecture that PPAD has a PCP-like complete problem, seeking a near equilibrium in which all but very few players have very little incentive to deviate. We show that, if one assumes that this problem requires exponential time, several open problems in this area are settled. The most important implication, proved via a ""birthday repetition"" reduction, is that the nO(log n) approximation scheme of Lipton et al. [23] for the Nash equilibrium of two-player games is essentially optimum. Two other open problems in the area are resolved once one assumes this conjecture, establishing that certain approximate equilibria are PPAD-complete: Finding a relative approximation of two-player Nash equilibria (without the well-supported restriction of [14]), and an approximate competitive equilibrium with equal incomes [10] with small clearing error and near-optimal Gini coefficient.",2015-04-09,https://www.semanticscholar.org/paper/13440bd165cdd629fe77032b517641243bcc9687,Information Technology Convergence and Services
2731,Automated design of virtual worlds for visualizing multivariate relations,"Interactive visualization systems provide a powerful means to explore complex data, especially when coupled with 3-D interaction and display devices to produce virtual worlds. While designing a quality static 2-D visualization is already a difficult task for most users, designing an interactive 3-D one is even more challenging. To address this problem, AutoVisual, a research system that designs interactive virtual worlds for visualizing and exploring multivariate relations of arbitrary arity, is being developed. AutoVisual uses worlds within worlds, an interactive visualization technique that exploits nested, heterogeneous coordinate systems to map multiple variables onto each spatial dimension. AutoVisual's designs are guided by user-specified visualization tasks, and by a catalog of design principles encoded using a rule-based language.<<ETX>>",1992-10-19,https://www.semanticscholar.org/paper/dcca8078ce4aafa9588b98c84a34214a7a3d0ee2,Proceedings Visualization '92
2732,Virtual worlds for visualizing information,"Virtual worlds are computer-generated environments created by coupling 3-D displays and interaction devices to powerful graphics workstations. The author describes work in the design of virtual worlds being carried out by Columbia's Computer Graphics and User Interfaces Group Two of the main themes of this group's research are exploiting true 3-D interaction and display devices to visualize and manipulate rich information spaces, and using artificial-intelligence techniques to automate the generation of effective graphics. The projects discussed address virtual worlds for visualizing multivariate data, hybrid user interfaces that merge 2-D and 3-D displays and interaction devices, and augmented realities in which the surrounding physical world is annotated with knowledge-based 3-D graphics.<<ETX>>",1992-12-01,https://www.semanticscholar.org/paper/de518d72d7577a09feacd0dda3d222d7e14830d2,Proceedings Supercomputing '92
724,Joint Cyber and Physical Attacks on Power Grids: Graph Theoretical Approaches for Information Recovery,"Recent events demonstrated the vulnerability of power grids to cyber attacks and to physical attacks. Therefore, we focus on joint cyber and physical attacks and develop methods to retrieve the grid state information following such an attack. We consider a model in which an adversary attacks a zone by physically disconnecting some of its power lines and blocking the information flow from the zone to the grid's control center. We use tools from linear algebra and graph theory and leverage the properties of the power flow DC approximation to develop methods for information recovery. Using information observed outside the attacked zone, these methods recover information about the disconnected lines and the phase angles at the buses. We identify sufficient conditions on the zone structure and constraints on the attack characteristics such that these methods can recover the information. We also show that it is NP-hard to find an approximate solution to the problem of partitioning the power grid into the minimum number of attack-resilient zones. However, since power grids can often be represented by planar graphs, we develop a constant approximation partitioning algorithm for these graphs. Finally, we numerically study the relationships between the grid's resilience and its structural properties, and demonstrate the partitioning algorithm on real power grids. The results can provide insights into the design of a secure control network for the smart grid.",2015-06-15,https://www.semanticscholar.org/paper/851ad77e848f708bc42dd8e885420fe7493308ef,Measurement and Modeling of Computer Systems
3525,Approximating the Minimum-Cost Maximum Flow is P-Complete,,1992-07-24,https://www.semanticscholar.org/paper/4a2c620056a97f1f7dcd09fdf63223668fbd9723,Information Processing Letters
2253,Sodium Salicylate Promotes Neutrophil Apoptosis by Stimulating Caspase-Dependent Turnover of Mcl-11,"Mcl-1 is an antiapoptotic member of the Bcl-2 family of proteins that plays a central role in cell survival of neutrophils and other cells. The protein is unusual among family members in that it has a very short half-life of 2–3 h. In this report, we show that sodium salicylate (at 10 mM) greatly enhances the rate at which neutrophils undergo apoptosis and, in parallel, greatly accelerates the turnover rate of Mcl-1, decreasing its half-life to only 90 min. Whereas constitutive and GM-CSF-modified Mcl-1 turnover is regulated by the proteasome, the accelerated sodium salicylate-induced Mcl-1 turnover is mediated largely via caspases. Sodium salicylate resulted in rapid activation of caspase-3, -8, -9, and -10, and salicylate-accelerated Mcl-1 turnover was partly blocked by caspase inhibitors. Sodium salicylate also induced dramatic changes in the activities of members of the MAPK family implicated in Mcl-1 turnover and apoptosis. For example, sodium salicylate blocked GM-CSF-stimulated Erk and Akt activation, but resulted in rapid and sustained activation of p38-MAPK, an event mimicked by okadaic acid that also accelerates Mcl-1 turnover and neutrophil apoptosis. These data thus shed important new insights into the dynamic and highly regulated control of neutrophil apoptosis that is effected by modification in the rate of Mcl-1 turnover.",2006-01-15,https://www.semanticscholar.org/paper/88f20cf9bb6e965aae3eaa093ba75b5930c869af,Journal of Immunology
513,On the predictability of coupled automata: an allegory about chaos,The authors show a sharp dichotomy between systems of identical automata with symmetric global control whose behavior is easy to predict and those whose behavior is hard to predict. The division pertains to whether the global control rule is invariant with respect to permutations of the states of the automaton. It is also shown that testing whether the global control rule has this invariance property is an undecidable problem. It is argued that there is a natural analog between complexity in the present model and chaos in dynamical systems.<<ETX>>,1990-10-22,https://www.semanticscholar.org/paper/78723b772ec90a57f66fc47e55fd7b8cee3737b9,Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
745,Small Approximate Pareto Sets for Biobjective Shortest Paths and Other Problems,"We investigate the problem of computing a minimum set of solutions that approximates within a specified accuracy $\epsilon$ the Pareto curve of a multiobjective optimization problem. We show that for a broad class of biobjective problems (containing many important widely studied problems such as shortest paths, spanning tree, matching, and many others), we can compute in polynomial time an $\epsilon$-Pareto set that contains at most twice as many solutions as the minimum set. Furthermore we show that the factor of 2 is tight for these problems; i.e., it is NP-hard to do better. We present upper and lower bounds for three or more objectives, as well as for the dual problem of computing a specified number $k$ of solutions which provide a good approximation to the Pareto curve.",2009-09-01,https://www.semanticscholar.org/paper/5430943cf37c3ffbba41198ebeca45db2c99064b,SIAM journal on computing (Print)
2978,Distinct Epigenomic Features in End-Stage Failing Human Hearts,"Background— The epigenome refers to marks on the genome, including DNA methylation and histone modifications, that regulate the expression of underlying genes. A consistent profile of gene expression changes in end-stage cardiomyopathy led us to hypothesize that distinct global patterns of the epigenome may also exist. Methods and Results— We constructed genome-wide maps of DNA methylation and histone-3 lysine-36 trimethylation (H3K36me3) enrichment for cardiomyopathic and normal human hearts. More than 506 Mb sequences per library were generated by high-throughput sequencing, allowing us to assign methylation scores to ≈28 million CG dinucleotides in the human genome. DNA methylation was significantly different in promoter CpG islands, intragenic CpG islands, gene bodies, and H3K36me3-enriched regions of the genome. DNA methylation differences were present in promoters of upregulated genes but not downregulated genes. H3K36me3 enrichment itself was also significantly different in coding regions of the genome. Specifically, abundance of RNA transcripts encoded by the DUX4 locus correlated to differential DNA methylation and H3K36me3 enrichment. In vitro, Dux gene expression was responsive to a specific inhibitor of DNA methyltransferase, and Dux siRNA knockdown led to reduced cell viability. Conclusions— Distinct epigenomic patterns exist in important DNA elements of the cardiac genome in human end-stage cardiomyopathy. The epigenome may control the expression of local or distal genes with critical functions in myocardial stress response. If epigenomic patterns track with disease progression, assays for the epigenome may be useful for assessing prognosis in heart failure. Further studies are needed to determine whether and how the epigenome contributes to the development of cardiomyopathy.",2011-11-29,https://www.semanticscholar.org/paper/293a1b0812f5dc2b130927873646f0828137dcfe,Circulation
2442,Hybrid UIs for Music Exploration in AR and VR,"We present hybrid user interfaces that facilitate interaction with music content in 3D, using a combination of 2D and 3D input and display devices. Participants will explore an online music library, some wearing AR or VR head-worn displays used alone or in conjunction with touch screens, and others using only touch screens. They will select genres, artists, albums, and songs, interacting through a combination of 3D hand-tracking and 2D multi-touch technologies.",2018-10-01,https://www.semanticscholar.org/paper/e069b87075ef3615af616dc74cee28872db21b96,2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)
1850,Combining Stochastic Block Models and Mixed Membership for Statistical Network Analysis,,2006-06-29,https://www.semanticscholar.org/paper/b82c2c26c44358e140b6e2eb5537563939cd3ab9,SNA@ICML
898,Deleting completed transactions,We derive necessary and suflicient conditions on when it is safe to forget (and remove) a completed transaction in several versions of conflict-graph-based schedulers. We show that the conditions can be applied repeatedly and analyze their complexity.,1985-06-01,https://www.semanticscholar.org/paper/bd10d8321209fc74fb3cc68ff277f82b4efa75a3,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems
2811,Galectins in acute and chronic inflammation,"Galectins are animal lectins that bind to β‐galactosides, such as lactose and N‐acetyllactosamine, in free form or contained in glycoproteins or glycolipids. They are located intracellularly or extracellularly. In the latter they exhibit bivalent or multivalent interactions with glycans on cell surfaces and induce various cellular responses, including production of cytokines and other inflammatory mediators, cell adhesion, migration, and apoptosis. Furthermore, they can form lattices with membrane glycoprotein receptors and modulate receptor properties. Intracellular galectins can participate in signaling pathways and alter biological responses, including apoptosis, cell differentiation, and cell motility. Current evidence indicates that galectins play important roles in acute and chronic inflammatory responses, as well as other diverse pathological processes. Galectin involvement in some processes in vivo has been discovered, or confirmed, through studies of genetically engineered mouse strains, each deficient in a given galectin. Current evidence also suggests that galectins may be therapeutic targets or employed as therapeutic agents for these inflammatory responses.",2012-04-01,https://www.semanticscholar.org/paper/ccc9d779756383e39c1f75771a60a4fbdb3a709b,Annals of the New York Academy of Sciences
1960,The cooperative estimation of distribution algorithm: a novel approach for semiconductor final test scheduling problems,,2014-10-01,https://www.semanticscholar.org/paper/7007373cdac029a224e4a69e6a4f75a0f6e489f2,Journal of Intelligent Manufacturing
632,The complexity of the capacitated tree problem,"We examine the complexity of a classical problem related to the design of centralized computer networks. Under very broad assumptions the problem is shown to be NP-complete, and hence most probably intractable. The same result holds for the “Euclidean” case of the problem; however, in the latter case a simple algorithm produces solutions with relative error almost certainly arbitrarily close to zero.",1978-09-01,https://www.semanticscholar.org/paper/44119d4db050c65e4156ec1b43cb6c7a85d531a6,Networks
2364,Impaired neutrophil killing in a patient with defective degranulation of myeloperoxidase.,"A case of recurrent, superficial abscesses in an 18 year old girl, is described. Staphylococcus aureus was the pathogen most often implicated and on several occasions the abscesses required surgical drainage. Defects in humoral immunity, neutrophil chemotaxis or opsonophagocytosis were not observed. However, her neutrophil's ability to kill ingested S. aureus in vitro was impaired. This was associated with impaired luminol-dependent chemiluminescence in response to stimulation by either latex beads, or the chemotactic peptide FMLP plus cytochalasin B. Oxygen uptake and superoxide anion production were normal but release of myeloperoxidase by this patient's neutrophils occurred more slowly and to a lower extent than in control cells. These data suggest that the recurrent infections and diminished in vitro neutrophil bactericidal activity observed in this patient are associated with impaired degranulation of myeloperoxidase.",1988-04-01,https://www.semanticscholar.org/paper/8fa6bd51379d11d50f5317c728a67e6b5c0fbeae,Journal of clinical & laboratory immunology
133,"Adaptive, Deadlock-Free Packet Routing in Torus Networks with Minimal Storage",,,https://www.semanticscholar.org/paper/fa198c2e553847f637d237944459fe1b8eb5540f,International Conference on Parallel Processing
287,Discretized Multinomial Distributions and Nash Equilibria in Anonymous Games,"We show that there is a polynomial-time approximation scheme for computing Nash equilibria in anonymous games with any fixed number of strategies (a very broad and important class of games), extending the two-strategy result of Daskalakis and Papadimitriou 2007. The approximation guarantee follows from a probabilistic result of more general interest: The distribution of the sum of n independent unit vectors with values ranging over {e1,...,ek}, where ei is the unit vector along dimension i of the k-dimensional Euclidean space, can be approximated by the distribution of the sum of another set of independent unit vectors whose probabilities of obtaining each value are multiples of 1/z for some integer z, and so that the variational distance of the two distributions is at most eps, where eps is bounded by an inverse polynomial in z and a function of k, but with no dependence on n. Our probabilistic result specifies the construction of a surprisingly sparse epsi-cover- under the total variation distance - of the set of distributions of sums of independent unit vectors, which is of interest on its own right.",2008-08-20,https://www.semanticscholar.org/paper/51146d4e060094b2d99bb4e3f7fe353b38b4fd22,2008 49th Annual IEEE Symposium on Foundations of Computer Science
763,Analysis of Recursive Probabilistic Models,,2006-10-23,https://www.semanticscholar.org/paper/723a564507f1fcb1f0ce3043c3b67f9d958d55ff,Automated Technology for Verification and Analysis
1322,Experimental discrimination between charge 2e/3 top quark and charge 4e/3 exotic quark production scenarios.,"We present the first experimental discrimination between the 2e/3 and 4e/3 top quark electric charge scenarios, using top quark pairs (tt) produced in pp collisions at (square root) s = 1.96 TeV by the Fermilab Tevatron Collider. We use 370 pb;{-1} of data collected by the D0 experiment and select events with at least one high transverse momentum electron or muon, high transverse energy imbalance, and four or more jets. We discriminate between b- and b-quark jets by using the charge and momenta of tracks within the jet cones. The data are consistent with the expected electric charge, |q|=2e/3. We exclude, at the 92% C.L., that the sample is solely due to the production of exotic quark pairs QQ with |q|=4e/3. We place an upper limit on the fraction of QQ pairs rho<0.80 at the 90% C.L.",2006-08-16,https://www.semanticscholar.org/paper/92249b39b5413e18cf0d0ccfa05b95f83a692b53,Physical Review Letters
1896,Electronic Design Automation (EDA) Scheduling System in IC Design Industry,"With the global competition and short life cycle in semiconductor industry, the IC design companies try to keep their competitiveness. Besides the human resources and advanced technology, enhancing the speed of the development is an important method to enhance market share. IC designers usually use electronic design automation tool to shorten the development. However, the EDA tools and resources are expensive and limited. Hence, it becomes a critical problem to allocate the resources and schedule the jobs. This study aimed to develop an IC design job scheduling decision support system framework. The framework enhances the throughput and reduces the idle. This study implements the proposed framework in practice. The empirical study conducted in IC design company to validate the developed solution and model implementation.",2019-04-01,https://www.semanticscholar.org/paper/74500f3a8dd0b456b9dea3111abc29fc69de1777,3D Structure from Multiple Images of Large-Scale Environments
2480,Evaluating subtle cueing in head-worn displays,"Goal-oriented visual search in Augmented Reality (AR) can be facilitated by using visual cues to call attention to the target. However, traditional use of explicit cues may degrade visual search performance. In contrast, Subtle Cueing has been previously proposed as an alternative to explicit cueing, but little is known about how well it performs in head-tracked head worn displays (HWDs).
 Using visual search research methods in simulated augmented reality environments, our user study found that Subtle Cueing improves visual search performance in HWDs, and revealed a phenomenon whereby subjects were not able to see the target, even though subjects were primed and the target was well within view.",2014-04-26,https://www.semanticscholar.org/paper/9fdceb03ad6a9579a20b207fde866566311a9792,Chinese CHI '14
228,"Algorithms, games, and evolution","Significance Theoretical biology was founded on the mathematical tools of statistics and physics. We believe there are productive connections to be made with the younger field of theoretical computer science, which shares with it an interest in complexity and functionality. In this paper, we find that the mathematical description of evolution in the presence of sexual recombination and weak selection is equivalent to a repeated game between genes played according to the multiplicative weight updates algorithm, an algorithm that has surprised computer scientists time and again in its usefulness. This equivalence is informative for the pursuit of two key problems in evolution: the role of sex and the maintenance of variation. Even the most seasoned students of evolution, starting with Darwin himself, have occasionally expressed amazement that the mechanism of natural selection has produced the whole of Life as we see it around us. There is a computational way to articulate the same amazement: “What algorithm could possibly achieve all this in a mere three and a half billion years?” In this paper we propose an answer: We demonstrate that in the regime of weak selection, the standard equations of population genetics describing natural selection in the presence of sex become identical to those of a repeated game between genes played according to multiplicative weight updates (MWUA), an algorithm known in computer science to be surprisingly powerful and versatile. MWUA maximizes a tradeoff between cumulative performance and entropy, which suggests a new view on the maintenance of diversity in evolution.",2014-06-16,https://www.semanticscholar.org/paper/61e2e06431c6245f3bd4f8668517044f6f459a75,Proceedings of the National Academy of Sciences of the United States of America
3604,Supporting SELL for High-Performance Computing,,2005-10-20,https://www.semanticscholar.org/paper/76669a17732fbc84e4f7f94ceb30ef34fa73c0cf,International Workshop on Languages and Compilers for Parallel Computing
701,Planar Graphs that Need Four Pages,,2020-05-28,https://www.semanticscholar.org/paper/8a87f3b42b21b3f3518f35f8ecbd1b914fe115bf,Journal of combinatorial theory. Series B (Print)
847,On the approximation of maximum satisfiability,"We present a 3/4 polynomial time approximation algorithm for the Maximum Satisfiability problem: Given a set of clauses, find a truth assignment that satisfies the maximum number of clauses. The algorithm applies to the weighted case as well, and involves nontrival application of network flow techniques.",1992-09-01,https://www.semanticscholar.org/paper/3a078ce26e2105fffb7f58fd1a58334184f4f84a,ACM-SIAM Symposium on Discrete Algorithms
3468,Approximation Algorithms for Single-Source Unsplittable Flow,"In the single-source unsplittable flow problem, we are given a network G, a source vertex s, and k commodities with sinks ti and real-valued demands $\rho_i,$ $1\leq i \leq k.$ We seek to route the demand $\rho_i$ of each commodity i along a single s-ti flow path so that the total flow routed across any edge e is bounded by the edge capacity ue. The conceptual difficulty of this NP-hard problem arises from combining packing constraints due to the existence of capacities with path selection in a graph of arbitrary topology. In this paper we give a generic framework, which yields approximation algorithms that are simpler than those previously known and achieve significant improvements upon the approximation ratios. Our framework, with appropriate subroutines, applies to all optimization versions previously considered and, unlike previous work, treats in a unified manner directed and undirected graphs. We provide extensions of our algorithms which yield the best possible approximation guarantees for restricted sets of demand values and an associated scheduling problem.",2002-03-01,https://www.semanticscholar.org/paper/4bff0aba150b151b329915f028ce66e1f7b831ef,SIAM journal on computing (Print)
2103,Data value development to enhance competitive advantage: a retrospective study of EDA systems for semiconductor fabrication,"In semiconductor manufacturing, reducing cycle time, producing high quality products, on-time delivery, continual reduction of costs and improving efficiency are the most direct and effective ways to create value for customers. However, the semiconductor manufacturing process is extremely complex. Without good support from fab engineers, creating value for customers is difficult. An Engineering Data Analysis (EDA) system is a very important off-line analysis-oriented system that can be used to support yield analysis and quality improvement for semiconductor fabrication. However, due to the data integration, system design, and requirement for cooperation among domain experts, IT specialists, and statisticians, to develop and deploy an in-house EDA system is difficult. A retrospective study of a semiconductor manufacturing company is presented; the company has more than ten years' experience of developing the in-house EDA system. In particular, the process of developing the system is reviewed, including design thinking and information design. An EDA system that is currently used in an 8-inch fab is introduced. This study identifies several hidden factors that may affect the successful development of an EDA system. Prescriptions for overcoming these difficulties are also given in this study.",,https://www.semanticscholar.org/paper/089666921f9db1f755e14a0ac47291e296da6de2,International Journal of Services Technology and Management
2710,Research frontiers in virtual reality,"The emergence of teleoperation and virtual environments has greatly increased interest in ""synthetic experience"", a mode of experience made possible by both these newer technologies and earlier ones, such as telecbrnmunicarion and sensory prosthetics. I maintain that understanding synthetic experience must begin by recognizing the fallacy of naive realism and with the recognition that the phenomenology of synthetic experience is conrinuous with that of ordinary experience. I demonstrate the continuity of synrhetic experience and normal perceptual experience with respect to two issues: the determination of' a person's phenomenal location in space and the experience of ""being in touch with"" near and remote objects. The emergence of teleoperation and virtual environments has greatly increased interest in ""synthetic experience"" [I], those forms of experience which are made possible both by these newer technologies and by earlier ones, such as telecommunication and sensory prosthetics. Recently, a number of authors have offered a variety of taxonomies and conceptual schemes for thinking about the experiential states associated with synthetic experience (e.g., presence. externalization) and the properties of the effectoridisplay arrangement that promote various degrees of perceptual realism [I, 2-91. Here I assert that an understanding of synthetic experience must begin by acknowledging that the phenomenology of synthetic experience is continuous with that of ordinary experience. In previous articles [6, 71, I have argued that in seeking to understand the phenomenology associated with the use of teleoperators and virtual environments, we must recognize the fallacy of naive realism. the unreflective view that the world we experience around us is one and the same as the ""physical world."" Vision, for example. is experienced as a transaction between observer and environment in which the eyes are mere windows on the physical world. This view fails to recognize that ordinary experience is mediate-that what we experience is an elaborate construction of o w senses and nervous system that is so highly functional a representation of the surrounding environment that we unsuspectingly act upon the former as if it were the latter. In its place, we need to substitute a form of representative realism that makes the distinction between the phenomenal world. that of which we are directly aware, and the physical world. that which underlies our phenomenal awareness but can only be known through inference. Among the facts that demand this alternate view is the mchromacy of human color vision-while color is part of the very fabric of the …",1994-07-24,https://www.semanticscholar.org/paper/fe4df9c4d855a94f0458e315298f60e2163fcac9,International Conference on Computer Graphics and Interactive Techniques
3156,SMART UNIX SVR4 support for multimedia applications,"Multimedia applications have dynamic and adaptive real-time requirements. Current scheduling practice, as typified by UNIX System V Release 4, lacks the necessary information and interfaces to meet these requirements. To address this problem, we have created the SMART (Scheduler for Multimedia And Real-Time) interface. It explicitly accounts for application-specific time constraints and provides dynamic feedback from the scheduler to applications to allow them to adapt to the system loading condition. This paper describes the design of the interface and its implementation in the Solaris UNIX operating system to provide an effective SVR4-conformant full featured environment for supporting multimedia applications.",1997-06-03,https://www.semanticscholar.org/paper/d7693012c522efbdc59f784214b711836048f58e,Proceedings of IEEE International Conference on Multimedia Computing and Systems
2726,Generating Cross-References for Multimedia Explanation,"When explanations include multiple media, such as text and illustrations, a reference to an object can be made through a combination of media. We call part of a presentation that references material elsewhere a cross-reference. We are concerned here with how textual expressions can refer to parts of accompanying illustrations. The illustration to which a cross-reference refers should also satisfy the specific goal of identifying an object for the user. Thus, producing an effective cross-reference not only involves text generation, but may also entail modifying or replacing an existing illustration and in some cases, generating an illustration where previously none was needed. In this paper, we describe the different types of cross-references that COMET (COordinated Multimedia Explanation Testbed) generates and show the roles that both its text and graphics generators play in this process.",1992-07-12,https://www.semanticscholar.org/paper/2cac67b2b327512ce2c44df4f97632db5b434715,AAAI Conference on Artificial Intelligence
1797,Variational Inference for the Nested Chinese Restaurant Process,"The nested Chinese restaurant process (nCRP) is a powerful nonparametric Bayesian model for learning tree-based hierarchies from data. Since its posterior distribution is intractable, current inference methods have all relied on MCMC sampling. In this paper, we develop an alternative inference technique based on variational methods. To employ variational methods, we derive a tree-based stick-breaking construction of the nCRP mixture model, and a novel variational algorithm that efficiently explores a posterior over a large set of combinatorial structures. We demonstrate the use of this approach for text and hand written digits modeling, where we show we can adapt the nCRP to continuous data as well.",2009-12-07,https://www.semanticscholar.org/paper/2f14e3b459dc78868851c372ae00a74519c3e1f4,Neural Information Processing Systems
208,Does Information Revelation Improve Revenue?,"We study the problem of optimal auction design in a valuation model, explicitly motivated by online ad auctions, in which there is two-way informational asymmetry, in the sense that private information is available to both the seller (the item type) and the bidders (their type), and the value of each bidder for the item depends both on his own and the item's type. Importantly, we allow arbitrary auction formats involving, potentially, several rounds of signaling from the seller and decisions by the bidders, and seek to find the optimum co-design of signaling and auction (we call this optimum the ""optimum augmented auction""). We characterize exactly the optimum augmented auction for our valuation model by establishing its equivalence with a multi-item Bayesian auction with additive bidders. Surprisingly, in the optimum augmented auction there is no signaling whatsoever, and in fact the seller need not access the available information about the item type until after the bidder chooses his bid. Suboptimal solutions to this problem, which have appeared in the recent literature, are shown to correspond to well-studied ways to approximate multi-item auctions by simpler formats, such as grand-bundling (this corresponds to Myerson's auction without any information revelation), selling items separately (this corresponds to Myerson's auction preceded by full information revelation as in [Fu et al. 2012]), and fractional partitioning (this corresponds to Myerson's auction preceded by optimal signaling). Consequently, all these solutions are separated by large approximation gaps from the optimum revenue.",2016-07-21,https://www.semanticscholar.org/paper/bad79181ff909385a33f6f11b7116e12ea9ce469,ACM Conference on Economics and Computation
1461,Investigation of the electromagnetic structure of eta and eta ' mesons by two-photon interactions.,"The TPC/Two-Gamma facility at the SLAC {ital e}{sup +}{ital e}{sup {minus}} storage ring PEP was used to study the reactions {gamma}{gamma}{sup *}{r arrow}{eta} and {gamma}{gamma}{sup *}{r arrow}{eta}{prime}. The {eta}{gamma}{sup *}{gamma} and {eta}{prime}{gamma}{sup *}{gamma} transition form factors were measured as functions of {ital Q}{sup 2}, the negative of the invariant mass squared of the tagged photon, in the range 0.1{lt}{ital Q}{sup 2}{lt}7 GeV{sup 2}. These determinations of the electromagnetic structure of the {eta} and {eta}{prime} mesons are consistent with both vector-meson dominance and QCD. They also provide new measurements of the pseudoscalar mixing angle and decay constants.",1990-01-08,https://www.semanticscholar.org/paper/7e68e8b1074df47dcc8ba48e8d68e900c0980dd0,Physical Review Letters
2531,SiteLens: situated visualization techniques for urban site visits,"Urban designers and urban planners often conduct site visits prior to a design activity to search for patterns or better understand existing conditions. We introduce SiteLens, an experimental system and set of techniques for supporting site visits by visualizing relevant virtual data directly in the context of the physical site, which we call situated visualization. We address alternative visualization representations and techniques for data collection, curation, discovery, comparison, manipulation, and provenance. A real use scenario is presented and two iterations of evaluation with faculty and students from the Columbia University Graduate School of Architecture, Planning and Preservation provide directions and insight for further investigation.",2009-04-04,https://www.semanticscholar.org/paper/44c9f67c40a6abfdb80b5cdda6de8fe7c939dd4a,International Conference on Human Factors in Computing Systems
1535,Adjusting for indirectly measured confounding using large-scale propensity score,,2021-10-23,https://www.semanticscholar.org/paper/06546042da3547154d7446832dd5943c0b92bcbe,Journal of Biomedical Informatics
3654,Type-Safe Linkage for C++,This paper describes the problems involved in generating names for overloaded functions in C++ and in linking to C programs. It also discusses how these problems relate to library building. It presents a solution that provides a degree of type-safe linkage. This eliminates several classes of errors from C++ and allows libraries to be composed more freely than has hitherto been possible. Finally the current encoding scheme for C++ names is presented.,,https://www.semanticscholar.org/paper/4ab04a6cf34213abb4bccaa51c8692d9318ce1a0,Computing Systems
3227,Tightly Bunched Herding Improves Cattle Performance in African Savanna Rangeland☆,,2018-07-01,https://www.semanticscholar.org/paper/7c92d54a511c1fc915a1f8c0d6001946b623f5a4,Rangeland Ecology & Management
1546,Assessing the Effects of Friend-to-Friend Texting onTurnout in the 2018 US Midterm Elections,"Recent mobile app technology lets people systematize the process of messaging their friends to urge them to vote. Prior to the most recent US midterm elections in 2018, the mobile app Outvote randomized an aspect of their system, hoping to unobtrusively assess the causal effect of their users’ messages on voter turnout. However, properly assessing this causal effect is hindered by multiple statistical challenges, including attenuation bias due to mismeasurement of subjects’ outcomes and low precision due to two-sided non-compliance with subjects’ assignments. We address these challenges, which are likely to impinge upon any study that seeks to randomize authentic friend-to-friend interactions, by tailoring the statistical analysis to make use of additional data about both users and subjects. Using meta-data of users’ in-app behavior, we reconstruct subjects’ positions in users’ queues. We use this information to refine the study population to more compliant subjects who were higher in the queues, and we do so in a systematic way which optimizes a proxy for the study’s power. To mitigate attenuation bias, we then use ancillary data of subjects’ matches to the voter rolls that lets us refine the study population to one with low rates of outcome mismeasurement. Our analysis reveals statistically significant treatment effects from friend-to-friend mobilization efforts ( 8.3, CI = (1.2, 15.3)) that are among the largest reported in the get-out-the-vote (GOTV) literature. While social pressure from friends has long been conjectured to play a role in effective GOTV treatments, the present study is among the first to assess these effects experimentally.",2021-04-19,https://www.semanticscholar.org/paper/933393df9068059dd49f555eaa67f5f9ce58aeec,The Web Conference
3185,Structural change in agriculture and farmers' social contacts: Insights from a Swiss mountain region,,2022-06-01,https://www.semanticscholar.org/paper/eba03ab1a7cd63e07aecc361e6fd1b82abb31ae4,Agricultural Systems
1863,"Learning with Scope, with Application to Information Extraction and Classification","In probabilistic approaches to classification and information extraction, one typically builds a statistical model of words under the assumption that future data will exhibit the same regularities as the training data. In many data sets, however, there are scope-limited features whose predictive power is only applicable to a certain subset of the data. For example, in information extraction from web pages, word formatting may be indicative of extraction category in different ways on different web pages. The difficulty with using such features is capturing and exploiting the new regularities encountered in previously unseen data. In this paper, we propose a hierarchical probabilistic model that uses both local/scope-limited features, such as word formatting, and global features, such as word content. The local regularities are modeled as an unobserved random parameter which is drawn once for each local data set. This random parameter is estimated during the inference process and then used to perform classification with both the local and global features--- a procedure which is akin to automatically retuning the classifier to the local regularities on each newly encountered web page. Exact inference is intractable and we present approximations via point estimates and variational methods. Empirical results on large collections of web data demonstrate that this method significantly improves performance from traditional models of global features alone.",2002-08-01,https://www.semanticscholar.org/paper/41eba1efcd55beb1b70db260dc6f070f7317b71b,Conference on Uncertainty in Artificial Intelligence
2284,Fcgamma receptors in autoimmune diseases.,"Fcgamma-receptors (Fcgamma-R) recognise the Fc portion of IgG and thus form a link between humoral and cellular immunity. These receptors are expressed by a variety of immune cells, and they function in the binding of immune complexes or IgG-opsonised particles, such as microbial pathogens. The are three major types of Fcgamma-R, namely Fcgamma-RI (CD64), Fcgamma-RII (CD32) and Fcgamma-RIII (CD16), and these differ in their ability to bind IgG and complexes. There are many isoforms of these receptors and a number of recently identified polymorphisms in their structure. This review describes the structure and function of these Fcgamma-Rs, and highlights how gene deficiencies and polymorphisms may contribute to the pathology of human diseases.",,https://www.semanticscholar.org/paper/5507a98d6d12cd96bf5b3bb78d42691f852e4ab4,European Journal of Clinical Investigation
2893,LDmat: efficiently queryable compression of linkage disequilibrium matrices,"Abstract Motivation Linkage disequilibrium (LD) matrices derived from large populations are widely used in population genetics in fine-mapping, LD score regression, and linear mixed models for Genome-wide Association Studies (GWAS). However, these matrices can reach large sizes when they are derived from millions of individuals; hence, moving, sharing and extracting granular information from this large amount of data can be cumbersome. Results We sought to address the need for compressing and easily querying large LD matrices by developing LDmat. LDmat is a standalone tool to compress large LD matrices in an HDF5 file format and query these compressed matrices. It can extract submatrices corresponding to a sub-region of the genome, a list of select loci, and loci within a minor allele frequency range. LDmat can also rebuild the original file formats from the compressed files. Availability and implementation LDmat is implemented in python, and can be installed on Unix systems with the command ‘pip install ldmat’. It can also be accessed through https://github.com/G2Lab/ldmat and https://pypi.org/project/ldmat/. Supplementary information Supplementary data are available at Bioinformatics online.",2023-02-01,https://www.semanticscholar.org/paper/60688664eef3ab172b8f2c4eaee7bcee2a7806d1,Bioinformatics
1216,Search for decay of a fermiophobic Higgs boson hf-->gammagamma with the D0 detector at sqrt s=1.96 TeV.,We report the results of a search for a narrow resonance decaying into two photons in 1.1 fb;{-1} of data collected by the D0 experiment at the Fermilab Tevatron Collider during the period 2002-2006. We find no evidence for such a resonance and set a lower limit on the mass of a fermiophobic Higgs boson of m_{h_{f}}>100 GeV at the 95% C.L. This exclusion limit exceeds those obtained in previous searches at the Fermilab Tevatron and covers a significant region of the parameter space B(h_{f}-->gammagamma) vs m_{h_{f}} which was not accessible at the CERN Large Electron-Positron Collider.,2008-07-29,https://www.semanticscholar.org/paper/4b7170d983febb0f8db5e0067a4b60fdb12453e7,Physical Review Letters
1778,Spatial distance dependent Chinese restaurant processes for image segmentation,"The distance dependent Chinese restaurant process (ddCRP) was recently introduced to accommodate random partitions of non-exchangeable data [1]. The dd-CRP clusters data in a biased way: each data point is more likely to be clustered with other data that are near it in an external sense. This paper examines the dd-CRP in a spatial setting with the goal of natural image segmentation. We explore the biases of the spatial ddCRP model and propose a novel hierarchical extension better suited for producing ""human-like"" segmentations. We then study the sensitivity of the models to various distance and appearance hyperparameters, and provide the first rigorous comparison of nonparametric Bayesian models in the image segmentation domain. On unsupervised image segmentation, we demonstrate that similar performance to existing nonparametric Bayesian models is possible with substantially simpler models and algorithms.",2011-12-12,https://www.semanticscholar.org/paper/ecfbd1a30a243f15610fa6b76907f8455560da3a,Neural Information Processing Systems
626,Efficient Search for Rationals,,,https://www.semanticscholar.org/paper/95b31c5b81fc523db34fe1902e7c84d970525c80,Information Processing Letters
3571,Scalable nonblocking concurrent objects for mission critical code,"The high degree of complexity and autonomy of future robotic space missions, such as Mars Science Laboratory (MSL), poses serious challenges in assuring their reliability and efficiency. Providing fast and safe concurrent synchronization is of critical importance to such autonomous embedded software systems. The application of nonblocking synchronization is known to help eliminate the hazards of deadlock, livelock, and priority inversion. The nonblocking programming techniques are notoriously difficult to implement and offer a variety of semantic guarantees and usability and performance trade-offs. The present software development and certification methodologies applied at NASA do not reach the level of detail of providing guidelines for the design of concurrent software. The complex task of engineering reliable and efficient concurrent synchronization is left to the programmer's ingenuity. A number of Software Transactional Memory (STM) approaches gained wide popularity because of their easy to apply interfaces, but currently fail to offer scalable nonblocking transactions. In this work we provide an in-depth analysis of the nonblocking synchronization semantics and their applicability in mission critical code. We describe a generic implementation of a methodology for scalable implementation of concurrent objects. Our performance evaluation demonstrates that our approach is practical and outperforms the application of nonblocking transactions by a large factor. In addition, we apply our Descriptor-based approach to provide a solution to the fundamental ABA problem. Our ABA prevention scheme, called the lambda-delta approach, outperforms by a large factor the use of garbage collection for the safe management of each shared location. It offers speeds comparable to the application of the architecture-specific CAS2 instruction used for version counting. The lambda-delta approach is an ABA prevention technique based on classification of concurrent operations and 3-step execution of a Descriptor object. A practical alternative to the application of CAS2 is particularly important for the engineering of embedded systems.",2009-10-25,https://www.semanticscholar.org/paper/51f9449505dfcd8525e026d5aa345754ff05c526,OOPSLA Companion
3277,"Stigmergy, collective actions, and animal social spacing","Significance Marking animals avoid locations recently visited by others. We conceptualized this time nonlocal avoidance behavior as stigmergy, a form of mediated interaction that gives rise to coordinated behavior from seemingly independent individuals. In so doing, the concept of stigmergy is used beyond the realm of eusocial insects. To link the population spatiotemporal patterns that emerge from the individual nonlocal rules of interaction, we construct a collective movement model whereby randomly moving animals have the tendency to avoid marks left by a conspecific, depending on the age of the mark. As a result, we are able to quantify animal decision-making processes in terms of current and past locations of other individuals, linking behavior to history-dependent actions. Collective animal behavior studies have led the way in developing models that account for a large number of individuals, but mostly have considered situations in which alignment and attraction play a key role, such as in schooling and flocking. By quantifying how animals react to one another’s presence, when interaction is via conspecific avoidance rather than alignment or attraction, we present a mechanistic insight that enables us to link individual behavior and space use patterns. As animals respond to both current and past positions of their neighbors, the assumption that the relative location of individuals is statistically and history independent is not tenable, underscoring the limitations of traditional space use studies. We move beyond that assumption by constructing a framework to analyze spatial segregation of mobile animals when neighbor proximity may elicit a retreat, and by linking conspecific encounter rate to history-dependent avoidance behavior. Our approach rests on the knowledge that animals communicate by modifying the environment in which they live, providing a method to analyze social cohesion as stigmergy, a form of mediated animal–animal interaction. By considering a population of animals that mark the terrain as they move, we predict how the spatiotemporal patterns that emerge depend on the degree of stigmergy of the interaction processes. We find in particular that nonlocal decision rules may generate a nonmonotonic dependence of the animal encounter rate as a function of the tendency to retreat from locations recently visited by other conspecifics, which has fundamental implications for epidemic disease spread and animal sociality.",2013-09-30,https://www.semanticscholar.org/paper/3b66088991868728d65dcc1c8848492a1c1aa747,Proceedings of the National Academy of Sciences of the United States of America
1893,Real Time Electronic Design Automation (EDA) Scheduling System in IC Design Industry,"Because of the global competition and short life cycle in semiconductor industry, the IC design companies try to keep their competitiveness. Besides the human resources and advanced technology, enhancing the speed of the development is important for market share and time to market. IC designers usually use electronic design automation tool to shorten and verify the development. However, the EDA tools and servers are expensive and limited. Hence, it becomes a critical problem to allocate the resources and schedule the jobs. This study aims to develop the decision support system framework for IC design job scheduling. The framework enhances the throughput and reduces waiting time. In practice, this study implements the proposed framework in an IC design service company in Taiwan as the empirical study. The empirical study reduced 77.4% of waiting time and 60% of makespan.",2019-05-24,https://www.semanticscholar.org/paper/54d3abf600bc226e8743b202150d678b59fa2d60,MSIE 2019
2544,Proceedings of the 2nd international conference on INtelligent TEchnologies for interactive enterTAINment,"The conference intends to stimulate interaction among academic researchers and commercial developers of interactive entertainment systems. In addition to paper presentations, posters and demos, the conference will foster discussions in topic centered workshops and special events such as the design garage. Underlying Interactive Device Technologies (mobile devices, home entertainment centers, haptic devices, wall screen displays, information kiosks, holographic displays, fog screens, distributed smart sensors, immersive screens and wearable devices), can provide through a variety of Media Delivery Infrastructures (multimedia networks, interactive radio, streaming technologies, DVB-T/M, ITV, P2P, satellite broadcasting, UMTS, Bluetooth, Broadband, VoIP) and a series of user centered Intelligent Computational Technologies and Interactive Applications for Entertainment as described.",2008-01-08,https://www.semanticscholar.org/paper/86abac22c2208d6e2709b306bb4e4064999f5c44,Intelligent Technologies for Interactive Entertainment
3378,Scheduling with Speed Predictions,"Algorithms with predictions is a recent framework that has been used to overcome pessimistic worst-case bounds in incomplete information settings. In the context of scheduling, very recent work has leveraged machine-learned predictions to design algorithms that achieve improved approximation ratios in settings where the processing times of the jobs are initially unknown. In this paper, we study the speed-robust scheduling problem where the speeds of the machines, instead of the processing times of the jobs, are unknown and augment this problem with predictions. Our main result is an algorithm that achieves a $\min\{\eta^2(1+\alpha), (2 + 2/\alpha)\}$ approximation, for any $\alpha \in (0,1)$, where $\eta \geq 1$ is the prediction error. When the predictions are accurate, this approximation outperforms the best known approximation for speed-robust scheduling without predictions of $2-1/m$, where $m$ is the number of machines, while simultaneously maintaining a worst-case approximation of $2 + 2/\alpha$ even when the predictions are arbitrarily wrong. In addition, we obtain improved approximations for three special cases: equal job sizes, infinitesimal job sizes, and binary machine speeds. We also complement our algorithmic results with lower bounds. Finally, we empirically evaluate our algorithm against existing algorithms for speed-robust scheduling.",2022-05-02,https://www.semanticscholar.org/paper/2ec801cd4d3daf636dbbafd49bc100ee8781b95b,arXiv.org
4,Weakly Supervised Attention Networks for Fine-Grained Opinion Mining and Public Health,"In many review classification applications, a fine-grained analysis of the reviews is desirable, because different segments (e.g., sentences) of a review may focus on different aspects of the entity in question. However, training supervised models for segment-level classification requires segment labels, which may be more difficult or expensive to obtain than review labels. In this paper, we employ Multiple Instance Learning (MIL) and use only weak supervision in the form of a single label per review. First, we show that when inappropriate MIL aggregation functions are used, then MIL-based networks are outperformed by simpler baselines. Second, we propose a new aggregation function based on the sigmoid attention mechanism and show that our proposed model outperforms the state-of-the-art models for segment-level sentiment classification (by up to 9.8% in F1). Finally, we highlight the importance of fine-grained predictions in an important public-health application: finding actionable reports of foodborne illness. We show that our model achieves 48.6% higher recall compared to previous models, thus increasing the chance of identifying previously unknown foodborne outbreaks.",2019-09-30,https://www.semanticscholar.org/paper/217be3119fe32cdfda39cec4af00ce646317d153,Conference on Empirical Methods in Natural Language Processing
651,Length of hospital stay and complications after percutaneous transluminal coronary angioplasty. Clinical and procedural predictors. Heparin Registry Investigators.,"BACKGROUND
Although several studies have established that the complications of percutaneous transluminal coronary angioplasty (PTCA) are related to clinical and angiographic variables such as advanced age and lesion complexity, it is uncertain whether the use of hospital resources after PTCA also depends on the same baseline variables. The purpose of this study was to identify the factors responsible for prolonged hospital stay after PTCA.


METHODS AND RESULTS
The study cohort included 591 consecutive patients undergoing conventional balloon angioplasty at nine medical centers in North America. Major or minor complications occurred in 91 patients (15.4%) and were observed to be related to several baseline characteristics, including unstable angina, multivessel coronary artery disease, patient age, and lesion complexity. Compared with a median length of hospital stay of 2.0 days after PTCA (25th, 75th percentiles: 2.0, 4.0) for the entire cohort of patients, the length of stay was increased in patients with unstable angina (3.0 days [2.0, 5.0]; P = .002), multivessel coronary artery disease (3.0 [2.0, 5.5]; P = .001), age > 65 years (3.0 [2.0, 5.5]; P = .02), complex lesions (3.0 [2.0, 6.0]; P = .001), and filling defects (6.0 [2.0, 11.0]; P < .001). The length of stay was more strikingly increased, however, in patients who experienced major or minor PTCA complications, such as emergency bypass surgery (9.0 days [8.0, 18.0]; P < .001), Q-wave or non-Q-wave myocardial infarction (8.0 [6.0, 15.5]; P < .001), transfusion unrelated to bypass surgery (8.0 [4.0, 12.0]; P < .001), or abrupt vessel closure (6.0 [3.0, 10.5]; P < .001). On stepwise multiple linear regression, PTCA complications appeared to be the strongest predictors of length of hospital stay (all P < .001) and overwhelmed the weaker relation between length of stay and several individual baseline variables. Inclusion of a composite clinical risk score (reflecting the presence of unstable angina, multivessel disease, advanced age, complex lesions, or filling defects) in the regression model confirmed that patients with several high-risk baseline variables had a significant increase in length of stay after PTCA (P = .003), but PTCA complications remained the strongest predictors of length of stay.


CONCLUSIONS
Although PTCA complications were correlated with baseline variables such as unstable angina, multivessel disease, advanced age, complex lesions, and filling defects, excess length of stay after PTCA was most strongly influenced by the development of minor and major PTCA complications. Because patients with several baseline risk factors experienced significantly prolonged hospitalizations, improved selection of patients may contribute to reductions in length of stay after PTCA. A greater reduction in resource use after PTCA, however, would be expected from developing new treatments to decrease PTCA complications rather than limiting the access of patients with unstable angina, advanced age, or complex lesions to PTCA.",1995-08-01,https://www.semanticscholar.org/paper/00cf52ce59123c6259257d77e1a2fe9e0a748407,Circulation
2873,Accelerated diabetic glomerulopathy in galectin‐3/AGE receptor 3 knockout mice,"Several molecules were shown to bind advanced glycation end products (AGEs) in vitro, but it is not known whether they all serve as AGE receptors and which functional role they play in vivo. We investigated the role of galectin‐3, a multifunctional lectin with (anti)adhesive and growth‐regulating properties, as an AGE receptor and its contribution to the development of diabetic glomerular disease, using a knockout mouse model. Galectin‐3 knockout mice obtained by gene ablation and the corresponding wild‐type mice were rendered diabetic with streptozotocin and killed 4 months later, together with age‐matched nondiabetic controls. Despite a comparable degree of metabolic derangement, galectin‐3‐deficient mice developed ac‐celerated glomerulopathy vs. the wild‐type animals, as evidenced by the more pronounced increase in protein‐uria, extracellular matrix gene expression, and mesan‐gial expansion. This was associated with a more marked renal/glomerular AGE accumulation, indicating it was attributable to the lack of galectin‐3 AGE receptor function. The galectin‐3‐deficient genotype was associated with reduced expression of receptors implicated in AGE removal (macrophage scavenger receptor A and AGE‐R1) and increased expression of those mediating cell activation (RAGE and AGE‐R2). These results show that the galectin‐3‐regulated AGE receptor pathway is operating in vivo and protects toward AGE‐induced tissue injury in contrast to that through RAGE.—Pugliese, G., Pricci, F., Iacobini, C., Leto, G., Amadio, L., Barsotti, P., Frigeri L., Hsu, D. K., Vlassara, H., Liu, F.‐T., Di Mario, U. Accelerated diabetic glomerulopathy in galectin‐3/AGE receptor 3 knockout mice. FASEB J. 15, 2471–2479 (2001)",2001-11-01,https://www.semanticscholar.org/paper/acd430ad1deac678ea6e8b09bfdd123708c68005,The FASEB Journal
308,Congestion games with malicious players,"We study the equilibria of non-atomic congestion games in which there are two types of players: rational players, who seek to minimize their own delay, and malicious players, who seek to maximize the average delay experienced by the rational players. We study the existence of pure and mixed Nash equilibria for these games, and we seek to quantify the impact of the malicious players on the equilibrium. One counter intuitive phenomenon which we demonstrate is the ""windfall of malice"": paradoxically, when a myopically malicious player gains control of a fraction of the flow, a fraction of the players change from rational to malicious, the new equilibrium may be more favorable for the remaining rational players than the previous equilibrium.",2007-06-11,https://www.semanticscholar.org/paper/907f522bd7eb3bae09bcf9bcb508328f4441d6e1,ACM Conference on Economics and Computation
602,A Fast Algorithm for Testing for Safety and Detecting Deadlocks in Locked Transaction Systems,,1981-09-01,https://www.semanticscholar.org/paper/63601adf49f75d97e1b0360cf90ac83d422cf5df,J. Algorithms
194,"Front Matter, Table of Contents, Preface, Conference Organization",,,https://www.semanticscholar.org/paper/fbd51e3e9fc4ea6dffd17f692425fb1b14831b1e,Information Technology Convergence and Services
3383,Matching Drivers to Riders: A Two-Stage Robust Approach,"Matching demand (riders) to supply (drivers) efficiently is a fundamental problem for ride-sharing platforms who need to match the riders (almost) as soon as the request arrives with only partial knowledge about future ride requests. A myopic approach that computes an optimal matching for current requests ignoring future uncertainty can be highly sub-optimal. In this paper, we consider a two-stage robust optimization framework for this matching problem where future demand uncertainty is modeled using a set of demand scenarios (specified explicitly or implicitly). The goal is to match the current request to drivers (in the first stage) so that the cost of first stage matching and the worst case cost over all scenarios for the second stage matching is minimized. We show that the two-stage robust matching is NP-hard under various cost functions and present constant approximation algorithms for different settings of our two-stage problem. Furthermore, we test our algorithms on real-life taxi data from the city of Shenzhen and show that they substantially improve upon myopic solutions and reduce the maximum wait time of the second-stage riders by an average of $30\%$ in our experimental results.",2020-11-06,https://www.semanticscholar.org/paper/1d0638fe19b3f006122b1582d47781e60986aec7,"International Workshop and International Workshop on Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques"
2375,Formation of myeloperoxidase compound II during aerobic stimulation of rat neutrophils,,1986-03-01,https://www.semanticscholar.org/paper/8df87bce3c6b886cbcdf59f6f78d2ad55f5d5199,Bioscience Reports
3148,Measuring thin-client performance using slow-motion benchmarking,"Modern thin-client systems are designed to provide the same graphical interfaces and applications available on traditional desktop computers while centralizing administration and allowing more efficient use of computing resources. Despite the rapidly increasing popularity of these client-server systems, there are few reliable analyses of their performance. Industry standard benchmark techniques commonly used for measuring desktop system performance are ill-suited for measuring the performance of thin-client systems because these benchmarks only measure application performance on the server, not the actual user-perceived performance on the client. To address this problem, we have developed slow-motion benchmarking, a new measurement technique for evaluating thin-client systems. In slow-motion benchmarking, performance is measured by capturing network packet traces between a thin client and its respective server during the execution of a slow-motion version of a conventional benchmark application. These results can then be used either independently or in conjunction with conventional benchmark results to yield an accurate and objective measure of the performance of thin-client systems. We have demonstrated the effectiveness of slow-motion benchmarking by using this technique to measure the performance of several popular thin-client systems in various network environments on Web and multimedia workloads. Our results show that slow-motion benchmarking solves the problems with using conventional benchmarks on thin-client systems and is an accurate tool for analyzing the performance of these systems.",2001-06-25,https://www.semanticscholar.org/paper/eb867b23e31dce148902b7ef879b63247d83f9c2,TOCS
3012,Tap,,2021-06-24,https://www.semanticscholar.org/paper/e530f36019b32bc0eb456a3e70632ac948f080a8,"Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services"
826,Testing Finite State Machines: Fault Detection,"Abstract We present simple randomized algorithms for the fault detection problem: Given a specification in the form of a deterministic finite state machine A and an implementation machine B, determine whether B is equal to A. If A has n states and p inputs, then in randomized polynomial time we can construct with high probability a checking sequence of length O(pn4 log n), i.e., a sequence that detects all faulty machines with at most n states. Better bounds can be obtained in certain cases. The techniques generalize to partially specified finite state machines.",1995-04-01,https://www.semanticscholar.org/paper/af20258b9728c146715ecf5515d58c88f61b42eb,Journal of computer and system sciences (Print)
3138,Inferring client response time at the web server,"As businesses continue to grow their World Wide Web presence, it is becoming increasingly vital for them to have quantitative measures of the client perceived response times of their web services. We present Certes (CliEnt Response Time Estimated by the Server), an online server-based mechanism for web servers to measure client perceived response time, as if measured at the client. Certes is based on a model of TCP that quantifies the effect that connection drops have on perceived client response time, by using three simple server-side measurements: connection drop rate, connection accept rate and connection completion rate. The mechanism does not require modifications to http servers or web pages, does not rely on probing or third party sampling, and does not require client-side modifications or scripting. Certes can be used to measure response times for any web content, not just HTML. We have implemented Certes and compared its response time measurements with those obtained with detailed client instrumentation. Our results demonstrate that Certes provides accurate server-based measurements of client response times in HTTP 1.0/1.1 [14] environments, even with rapidly changing workloads. Certes runs online in constant time with very low overhead. It can be used at web sites and server farms to verify compliance with service level objectives.",2002-06-15,https://www.semanticscholar.org/paper/3fd4dd73277e3c1bb6a5db95eac2108531ceb0f2,Measurement and Modeling of Computer Systems
273,Comparing Trade-off Based Models of the Internet,"We introduce and evaluate several new models of network growth. Our models are extensions of the FKP model, modifying and improving it in various dimensions. In all these models nodes arrive one by one, and each node is connected to previous nodes by optimizing a trade-off between a geometric objective (""last mile cost"") and a topological objective (""position in the network""). Our new models differ from the original FKP model in directions inspired by the real Internet: two or more edges are attached to each arriving node (while the FKP model produces a tree); these edges are chosen according to various criteria such as robustness; edges may be added to the network between old nodes; or only certain ""fertile"" nodes (an attribute that changes dynamically) are capable of attracting new edges. We evaluate these models, and compare them with the graph of the Internet's autonomous systems, with respect to a suite of many test parameters (such as average degree, power law exponent, and local clustering rank) proposed in the literature; to this end we have developed the network generation and measurement system Pandora.",2009-06-30,https://www.semanticscholar.org/paper/274a29fbc41855812cfa5493fabcabc65d52550f,Fundamenta Informaticae
1563,Skill Rating for Multiplayer Games. Introducing Hypernode Graphs and their Spectral Theory,"We consider the skill rating problem for multiplayer games, that is how to infer player skills from game outcomes in multiplayer games. We formulate the problem as a minimization problem arg min s s T ∆s where ∆ is a positive semidefinite matrix and s a real-valued function, of which some entries are the skill values to be inferred and other entries are constrained by the game outcomes. We leverage graph-based semi-supervised learning (SSL) algorithms for this problem. We apply our algorithms on several data sets of multiplayer games and obtain very promising results compared to Elo Duelling (see Elo, 1978) and TrueSkill (see Herbrich et al., 2006). As we leverage graph-based SSL algorithms and because games can be seen as relations between sets of players, we then generalize the approach. For this aim, we introduce a new finite model, called hypernode graph, defined to be a set of weighted binary relations between sets of nodes. We define Laplacians of hy-pernode graphs. Then, we show that the skill rating problem for multiplayer games can be formulated as arg min s s T ∆s where ∆ is the Laplacian of a hypernode graph constructed from a set of games. From a fundamental perspective, we show that hypernode graph Laplacians are symmetric positive semidefinite matrices with constant functions in their null space. We show that problems on hypernode graphs can not be solved with graph constructions and graph kernels. We relate hypernode graphs to signed graphs showing that positive relations between groups can lead to negative relations between individuals.",,https://www.semanticscholar.org/paper/e9ca8923abf66839bbf212262ab371f34b809380,Journal of machine learning research
1895,Management Suggestions for Process Control of Semiconductor Manufacturing: An Operations Research and Data Science Perspective,,2019-09-21,https://www.semanticscholar.org/paper/6bc75b9064dd7f7094d63a3c4f78ee01c90933ac,Computational Intelligence and Optimization Methods for Control Engineering
1665,Variational Inference via \chi Upper Bound Minimization,"Variational inference (VI) is widely used as an efficient alternative to Markov chain Monte Carlo. It posits a family of approximating distributions $q$ and finds the closest member to the exact posterior $p$. Closeness is usually measured via a divergence $D(q || p)$ from $q$ to $p$. While successful, this approach also has problems. Notably, it typically leads to underestimation of the posterior variance. In this paper we propose CHIVI, a black-box variational inference algorithm that minimizes $D_{\chi}(p || q)$, the $\chi$-divergence from $p$ to $q$. CHIVI minimizes an upper bound of the model evidence, which we term the $\chi$ upper bound (CUBO). Minimizing the CUBO leads to improved posterior uncertainty, and it can also be used with the classical VI lower bound (ELBO) to provide a sandwich estimate of the model evidence. We study CHIVI on three models: probit regression, Gaussian process classification, and a Cox process model of basketball plays. When compared to expectation propagation and classical VI, CHIVI produces better error rates and more accurate estimates of posterior variance.",2016-11-01,https://www.semanticscholar.org/paper/d56117c84a559670f5c4e6e2993a2f117dc34be9,Neural Information Processing Systems
2613,Authoring 3D hypermedia for wearable augmented and virtual reality,"Most existing authoring systems for wearable augmentedand virtual reality experiences concentrate oncreating separate media objects and embedding themwithin the user's surroundings. In contrast, designingnarrative multimedia experiences for such environmentsis still largely a tedious manual task. We present an authoringtool for creating and editing 3D hypermedia narrativesthat are interwoven with a wearable computeruser's surrounding environment. Our system is designedfor use by authors who are not programmers, and allowsthem to preview their results on a desktop workstation, aswell as with an augmented or virtual reality system.",2003-10-21,https://www.semanticscholar.org/paper/2c5e9cf18d7eaf6cf250dca6f72f3ee5765106a9,"Seventh IEEE International Symposium on Wearable Computers, 2003. Proceedings."
1626,Frequentist Consistency of Variational Bayes,"ABSTRACT A key challenge for modern Bayesian statistics is how to perform scalable inference of posterior distributions. To address this challenge, variational Bayes (VB) methods have emerged as a popular alternative to the classical Markov chain Monte Carlo (MCMC) methods. VB methods tend to be faster while achieving comparable predictive performance. However, there are few theoretical results around VB. In this article, we establish frequentist consistency and asymptotic normality of VB methods. Specifically, we connect VB methods to point estimates based on variational approximations, called frequentist variational approximations, and we use the connection to prove a variational Bernstein–von Mises theorem. The theorem leverages the theoretical characterizations of frequentist variational approximations to understand asymptotic properties of VB. In summary, we prove that (1) the VB posterior converges to the Kullback–Leibler (KL) minimizer of a normal distribution, centered at the truth and (2) the corresponding variational expectation of the parameter is consistent and asymptotically normal. As applications of the theorem, we derive asymptotic properties of VB posteriors in Bayesian mixture models, Bayesian generalized linear mixed models, and Bayesian stochastic block models. We conduct a simulation study to illustrate these theoretical results. Supplementary materials for this article are available online.",2017-05-09,https://www.semanticscholar.org/paper/534403a65fa011cc29a09e29864a88cd6216a0dd,Journal of the American Statistical Association
2954,Characterization of functional methylomes by next-generation capture sequencing identifies novel disease-associated variants,,2015-05-29,https://www.semanticscholar.org/paper/1d1fa74d6a9d3e9c7adf1907ca78162020550591,Nature Communications
796,Hierarchical State Machines,,2000-08-17,https://www.semanticscholar.org/paper/77b54ddc90953ac13c62a77197b85ce22c457b68,IFIP TCS
1073,Production rate measurement of Tritium and other cosmogenic isotopes in Germanium with CDMSlite,,2018-06-19,https://www.semanticscholar.org/paper/aa29bcb69d2d4e84c26c86fe2ea0a3dbc9864e7f,Astroparticle physics
1989,Semiconductor manufacturing,,2012-10-18,https://www.semanticscholar.org/paper/03c5a168e77720077204b78b6dd63ff76a2fbdb7,Flexible Services and Manufacturing Journal
614,Symmetric Space-Bounded Computation (Extended Abstract),,1980-07-14,https://www.semanticscholar.org/paper/0e6e6ad899f7f35dd294d73c8cd081f76f03f6ab,"International Colloquium on Automata, Languages and Programming"
2367,Myeloperoxidase secretion during phagocytosis: a case of a patient with impaired bactericidal activity.,"We describe a case of a 5-year-old male patient with prolonged and extensive osteomyelitis of the left femur. Staphylococcus aureus was grown from blood cultures taken upon admission and also from pus drained from an incised hip joint. A defect in immune function was suspected and neutrophil function was assessed. Chemotaxis and phagocytosis were normal, but phagocytosed S. aureus were not killed as efficiently as in control neutrophils. No inherent defect in the ability of these neutrophils to generate reactive oxidants was observed, but an unusual luminol-dependent chemiluminescence response was obtained during phagocytosis of latex beads or opsonized S. aureus: This was characterized by an initial rapid, but transient increase occurring within 1 min of addition of phagocytic stimulus. Whereas during phagocytosis of latex beads by control neutrophils less than 1% of the total myeloperoxidase activity was detected extracellularly, up to 15% was released from the patient's neutrophils. We propose that release of myeloperoxidase from the patient's neutrophils during phagocytosis reduces the intraphagosomal concentration of this enzyme and thus impairs the efficiency of intracellular killing of S. aureus.",1988-10-01,https://www.semanticscholar.org/paper/fbd28b87aad15f7c32baa1f08c378483b4164b9f,Journal of clinical & laboratory immunology
3101,An Application Streaming Service for Mobile Handheld Devices,"Mobile handheld devices such as PDAs and smartphones are increasingly being used by service providers to deliver application functionality similar to that found in traditional desktop computing environments. However, these handheld applications can be quite slow and often lack important functionality compared to their desktop counterparts. We have developed PASSPORT, (PDA application streaming service portal) a thin-client solution that leverages more powerful servers to run full-function desktop applications and then simply stream screen updates to the PDA for display. PASSPORT uses server-side screen scaling to provide high-fidelity display and seamless mobility across a broad range of different clients and screen sizes, including both portrait and landscape viewing modes. PASSPORT also leverages existing PDA control buttons to improve system usability and maximize available screen resolution for application display. We have implemented PASSPORT on Windows PDAs and demonstrate that it can provide superior application performance and functionality compared to the traditional approach of running applications directly on handheld devices",2006-09-18,https://www.semanticscholar.org/paper/beae55a7523354a298c84f3aeac1c99affa8156e,IEEE International Conference on Services Computing
120,Optimizing queries over multimedia repositories,"Repositories of multimedia objects having multiple types of attributes (e.g., image, text) are becoming increasingly common. A selection on these attributes will typically produce not just a set of objects, as in the traditional relational query model (filtering), but also a grade of match associated with each object, indicating how well the object matches the selection condition (ranking). Also, multimedia repositories may allow access to the attributes of each object only through indexes. We investigate how to optimize the processing of queries over multimedia repositories. A key issue is the choice of the indexes used to search the repository. We define an execution space that is search-minimal, i.e., the set of indexes searched is minimal. Although the general problem of picking an optimal plan in the search-minimal execution space is NP-hard, we solve the problem efficiently when the predicates in the query are independent. We also show that the problem of optimizing queries that ask for a few top-ranked objects can be viewed, in many cases, as that of evaluating selection conditions. Thus, both problems can be viewed together as an extended filtering problem.",1996-06-01,https://www.semanticscholar.org/paper/1613abd9402dbd7461d652d74071cb91ef7a21b7,ACM SIGMOD Conference
1936,A Novel Route Selection and Resource Allocation Approach to Improve the Efficiency of Manual Material Handling System in 200-mm Wafer Fabs for Industry 3.5,"Motivated by realistic needs to enhance the productivity for 200-mm wafer fabs, this paper aims to propose a novel approach for manual material handling system (MMHS) to mimic functionalities of the automated material handling system in the advanced fabs without intensive capital investment to deliver the wafer lots manually and systematically. In particular, a mathematical model is developed to optimize the routing plan with two objectives that minimize the total traveling distance in all routes or minimize the number of manpower needed in all routes. Furthermore, a route planning approach is proposed to utilize the routes that reduce the technician traveling distance and transportation time for implementation. Also, a manpower loading index was developed for evaluating the number of needed technicians in the proposed MMHS. To estimate the validity of the proposed MMHS, we developed a simulation environment based on empirical data with different transportation requirement scenarios for comparison. The results have shown practical viability of the proposed approach.",2016-07-19,https://www.semanticscholar.org/paper/892701e35b21a935ed76b10a69f12b9c340cc9db,IEEE Transactions on Automation Science and Engineering
2330,Cytokine expression by inflammatory neutrophils.,"Bloodstream neutrophils do not express mRNA for interleukin-1 beta (IL-1 beta), but transcripts for this cytokine are rapidly induced following exposure to recombinant granulocyte-macrophage colony-stimulating factor (rGM-CSF) in vitro. Levels of IL-1 beta mRNA reach maximal values 1 h after exposure to rGM-CSF and then decline to near basal levels by 4 h. Similarly, rGM-CSF treatment of blood neutrophils in vitro induced increases in levels of mRNA for IL-6 and tumour necrosis factor-alpha (TNF-alpha). RNA extracted from neutrophils isolated from the synovial fluid of patients with rheumatoid arthritis expressed low, but significant levels of IL-1 beta mRNA that were between 0.5 and 3% of the levels that could be maximally induced by rGM-CSF treatment of blood neutrophils. However, transcripts for TNF-alpha and IL-6 were not detected in these synovial fluid neutrophils. mRNA for transforming growth factor-beta (TGF-beta) was constitutively expressed in blood and synovial fluid neutrophils and transcripts for this cytokine were not altered by rGM-CSF exposure. Because of the transient nature of IL-1 beta expression by activated neutrophils, we propose that the low levels of expression of mRNA for this cytokine in the synovial fluid neutrophils represents expression by a small, perhaps newly-recruited and activated, sub-population of cells. IL-1 beta expression by this sub-population may thus contribute to the pathogenesis of rheumatoid disease.",1994-03-01,https://www.semanticscholar.org/paper/a784bda910605f9818bc6788b821ed7118d19d67,FEMS Immunology & Medical Microbiology
1945,Multistage production distribution under uncertain demands with integrated discrete particle swarm optimization and extended priority-based hybrid genetic algorithm,,2015-01-08,https://www.semanticscholar.org/paper/8128eee3739e1f7abf163c1d5487325892dcd099,Fuzzy Optimization and Decision Making
3045,"The Design, Implementation, and Evaluation of Cells: A Virtual Smartphone Architecture","Smartphones are increasingly ubiquitous, and many users carry multiple phones to accommodate work, personal, and geographic mobility needs. We present Cells, a virtualization architecture for enabling multiple virtual smartphones to run simultaneously on the same physical cellphone in an isolated, secure manner. Cells introduces a usage model of having one foreground virtual phone and multiple background virtual phones. This model enables a new device namespace mechanism and novel device proxies that integrate with lightweight operating system virtualization to multiplex phone hardware across multiple virtual phones while providing native hardware device performance. Cells virtual phone features include fully accelerated 3D graphics, complete power management features, and full telephony functionality with separately assignable telephone numbers and caller ID support. We have implemented a prototype of Cells that supports multiple Android virtual phones on the same phone. Our performance results demonstrate that Cells imposes only modest runtime and memory overhead, works seamlessly across multiple hardware devices including Google Nexus 1 and Nexus S phones, and transparently runs Android applications at native speed without any modifications.",2012-08-01,https://www.semanticscholar.org/paper/d73fde4fae0c32e574b4950a8e0ee6d7324831d5,TOCS
1767,A topographic latent source model for fMRI data,,2011-07-01,https://www.semanticscholar.org/paper/5d311ef363f831a903ead8997c493be51609346a,NeuroImage
725,On the Complexity of Optimal Lottery Pricing and Randomized Mechanisms,"We study the optimal lottery problem and the optimal mechanism design problem in the setting of a single unit-demand buyer with item values drawn from independent distributions. Optimal solutions to both problems are characterized by a linear program with exponentially many variables. For the menu size complexity of the optimal lottery problem, we present an explicit, simple instance with distributions of support size 2, and show that exponentially many lotteries are required to achieve the optimal revenue. We also show that, when distributions have support size 2 and share the same high value, the simpler scheme of item pricing can achieve the same revenue as the optimal menu of lotteries. The same holds for the case of two items with support size 2 (but not necessarily the same high value). For the computational complexity of the optimal mechanism design problem, we show that unless the polynomial-time hierarchy collapses (more exactly, PNP = P#P), there is no universal efficient randomized algorithm to implement an optimal mechanism even when distributions have support size 3.",2015-10-17,https://www.semanticscholar.org/paper/bca2b5cccf4379e1924571b4f0c379402b13352d,IEEE Annual Symposium on Foundations of Computer Science
868,Towards an architecture-independent analysis of parallel algorithms,"A simple and efficient method for evaluating the performance of an algorithm, rendered as a directed acyclic graph, on any parallel computer is presented. The crucial ingredient is an efficient approximation algorithm for a particular scheduling problem. The only parameter of the parallel computer needed by our method is the message-to-instruction ratio $\tau$. Although the method used in this paper does not take into account the number of processors available, its application to several common algorithms shows that it is surprisingly accurate.",1990-04-01,https://www.semanticscholar.org/paper/6148ddf9fe6a835f4f92a591ff135a1c6bba70e1,Symposium on the Theory of Computing
953,Development of a novel hyaluronic acid membrane for the treatment of ocular surface diseases,,2021-01-27,https://www.semanticscholar.org/paper/7844a97100a5b2f43cf803d1a927c2020a4ff381,Scientific Reports
2074,A Case Study to Evaluate the Productivity Changes of the Thermal Power Plants of the Taiwan Power Company,"This paper developed an approach based on data envelopment analysis and Malmquist productivity index to investigate the performance of power plants and conducted an empirical study with eight thermal power plants in Taiwan. The analysis results show the productivity improvements, and thus, help Taiwan Power Company to monitor and diagnose changes in the productivity of its thermal power plants. Furthermore, this study also provides specific directions for improvements to increase competitiveness in the face of the continuing liberalization of the Taiwanese power generation market.",2007-08-20,https://www.semanticscholar.org/paper/e635f7f03dde899bdde25e91de178372d2edaa51,IEEE transactions on energy conversion
2095,Decision support system for delivery mode of backend,"This study develops a decision support system for delivery mode based on the relationship of entire supply chain of backend in semiconductor industry. In particular, it is a critical decision for delivery mode including quantity and timing in the supply chain operations for semiconductor industry. In practice, the existing way relies on engineer's experience to making decisions and taking more than 4 hours to dispatch. This study aims to develop a decision support system for delivery mode embedded optimal dispatching model. And it enables to confirm relationship of entire supply chain having on game theory. We apply the DSS at a semiconductor backend to verify it in real setting. The results show that it can reduce efficiently either total cycle time or implement time of staff with cost-effective assignment. The developed DSS assists decision makers in making rapid and accurate decisions in real time given the circumstances of limited information of various order mix",2004-12-08,https://www.semanticscholar.org/paper/5488b9bd11af1ed1c821ef57de331b3bbed3dc64,Electronic Packaging Technology Conference
2192,"Low‐density granulocytes: functionally distinct, immature neutrophils in rheumatoid arthritis with altered properties and defective TNF signalling","Our aim was to determine whether rheumatoid arthritis (RA) low‐density granulocytes (LDGs) are functionally different from RA neutrophils. LDGs from 32 RA patients were characterized using flow cytometry and quantitative PCR (qPCR). RNA sequencing (RNA‐Seq) was carried out on paired RA LDGs and neutrophils (n = 4) and validated using qPCR. Functional assays included chemotaxis, phagocytosis, reactive oxygen species (ROS) production, cell‐cycle analysis, apoptosis, neutrophil extracellular trap (NET)osis, and measurement of cytokine production (n ≥ 5 paired RA LDGs/neutrophils). RA LDGs had a substantially altered transcriptome, expressing >5000 genes at significantly different levels compared with RA neutrophils, including elevated levels of transcripts for granule proteins [including elastase and myeloperoxidase (MPO)] and cell‐cycle genes [including cyclin‐dependent kinase (CDK)2, CDK4, and CDK6]. Approximately 1% of RA LDGs stained positive for the G2/S phase of the cell cycle. RA LDGs had a significantly lower constitutive rate of apoptosis compared with RA neutrophils and did not respond to TNF‐α in culture. Expression of transcripts for cytokines and cytokine receptors was lower in RA LDGs. NET formation was lower in LDGs in response to PMA compared with RA neutrophils. Chemotaxis and phagocytosis was lower in RA LDGs compared with neutrophils. RA LDGs produced significantly lower amounts of ROS in response to fMLP following priming with TNF‐α. Expression of TNFR1 and ‐2 mRNA and protein was significantly lower in LDGs. We conclude that RA LDGS are functionally different from RA neutrophils, representing an immature neutrophil population within peripheral blood. Their enhanced survival properties and decreased TNF signaling are likely to have important consequences for disease pathology and response to therapy.",2017-02-01,https://www.semanticscholar.org/paper/8ac0de14582575f30347e5cb2fc99e2c8121534e,Journal of Leukocyte Biology
15,k-Shape: Efficient and Accurate Clustering of Time Series,"The proliferation and ubiquity of temporal data across many disciplines has generated substantial interest in the analysis and mining of time series. Clustering is one of the most popular data mining methods, not only due to its exploratory power, but also as a preprocessing step or subroutine for other techniques. In this paper, we present k-Shape, a novel algorithm for time-series clustering. k-Shape relies on a scalable iterative refinement procedure, which creates homogeneous and well-separated clusters. As its distance measure, k-Shape uses a normalized version of the cross-correlation measure in order to consider the shapes of time series while comparing them. Based on the properties of that distance measure, we develop a method to compute cluster centroids, which are used in every iteration to update the assignment of time series to clusters. To demonstrate the robustness of k-Shape, we perform an extensive experimental evaluation of our approach against partitional, hierarchical, and spectral clustering methods, with combinations of the most competitive distance measures. k-Shape outperforms all scalable approaches in terms of accuracy. Furthermore, k-Shape also outperforms all non-scalable (and hence impractical) combinations, with one exception that achieves similar accuracy results. However, unlike k-Shape, this combination requires tuning of its distance measure and is two orders of magnitude slower than k-Shape. Overall, k-Shape emerges as a domain-independent, highly accurate, and highly efficient clustering approach for time series with broad applications.",2015-05-27,https://www.semanticscholar.org/paper/8278ca04c4ffafef80abfbe0ce3c6cfc07b2792d,SIGMOD Conference
2526,Perceptual issues in augmented reality revisited,"This paper provides a classification of perceptual issues in augmented reality, created with a visual processing and interpretation pipeline in mind. We organize issues into ones related to the environment, capturing, augmentation, display, and individual user differences. We also illuminate issues associated with more recent platforms such as handhelds or projector-camera systems. Throughout, we describe current approaches to addressing these problems, and suggest directions for future research.",2010-11-22,https://www.semanticscholar.org/paper/db5b0202e1de96f151a3efb36f68aeb332cf2eea,2010 IEEE International Symposium on Mixed and Augmented Reality
3169,"A Framework for Fast, Large-scale, Semi-Automatic Inference of Animal Behavior from Monocular Videos","An automatic, quick, accurate, and scalable method for animal behavior inference using only videos of animals offers unprecedented opportunities to understand complex biological phenomena and answer challenging ecological questions. The advent of sophisticated machine learning techniques now allows the development and implementation of such a method. However, apart from developing a network model that infers animal behavior from video inputs, the key challenge is to obtain sufficient labeled (annotated) data to successfully train that network - a laborious task that needs to be repeated for every species and/or animal system. Here, we propose solutions for both problems, i) a novel methodology for rapidly generating large amounts of annotated data of animals from videos and ii) using it to reliably train deep neural network models to infer the different behavioral states of every animal in each frame of the video. Our method’s workflow is bootstrapped with a relatively small amount of manually-labeled video frames. We develop and implement this novel method by building upon the open-source tool Smarter-LabelMe, leveraging deep convolutional visual detection and tracking in combination with our behavior inference model to quickly produce large amounts of reliable training data. We demonstrate the effectiveness of our method on aerial videos of plains and Grévy’s Zebras (Equus quagga and Equus grevyi). We fully open-source the code1 of our method as well as provide large amounts of accurately-annotated video datasets2 of zebra behavior using our method. A video abstract of this paper is available here3.",2023-08-02,https://www.semanticscholar.org/paper/98e3b68ea3e6d9df7bb9f4ae3b5aa4b92f547024,bioRxiv
2237,Evidence That RSV Directly Infects Neutrophils in the Airways and Blood of Infants with Severe Bronchiolitis.,,2009-04-01,https://www.semanticscholar.org/paper/3fe7018a983f41c8e1195059e35a94fe59008698,Asian Test Symposium
3591,Evolving a language in and for the real world: C++ 1991-2006,"This paper outlines the history of the C++ programming language from the early days of its ISO standardization (1991), through the 1998 ISO standard, to the later stages of the C++0x revision of that standard (2006). The emphasis is on the ideals, constraints, programming techniques, and people that shaped the language, rather than the minutiae of language features. Among the major themes are the emergence of generic programming and the STL (the C++ standard library's algorithms and containers). Specific topics include separate compilation of templates, exception handling, and support for embedded systems programming. During most of the period covered here, C++ was a mature language with millions of users. Consequently, this paper discusses various uses of C++ and the technical and commercial pressures that provided the background for its continuing evolution.",2007-06-09,https://www.semanticscholar.org/paper/ba49c311278c079272484bfb5cb01ac8d023a6d9,History of Programming Languages
991,"Systemic Comorbidities of Dry Eye Syndrome: The Korean National Health and Nutrition Examination Survey V, 2010 to 2012","Purpose: To identify systemic comorbidities in patients with dry eye syndrome in South Korea. Methods: From 2010 to 2012, 17,364 participants aged 20 or older were randomly included in the nationwide Korean National Health and Nutrition Examination Survey V. The prevalence of dry eye syndrome and demographics of these patients were investigated. We performed conditional logistic regression analyses based on age, sex, residential area, education level, occupation type, and household income level to obtain the odds ratio for each systemic comorbidity among subjects with and without dry eye syndrome. Results: The prevalence of dry eye syndrome in this study was 10.4%. Age [adjusted odds ratio (AOR): 1.02], female gender (AOR: 3.01), and indoor occupation (AOR: 1.30) were associated with a higher prevalence of dry eye syndrome and found to be less prevalent in those residing in rural areas (AOR: 0.73) and with lower education levels (AOR: 0.66–0.99). With regard to systemic comorbidities, dyslipidemia (AOR: 1.63), degenerative arthritis (AOR: 1.56), rheumatoid arthritis (AOR: 1.44), thyroid disease (AOR: 1.79), and renal failure (AOR: 2.56) were associated with a significantly higher prevalence of dry eye syndrome. Conclusions: We found that patients with dry eye syndrome have a higher prevalence of several systemic comorbidities. A more comprehensive therapeutic approach considering the effect of systemic medication may be necessary in these patients.",2016-02-01,https://www.semanticscholar.org/paper/647b5b585d9e7dbf5869b431b226e59638048163,Cornea
791,Approximation of Multiobjective Optimization Problems,,2001-08-08,https://www.semanticscholar.org/paper/7293420513488a517a33da2e8f1ccba18cabc9e9,Workshop on Algorithms and Data Structures
3480,Approximation Algorithms for the Minimum Bends Traveling Salesman Problem,,2000-04-01,https://www.semanticscholar.org/paper/aea0f6bd91a4a3b2a7fd459919d71fd4301f67f3,Conference on Integer Programming and Combinatorial Optimization
623,An optimality theory of concurrency control for databases,,1979-05-30,https://www.semanticscholar.org/paper/42cf741f1b38315c3376c301601cfee74571c6b6,ACM SIGMOD Conference
3306,"Social relationships and reproductive state influence leadership roles in movements of plains zebra, Equus burchellii
",,2007-05-01,https://www.semanticscholar.org/paper/02f6d0ca8b1d8bc6bf480312658bced412abf759,Animal Behaviour
1951,A Framework for Root Cause Detection of Sub-Batch Processing System for Semiconductor Manufacturing Big Data Analytics,"Root cause detecting and rapid yield ramping for advanced technology nodes are crucial to maintain competitive advantages for semiconductor manufacturing. Since the data structure is increasingly complicated in a fully automated wafer fabrication facility, it is difficult to diagnose the whole production system for fault detection. A number of approaches have been proposed for fault diagnosis and root cause detection. However, many constraints in real settings restrict the usage of conventional approaches, due to the big data with complicated data structure. In particular, a batch may not be considered as a run in the present sub-batch processing system for wafer fabrication, in which the processing paths of the wafers in a batch could be different. Motivated by realistic needs, this paper aims to develop a root cause detection framework for the sub-batch processing system. Briefly, the proposed framework consists of three phases: data preparation, data dimension reduction, and the sub-batch processing model construction and evaluation. The proposed approach has been validated by a sequence of simulations and an empirical study conducted in a leading semiconductor manufacturing company in Taiwan. The results have shown practical viability of the proposed approach. Indeed, the developed approach is incorporated in the engineering data analysis system in this case company.",2014-09-09,https://www.semanticscholar.org/paper/0baf6e977362cc537010d76d50613051c5c1d6c5,IEEE transactions on semiconductor manufacturing
2594,VITA: visual interaction tool for archaeology (demo),"VITA (Visual Interaction Tool for Archaeology) is an experimental collaborative mixed reality system for offsite visualization of an archaeological dig. Our demonstration of VITA allows multiple users to visualize the dig site in a mixed reality environment in which tracked, see-through, head-worn displays are combined with a multi-user, multi-touch, projected table surface, a large screen display, and tracked hand-held displays. VITA augments existing archaeological analysis methods with new ways to organize, visualize, and combine the standard 2D information available from an excavation (drawings, pictures, and notes) with textured, laser range-scanned 3D models of objects and the site itself. Users can combine speech, touch, and 3D hand gestures to interact multimodally with the environment.",2004-10-15,https://www.semanticscholar.org/paper/261c983b2d9f625b1bc582458a468605a1432684,ACM SIGMM workshop on Experiential Telepresence
2082,Segmented WIP Control for Cycle Time Reduction,"In semiconductor manufacturing, it is difficult for engineers to control complicated and often imbanlanced heavily loaded production routes well. This study aims to propose a framework of segmented WIP control for the production routes to control and reduce cycle time effectively. Furthermore, Starvation Avoidance (SA) method is employed to maintain high utilization of the Capacity Constrained Resources (CCR) in light of low WIP situations. A simulation is designed and validated with a small-scale model based on real data collected in a wafer fab in Taiwan. The results demonstrated its qualified viability.",2006-09-01,https://www.semanticscholar.org/paper/fba059d90442c8c15d9efaf10299e6068bd5ac52,2006 IEEE International Symposium on Semiconductor Manufacturing
585,Games against nature,"In Com,puter Science, important concepts usually come with a plethora of alternative characterizations. The class PSPACE, for example, can be defined either as the class of problelns solvable in polynomially bounded space by a multitape Turing machine or some similar model, or, equivalently, as the class of problems reducible to some polynomial-depth combinatorial two-person game [Stocklneyer and Meyer, Schaefer]. Alternation [Chandra and Stockmeyer] is an interesting variant of the latter point of view. A more recent, also ""problem-oriented"" characterization of PSPACE is the one in terms of problems involving periodic objects [Orlin]. In this paper, we propose a new characterization of PSPACE, based on some of the most classical and well-looked at problems in Optimization: decision-making under uncertainty. Problems in this class are usually characterized by a discretetime random process, the parameters of which can be influenced by dynamic decisions. Decisions are based on the current state. The goal is to minimize thge expectation of some cost functional of the history of states and decisions. There is a vast .literature on the numerous variants of this problem; the reader is referred for a start to the books by [Denardo, Der.man, IIoward, Bertsekas]. Typically, such a problem is solved by dynamic programming (ill fact, decision-making under uncertainty. was the original and intend.ed application of this technique), with a time and space cost which are usually exponential in the description of the input. In a handful of now classical cases, more clever special techniques have yielded polynomialtime algorithms [Howard, Derman]. Linear programming is sometimes employed. We can formulate a decision problem under uncertain.ty as a new sort of game, in which one",1983-11-07,https://www.semanticscholar.org/paper/edd8c619a47ccb3c3d7f79cd29c95b7ca682413e,24th Annual Symposium on Foundations of Computer Science (sfcs 1983)
691,Reducing Tarski to Unique Tarski (in the Black-box Model),We study the problem of finding a Tarski fixed point over the k -dimensional grid [ n ] k . We give a black-box reduction from the Tarski problem to the same problem with an additional promise that the input function has a unique fixed point. It implies that the Tarski problem and the unique Tarski problem have exactly the same query complexity. Our reduction is based on a novel notion of partial-information functions which we use to fool algorithms for the unique Tarski problem as if they were working on a monotone function with a unique fixed point,,https://www.semanticscholar.org/paper/48f4375253789920bfea64a3d3abaf86d6e30b42,Electron. Colloquium Comput. Complex.
2898,Multiset correlation and factor analysis enables exploration of multi-omics data,,2023-07-01,https://www.semanticscholar.org/paper/e85b97c06a76184f4066362809598cdb1770d928,Cell Genomics
2939,A Birth-Death Process for Feature Allocation,"Konstantina's research leading to these results has received funding from the
European Research Council under the European Union's Seventh Framework
Programme (FP7/2007-2013) ERC grant agreement no. 617411.
EPSRC Grant EP/N014162/1
ATI Grant EP/N510129/1


Institutions involved:
Oxford University, 
Cambridge University,
Stanford University",2017-07-17,https://www.semanticscholar.org/paper/311dbdf39b17668efdcb03457f824fd6c0e716ca,International Conference on Machine Learning
328,Algorithmic Problems in Ad Hoc Networks,,2005-06-30,https://www.semanticscholar.org/paper/27862eab7e0a36d732e788b2080a25b7a9636f30,International Conference on Distributed Computing in Sensor Systems
2936,Genetic regulation of gene expression and splicing during a 10-year period of human aging,,2019-11-04,https://www.semanticscholar.org/paper/e5440fdee0c0de339c5fa4507f0e6d9323b7c66f,Genome Biology
2972,The Supervised IBP: Neighbourhood Preserving Infinite Latent Feature Models,"We propose a probabilistic model to infer supervised latent variables in the Hamming space from observed data. Our model allows simultaneous inference of the number of binary latent variables, and their values. The latent variables preserve neighbourhood structure of the data in a sense that objects in the same semantic concept have similar latent values, and objects in different concepts have dissimilar latent values. We formulate the supervised infinite latent variable problem based on an intuitive principle of pulling objects together if they are of the same type, and pushing them apart if they are not. We then combine this principle with a flexible Indian Buffet Process prior on the latent variables. We show that the inferred supervised latent variables can be directly used to perform a nearest neighbour search for the purpose of retrieval. We introduce a new application of dynamically extending hash codes, and show how to effectively couple the structure of the hash codes with continuously growing structure of the neighbourhood preserving infinite latent feature space.",2013-08-11,https://www.semanticscholar.org/paper/e4a1b4d5211f05bd1dd0d9fc51cd3f9e639855ab,Conference on Uncertainty in Artificial Intelligence
2876,Human Galectin-3 Is a Novel Chemoattractant for Monocytes and Macrophages1,"Galectin-3 is a β-galactoside-binding protein implicated in diverse biological processes. We found that galectin-3 induced human monocyte migration in vitro in a dose-dependent manner, and it was chemotactic at high concentrations (1.0 μM) but chemokinetic at low concentrations (10–100 nM). Galectin-3-induced monocyte migration was inhibited by its specific mAb and was blocked by lactose and a C-terminal domain fragment of the protein, indicating that both the N-terminal and C-terminal domains of galectin-3 are involved in this activity. Pertussis toxin (PTX) almost completely blocked monocyte migration induced by high concentrations of galectin-3. Galectin-3 caused a Ca2+ influx in monocytes at high, but not low, concentrations, and both lactose and PTX inhibited this response. There was no cross-desensitization between galectin-3 and any of the monocyte-reactive chemokines examined, including monocyte chemotactic protein-1, macrophage inflammatory protein-1α, and stromal cell-derived factor-1α. Cultured human macrophages and alveolar macrophages also migrated toward galectin-3, but not monocyte chemotactic protein-1. Finally, galectin-3 was found to cause monocyte accumulation in vivo in mouse air pouches. These results indicate that galectin-3 is a novel chemoattractant for monocytes and macrophages and suggest that the effect is mediated at least in part through a PTX-sensitive (G protein-coupled) pathway.",2000-08-15,https://www.semanticscholar.org/paper/6b839c691c98fda8340cac159e81e9b90aff40d6,Journal of Immunology
3482,Rounding algorithms for a geometric embedding of minimum multiway cut,"Given an undirected graph with edge costs and a subset of k ≥ 3 nodes called terminals, a multiway, or k-way, cut is a subset of the edges whose removal disconnects each terminal from the others. The multiway cut problem is to find a minimum-cost multiway cut. T his problem is Max-SNP hard. Recently Calinescu, Karloff, and Rabani (STOC’98) gave a novel geometric relaxation of the problem and a rounding scheme that produced a (3/2 − 1/k)-approximation algorithm. In this paper, we study their geometric relaxation. In parti cular, we study the worst-case ratio between the value of the relaxation and the value of the minimum multicut (the so-called integrality gap of the relaxation). For k = 3, we show the integrality gap is 12/11, giving tight upper and lower bounds. That is, we exhibit a family of graphs with integrality gaps arbit rarily close to 12/11 and give an algorithm that finds a cut of value 12/11 times the relaxation value. Our lower bound shows that this is the best",1999-05-01,https://www.semanticscholar.org/paper/1dee324b815b462f7f9715cbed197049a628e163,Symposium on the Theory of Computing
384,On Approximating a Scheduling Problem,,,https://www.semanticscholar.org/paper/4b5e7737ab3707bdf2655065c3b11c72358a3873,Journal of combinatorial optimization
2452,Personalized Compass: A Compact Visualization for Direction and Location,"Maps on mobile/wearable devices often make it difficult to determine the location of a point of interest (POI). For example, a POI may exist outside the map or on a background with no meaningful cues. To address this issue, we present Personalized Compass, a self-contained compact graphical location indicator. Personalized Compass uses personal a priori POIs to establish a reference frame, within which a POI in question can then be localized. Graphically, a personalized compass combines a multi-needle compass with an abstract overview map. We analyze the characteristics of Personalized Compass and the existing Wedge technique, and report on a user study comparing them. Personalized Compass performs better for four inference tasks, while Wedge is better for a locating task. Based on our analysis and study results, we suggest the two techniques are complementary and offer design recommendations.",2016-05-07,https://www.semanticscholar.org/paper/222224c3866d418cb323a7455dd354de6f634306,International Conference on Human Factors in Computing Systems
2665,The importance of being mobile: some social consequences of wearable augmented reality systems,"What are the consequences of mobility for augmented reality? This paper explores some of the issues that I believe will be raised by the development and future commonplace adoption of mobile, wearable, augmented reality systems. These include: social influences on tracking accuracy, the importance of appearance and comfort, an increase in collaborative applications, integration with other devices, and implications for personal privacy.",1999-10-20,https://www.semanticscholar.org/paper/917356f2ed3c0a97997c012476787695b6e2e5bd,International Workshop on Automated Reasoning
765,Recursion and Probability,,,https://www.semanticscholar.org/paper/eb1c8d54bcdf24c41226bf3e57a0356089e225da,IFIP TCS
1770,Distance Dependent Infinite Latent Feature Models,"Latent feature models are widely used to decompose data into a small number of components. Bayesian nonparametric variants of these models, which use the Indian buffet process (IBP) as a prior over latent features, allow the number of features to be determined from the data. We present a generalization of the IBP, the distance dependent Indian buffet process (dd-IBP), for modeling non-exchangeable data. It relies on distances defined between data points, biasing nearby data to share more features. The choice of distance measure allows for many kinds of dependencies, including temporal and spatial. Further, the original IBP is a special case of the dd-IBP. We develop the dd-IBP and theoretically characterize its feature-sharing properties. We derive a Markov chain Monte Carlo sampler for a linear Gaussian model with a dd-IBP prior and study its performance on real-world non-exchangeable data.",2011-10-25,https://www.semanticscholar.org/paper/7d9184edefd59b8692f9f41c038446afacea931f,IEEE Transactions on Pattern Analysis and Machine Intelligence
3130,Thin Client Performance for Remote 3-D Image Display,"Several trends in biomedical computing are converging in a way that will require new approaches to telehealth image display. Image viewing is becoming an ""anytime, anywhere"" activity. In addition, organizations are beginning to recognize that healthcare providers are highly mobile and optimal care requires providing information wherever the provider and patient are. Thin-client computing is one way to support image viewing this complex environment. However little is known about the behavior of thin client systems in supporting image transfer in modern heterogeneous networks. Our results show that using thin-clients can deliver acceptable performance over conditions commonly seen in wireless networks if newer protocols optimized for these conditions are used.",,https://www.semanticscholar.org/paper/55bd56ec8241880b92972cbb2b59e2136a4cc745,American Medical Informatics Association Annual Symposium
1248,Search for large extra spatial dimensions in the dielectron and diphoton channels in pp[over ] collisions at sqrt[s]=1.96 TeV.,We report on a search for large extra spatial dimensions in the dielectron and diphoton channels using a data sample of 1.05 fb;{-1} of pp[over ] collisions at a center-of-mass energy of 1.96 TeV collected by the D0 detector at the Fermilab Tevatron Collider. The invariant mass spectrum of the data agrees well with the prediction of the standard model. We find the most restrictive 95% C.L. lower limits on the effective Planck scale between 2.1 and 1.3 TeV for 2 to 7 extra dimensions.,2008-09-16,https://www.semanticscholar.org/paper/b18bea4b54a690146748833fb25aed470d24eacf,Physical Review Letters
2227,Relative α1-anti-trypsin deficiency in systemic sclerosis,"Objective. Neutrophil elastase is secreted by neutrophils during activation and circulates in the plasma where it can play a role in inflammation and fibrosis. This study examines the role of neutrophil elastase in SSc, a systemic CTD that is typified by vascular dysfunction, tissue fibrosis and inflammation. Methods. Serum neutrophil elastase and α1-anti-trypsin concentrations were assessed in SSc patients and healthy controls by ELISA. Serum neutrophil elastase activity was assessed by the elastase-dependent conversion of methoxy-succinyl-alanyl-alanyl-prolyl-valyl-p-nitroanilide to p-nitroanilide using a colourimetric assay. Elastase concentration and activity were correlated with clinical disease features. Results. Serum neutrophil elastase concentration and activity were equivalent in patients and controls; however, in SSc serum, there was an increase in elastase activity for each unit of elastase concentration (P = 0.03). This was due to a decrease in serum α1-anti-trypsin concentrations (P = 0.04). Serum elastase concentration (P = 0.03) and activity (P = 0.02) were significantly lower in RNP-positive patients and serum elastase concentrations were lower in ANA-positive patients (P = 0.003). Conclusions. Relative deficiency in serum α1-anti-trypsin concentrations in SSc could have important and pathogenically relevant effects since elastase has pro-inflammatory and pro-fibrotic roles. Elastase inhibitors are available in clinical practice and could represent potential therapeutic options in SSc.",2011-03-30,https://www.semanticscholar.org/paper/22933f51054d844e747fc011821721c263a21ef4,Rheumatology
717,A Polynomial Time Algorithm for Computing Extinction Probabilities of Multitype Branching Processes,We show that one can approximate the least fixed point solution for a multivariate system of monotone probabilistic polynomial equations in time polynomial in both the encoding size of the system o...,2017-09-26,https://www.semanticscholar.org/paper/b6fa400f80969db103d60af72f7f4c3955474e98,SIAM journal on computing (Print)
3381,Estimating the Longest Increasing Subsequence in Nearly Optimal Time,"Longest Increasing Subsequence (LIS) is a fundamental statistic of a sequence, and has been studied for decades. While the LIS of a sequence of length n can be computed exactly in time $O(n\log n)$, the complexity of estimating the (length of the) LIS in sublinear time, especially when LIS $\ll n$, is still open. We show that for any $n\in\mathbb{N}$ and $\lambda=o(1)$, there exists a (randomized) non-adaptive algorithm that, given a sequence of length n with LIS $\geq\lambda n$, approximates the LIS up to a factor of $1/\lambda^{o(1)}$ in $ n^{o(1)}/\lambda$ time. Our algorithm improves upon prior work substantially in terms of both approximation and run-time: (i) we provide the first sub-polynomial approximation for LIS in sub-linear time; and (ii) our run-time complexity essentially matches the trivial sample complexity lower bound of $\Omega(1/\lambda)$, which is required to obtain any non-trivial approximation of the LIS. As part of our solution, we develop two novel ideas which may be of independent interest. First, we define a new Genuine-LIS problem, in which each sequence element may be either genuine or corrupted. In this model, the user receives unrestricted access to the actual sequence, but does not know a priori which elements are genuine. The goal is to estimate the LIS using genuine elements only, with the minimal number of tests for genuineness. The second idea, Precision Tree, enables accurate estimations for composition of general functions from “coarse” (sub-)estimates. Precision Tree essentially generalizes classical precision sampling, which works only for summations. As a central tool, the Precision Tree is pre-processed on a set of samples, which thereafter is repeatedly used by multiple components of the algorithm, improving their amortized complexity.",2021-12-09,https://www.semanticscholar.org/paper/475af2fc6bb7b11e3853f8587c5fc3378e614f41,IEEE Annual Symposium on Foundations of Computer Science
2514,Enabling large-scale outdoor mixed reality and augmented reality,"While there is significant recent progress in technologies supporting augmented reality for small indoor environments, there is still much work to be done for large outdoor environments. This workshop focuses primarily on research that enables high-quality outdoor Mixed Reality (MR) and Augmented Reality (AR) applications. These research topics include, but are not restricted to: — 3D geo-referenced data (images, point clouds, and models) — Algorithms for object recognition from large databases of geo-referenced data — Algorithms for object tracking in outdoor environment — Multi-cue fusion to achieve improved performance of object detection and tracking — Novel representation schemes to facilitate large-scale content distribution — 3D reasoning to support intelligent augmentation — Novel and improved mobile capabilities for data capture (device sensors), processing, and display — Applications, experiences, and user interface techniques. The workshop will also showcase existing prototypes of applications enabled by these technologies: mirror worlds, high-fidelity virtual environments, applications of panoramic imagery, and user studies relating to these media types. This workshop aims to bring together academic and industrial researchers and to foster discussion amongst participants on the current state of the art and future directions for technologies that enable large-scale outdoor MR and AR applications. The workshop will start with a session in which position statements and overviews of the state of the art are presented. In the afternoon, we will follow up with discussion sessions and a short closing session.",,https://www.semanticscholar.org/paper/dfc4eac460e342bdccbe543d8f1f4a45450fd26c,2011 10th IEEE International Symposium on Mixed and Augmented Reality
810,Model checking of hierarchical state machines,"Model checking is emerging as a practical tool for detecting logical errors in early stages of system design. We investigate the model checking of sequential hierarchical (nested) systems, i.e., finite-state machines whose states themselves can be other machines. This nesting ability is common in various software design methodologies, and is available in several commercial modeling tools. The straightforward way to analyze a hierarchical machine is to flatten it (thus incurring an exponential blow up) and apply a model-checking tool on the resulting ordinary FSM. We show that this flattening can be avoided. We develop algorithms for verifying linear-time requirements whose complexity is polynomial in the size of the hierarchical machine. We also address the verification of branching time requirements and provide efficient algorithms and matching lower bounds.",1998-11-01,https://www.semanticscholar.org/paper/d1278ca288db67c98e0d1f4ab42bb3c384bf3b14,TOPL
2570,Mobile augmented reality interaction techniques for authoring situated media on-site,"We present a set of mobile augmented reality interaction techniques for authoring situated media: multimedia and hypermedia that are embedded within the physical environment. Our techniques are designed for use with a tracked hand-held tablet display with an attached camera, and rely on ""freezing"" the frame for later editing.",2006-10-22,https://www.semanticscholar.org/paper/404682c45eacebc7bf01237cc7a6f400d91a6522,2006 IEEE/ACM International Symposium on Mixed and Augmented Reality
1574,The Dynamic Embedded Topic Model,"Topic modeling analyzes documents to learn meaningful patterns of words. For documents collected in sequence, dynamic topic models capture how these patterns vary over time. We develop the dynamic embedded topic model (D-ETM), a generative model of documents that combines dynamic latent Dirichlet allocation (D-LDA) and word embeddings. The D-ETM models each word with a categorical distribution parameterized by the inner product between the word embedding and a per-time-step embedding representation of its assigned topic. The D-ETM learns smooth topic trajectories by defining a random walk prior over the embedding representations of the topics. We fit the D-ETM using structured amortized variational inference with a recurrent neural network. On three different corpora---a collection of United Nations debates, a set of ACL abstracts, and a dataset of Science Magazine articles---we found that the D-ETM outperforms D-LDA on a document completion task. We further found that the D-ETM learns more diverse and coherent topics than D-LDA while requiring significantly less time to fit.",2019-07-12,https://www.semanticscholar.org/paper/853be905f533fb347d58c463a61bc365e133c2ca,arXiv.org
965,Angular Location of Retinal Nerve Fiber Layer Defect: Association With Myopia and Open-Angle Glaucoma,"Purpose To compare retinal nerve fiber layer (RNFL) defects’ angle measurements determined from the center of the optic disc and Bruch's membrane opening (BMO), as a function of myopia and open-angle glaucoma (OAG) subtypes. Methods In total, 118 patients with OAG were grouped by axial length (AL; high myopia, AL >26 mm; mild to moderate myopia, 24 ≤ AL ≤26 mm; nonmyopia, AL <24 mm) and OAG subtype (normal-tension glaucoma [NTG], high-tension glaucoma [HTG]). The disc and BMO centers were determined by a merged image of red-free fundus photography and spectral-domain optical coherence tomography. The angular location of the RNFL defect close to the fovea (angle α) was measured from the disc center and BMO center, respectively (angle αdisc and angle αBMO). The difference between angle αdisc and αBMO (Δα), as well as the RNFL defect width (angle γ), was evaluated. Results Angle αdisc was smaller in myopic eyes and correlated significantly with AL (P = 0.001), whereas it did not differ among OAG subgroups. Angle αBMO and angle γ were not different in the myopic and OAG subgroups. The Δ α was larger for eyes with higher degree of myopia and had significant correlation with AL (P < 0.001) and was larger in NTG eyes than in HTG eyes (P = 0.023). Conclusions The angular location of the RNFL defect measured from the disc center, but not from the BMO center, was closer to the fovea for glaucomatous eyes with higher values of AL. The present study may facilitate understanding of the characteristic locational pattern of the RNFL defect in myopic glaucomatous eyes.",2020-09-01,https://www.semanticscholar.org/paper/3bf60b64d46409a6ff149bf89004dcdd58753120,Investigative Ophthalmology and Visual Science
2351,Receptor expression and oxidase activity in human neutrophils: Regulation by granulocyte-macrophage colony-stimulating factor and dependence upon protein biosynthesis,,1990-08-01,https://www.semanticscholar.org/paper/6d353860c0f44e12c6c7c74d9164ad9989defdb6,Bioscience Reports
3080,The REmote Patient Education in a Telemedicine Environment Architecture (REPETE).,"The objective of the study was to develop and implement an architecture for remote training that can be used in the narrowband home telemedicine environment. A remote training architecture, the REmote Patient Education in a Telemedicine Environment (REPETE) architecture, using a remote control protocol (RCP) was developed. A set of design criteria was specified. The developed architecture was integrated into the IDEATel home telemedicine unit (HTU) and evaluated against these design criteria using a combination of technical and expert evaluations. Technical evaluation of the architecture demonstrated that remote cursor movements and positioning displayed on the HTU were smooth and effectively real-time. The trainers were able to observe within approximately 2 seconds lag what the patient sees on their HTU screen. Evaluation of the architecture by experts was favorable. Responses to a Likert scale questionnaire regarding audio quality and remote control performance indicated that the expert evaluators thought that the audio quality and remote control performance were adequate for remote training. All evaluators strongly agreed that the system would be useful for training patients. The REPETE architecture supports basic training needs over a narrowband dial-up connection. We were able to maintain an audio chat simultaneously with performing a remote training session, while maintaining both acceptable audio quality and remote control performance. The RCP provides a mechanism to provide training without requiring a trainer to go to the patient's home and effectively supports deictic referencing to on screen objects.",2008-05-01,https://www.semanticscholar.org/paper/8498a23215226060b06e8b694e359a32f4128695,Telemedicine journal and e-health
777,"Testing, optimization, and games",,2004-07-12,https://www.semanticscholar.org/paper/9ebc48c4e82d1a3576cc8ee64e6f6333440e7519,"Proceedings of the 19th Annual IEEE Symposium on Logic in Computer Science, 2004."
2995,Spoq: Scaling Machine-Checkable Systems Verification in Coq,"System software is often large and complex, resulting in many vulnerabilities that can potentially be exploited to compromise the security of a system. Formal verification offers a potential solution to creating bug-free software, but a key impediment to its adoption remains proof cost. We present Spoq, a highly automated verification framework to construct machine-checkable proofs in Coq for system software with much less proof cost. Spoq introduces a novel program structure reconstruction technique to leverage LLVM to translate C code into Coq, supporting full C semantics, including C macros, inline assembly, and compiler directives, so that source code no longer has to be manually modified to be verified. Spoq leverages a layering proof strategy and introduces novel Coq tactics and transformation rules to automatically generate layer specifications and refinement proofs to simplify verification of concurrent system software. Spoq also supports easy integration of manually written layer specifications and refinement proofs. We use Spoq to verify a multiprocessor KVM hypervisor implementation. Verification using Spoq required 70% less proof effort than the manually written specifications and proofs to verify an older implementation. Furthermore, the proofs using Spoq hold for the unmodified implementation that is directly compiled and executed.",,https://www.semanticscholar.org/paper/0421c0442d6efc5ff4a84ba1711e2bf2f2f3e259,USENIX Symposium on Operating Systems Design and Implementation
2991,Infinite Sparse Factor Analysis and Infinite Independent Components Analysis,,2007-09-09,https://www.semanticscholar.org/paper/76e171e8de3fe77d4532ed235e0a0669e420b782,International Conference on Agents
459,Reversible Simulation of Space-Bounded Computations,,1995-05-29,https://www.semanticscholar.org/paper/fb334c05d3a3b7d2d8362118acfa8bc9f7ace2e5,Theoretical Computer Science
343,Networks and Games,,2004-12-19,https://www.semanticscholar.org/paper/3aa9447d39762e7cd2d298f8c99c3e7ad77c810d,International Conference on High Performance Computing
3531,Leighton-Rao might be practical: faster approximation algorithms for concurrent flow with uniform capacities,"In this paper, we describe new algorithms for approximately solving the concurrent multicommodity flow problem with uniform capacities. Our algorithms are much faster than previously known algorithms. Besides being an important problem in its own right, the concurrent flow problem has many interesting applications. Leighton and Rao used concurrent flow to find an approximately ""sparsest cut"" in a graph, and thereby approximately solve a wide variety of graph problems, including minimum feedback arc set, minimum cut linear arrangement, and minimum area layout. We show that their method might be practical by giving an O(m~logm) expected-time randomized algorithm for their concurrent flow problem on an m-edge graph. l~aghavan and Thompson used concurrent flow to approximately solve a channel width minimization problem in VLSI. We give an O(k3/2(m+n log n)) expectedtime randomized algorithm and an O(k min{n, k}(m + n log n) log k) deterministic algorithm for this problem when the channel width is O(logn), where k denotes the number of wires to be routed in an n-node, m-edge network, *Research partially suppor ted by ONR grant N00014-88-K0243 and DARPA grant N00039-88-C0113 at Harvard University tSuppor t provided by Air Force Contract AFOSI:t-86-0078, and by an NSF PYI awarded to David Shmoya, with matching funds from IBM, Sun Microsysterns, and UPS. t Research partially supported by NSF grant DMS87-06133. Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission. 1 I n t r o d u c t i o n The multicommodity flow problem involves shipping several different commodities from their respective sources to their sinks in a single network with the total amount of flow going through an edge limited by its capacity. Here we consider the concurrent flow problem with uniform capacities, which is the problem of finding a multicommodity flow that minimizes the maximum total flow on any edge (the congestion). An important special case of the problem, called the unit-demand unit-capacity concurrent flow problem, arises when the amount of flow that needs to be shipped is the same for each commodity. For both versions of the problem we give algorithms that, for any positive e, find a solution whose congestion is no more than (1 + e) times the minimum congestion. Our algorithms significantly improve the time required for finding such approximately optimal solutions. We shall use n, m and k to denote the number of nodes, edges and commodities. Leighton and Rao [7] showed how to use the solution to a unit-capacity unit-demand concurrent fiow problem to find an approximate ""sparsest cut"" of a graph. As a consequence, they gave the first polylog-timesoptimal approximation algorithms for a wide variety of graph problems. The computational bottleneck of their method is solving a unit-capacity unit-demand concurrent flow problem with O(n) commodities. They appealed to linear programming techniques to show that the problem can be solved in polynomial time. The new approximation algorithm greatly improves the resulting running time. T h e o r e m 1.1 For any fixed 0 < e < 1, a (1 + e)-factor approximation to the unit-capacity unit-demand concurrent flow problem can be found by a randomized algorithm in O((k + m)mlogm) time, where the constant depends",1990-04-01,https://www.semanticscholar.org/paper/f9aea0cac404108bbbc71eb1f91afdc89a4a4980,Symposium on the Theory of Computing
1703,Population Empirical Bayes,"Bayesian predictive inference analyzes a dataset to make predictions about new observations. When a model does not match the data, predictive accuracy suffers. We develop population empirical Bayes (POP-EB), a hierarchical framework that explicitly models the empirical population distribution as part of Bayesian analysis. We introduce a new concept, the latent dataset, as a hierarchical variable and set the empirical population as its prior. This leads to a new predictive density that mitigates model mismatch. We efficiently apply this method to complex models by proposing a stochastic variational inference algorithm, called bumping variational inference (BUMP-VI). We demonstrate improved predictive accuracy over classical Bayesian inference in three models: a linear regression model of health data, a Bayesian mixture model of natural images, and a latent Dirichlet allocation topic model of scientific documents.",2014-11-02,https://www.semanticscholar.org/paper/7b22ad7650211047208dfa2cd1cc69cea71851f8,Conference on Uncertainty in Artificial Intelligence
967,Relationship between 3D-MRI Eyeball shape and Optic Nerve Head Morphology.,,2020-09-08,https://www.semanticscholar.org/paper/9564af6a45630e0c305efbd424094caddcc1a17d,"Ophthalmology (Rochester, Minn.)"
113,Merging Ranks from Heterogeneous Internet Sources,"Many sources on the Internet and elsewhere rank the objects in query results according to how well these objects match the original query. For example, a real-estate agent might rank the available houses according to how well they match the user's preferred location and price. In this environment, ``meta-brokers'' usually query multiple autonomous, heterogeneous sources that might use varying result-ranking strategies. A crucial problem that a meta-broker then faces is extracting from the underlying sources the top objects for a user query according to the meta-broker's ranking function. This problem is challenging because these top objects might not be ranked high by the sources where they appear. In this paper we discuss strategies for solving this ``meta-ranking'' problem. In particular, we present a condition that a source must satisfy so that a meta-broker can extract the top objects for a query from the source without examining its entire contents. Not only is this condition necessary but it is also sufficient, and we show an efficient algorithm to extract the top objects from sources that satisfy the given condition.",1997-08-25,https://www.semanticscholar.org/paper/15c5cd0f4bc3fa966924a8a76118f6011dffba41,Very Large Data Bases Conference
1853,Correlated Topic Models,"Topic models, such as latent Dirichlet allocation (LDA), can be useful tools for the statistical analysis of document collections and other discrete data. The LDA model assumes that the words of each document arise from a mixture of topics, each of which is a distribution over the vocabulary. A limitation of LDA is the inability to model topic correlation even though, for example, a document about genetics is more likely to also be about disease than x-ray astronomy. This limitation stems from the use of the Dirichlet distribution to model the variability among the topic proportions. In this paper we develop the correlated topic model (CTM), where the topic proportions exhibit correlation via the logistic normal distribution [1]. We derive a mean-field variational inference algorithm for approximate posterior inference in this model, which is complicated by the fact that the logistic normal is not conjugate to the multinomial. The CTM gives a better fit than LDA on a collection of OCRed articles from the journal Science. Furthermore, the CTM provides a natural way of visualizing and exploring this and other unstructured data sets.",2005-12-05,https://www.semanticscholar.org/paper/e49da956b23ed295541c80939d4a1261d0a1022f,Neural Information Processing Systems
2742,Editable graphical histories: the video,"INTRODUCTION Command histories are an important component of good user interfaces [3]. They allow users to review sequences of previously executed commands, and can provide an interface to an undo facility. Furthermore, they can support either a simple redo mechanism, or a more sophisticated macro-by-demonstration capability. Providing a visual representation of command histories in a graphical user interface presents a number of difficulties. Editable graphical histories [1, 2], a history representation that we have developed based on a comic-strip metaphor, overcomes many of these. Here we describe editable graphical histories, and in the accompanying videotape we demonstrate our test-bed implementation in the graphical editing mode of Chimera, a multi-modal editor.",1991-03-01,https://www.semanticscholar.org/paper/acf272f3f2d5f40cad6842f7f50f462e4c5ab94f,International Conference on Human Factors in Computing Systems
1010,Retinal nerve fiber layer defect and cerebral small vessel disease.,"PURPOSE
To determine whether retinal nerve fiber layer defect (RNFLD) is associated with cerebral small vessel diseases (SVDs) and to identify risk factors for RNFLD.


METHODS
A total of 4421 Korean subjects who underwent health checkups including brain magnetic resonance imaging (MRI) and fundus photography between January 2008 and October 2009 were included in this study. Co-morbid systemic diseases including hypertension, diabetes mellitus, and stroke or ocular diseases were evaluated using detailed questionnaires and medical records. Two experienced ophthalmologists assessed RNFLD on fundus photographs, according to the definition that describes the condition as marked thinning or absence of retinal nerve fiber layer bundles.


RESULTS
RNFLD was detected in 238 of 4395 eligible subjects, and the estimated prevalence was 5.4%. Multivariate regression analysis results showed the prevalence of RNFLD to be significantly higher in hypertensive subjects (odds ratio [OR], 1.73; 95% confidence interval [CI], 1.28-2.34), in those with cerebral SVD based on MRI (OR, 1.58; 95% CI, 1.17-2.12), and in male (OR, 1.47; 95% CI, 1.10-1.96) and older subjects (OR, 1.02; 95% CI, 1.00-1.03). Among the cases of cerebral SVD, white matter lesions (WMLs) were associated with RNFLD, whereas lacunar infarctions were not significantly associated with it.


CONCLUSIONS
The results indicate that RNFLD may be related to the presence of cerebral SVD, particularly WMLs. Furthermore, being older and male and having hypertension increase the risk of RNFLD.",2011-08-01,https://www.semanticscholar.org/paper/9525138586f220e04398fc21edadc191949565d9,Investigative Ophthalmology and Visual Science
2871,Association of a macrophage galactoside‐binding protein with Mycobacterium‐containing phagosomes,"Mycobacteria reside intracellularly in a vacuole that allows it to circumvent the antimicrobial environment of the host macrophage. Although the mycobacterial phagosome exhibits selective fusion with vesicles of the endosomal system, identification of host and bacterial factors associated with phagosome bio‐genesis is limited. To identify these potential factors, mAbs were generated to a membrane preparation of mycobacterial phagosomes isolated from M. tuberculosis‐infected macrophages. A mAb recognizing a 32–35 kDa macrophage protein associated with the phagosomal membrane of Mycobacterium was identified. N‐terminal sequence analysis identified this protein as Mac‐2 or galectin‐3, a galactoside‐binding protein of macrophages. Galectin‐3 (gal‐3) was shown to accumulate in Mycobacterium‐containing phagosomes during the course of infection. This accumu‐lation was specific for phagosomes containing live mycobacteria and occurred primarily at the cytosolic face of the phagosome membrane. In addition, bind‐ing of gal‐3 to mycobacterial phosphatidylinositol mannosides (PIMs) demonstrated a novel interaction between host carbohydrate‐binding proteins and released mycobacterial glycolipids. Infection of macrophages from gal‐3‐deficient mice indicated that the protein did not play a role in infection in vitro. In contrast, infection of gal‐3‐deficient mice revealed a reduced capacity to clear late but not early infection.",2002-03-01,https://www.semanticscholar.org/paper/8024de6479d2caff1b87e30c6ae459620e902cfe,Cellular Microbiology
3570,Semantically Enhanced Containers for Concurrent Real-Time Systems,"Future space missions, such as Mars Science Laboratory, are built upon computing platforms providing a high degree of autonomy and diverse functionality. The increased sophistication of robotic spacecraft has skyrocketed the complexity and cost of its software development and validation. The engineering of autonomous spacecraft software relies on the availability and application of advanced methods and tools that deliver safe concurrent synchronization as well as enable the validation of domain-specific semantic invariants. The software design and certification methodologies applied at NASA do not reach the level of detail of providing guidelines for the development of reliable concurrent software. To achieve effective and safe concurrent interactions as well as guarantee critical domain-specific properties in code, we introduce the notion of a Semantically Enhanced Container (SEC). A SEC is a data structure engineered to deliver the flexibility and usability of the popular ISO C++ Standard Template Library containers, while at the same time it is hand-crafted to guarantee domain-specific policies. We demonstrate the SEC proof-of-concept by presenting a shared nonblocking SEC vector. To eliminate the hazards of the ABA problem (a fundamental problem in lock-free programming), we introduce an innovative library for querying C++ semantic information. Our SEC design aims at providing an effective model for shared data access within the JPL's Mission Data System. Our test results show that the SEC vector delivers significant performance gains (a factor of 3 or more) in contrast to the application of nonblocking synchronization amended with the traditional ABA avoidance scheme.",2009-04-14,https://www.semanticscholar.org/paper/4cd6b68ec9a2ffd18834b1750e64c670bc7b01da,European Conference on the Engineering of Computer-Based Systems
1143,Search for dark photons from supersymmetric hidden valleys.,"We search for a new light gauge boson, a dark photon, with the D0 experiment. In the model we consider, supersymmetric partners are pair produced and cascade to the lightest neutralinos that can decay into the hidden sector state plus either a photon or a dark photon. The dark photon decays through its mixing with a photon into fermion pairs. We therefore investigate a previously unexplored final state that contains a photon, two spatially close leptons, and large missing transverse energy. We do not observe any evidence for dark photons and set a limit on their production.",2009-05-11,https://www.semanticscholar.org/paper/15055385a0615ead20447ab729a331fe9e6990b4,Physical Review Letters
2468,From GPS and virtual globes to spatial computing - 2020,,2015-10-01,https://www.semanticscholar.org/paper/6e36881442c538c7273554eddde473bf4ef5c8c8,GeoInformatica
3232,Temporal structuring of vigilance behaviour by female Thomson's gazelles with hidden fawns,,2018-11-01,https://www.semanticscholar.org/paper/cf9f84c44e8ca0e6cf867b66fee55bf28a6bd404,Animal Behaviour
3470,Improved bicriteria existence theorems for scheduling,"Two common objectives for evaluating a schedule are the makespan, or schedule length, and the average completion time. In this note, we give improved bounds on the existence of schedules that simultaneously optimize both criteria. In a scheduling problem, we are given n jobs and m machines. With each job j we associate a nonnegative weight wj . A schedule is an assignment of jobs to machines over time, and yields a completion time Cj for each job j. We then define the average completion time as ∑n j=1 wjCj and the makespan as Cmax = maxj Cj . We use C opt max and ∑ wjC ∗ j to denote the optimal makespan and average completion time. We will give results which will hold for a wide variety of combinatorial scheduling problems. In particular, we require that valid schedules for the problem satisfy two very general conditions. First, if we take a valid schedule S and remove from it all jobs that complete after time t, the schedule remains a valid schedule for those jobs that remain. Second, given two valid schedules S1 and S2 for two sets J1 and J2 of jobs (where J1 ∩ J2 is potentially nonempty), the composition of S1 and S2, obtained by appending S2 to the end of S1, and removing from S2 all jobs that are in J1 ∩ J2, is a valid schedule for J1 ∪ J2. For the rest of this note we will make claims about “any” scheduling problem, and mean any problem that satisfies the two conditions above. In addition, if a schedule has Cmax ≤ αC opt max and ∑",2002-05-09,https://www.semanticscholar.org/paper/d33a63896f497e5f9d4bb23b59a22ec4f2a343f8,ACM-SIAM Symposium on Discrete Algorithms
3172,Savannas are vital but overlooked carbon sinks,,2022-01-28,https://www.semanticscholar.org/paper/022618327c8a064d0544b2bb7562de2c08f45d30,Science
3511,Short Superstrings and the Structure of Overlapping Strings,"Given a collection of strings S = [s1,...,sn] over an alphabet sigma, a superstring alpha of S is a string containing each si as a substring, that is, for each i, 1 < or = i < or = n, alpha contains a block of magnitude of si consecutive characters that match si exactly. The shortest superstring problem is the problem of finding a superstring alpha of minimum length. The shortest superstring problem has applications in both computational biology and data compression. The shortest superstring problem is NP-hard (Gallant et al., 1980); in fact, it was recently shown to be MAX SNP-hard (Blum et al., 1994). Given the importance of the applications, several heuristics and approximation algorithms have been proposed. Constant factor approximation algorithms have been given in Blum et al. (1994) (factor of 3), Teng and Yao (1993) (factor of 2 8/9), Czumaj et al. (1994) (factor of 2 5/6), and Kosaraju et al. (1994) (factor of 2 50/63). Informally, the key to any algorithm for the shortest superstring problem is to identify sets of strings with large amounts of similarity, or overlap. Although the previous algorithms and their analyses have grown increasingly sophisticated, they reveal remarkably little about the structure of strings with large amounts of overlap. In this sense, they are solving a more general problem than the one at hand. In this paper, we study the structure of strings with large amounts of overlap and use our understanding to give an algorithm that finds a superstring whose length is no more than 2 3/4 times that of the optimal superstring. Our algorithm runs in O(magnitude of S + n3) time, which matches that of previous algorithms. We prove several interesting properties about short periodic strings, allowing us to answer questions of the following form: Given a string with some periodic structure, characterize all the possible periodic strings that can have a large amount of overlap with the first string.",,https://www.semanticscholar.org/paper/d72513c2783daa0d289ed04c4bec9e877df0678b,J. Comput. Biol.
3689,Understanding Zero-Shot Adversarial Robustness for Large-Scale Models,"Pretrained large-scale vision-language models like CLIP have exhibited strong generalization over unseen tasks. Yet imperceptible adversarial perturbations can significantly reduce CLIP's performance on new tasks. In this work, we identify and explore the problem of \emph{adapting large-scale models for zero-shot adversarial robustness}. We first identify two key factors during model adaption -- training losses and adaptation methods -- that affect the model's zero-shot adversarial robustness. We then propose a text-guided contrastive adversarial training loss, which aligns the text embeddings and the adversarial visual features with contrastive learning on a small set of training data. We apply this training loss to two adaption methods, model finetuning and visual prompt tuning. We find that visual prompt tuning is more effective in the absence of texts, while finetuning wins in the existence of text guidance. Overall, our approach significantly improves the zero-shot adversarial robustness over CLIP, seeing an average improvement of over 31 points over ImageNet and 15 zero-shot datasets. We hope this work can shed light on understanding the zero-shot adversarial robustness of large-scale models.",2022-12-14,https://www.semanticscholar.org/paper/16596dd03fa40ba278f9533ea9986982dcc81fb6,International Conference on Learning Representations
84,PERSIVAL demo: categorizing hidden-web resources,"The information available in electronic form continues to grow at an exponential rate and this trend is expected to continue. Although traditional search engines like AltaVista can address common information needs, they ignore the often valuable information that is “hidden” behind search interfaces, the so-called “hidden web.” Automating the classification of “hidden web” resources is challenging, since the contents of these collections are available only by querying, not by traditional crawling. For example, consider the PubMed medical database from the National Library of Medicine, which stores medical bibliographic information and links to full-text journals accessible through the web. This database is accessible through a query interface. A query to PubMed with keyword “cancer” returns 1,313,266 matches, which are high-quality citations to medical articles, stored locally at the PubMed site. The contents of PubMed are not “crawlable” by traditional search engines. Thus, a query on AltaVista for all the pages in the PubMed site with keyword “cancer” returns only 16,380 matches. Hence, techniques that need to have the documents available for inspection are not applicable to analyze and classify the “hidden web” resources. The ability to access these resources and organize them for subsequent use is a central component of the Digital Libraries Initiative – Phase 2 (DLI2) project at Columbia University. The project is named PERSIVAL and its main goal is to provide personalized access to a distributed patient care digital library with all kinds of collections. The manual inspection and classification of these resources is a non-scalable solution, so we developed a novel technique to automate this task.",,https://www.semanticscholar.org/paper/345dfe61c6932e0c0c6040fd8e00b4b55d603de6,ACM/IEEE Joint Conference on Digital Libraries
751,"Equilibria, Fixed Points, and Complexity Classes","Many models from a variety of areas involve the computation of an equilibrium or fixed point of some kind. Examples include Nash equilibria in games; market equilibria; computing optimal strategies and the values of competitive games (stochastic and other games); stable configurations of neural networks; analysing basic stochastic models for evolution like branching processes and for language like stochastic context-free grammars; and models that incorporate the basic primitives of probability and recursion like recursive Markov chains. It is not known whether these problems can be solved in polynomial time. There are certain common computational principles underlying different types of equilibria, which are captured by the complexity classes PLS, PPAD, and FIXP. Representative complete problems for these classes are, respectively, pure Nash equilibria in games where they are guaranteed to exist, (mixed) Nash equilibria in two-player normal form games, and (mixed) Nash equilibria in normal form games with three (or more) players. This paper reviews the underlying computational principles and the corresponding classes.",2008-02-01,https://www.semanticscholar.org/paper/38a305e7255b1a9a448741a60dbfd3ce290ca910,Symposium on Theoretical Aspects of Computer Science
2738,Software technology for wireless mobile computing,"Some of the possibilities and requirements for mobile computing on wireless local area networks (LANs) are discussed from the systems software viewpoint. The design of the Student Electronic Notebook (SEN) is sketched to provide a partial catalog of problems in building a real system for wireless mobile computing. This project was initiated to investigate the potential of wireless mobile computing to reshape education. Some of the key directions for research in software technology for wireless, mobile computing are examined. Some of the authors' experience with wireless LANs is related.<<ETX>>",1991-11-01,https://www.semanticscholar.org/paper/452335899f0bf0164add7de30f112047ee80d526,IEEE Network
879,Embedding Planar Graphs in Four Pages,,,https://www.semanticscholar.org/paper/cdf9707d7e0b46bfa79f60642a3aac665f438cef,Journal of computer and system sciences (Print)
1804,Simultaneous image classification and annotation,"Image classification and annotation are important problems in computer vision, but rarely considered together. Intuitively, annotations provide evidence for the class label, and the class label provides evidence for annotations. For example, an image of class highway is more likely annotated with words “road,” “car,” and “traffic” than words “fish,” “boat,” and “scuba.” In this paper, we develop a new probabilistic model for jointly modeling the image, its class label, and its annotations. Our model treats the class label as a global description of the image, and treats annotation terms as local descriptions of parts of the image. Its underlying probabilistic assumptions naturally integrate these two sources of information. We derive an approximate inference and estimation algorithms based on variational methods, as well as efficient approximations for classifying and annotating new images. We examine the performance of our model on two real-world image data sets, illustrating that a single model provides competitive annotation performance, and superior classification performance.",2009-06-20,https://www.semanticscholar.org/paper/7fdf31d5ebdd293b3027e6555e256a936ff5515a,2009 IEEE Conference on Computer Vision and Pattern Recognition
732,Polynomial Time Algorithms for Branching Markov Decision Processes and Probabilistic Min(Max) Polynomial Bellman Equations,,2012-02-21,https://www.semanticscholar.org/paper/452dbc46ce40de491066df9abc142637b0190481,"International Colloquium on Automata, Languages and Programming"
2287,Apoptosis is rapidly triggered by antisense depletion of MCL-1 in differentiating U937 cells.,"Mcl-1 is a member of the Bcl-2 protein family, which has been shown to delay apoptosis in transfection and/or overexpression experiments. As yet no gene knockout mice have been engineered, and so there is little evidence to show that loss of Mcl-1 expression is sufficient to trigger apoptosis. U937 cells constitutively express the antiapoptotic protein Bcl-2; but during differentiation, in response to the phorbol ester PMA (phorbol 12 beta-myristate 13 alpha-acetate), Mcl-1 is transiently induced. The purpose of this investigation was to determine the functional role played by Mcl-1 in this differentiation program. Mcl-1 expression was specifically disrupted by chimeric methylphosphonate/phosphodiester antisense oligodeoxynucleotides to just 5% of control levels. The depletion of Mcl-1 messenger RNA (mRNA) and protein was both rapid and specific, as indicated by the use of control oligodeoxynucleotides and analysis of the expression of other BCL2 family members and PMA-induced tumor necrosis factor-alpha (TNF-alpha). Specific depletion of Mcl-1 mRNA and protein, in the absence of changes in cellular levels of Bcl-2, results in a rapid entry into apoptosis. Levels of the proapoptotic protein Bax remained unchanged during differentiation, while Bak expression doubled within 24 hours. Apoptosis was detected within 4 hours of Mcl-1 antisense treatment by a variety of parameters including a novel live cell imaging technique allowing correlation of antisense treatment and apoptosis in individual cells. The induction of Mcl-1 is required to prevent apoptosis during differentiation of U937 cells, and the constitutive expression of Bcl-2 is unable to compensate for the loss of Mcl-1. (Blood. 2000;96:1756-1763)",2000-09-01,https://www.semanticscholar.org/paper/7ec2e261e337b086d8089fb39d5712893294c9f0,Blood
252,An analytical contrast between fitness maximization and selection for mixability.,,2011-03-21,https://www.semanticscholar.org/paper/615815563886fd7152c59483230c7c6ebf5fcc27,Journal of Theoretical Biology
3664,An overview of C++ (abstract only),,1986-10-01,https://www.semanticscholar.org/paper/cba93c9362c4bea2e55db5eac1d7f19ef007dea4,OOPWORK '86
1513,Amortized Variational Inference: When and Why?,"Amortized variational inference (A-VI) is a method for approximating the intractable posterior distributions that arise in probabilistic models. The defining feature of A-VI is that it learns a global inference function that maps each observation to its local latent variable's approximate posterior. This stands in contrast to the more classical factorized (or mean-field) variational inference (F-VI), which directly learns the parameters of the approximating distribution for each latent variable. In deep generative models, A-VI is used as a computational trick to speed up inference for local latent variables. In this paper, we study A-VI as a general alternative to F-VI for approximate posterior inference. A-VI cannot produce an approximation with a lower Kullback-Leibler divergence than F-VI's optimal solution, because the amortized family is a subset of the factorized family. Thus a central theoretical problem is to characterize when A-VI still attains F-VI's optimal solution. We derive conditions on both the model and the inference function under which A-VI can theoretically achieve F-VI's optimum. We show that for a broad class of hierarchical models, including deep generative models, it is possible to close the gap between A-VI and F-VI. Further, for an even broader class of models, we establish when and how to expand the domain of the inference function to make amortization a feasible strategy. Finally, we prove that for certain models -- including hidden Markov models and Gaussian processes -- A-VI cannot match F-VI's solution, no matter how expressive the inference function is. We also study A-VI empirically. On several examples, we corroborate our theoretical results and investigate the performance of A-VI when varying the complexity of the inference function. When the gap between A-VI and F-VI can be closed, we find that the required complexity of the function need not scale with the number of observations, and that A-VI often converges faster than F-VI.",2023-07-20,https://www.semanticscholar.org/paper/376f88d661e1b299bfdef13c9a404cbc76b9566b,arXiv.org
428,A Microeconomic View of Data Mining,,1998-12-01,https://www.semanticscholar.org/paper/e0c4fac2c53c3ef840cf6ba278eaebad61a1e47d,Data mining and knowledge discovery
2920,Integrative genetic analysis of the amyotrophic lateral sclerosis spinal cord implicates glial activation and suggests new risk genes,"Amyotrophic lateral sclerosis (ALS) is a progressively fatal neurodegenerative disease affecting motor neurons in the brain and spinal cord. We used 380 post-mortem tissue RNA-seq transcriptomes from 154 ALS cases and 49 control individuals from cervical, thoracic, and lumbar spinal cord segments to investigate the gene expression response to ALS. We observed an increase in microglia and astrocyte expression, accompanied by a decrease in oligodendrocytes. By creating a gene co-expression network in the ALS samples, we identify several activated microglia modules that negatively correlate with retrospective disease duration. We map molecular quantitative trait loci and find several potential ALS risk loci that may act through gene expression or splicing in the spinal cord and assign putative cell-types for FNBP1, ACSL5, SH3RF1 and NFASC. Finally, we outline how repeat expansions that alter splicing of C9orf72 are tagged by common variants, and use this to suggest ATXN3 as a putative risk gene.",2021-09-02,https://www.semanticscholar.org/paper/d649792fe67746960ee38f5ef5ca0fbb674e55eb,medRxiv
3566,ViewpointWhat should we teach new software developers? Why?,Fundamental changes to computer science education are required to better address the needs of industry.,,https://www.semanticscholar.org/paper/eda811221528156924ab2eb95b578c92fab609e0,CACM
3588,Open multi-methods for c++,"Multiple dispatch - the selection of a function to be invoked based on the dynamic type of two or more arguments - is a solution to several classical problems in object-oriented programming. Open multi-methods generalize multiple dispatch towards open-class extensions, which improve separation of concerns and provisions for retroactive design. We present the rationale, design, implementation, and performance of a language feature, called open multi-methods, for C++. Our open multi-methods support both repeated and virtual inheritance. Our call resolution rules generalize both virtual function dispatch and overload resolution semantics. After using all information from argument types, these rules can resolve further ambiguities by using covariant return types. Great care was taken to integrate open multi-methods with existing C++ language features and rules. We describe a model implementation and compare its performance and space requirements to existing open multi-method extensions and workaround techniques for C++. Compared to these techniques, our approach is simpler to use, catches more user mistakes, and resolves more ambiguities through link-time analysis, runs significantly faster, and requires less memory. In particular, the runtime cost of calling an open multi method is constant and less than the cost of a double dispatch (two virtual function calls). Finally, we provide a sketch of a design for open multi-methods in the presence of dynamic loading and linking of libraries.",2007-10-01,https://www.semanticscholar.org/paper/3710838c07fca4a6e55af64b1a7d1c563e3f04a7,International Conference on Generative Programming: Concepts and Experiences
2787,"Role of skin and gut microbiota in the pathogenesis of psoriasis, an inflammatory skin disease",,2020-06-01,https://www.semanticscholar.org/paper/a1a55251af8869f143444ab813cada3a2ba56031,Medicine in Microecology
729,"Stochastic Context-Free Grammars, Regular Languages, and Newton's Method",,2013-02-26,https://www.semanticscholar.org/paper/749bdc64263934e431592875f2e6f5565ffab121,"International Colloquium on Automata, Languages and Programming"
3709,Towards a Unifying Framework for Formal Theories of Novelty,"Managing inputs that are novel, unknown, or out-of-distribution is critical as an agent moves from the lab to the open world. Novelty-related problems include being tolerant to novel perturbations of the normal input, detecting when the input includes novel items, and adapting to novel inputs. While significant research has been undertaken in these areas, a noticeable gap exists in the lack of a formalized definition of novelty that transcends problem domains. As a team of researchers spanning multiple research groups and different domains, we have seen, first hand, the difficulties that arise from ill-specified novelty problems, as well as inconsistent definitions and terminology. Therefore, we present the first unified framework for formal theories of novelty and use the framework to formally define a family of novelty types. Our framework can be applied across a wide range of domains, from symbolic AI to reinforcement learning, and beyond to open world image recognition. Thus, it can be used to help kick-start new research efforts and accelerate ongoing work on these important novelty-related problems.",2021-05-18,https://www.semanticscholar.org/paper/357411070c0779e6c29db11532e690db2bb8a64c,AAAI Conference on Artificial Intelligence
684,Multiple Instance Learning with Manifold Bags,"In many machine learning applications, labeling every instance of data is burdensome. Multiple Instance Learning (MIL), in which training data is provided in the form of labeled bags rather than labeled instances, is one approach for a more relaxed form of supervised learning. Though much progress has been made in analyzing MIL problems, existing work considers bags that have a finite number of instances. In this paper we argue that in many applications of MIL (e.g. image, audio, etc.) the bags are better modeled as low dimensional manifolds in high dimensional feature space. We show that the geometric structure of such manifold bags affects PAC-learnability. We discuss how a learning algorithm that is designed for finite sized bags can be adapted to learn from manifold bags. Furthermore, we propose a simple heuristic that reduces the memory requirements of such algorithms. Our experiments on real-world data validate our analysis and show that our approach works well.",2011-06-28,https://www.semanticscholar.org/paper/097968d3115689d1df054b8678bc290df5d63bc5,International Conference on Machine Learning
579,Updaies of relational views,"We study the problem of translating updates of database views. View updates are disambiguated by requning that a specified view complement (i.e. a second view which contnms all the information omitted from the given view) remains constant during the translation. We study some of the computational problems related to the application of this general methodology in the context of relational databases. We restrict our attention to projective views of databases which consist of a single relation and satisfy functional dependencies. We first characterize complementary views and show that finding a minimum complement of a given view is NP-complete. We then study in detail the problem of translating the insertion of a tuple into a view and extend our results to the cases of deletion and replacement of a tuple. Finally we define and study a new kind of dependencies the explicit functional dependencies, which intuitively state that some part of the database information can be computed from the rest.",1983-03-21,https://www.semanticscholar.org/paper/2128bd7851a0303a86f1ae8660eac005a734a7b2,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems
2847,The role of galectin-3 in promotion of the inflammatory response.,"Galectin-3 is a member of a family of beta-galactoside-binding animal lectins and is distinct from other members by the presence of tandem repeats in its N-terminal region. Like other members, galectin-3 lacks a classical signal sequence, but the protein is secreted by a nonclassical secretary pathway and can function extracellularly in an autocrine or paracrine fashion. Galectin-3 is able to oligomerize and participate in multivalent interactions with cell surface and extracellular matrix glycans, through lectin-carbohydrate interactions, thus affecting cellular functions. Galectin-3 is detectable in the cytosol and nucleus and the intracellular protein may bind to other cytoplasmic and nuclear proteins by protein-protein interactions. In this manner, galectin-3 is able to influence intracellular signaling pathways and exert various functions. Galectin-3 is expressed by virtually all immune and inflammatory cell types, either constitutively or in a inducible fashion. A large body of work has demonstrated the role of galectin-3 in regulation of the functions of these cells. The use of galectin-3-deficient mice has provided additional evidence for this protein's contribution to the inflammatory response. Thus, galectin-3 may be a therapeutic target for various inflammatory diseases.",2007-09-01,https://www.semanticscholar.org/paper/2e4de81443f0f3241b115c769fed770880baa8b2,Drug News and Perspectives
2022,"A multi-period inventory model to incorporate with inventory age, accounting principle, and product structure: A case study in a make-to-stock semiconductor integrated device manufacturer","Statement of Financial Accounting Standards (SFAS) No.10 has been declared as accounting principle for allowance for reduction of inventory to market in Taiwan since December 31, 2008. It has become more difficult for semiconductor integrated device manufacturers, using make-to-stock manufacturing strategy and possessing inventory accounting for about 14% total costs, to maintain robust records in financial statements. A case company has to write down loss of 2% to 100% of total inventory cost for products with inventory ages of 3 to more than 18 months. However, average cycle times of producing flash memory are about three months. In other words, when system variation and safety stock policy are further taken into account, the company has to write down allowance for reduction of inventory to market for most work-in-process inventory. However, little research has been done with these regards to address practical management of operations according to inventory aging process. Therefore, this study aims to propose a holistic inventory model to incorporate with inventory ages, accounting principles, and product structures (e.g., bill of material) for robust estimation of inventory cost to reduce the impact of carrying value fluctuation of inventory. An empirical study will be conducted in a Taiwanese semiconductor manufacturer.",2010-07-25,https://www.semanticscholar.org/paper/1d55c625917e08250612314a94f0e7d8d8ebe16f,The 40th International Conference on Computers & Indutrial Engineering
3377,Learning-Augmented Online Packet Scheduling with Deadlines,"The modern network aims to prioritize critical traffic over non-critical traffic and effectively manage traffic flow. This necessitates proper buffer management to prevent the loss of crucial traffic while minimizing the impact on non-critical traffic. Therefore, the algorithm's objective is to control which packets to transmit and which to discard at each step. In this study, we initiate the learning-augmented online packet scheduling with deadlines and provide a novel algorithmic framework to cope with the prediction. We show that when the prediction error is small, our algorithm improves the competitive ratio while still maintaining a bounded competitive ratio, regardless of the prediction error.",2023-05-11,https://www.semanticscholar.org/paper/60ae6a3626b3150bb0a80ead1062c5b314585a5d,arXiv.org
3708,UnweaveNet: Unweaving Activity Stories,"Our lives can be seen as a complex weaving of activities; we switch from one activity to another, to maximise our achievements or in reaction to demands placed upon us. Observing a video of unscripted daily activities, we parse the video into its constituent activity threads through a process we call unweaving. To accomplish this, we introduce a video representation explicitly capturing activity threads called a thread bank, along with a neural controller capable of detecting goal changes and resuming of past activities, together forming UnweaveNet. We train and evaluate UnweaveNet on sequences from the unscripted egocentric dataset EPIC-KITCHENS. We propose and showcase the efficacy of pretraining UnweaveNet in a self-supervised manner.",2021-12-19,https://www.semanticscholar.org/paper/0fb956fe2bc1272ed53fecb47060c0c3dcff739c,Computer Vision and Pattern Recognition
1340,Search for neutral supersymmetric Higgs Bosons in multijet events at sqrt[s]=1.96 TeV.,"We have performed a search for neutral Higgs bosons produced in association with bottom quarks in pp collisions, using 260 pb-1 of data collected with the D0 detector in Run II of the Fermilab Tevatron Collider. The cross sections for these processes are enhanced in many extensions of the standard model (SM), such as in its minimal supersymmetric extension at large tanbeta. The results of our analysis agree with expectations from the SM, and we use our measurements to set upper limits on the production of neutral Higgs bosons in the mass range of 90 to 150 GeV.",2005-04-09,https://www.semanticscholar.org/paper/34eed6b775a121ae006f75c06bf4f04fc9f96cdf,Physical Review Letters
181,Smoothed Analysis of Discrete Tensor Decomposition and Assemblies of Neurons,"We analyze linear independence of rank one tensors produced by tensor powers of randomly perturbed vectors. This enables efficient decomposition of sums of high-order tensors. Our analysis builds upon [BCMV14] but allows for a wider range of perturbation models, including discrete ones. We give an application to recovering assemblies of neurons. 
Assemblies are large sets of neurons representing specific memories or concepts. The size of the intersection of two assemblies has been shown in experiments to represent the extent to which these memories co-occur or these concepts are related; the phenomenon is called association of assemblies. This suggests that an animal's memory is a complex web of associations, and poses the problem of recovering this representation from cognitive data. Motivated by this problem, we study the following more general question: Can we reconstruct the Venn diagram of a family of sets, given the sizes of their $\ell$-wise intersections? We show that as long as the family of sets is randomly perturbed, it is enough for the number of measurements to be polynomially larger than the number of nonempty regions of the Venn diagram to fully reconstruct the diagram.",2018-10-28,https://www.semanticscholar.org/paper/6e4fab41d34251a186496d86926e2a0871e42a7e,Neural Information Processing Systems
995,Spectral-domain Optical Coherence Tomography in Manifest Glaucoma: Its Additive Role in Structural Diagnosis.,,2016-11-01,https://www.semanticscholar.org/paper/d582b775b4321e97bf6bbafae69beb5709acea42,American journal of ophthalmology-glaucoma
1242,Search for W' boson resonances decaying to a top quark and a bottom quark.,"We search for the production of a heavy W' gauge boson that decays to third generation quarks in 0.9 fb-1 of pp collisions at square root(s)=1.96 TeV, collected with the D0 detector at the Fermilab Tevatron collider. We find no significant excess in the final-state invariant mass distribution and set upper limits on the production cross section times branching fraction. For a left-handed W' boson with SM couplings, we set a lower mass limit of 731 GeV. For right-handed W' bosons, we set lower mass limits of 739 GeV if the W' boson decays to both leptons and quarks and 768 GeV if the W' boson decays only to quarks. We also set limits on the coupling of the W' boson to fermions as a function of its mass.",2008-03-01,https://www.semanticscholar.org/paper/9c2193299f46424b21113f68d2cbccb05ad90ff7,Physical Review Letters
3453,LP decoding achieves capacity,"We give a linear programming (LP) decoder that achieves the capacity (optimal rate) of a wide range of probabilistic binary communication channels. This is the first such result for LP decoding. More generally, as far as the authors are aware this is the first known polynomial-time capacity-achieving decoder with the maximum-likelihood (ML) certificate property---where output codewords come with a proof of optimality. Additionally, this result extends the capacity-achieving property of expander codes beyond the binary symmetric channel to a larger family of communication channels.Perhaps most importantly, since LP decoding performs well in practice on turbo codes and low-density parity-check (LDPC) codes (comparable to the popular ""belief propagation"" algorithm), this result exhibits the power of a new, widely applicable ""dual witness"" technique (Feldman, Malkin, Servedio, Stein and Wainwright, ISIT '04) for bounding decoder performance.For expander codes over an adversarial channel, we prove that LP decoding corrects a constant fraction of errors. To show this, we provide a new combinatorial characterization of error events that is of independent interest, and which we expect will lead to further improvements.",2005-01-23,https://www.semanticscholar.org/paper/10197d1e071a5dc054c075a9856e1808b7613a05,ACM-SIAM Symposium on Discrete Algorithms
3432,FairTorrent: bringing fairness to peer-to-peer systems,"Peer-to-Peer file-sharing applications suffer from a fundamental problem of unfairness. Free-riders cause slower download times for others by contributing little or no upload bandwidth while consuming much download bandwidth. Previous attempts to address this fair bandwidth allocation problem suffer from slow peer discovery, inaccurate predictions of neighboring peers' bandwidth allocations, underutilization of bandwidth, and complex parameter tuning. We present FairTorrent, a new deficit-based distributed algorithm that accurately rewards peers in accordance with their contribution. A FairTorrent peer simply uploads the next data block to a peer to whom it owes the most data as measured by a deficit counter. FairTorrent is resilient to exploitation by free-riders and strategic peers, is simple to implement, requires no bandwidth over-allocation, no prediction of peers' rates, no centralized control, and no parameter tuning. We implemented FairTorrent in a BitTorrent client without modifications to the BitTorrent protocol, and evaluated its performance against other widely-used BitTorrent clients. Our results show that FairTorrent provides up to two orders of magnitude better fairness, up to five times better download times for contributing peers, and 60% to 100% better performance on average in live BitTorrent swarms.",2009-12-01,https://www.semanticscholar.org/paper/691fd792034ffa74a93738eebf8bbe5d1d7c416c,Conference on Emerging Network Experiment and Technology
1168,Direct measurement of the W boson width.,"We present a direct measurement of the width of the W boson using the shape of the transverse mass distribution of W --> enu candidate events. Data from approximately 1 fb(-1) of integrated luminosity recorded at square root of s = 1.96 TeV by the D0 detector at the Fermilab Tevatron pp collider are analyzed. We use the same methods and data sample that were used for our recently published W boson mass measurement, except for the modeling of the recoil, which is done with a new method based on a recoil library. Our result, 2.028 +/- 0.072 GeV, is in agreement with the predictions of the standard model.",2009-09-25,https://www.semanticscholar.org/paper/8e538330e7a8d93582bf903a71408bfcfa1e9db4,Physical Review Letters
276,A Note on Strictly Competitive Games,,2009-12-09,https://www.semanticscholar.org/paper/840f716e5b5de23dd14f5a1c1ee8914dec36500d,Workshop on Internet and Network Economics
703,Computational Complexity of the Hylland-Zeckhauser Scheme for One-Sided Matching Markets,"In 1979, Hylland and Zeckhauser \cite{hylland} gave a simple and general scheme for implementing a one-sided matching market using the power of a pricing mechanism. Their method has nice properties -- it is incentive compatible in the large and produces an allocation that is Pareto optimal -- and hence it provides an attractive, off-the-shelf method for running an application involving such a market. With matching markets becoming ever more prevalant and impactful, it is imperative to finally settle the computational complexity of this scheme. We present the following partial resolution: 1. A combinatorial, strongly polynomial time algorithm for the special case of $0/1$ utilities. 2. An example that has only irrational equilibria, hence proving that this problem is not in PPAD. Furthermore, its equilibria are disconnected, hence showing that the problem does not admit a convex programming formulation. 3. A proof of membership of the problem in the class FIXP. We leave open the (difficult) question of determining if the problem is FIXP-hard. Settling the status of the special case when utilities are in the set $\{0, {\frac 1 2}, 1 \}$ appears to be even more difficult.",2020-04-03,https://www.semanticscholar.org/paper/cb3815c6c806b8672ea3d64c395a3e8b6bddc0a6,Information Technology Convergence and Services
3124,MobiDesk: mobile virtual desktop computing,"We present MobiDesk, a mobile virtual desktop computing hosting infrastructure that leverages continued improvements in network speed, cost, and ubiquity to address the complexity, cost, and mobility limitations of today's personal computing infrastructure. MobiDesk transparently virtualizes a user's computing session by abstracting underlying system resources in three key areas: display, operating system, and network. It provides a thin virtualization layer that decouples a user's computing session from any particular end-user device, and moves all application logic to hosting providers. The virtualization layer decouples a user's computing session from the underlying operating system and server instance, enabling high-availability service by transparently migrating sessions from one server to another during server maintenance or upgrades. We have implemented a prototype in Linux that works with existing unmodified applications and operating system kernels. Our experimental results demonstrate that MobiDesk has very low virtualization overhead, can provide a full featured desktop experience including full-motion video support, and is able to migrate users' sessions efficiently and reliably for high-availability, while maintaining existing network connections.",2004-09-26,https://www.semanticscholar.org/paper/6e1f042567fe5fc13a99d55e280838a186261a52,ACM/IEEE International Conference on Mobile Computing and Networking
1599,Avoiding Latent Variable Collapse With Generative Skip Models,"Variational autoencoders learn distributions of high-dimensional data. They model data with a deep latent-variable model and then fit the model by maximizing a lower bound of the log marginal likelihood. VAEs can capture complex distributions, but they can also suffer from an issue known as ""latent variable collapse,"" especially if the likelihood model is powerful. Specifically, the lower bound involves an approximate posterior of the latent variables; this posterior ""collapses"" when it is set equal to the prior, i.e., when the approximate posterior is independent of the data. While VAEs learn good generative models, latent variable collapse prevents them from learning useful representations. In this paper, we propose a simple new way to avoid latent variable collapse by including skip connections in our generative model; these connections enforce strong links between the latent variables and the likelihood function. We study generative skip models both theoretically and empirically. Theoretically, we prove that skip models increase the mutual information between the observations and the inferred latent variables. Empirically, we study images (MNIST and Omniglot) and text (Yahoo). Compared to existing VAE architectures, we show that generative skip models maintain similar predictive performance but lead to less collapse and provide more meaningful representations of the data.",2018-07-12,https://www.semanticscholar.org/paper/5f4bee489f595bd3d3dda7fd88de8d79b006aa52,International Conference on Artificial Intelligence and Statistics
3718,Learning Goals from Failure,"We introduce a framework that predicts the goals behind observable human action in video. Motivated by evidence in developmental psychology, we leverage video of unintentional action to learn video representations of goals without direct supervision. Our approach models videos as contextual trajectories that represent both low-level motion and high-level action features. Experiments and visualizations show our trained model is able to predict the underlying goals in video of unintentional action. We also propose a method to ""automatically correct"" unintentional action by leveraging gradient signals of our model to adjust latent trajectories. Although the model is trained with minimal supervision, it is competitive with or outperforms baselines trained on large (supervised) datasets of successfully executed goals, showing that observing unintentional action is crucial to learning about goals in video.",2020-12-16,https://www.semanticscholar.org/paper/1193fc121f9be24a5da562745dd3a07a1e1a7269,Computer Vision and Pattern Recognition
2399,The effect of inhibitors on the oxygen kinetics of terminal oxidases of Acanthamoeba castellanii.,"1. Respiration of growing cultures of Acanthamoeba castellanii is inhibited less than 60% by azide (35 mM); the respiration of early-exponential-phase cultures differs from that of late-exponential-phase cultures in being stimulated by up to 120% by low concentrations (less than 1 mM) of this inhibitor. Azide (0.5 mM) plus 1 mM-salicylhydroxamic acid gives 80% inhibition of respiration in early- or late-exponential-phase cultures. 2. Lineweaver-Burk plots of 1/v against 1/[O2] for growing and stationary-phase cultures give values of less than 1 muM for the apparent Km for oxygen. 3. These values are not significantly altered when determined in the presence of 1 mM-salicylhydroxamic acid. 4. Higher values (greater than 7 muM) for apparent Km values for oxygen were obtained in the presence of azide, which gives non-linear Lineweaver-Burk plots. 5. Competitive inhibition of respiration by CO occurs with Ki 2.4 muM. 6. The results are discussed in terms of the presence of three terminal oxidases in this organism, namely two oxidases with high affinities for oxygen (cytochrome c oxidase of the main phosphorylating electron-transport chain and the salicylhydroxamic acid-sensitive oxidase) and a third oxidase with a low affinity for oxygen, sensitive to inhibition by cyanide but not by azide or salicylhydroxamic acid. The relative contributions to oxygen utilization by these oxidases change during the growth of a batch culture.",1979-07-15,https://www.semanticscholar.org/paper/e768a21bea0cac9381e81f960403c471d5b72158,Biochemical Journal
165,Energy Equilibria in Proof-of-Work Mining,"The Bitcoin protocol induces miners, through monetary rewards, to expend energy in order to add blocks to the chain. We show that, when energy costs are substantial and taken into account, counterintuitive and unintended strategic behavior results: In a simple bounded-horizon setting with two identical miners there is a unique pure symmetric equilibrium in which both miners first ""slow down"" in order to decrease the crypto complexity and then take advantage of this decrease. If miners have different energy efficiencies and are restricted to choose the same hash rate for many epochs, there is a unique pure equilibrium in which miners either participate at low levels that depend in intricate ways on all the other miners' efficiencies, or choose to abstain from mining if their efficiency is too low. In the general setting in which miners can adapt their hash rates over time, we show that, unless the number of miners is very small, the only possible pure equilibria are rather chaotic, with miners quitting and starting again periodically --- or there is no pure equilibrium at all. We discuss the implications of these results for the stability of proof-of-work protocols.",2019-06-17,https://www.semanticscholar.org/paper/041a0308b938fd151f92a25c2f7cefabd2c31648,ACM Conference on Economics and Computation
723,Joint Cyber and Physical Attacks on Power Grids,"Recent events demonstrated the vulnerability of power grids to cyber attacks and to physical attacks. Therefore, we focus on joint cyber and physical attacks and develop methods to retrieve the grid state information following such an attack. We consider a model in which an adversary attacks a zone by physically disconnecting some of its power lines and blocking the information flow from the zone to the grid's control center. We use tools from linear algebra and graph theory and leverage the properties of the power flow DC approximation to develop methods for information recovery. Using information observed outside the attacked zone, these methods recover information about the disconnected lines and the phase angles at the buses. We identify sufficient conditions on the zone structure and constraints on the attack characteristics such that these methods can recover the information. We also show that it is NP-hard to find an approximate solution to the problem of partitioning the power grid into the minimum number of attack-resilient zones. However, since power grids can often be represented by planar graphs, we develop a constant approximation partitioning algorithm for these graphs. Finally, we numerically study the relationships between the grid's resilience and its structural properties, and demonstrate the partitioning algorithm on real power grids. The results can provide insights into the design of a secure control network for the smart grid.",2015-06-15,https://www.semanticscholar.org/paper/2c69e498329378f828f7aec28d80f99b159ff433,Measurement and Modeling of Computer Systems
2423,COVIZ: Visualization of Effects of COVID-19 on New York City Through Socially Impactful Virtual Reality,"This work is the product of a collaboration between students studying computer science and social work to visualize the impacts and effects of COVID-19 in New York City in a virtual environment (VE). As a proof of concept, the team chose two datasets from NYC Open Data; COVID-19 infection cases and rates per zip code and vehicular traffic rates within the five boroughs of New York City. To foster unexplored insights into the relationship between these data, we developed a virtual reality application that provides a stronger sense of embodiment and ownership of urban visualization analysis when manipulating 3D virtual maps for comparison in a VE.",2021-03-01,https://www.semanticscholar.org/paper/d4161ffacb19f86e87b46a75cee56cd4490e589e,2021 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)
1026,Control and locomotion of hydrodynamically coupled rigid spheres,"The coupling interactions between two spherical swimmers in an ideal fluid have known analytic approximations for certain types of motion. We apply these results to produce locomotion and coordination through the actuation of internal masses inside each swimmer. Through control of either both swimmers or one of them, the latter case taking advantage of compliance in the passive swimmer, desired motion along the spheres' line of centers can be achieved. We subsequently treat general 2D motion as a superposition of motion components along and perpendicular to the spheres' line of centers, leading to a derivation of a full model of controlled locomotion and coordination of two spheres in a planar fluid.",2017-05-01,https://www.semanticscholar.org/paper/0e613a415a30ac06a937644bcd070284bd43e4eb,American Control Conference
3311,Herbivore-initiated interaction cascades and their modulation by productivity in an African savanna,"Despite conceptual recognition that indirect effects initiated by large herbivores are likely to have profound impacts on ecological community structure and function, the existing literature on indirect effects focuses largely on the role of predators. As a result, we know neither the frequency and extent of herbivore-initiated indirect effects nor the mechanisms that regulate their strength. We examined the effects of ungulates on taxa (plants, arthropods, and an insectivorous lizard) representing several trophic levels, using a series of large, long-term, ungulate-exclusion plots that span a landscape-scale productivity gradient in an African savanna. At each of six sites, lizards, trees, and the numerically dominant order of arthropods (Coleoptera) were more abundant in the absence of ungulates. The effect of ungulates on arthropods was mediated by herbaceous vegetation cover. The effect on lizards was simultaneously mediated by both tree density (lizard microhabitat) and arthropod abundance (lizard food). The magnitudes of the experimental effects on all response variables (trees, arthropods, and lizards) were negatively correlated with two distinct measures of primary productivity. These results demonstrate strong cascading effects of ungulates, both trophic and nontrophic, and support the hypothesis that productivity regulates the strength of these effects. Hence, the strongest indirect effects (and thus, the greatest risks to ecosystem integrity after large mammals are extirpated) are likely to occur in low-productivity habitats.",2007-01-02,https://www.semanticscholar.org/paper/c739ed724b8a43409ac3acedc31dd1a7786e7872,Proceedings of the National Academy of Sciences of the United States of America
2463,Breaking the Barriers to True Augmented Reality,"In recent years, Augmented Reality (AR) and Virtual Reality (VR) have gained considerable commercial traction, with Facebook acquiring Oculus VR for \$2 billion, Magic Leap attracting more than \$500 million of funding, and Microsoft announcing their HoloLens head-worn computer. Where is humanity headed: a brave new dystopia-or a paradise come true? 
In this article, we present discussions, which started at the symposium ""Making Augmented Reality Real"", held at Nara Institute of Science and Technology in August 2014. Ten scientists were invited to this three-day event, which started with a full day of public presentations and panel discussions (video recordings are available at the event web page), followed by two days of roundtable discussions addressing the future of AR and VR.",2015-12-17,https://www.semanticscholar.org/paper/27871230af68b3567193f2417877e0a9bc9f98c6,arXiv.org
568,Intractable problems in control theory,"This paper is a study of the apparent intractability of problems in decentralized decision-making, using the concepts and methods of Complexity Theory. We first establish that the discrete version of an important paradigm for this area, proposed by Witsenhausen, is NP-complete, thus explaining the failures reported in the literature to attack it computationally. In the rest of the paper we show that the computational intractability of the discrete version of a control problem can imply that there are no satisfactory (continuous) algorithms for the continuous version. To this effect, we develop a theory of continuous algorithms and their complexity, and an analytical methodology, which can prove quite interesting by themselves.",1985-12-01,https://www.semanticscholar.org/paper/f76fc150541640fc710990606a69ba79863282c4,IEEE Conference on Decision and Control
3560,Dynamic algorithm selection for runtime concepts,,2010-09-01,https://www.semanticscholar.org/paper/3fba9cbfd08e6cdeb0882be29a77bfc806aaaad1,Science of Computer Programming
438,"Panarity, Revisited (Extended Abstract)",,1997-08-06,https://www.semanticscholar.org/paper/ba925b0e259de3d3fdd38df72a42c7682144a91c,Workshop on Algorithms and Data Structures
2773,Interactive Multimedia Explanation for Equipment Maintenance and Repair,,,https://www.semanticscholar.org/paper/24336b2326d3bf2935251a058d67bcd5e30f9244,Human Language Technology - The Baltic Perspectiv
2932,Sparse discriminative latent characteristics for predicting cancer drug sensitivity from genomic features,"Drug screening studies typically involve assaying the sensitivity of a range of cancer cell lines across an array of anti-cancer therapeutics. Alongside these sensitivity measurements high dimensional molecular characterizations of the cell lines are typically available, including gene expression, copy number variation and genomic mutations. We propose a sparse multitask regression model which learns discriminative latent characteristics that predict drug sensitivity and are associated with specific molecular features. We use ideas from Bayesian nonparametrics to automatically infer the appropriate number of these latent characteristics. The resulting analysis couples high predictive performance with interpretability since each latent characteristic involves a typically small set of drugs, cell lines and genomic features. Our model uncovers a number of drug-gene sensitivity associations missed by single gene analyses. We functionally validate one such novel association: that increased expression of the cell-cycle regulator C/EBPδ decreases sensitivity to the histone deacetylase (HDAC) inhibitor panobinostat.",2019-05-01,https://www.semanticscholar.org/paper/2ac952da5609e2823841d9472afa7d1bea4aa558,PLoS Comput. Biol.
2439,Collaborative Virtual Reality for Low-Latency Interaction,"In collaborative virtual environments, users must often perform tasks requiring coordinated action between multiple parties. Some cases are symmetric, in which users work together on equal footing, while others are asymmetric, in which one user may have more experience or capabilities than another (e.g., one may guide another in completing a task). We present a multi-user virtual reality system that supports interactions of both these types. Two collaborating users, whether co-located or remote, simultaneously manipulate the same virtual objects in a physics simulation, in tasks that require low latency networking to perform successfully. We are currently applying this approach to motor rehabilitation, in which a therapist and patient work together.",2018-10-11,https://www.semanticscholar.org/paper/987cffb26eaf1941c142b3de9499c646a113b9cc,ACM Symposium on User Interface Software and Technology
2571,Vertical Vergence Calibration for Augmented Reality Displays,"Stereo and bi-ocular head-mounted displays (HMDs) require the user to fuse two images into a coherent picture of the threedimensional world. The human visual system performs this task constantly, but when the input images contain both real and graphical depictions, the problem becomes more difficult. A vertical disparity in the graphics causes diplopia for users trying to fuse the real and virtual objects simultaneously. We implement three methods to measure and correct this disparity and assess them with a collection of a single model of optical see-through HMD.",2006-03-25,https://www.semanticscholar.org/paper/5be7e31f4acbb26e1fb77fcc816c05cb94845d9b,IEEE Conference on Virtual Reality and 3D User Interfaces
3733,Oops! Predicting Unintentional Action in Video,"From just a short glance at a video, we can often tell whether a person's action is intentional or not. Can we train a model to recognize this? We introduce a dataset of in-the-wild videos of unintentional action, as well as a suite of tasks for recognizing, localizing, and anticipating its onset. We train a supervised neural network as a baseline and analyze its performance compared to human consistency on the tasks. We also investigate self-supervised representations that leverage natural signals in our dataset, and show the effectiveness of an approach that uses the intrinsic speed of video to perform competitively with highly-supervised pretraining. However, a significant gap between machine and human performance remains.",2019-11-25,https://www.semanticscholar.org/paper/2324d55ec54b9a12c4ac5353c51bcfa8440f7b6a,Computer Vision and Pattern Recognition
702,Homa: An Efficient Topology and Route Management Approach in SD-WAN Overlays,"This paper presents an efficient topology and route management approach in Software-Defined Wide Area Networks (SD-WAN). Traditional WANs suffer from low utilization and lack of global view of the network. Therefore, during failures, topology/service/traffic changes, or new policy requirements, the system does not always converge to the global optimal state. Using Software Defined Networking architectures in WANs provides the opportunity to design WANs with higher fault tolerance, scalability, and manageability. We exploit the correlation matrix derived from monitoring system between the virtual links to infer the underlying route topology and propose a route update approach that minimizes the total route update cost on all flows. We formulate the problem as an integer linear programming optimization problem and provide a centralized control approach that minimizes the total cost while satisfying the quality of service (QoS) on all flows. Experimental results on real network topologies demonstrate the effectiveness of the proposed approach in terms of disruption cost and average disrupted flows.",2020-07-01,https://www.semanticscholar.org/paper/99194efa76b8afcfec823a3de3ba383bc689bb48,IEEE Conference on Computer Communications
2823,The bone marrow compartment is modified in the absence of galectin-3,,2011-11-27,https://www.semanticscholar.org/paper/9f731e6e25016bd5fa095af39d612128337bfff6,Cell and Tissue Research
2763,Editable graphical histories,"The authors have designed a testbed system that creates a series of automatically generated panels that depict in chronological order the important events in the history of a users session with Chimera, a graphical editor. The authors' system heuristically determines the contents of each panel and the actions that it illustrates. The user can scroll through the sequence of panels, reviewing actions at different levels of detail, and selectively undoing, modifying, and redoing previous actions.<<ETX>>",1988-10-10,https://www.semanticscholar.org/paper/4551f016f4dad887465c7b83839ed41ade8f49ca,[Proceedings] 1988 IEEE Workshop on Visual Languages
2044,Analysing inspection frequency for wafer bumping process and an empirical study of UNISON decision framework,"This study aims to develop a UNISON decision framework for analysing the inspection frequency decisions for the advanced wafer bumping process. Since the accumulated values of the wafers fabricated in wafer fabs are high, process excursion and defects in bumping will cause serious loss. Thus, it is critical to determine the inspection frequency to fulfil the requirement of mass production while ensuring the yield. For validation, an empirical study was conducted in a bumping company and the results showed practical viability of the proposed approach.",2008-03-13,https://www.semanticscholar.org/paper/35f644564bf1da938be2800a9fd6f4de1b99998a,International Journal of Manufacturing Technology and Management (IJMTM)
3181,Social dilemmas of sociality due to beneficial and costly contagion,"Levels of sociality in nature vary widely. Some species are solitary; others live in family groups; some form complex multi-family societies. Increased levels of social interaction can allow for the spread of useful innovations and beneficial information, but can also facilitate the spread of harmful contagions, such as infectious diseases. It is natural to assume that these contagion processes shape the evolution of complex social systems, but an explicit account of the dynamics of sociality under selection pressure imposed by contagion remains elusive. We consider a model for the evolution of sociality strategies in the presence of both a beneficial and costly contagion. We study the dynamics of this model at three timescales: using a susceptible-infectious-susceptible (SIS) model to describe contagion spread for given sociality strategies, a replicator equation to study the changing fractions of two different levels of sociality, and an adaptive dynamics approach to study the long-time evolution of the population level of sociality. For a wide range of assumptions about the benefits and costs of infection, we identify a social dilemma: the evolutionarily-stable sociality strategy (ESS) is distinct from the collective optimum—the level of sociality that would be best for all individuals. In particular, the ESS level of social interaction is greater (respectively less) than the social optimum when the good contagion spreads more (respectively less) readily than the bad contagion. Our results shed light on how contagion shapes the evolution of social interaction, but reveals that evolution may not necessarily lead populations to social structures that are good for any or all.",2022-02-20,https://www.semanticscholar.org/paper/9a8161c23ea8cd73c3c4cb1b25b5595ed8230e77,PLoS Comput. Biol.
1272,Measurement of the charge asymmetry in semileptonic Bs0 decays.,"We have performed the first direct measurement of the time-integrated flavor untagged charge asymmetry in semileptonic Bs0 decays ASLs,unt by comparing the decay rate of Bs0-->micro+Ds-nuX, where Ds- -->phipi- and phi-->K+K-, with the charge-conjugate Bs0 decay rate. This sample was selected from 1.3 fb-1 of data collected by the D0 experiment in run II of the Fermilab Tevatron collider. We obtain ASLs,unt=[1.23+/-0.97(stat)+/-0.17(syst)]x10(-2). Assuming that Deltam(s)/Gamma(s)>>1, this result can be translated into a measurement of the CP-violating phase in Bs0 mixing: DeltaGamma(s)/Deltam(s)tanphi(s)=[2.45+/-1.93(stat)+/-0.35(syst)]x10(-2).",2007-01-06,https://www.semanticscholar.org/paper/280217fc5db85b8c11456930032b55c98130e5dc,Physical Review Letters
2168,Role of antimicrobial peptides in atopic dermatitis,"Host defense peptides (HDPs) or antimicrobial peptides (AMPs) are short cationic amphipathic peptides of divergent sequences, which are part of the innate immune system and produced by various types of cells and tissues. The predominant role of HDPs is to respond to and protect humans against infection and inflammation. Common human HDPs include defensins, cathelicidin, psoriasin, dermcidin, and ribonucleases, but these peptides may be dysregulated in the skin of patients with atopic dermatitis (AD). Current evidence suggests that the antimicrobial properties and immunomodulatory effects of HDPs are involved in AD pathogenesis, making HDPs research a promising area for predicting disease severity and developing novel treatments for AD. In this review, we describe a potential role for human HDPs in the development, exacerbation, and progression of AD and propose their potential therapeutic benefits.",2021-08-25,https://www.semanticscholar.org/paper/d5727a13c5cd7f38b9f5a7573a117c87d88a6d6d,International Journal of Dermatology
2438,Leveraging Patient-Reported Outcomes Using Data Visualization,"Abstract Background Health care organizations increasingly use patient-reported outcomes (PROs) to capture patients' health status. Although federal policy mandates PRO collection, the challenge remains to better engage patients in PRO surveys, and ensure patients comprehend the surveys and their results. Objective This article identifies the design requirements for an interface that assists patients with PRO survey completion and interpretation, and then builds and evaluates the interface. Methods We employed a user-centered design process that consisted of three stages. First, we conducted qualitative interviews and surveys with 13 patients and 11 health care providers to understand their perceptions of the value and challenges associated with the use of PRO measures. Second, we used the results to identify design requirements for an interface that collects PROs, and designed the interface. Third, we conducted usability testing with 12 additional patients in a hospital setting. Results In interviews, patients and providers reported that PRO surveys help patients to reflect on their symptoms, potentially identifying new opportunities for improved care. However, 6 out of 13 patients reported significant difficultly in understanding PRO survey questions, answer choices and results. Therefore, we identified aiding comprehension as a key design requirement, and incorporated visualizations into our interface design to aid comprehension. In usability testing, patients found the interface highly usable. Conclusion Future interfaces designed to collect PROs may benefit from employing strategies such as visualization to aid comprehension and engage patients with surveys.",2018-07-01,https://www.semanticscholar.org/paper/954011ab14b181af811c2112c59977e03486ec56,Applied Clinical Informatics
364,The new problems,"The Brown web site announcing the death of Paris Kanellakis was one of the 90,000 active websites, running on one of 13 million IP servers. Since that time, the Internet and the worldwide web have grown by several orders of magnitude, and they have changed the way the world communicates, learns, expresses itself, and does business. The research agenda in Theory and Databases has also been affected: The Internet and the web are the first computational artefacts that were not designed in any direct, conventional sense, and must therefore be understood by the scientific method: the development and verification of falsifiable theories. There is an emergent field that uses concepts from Game Theory, Graph Theory, and Algorithms and Complexity in order to develop a mathematical methodology appropriate for such study.",2003-06-08,https://www.semanticscholar.org/paper/ea84a732ca2c6086b00cc811bfddbd194d73e7ff,PCK50
3062,RSIO: automatic user interaction detection and scheduling,"We present RSIO, a processor scheduling framework for improving the response time of latency-sensitive applications by monitoring accesses to I/O channels and inferring when user interactions occur. RSIO automatically identifies processes involved in a user interaction and boosts their priorities at the time the interaction occurs to improve system response time. RSIO also detects processes indirectly involved in processing an interaction, automatically accounting for dependencies and boosting their priorities accordingly. RSIO works with existing schedulers and requires no application modifications to identify periods of latency-sensitive application activity. We have implemented RSIO in Linux and measured its effectiveness on microbenchmarks and real applications. Our results show that RSIO is easy to use and can provide substantial improvements in system performance for latency-sensitive applications.",2010-06-12,https://www.semanticscholar.org/paper/80bd596d8090bd33ab7f3e6df84a6282e886eea3,Measurement and Modeling of Computer Systems
1747,Sparse stochastic inference for latent Dirichlet allocation,We present a hybrid algorithm for Bayesian topic models that combines the efficiency of sparse Gibbs sampling with the scalability of online stochastic inference. We used our algorithm to analyze a corpus of 1.2 million books (33 billion words) with thousands of topics. Our approach reduces the bias of variational inference and generalizes to many Bayesian hidden-variable models.,2012-06-26,https://www.semanticscholar.org/paper/481eb978677eaae4e01639f03212fd81d1a5a448,International Conference on Machine Learning
3104,Group round robin,"We present group round-robin (GRR) scheduling, a hybrid fair packet scheduling framework based on a grouping strategy that narrows down the traditional trade-off between fairness and computational complexity. GRR combines its grouping strategy with a specialized round-robin scheduling algorithm that utilizes the properties of GRR groups to schedule flows within groups in a manner that provides O(1) bounds on fairness with only O(1) time complexity. Under the practical assumption that GRR employs a small constant number of groups, we apply GRR to popular fair queuing scheduling algorithms and show how GRR can be used to achieve constant bounds on fairness and time complexity for these algorithms. We also present and prove new results on the fairness bounds for several of these fair queuing algorithms using a consistent fairness measure. We analyze the behavior of GRR and present experimental results that demonstrate how GRR can be combined with existing scheduling algorithms to provide much lower scheduling overhead and more than an order of magnitude better scheduling accuracy in practice than scheduling algorithms without GRR.",2005-10-01,https://www.semanticscholar.org/paper/13624d2ca68720ac1a5d1ef3df597c352f146fb9,Symposium on Architectures for Networking and Communications Systems
3158,Integrated Processors Scheduling for Multimedia,,1995-04-19,https://www.semanticscholar.org/paper/cc1f69155d69260ea1b09874fdb8933d6e41da70,International Workshop on Network and Operating System Support for Digital Audio and Video
581,Concurrency Control by Locking,"We present a geometric method for studying concurrency control by locking. When there are only two transactions, our method yields an exact characterization of safe locking policies and also of deadlock-free locking policies. Our results can be extended to more than two transactions, but in that case the problem becomes NP-complete.",1983-05-01,https://www.semanticscholar.org/paper/62310dd997c8e71acbea6e4cc4907ad974d26fa1,SIAM journal on computing (Print)
2647,User interface management techniques for collaborative mobile augmented reality,,2001-10-01,https://www.semanticscholar.org/paper/c34b386d148b8bd270ef656a811ef85fcee87a3b,Computers & graphics
2562,Designing a mobile user interface for automated species identification,"Biological research in the field is constrained by the speed and difficulty of species determination, as well as by access to relevant information about the species encountered. However, recent work on vision-based algorithms raises the promise of rapid botanical species identification. The potential for mobile vision-based identification provides opportunities for new user interface techniques. To explore these issues, we present LeafView, a Tablet-PC-based user interface for an electronic field guide that supports automated identification of botanical species in the field. We describe a user interface design based on an ethnographic study of botanists, field tests of working prototypes by botanists at the Smithsonian Institution on Plummers Island, Maryland, and observations at an internal exhibition at the Smithsonian at which other staff members tried the prototypes. We present functionality specific to mobile identification and collection in the electronic field guide and use this to motivate discussion of mobile identification in general.",2007-04-29,https://www.semanticscholar.org/paper/b6c5cf1b7544ed141bdcf19a6c452d6345988d75,International Conference on Human Factors in Computing Systems
3537,Object-Oriented Programming without Inheritance (Invited Talk),"Object-oriented programming is often characterized as encapsulation plus polymorphism plus inheritance. The original Simula67 demonstrated that we could do without encapsulation and Kristen Nygaard insisted that some OOP could be done without inheritance. I present generic programming as providing encapsulation plus polymorphism. In C++, this view is directly supported by language facilities, such as classes, templates and (only recently) concepts. I show a range of type-and-resource-safe techniques covering a wide range of applications including containers, algebraic concepts, and numerical and non-numerical algorithms.",,https://www.semanticscholar.org/paper/1cd7db9c5b0fa292941a834a8cda55b7a8d82cd2,European Conference on Object-Oriented Programming
136,Fully-adaptive routing: packet switching performance and wormhole algorithms,No abstract available,1991-08-01,https://www.semanticscholar.org/paper/c60b14354b0ac82c2f883f739e16c6720ebe870a,Proceedings of the 1991 ACM/IEEE Conference on Supercomputing (Supercomputing '91)
979,Change of β-Zone Parapapillary Atrophy During Axial Elongation: Boramae Myopia Cohort Study Report 3.,"Purpose
To investigate changes of β-zone parapapillary atrophy (PPA) during axial elongation.


Methods
Change of β-zone PPA was evaluated by spectral-domain optical coherence tomography (SD-OCT) in myopic children for 2 years, prospectively. Using the infrared images acquired by a fixed scan circle in the glaucoma progression analysis (GPA) mode, the retinal pigment epithelial opening (RPEO) and the clinical disc margin (CDM) were manually delineated. The area and position of β-zone PPA was calculated as the differences from those of the RPEO and CDM, respectively. The β-zone PPA was further differentiated into βBM PPA (β-zone PPA with Bruch's membrane [BM]) and γ-zone PPA (β-zone PPA without BM). The change of β-zone PPA was compared between the first and final visits.


Results
The area of β-zone PPA increased in 35 eyes (76%). This increase was associated with RPEO area increase and CDM area decrease. The center of β-zone PPA moved along the direction of vascular trunk dragging, but to a lesser extent. The β-zone PPA enlargement was correlated with the extent of vascular trunk dragging (P = 0.014). In all eyes with β-zone PPA increase, the γ-zone portion had increased. Even in childhood, βBM PPA existed next to their γ-zone PPA in 11 eyes (24%), including 4 eyes that showed increase of both γ-zone and βBM portion during axial elongation.


Conclusions
Enlargement of β-zone PPA during axial elongation was affected by the extent and direction of vascular trunk dragging, thus implicating disproportionate growth between the retina and sclera.",2018-08-01,https://www.semanticscholar.org/paper/56a294c9c98c559eec46ced7460dc4ee0b998487,Investigative Ophthalmology and Visual Science
1397,Search for the scalar top quark in pp collisions at square root[s] = 1.8 TeV.,"We have performed a search for scalar top quark (stop) pair production in the inclusive electron-muon-missing transverse energy final state, using a sample of pp events corresponding to 108.3 pb(-1) of data collected with the D0 detector at Fermilab. The search is done in the framework of the minimal supersymmetric standard model assuming that the sneutrino is the lightest supersymmetric particle. For the dominant decays of the lightest stop, t-->b chi+1 and t-->blnu, no evidence for signal is found. We derive cross-section limits as a function of stop ( t ), chargino ( chi+1), and sneutrino ( nu) masses.",,https://www.semanticscholar.org/paper/a475f22b553f6df9599a9e84684bfaabebbcb731,Physical Review Letters
411,Proceedings of the eighteenth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems,,1999-05-01,https://www.semanticscholar.org/paper/950e5917cb176b72015d8f64f837601b2740642e,ACM SIGMOD Conference
152,On the complexity of dynamic mechanism design,,2022-03-01,https://www.semanticscholar.org/paper/93022f2ecb0c64dd2b27a095d1d10c5a1f0ff143,Games Econ. Behav.
1633,Zero-Inflated Exponential Family Embeddings,"Word embeddings are a widely-used tool to analyze language, and exponential family embeddings (Rudolph et al., 2016) generalize the technique to other types of data. One challenge to fitting embedding methods is sparse data, such as a document/term matrix that contains many zeros. To address this issue, practitioners typically downweight or subsample the zeros, thus focusing learning on the non-zero entries. In this paper, we develop zero-inflated embeddings, a new embedding method that is designed to learn from sparse observations. In a zero-inflated embedding (ZIE), a zero in the data can come from an interaction to other data (i.e., an embedding) or from a separate process by which many observations are equal to zero (i.e. a probability mass at zero). Fitting a ZIE naturally downweights the zeros and dampens their influence on the model. Across many types of data— language, movie ratings, shopping histories, and bird watching logs—we found that zero-inflated embeddings provide improved predictive performance over standard approaches and find better vector representation of items.",2017-07-17,https://www.semanticscholar.org/paper/b0f7e146bff2f5be42c4d84eac2f7309310c54b7,International Conference on Machine Learning
3522,An algorithm for minimum cuts,"A minimum cut is a set of edges of minimum weight whose removal disconnects a given graph. Minimum cut algorithms historically applied duality with maximum ows and thus had the same (mn) running time as maximum ow algorithms. More recent algorithms which are not based on maximum ows also require (mn) time. In this paper, we present the rst algorithm that breaks the (mn) \max-ow barrier"" for nding minimum cuts in weighted undirected graphs. We give a strongly polynomial randomized algorithm which nds a minimum cut with high probability in O(n 2 log 3 n) time. This suggests that the min-cut problem might be fundamentally easier to solve than the maximum ow problem. Our algorithm can be implemented in RNC using only n 2 processors|this is the rst eecient RNC algorithm for the min-cut problem. Our algorithm is simple and uses no complicated data structures.",,https://www.semanticscholar.org/paper/3271c7f71242e850f0b8bf9e3adfaa17d9dc34b8,Symposium on the Theory of Computing
3394,A general framework for handling commitment in online throughput maximization,,2018-11-20,https://www.semanticscholar.org/paper/0e0cced03fbc595cd997c6ede35c93b54017a6c9,Conference on Integer Programming and Combinatorial Optimization
604,The Complexity of Searching a Graph (Preliminary Version),"T. Parsons proposed and partially analyzed the following pursuit-evasion problem on graphs: A team of searchers traverse the edges of a graph G in pursuit of a fugitive, who moves along the edges of the graph with complete knowledge of the locations of the pursuers. What is the smallest number s(G) of searchers that will suffice for guaranteeing capture of the fugitive? We show that determining whether s(G) ≤ K, for a given integer K, is NP-hard for general graphs but can be solved in linear time for trees. We also provide a structural characterization of those graphs with s(G) ≤ K for K = 1,2,3.",1981-10-28,https://www.semanticscholar.org/paper/79d7ce3bdaa83d3cf3293ecaac91216edecdf8f4,IEEE Annual Symposium on Foundations of Computer Science
2607,BRIDGING THE GAPS: HYBRID TRACKING FOR ADAPTIVE MOBILE AUGMENTED REALITY,"Tracking accuracy in a location-aware mobile system can change dynamically as a function of the user's location and other variables specific to the tracking technologies used. This is especially problematic for mobile augmented reality systems, which ideally require extremely precise position tracking for the user's head, but which may not always be able to achieve that level of accuracy. While it is possible to ignore variable positional accuracy in an augmented reality user interface, this can make for a confusing system; for example, when accuracy is low, virtual objects that are nominally registered with real ones may be too far off to be of use. To address this problem, we describe an experimental mobile augmented reality system that: (1) employs multiple position-tracking technologies, including ones that apply heuristics based on environmental knowledge; (2) coordinates these concurrently monitored tracking systems; and (3) automatically adapts the user interface to varying degrees of confidence in tracking accuracy. We share our experiences with managing these multiple tracking technologies, employing various techniques to facilitate smooth and reasonable “hand-offs” between the cooperating systems. We present these results in the context of an intelligent navigational guidance system that helps users to orient themselves in an unfamiliar environment, using path planning to guide them toward destinations they choose, and sometimes towards ones the system infers as equally relevant.",2004-07-01,https://www.semanticscholar.org/paper/b3d6b80c4c3b6f95b8bf56c1450154439a3820ff,Applied Artificial Intelligence
3338,"Sperm competition in the water strider, Gerris remigis
",,1989-10-01,https://www.semanticscholar.org/paper/b41ca951dce8725edc7daf0ad12ae82fe97b884e,Animal Behaviour
292,On the Hardness of Being Truthful,"The central problem in computational mechanism design is the tension between incentive compatibility and computational efficiency. We establish the first significant approximability gap between algorithms that are both truthful and computationally-efficient, and algorithms that only achieve one of these two desiderata. This is shown in the context of a novel mechanism design problem which we call the combinatorial public project problem (cppp). cpppis an abstraction of many common mechanism design situations, ranging from elections of kibbutz committees to network design.Our result is actually made up of two complementary results -- one in the communication-complexity model and one in the computational-complexity model. Both these hardness results heavily rely on a combinatorial characterization of truthful algorithms for our problem. Our computational-complexity result is one of the first impossibility results connecting mechanism design to complexity theory; its novel proof technique involves an application of the Sauer-Shelah Lemma and may be of wider applicability, both within and without mechanism design.",2008-10-25,https://www.semanticscholar.org/paper/8c573c11f35f33bc31a00fea5331f6e04329935e,2008 49th Annual IEEE Symposium on Foundations of Computer Science
2677,Computer vision in 3D interactivity (panel),"With microprocessor clock rates in excess of 350MHz, SIMD integer instructions commonplace, and shared memory multiprocessing available for under $3,000.00, integration of computer vision with 3D graphics is now more practical than ever. Tracking the user’s head, hands, and body, and detecting gestures, is one obvious direction to explore to eliminate encumbering sensors and enable new modes of interaction. Another direction is using computer vision techniques to understand 3D structure and camera parameters in multi-view imagebased scenes for the purpose of re-rendering the scenes as a user directs. Yet another is giving animated characters visual awareness of users and other characters to enable richer interactions. What will be the most compelling integration of computer vision with 3D graphics?",1998-07-21,https://www.semanticscholar.org/paper/9d5253924091fb0509a019a243f91897ba2db2eb,International Conference on Computer Graphics and Interactive Techniques
2959,An Empirical Study of Stochastic Variational Algorithms for the Beta Bernoulli Process,"Stochastic variational inference (SVI) is emerging as the most promising candidate for scaling inference in Bayesian probabilistic models to large datasets. However, the performance of these methods has been assessed primarily in the context of Bayesian topic models, particularly latent Dirichlet allocation (LDA). Deriving several new algorithms, and using synthetic, image and genomic datasets, we investigate whether the understanding gleaned from LDA applies in the setting of sparse latent factor models, specifically beta process factor analysis (BPFA). We demonstrate that the big picture is consistent: using Gibbs sampling within SVI to maintain certain posterior dependencies is extremely effective. However, we find that different posterior dependencies are important in BPFA relative to LDA. Particularly, approximations able to model intra-local variable dependence perform best.",2015-06-26,https://www.semanticscholar.org/paper/c0cc5cc0968554d766d3deadbe2fbe5c10b96994,International Conference on Machine Learning
2360,Granulocyte‐macrophage colony‐stimulating factor (GM‐CSF) primes the respiratory burst and stimulates protein biosynthesis in human neutrophils,,1989-10-09,https://www.semanticscholar.org/paper/e2cf681775445883e3df1d9595f28a43732e5c62,FEBS Letters
3278,Fusing enacted and expected mimicry generates a winning strategy that promotes the evolution of cooperation,"Although cooperation and trust are essential features for the development of prosperous populations, they also put cooperating individuals at risk for exploitation and abuse. Empirical and theoretical evidence suggests that the solution to the problem resides in the practice of mimicry and imitation, the expectation of opponent’s mimicry and the reliance on similarity indices. Here we fuse the principles of enacted and expected mimicry and condition their application on two similarity indices to produce a model of mimicry and relative similarity. Testing the model in computer simulations of behavioral niches, populated with agents that enact various strategies and learning algorithms, shows how mimicry and relative similarity outperforms all the opponent strategies it was tested against, pushes noncooperative opponents toward extinction, and promotes the development of cooperative populations. The proposed model sheds light on the evolution of cooperation and provides a blueprint for intentional induction of cooperation within and among populations. It is suggested that reducing conflict intensities among human populations necessitates (i) instigation of social initiatives that increase the perception of similarity among opponents and (ii) efficient lowering of the similarity threshold of the interaction, the minimal level of similarity that makes cooperation advisable.",2013-06-03,https://www.semanticscholar.org/paper/51bbb26244e0a7f18bb467f13700cd321723fb5f,Proceedings of the National Academy of Sciences of the United States of America
3552,Rejuvenating C++ programs through demacrofication,"We describe how legacy C++ programs can be rejuvenated using C++11 features such as generalized constant expressions, perfect forwarding, and lambda expressions. In general, this work develops a correspondence between different kinds of macros and the C++ declarations to which they should be transformed. We have created a set of demacrofication tools to assist a developer in the rejuvenation of C++ programs. To evaluate the work, we have applied the rejuvenation tools to a number of C++ libraries to assess the extent to which these libraries might be improved by demacrofication. Results indicate that between 68 and 98% of potentially refactorable macros could be transformed into C++11 declarations. Additional experiments demonstrate why these numbers are not readily achieved using fully automated rejuvenation tools. We also discuss some techniques to further assist in automating rejuvenation process.",2012-09-23,https://www.semanticscholar.org/paper/de91c41c6335f1541d2c56b25711629ce7f8b552,International Conference on Smart Multimedia
1297,Study of the decay Bs(0)-->Ds(*)Ds(*).,"We report a study of the decay Bs(0)-->Ds(*)Ds(*) using a data sample corresponding to 1.3 fb(-1) of integrated luminosity collected by the D0 experiment in 2002-2006 during run II of the Fermilab Tevatron collider. One Ds(*) meson was partially reconstructed in the decay Ds-->phi mu nu, and the other Ds(*) meson was identified using the decay Ds-->phi pi where no attempt was made to distinguish Ds and Ds(*) states. For the branching fraction Br(Bs(0)-->Ds(*)Ds(*)) we obtain a 90% C.L. range [0.002,0.080] and central value 0.039(-0.017)(+0.019)(stat)(-0.015)(+0.016)(syst). This was subsequently used to make the most precise estimate of the width difference DeltaGamma(s)CP in the Bs(0)-Bs(0) system: DeltaGamma(s)CP/Gamma(s)=0.079(-0.035)(+0.038)(stat)(-0.030)(+0.031)(syst).",2007-12-12,https://www.semanticscholar.org/paper/ec3113786334c084939ac8442613bd817bae969c,Physical Review Letters
2793,Galectin‐3 regulates inflammasome activation in cholestatic liver injury,"Macrophage activation is an important feature of primary biliary cholangitis (PBC) pathogenesis and other cholestatic liver diseases. Galectin‐3 (Gal3), a pleiotropic lectin, is produced by monocytic cells and macrophages. However, its role in PBC has not been addressed. We hypothesized that Gal3 is a key to induce NOD‐like receptor family, pyrin domain containing 3 (NLRP3) inflammasome in macrophages and in turn to propagate proinflammatory IL‐17 signaling. In liver tissues from patients with PBC and dnTGF‐βRII mice, a model of autoimmune cholangitis, the expression of Gal3, NLRP3, and the adaptor protein adaptor apoptosis‐associated speck like protein was induced, with the downs tream activation of caspase‐1 and IL‐1β. Inwild‐typehepaticmacrophages, deoxycholic acid induced the association of Gal3 and NLRP3with direct activation of the inflammasome, resulting in an increase in IL‐1β. Downstreamretinoid‐related orphan receptor CmRNA, IL‐17A, and IL‐17Fwere induced. In Gal3‐/‐ macrophages, no inflammasome activation was detected. To confirm the key role of Gal3 in the pathogenesis of cholestatic liver injury, we generated dnTGF‐βRII/galectin‐3‐/ (dn/Gal3‐/) mice, which showed impaired inflammasome activation alongwith significantly improved inflammation and fibrosis. Taken together, our data point to a novel role of Gal3 as an initiator of inflammatory signaling in autoimmune cholangitis, mediating the activation of NLRP3 inflammasome and inducing IL‐17 proinflammatory cascades. These studies provide a rationale to target Gal3 in autoimmune cholangitis and potentially other cholestatic diseases.—Tian, J., Yang, G., Chen, H.‐Y., Hsu, D. K., Tomilov, A., Olson, K.A., Dehnad, A., Fish, S. R., Cortopassi, G.,Zhao, B., Liu, F.‐T., Gershwin, M.E., Török, N. J., Jiang, J. X. Galectin‐3 regulates inflammasome activation in cholestatic liver injury. FASEB J. 30, 4202–4213 (2016). www.fasebj.org",2016-09-14,https://www.semanticscholar.org/paper/56e91c5299d90f6d375625def0937cc7b195f2d7,The FASEB Journal
1919,An empirical study of bio manufacturing for the scheduling of hepatitis in vitro diagnostic device with constrained process time window,,2017-12-01,https://www.semanticscholar.org/paper/4287c819ec539cf27a3dcfab6d1c3b1565055d32,Computers & industrial engineering
337,An economic model of the worldwide web,"We believe that much novel insight into the worldwide web can be obtained from taking into account the important fact that it is created, used, and run by selfish optimizing agents: users, document authors, and search engines. On-going theoretical and experimental analysis of a simple abstract model of www creation and search based on user utilities illustrates this point: We find that efficiency is higher when the utilities are more clustered, and that power-law statistics of document degrees emerge very naturally in this context. More importantly, our work sets up many more elaborate questions, related, e.g., to www search algorithms seen as author incentives, to search engine spam, and to search engine quality and competition.",2005-05-10,https://www.semanticscholar.org/paper/bb604ec86a36e95920ad0468590bc587532d23ae,The Web Conference
2518,Focus and Context in Mixed Reality by Modulating First Order Salient Features,"We present a technique for dynamically directing a viewer's attention to a focus object by analyzing and modulating bottom-up salient features of a video feed. Rather than applying a static modulation strategy, we inspect the original image's saliency map, and modify the image automatically to favor the focus object. Image fragments are adaptively darkened, lightened and manipulated in hue according to local contrast information rather than global parameters. The goal is to suggest rather than force the attention of the user towards a specific location. The technique's goal is to apply only minimal changes to an image, while achieving a desired difference of saliency between focus and context regions of the image. Our technique exhibits temporal and spatial coherence and runs at interactive frame rates using GPU shaders. We present several application examples from the field of Mixed Reality, or more precisely Mediated Reality.",2010-06-24,https://www.semanticscholar.org/paper/273182f7df098bbb979d19905ab60823b41d241d,International Symposium on Smart Graphics
728,The Complexity of Optimal Multidimensional Pricing,"We resolve the complexity of revenue-optimal deterministic auctions in the unit-demand single-buyer Bayesian setting, i.e., the optimal item pricing problem, when the buyer's values for the items are independent. We show that the problem of computing a revenue-optimal pricing can be solved in polynomial time for distributions of support size 2 and its decision version is NP-complete for distributions of support size 3. We also show that the problem remains NP-complete for the case of identical distributions.",2013-11-08,https://www.semanticscholar.org/paper/2faada3bca28233d0c9ef1b39edff258bef5a279,ACM-SIAM Symposium on Discrete Algorithms
230,"Algorithms, complexity, and the sciences","Significance The theory of algorithms and complexity, of which the basic concepts and results are being reviewed here, is an important and consequential domain of mathematical investigation that is unfamiliar to most PNAS readers. Furthermore, in the second half of the article we focus on recent and current research, applying these concepts and results to help elucidate certain central theoretical problems in the social and life sciences, including market equilibria and the theory of evolution. Algorithms, perhaps together with Moore’s law, compose the engine of the information technology revolution, whereas complexity—the antithesis of algorithms—is one of the deepest realms of mathematical investigation. After introducing the basic concepts of algorithms and complexity, and the fundamental complexity classes P (polynomial time) and NP (nondeterministic polynomial time, or search problems), we discuss briefly the P vs. NP problem. We then focus on certain classes between P and NP which capture important phenomena in the social and life sciences, namely the Nash equlibrium and other equilibria in economics and game theory, and certain processes in population genetics and evolution. Finally, an algorithm known as multiplicative weights update (MWU) provides an algorithmic interpretation of the evolution of allele frequencies in a population under sex and weak selection. All three of these equivalences are rife with domain-specific implications: The concept of Nash equilibrium may be less universal—and therefore less compelling—than has been presumed; selection on gene interactions may entail the maintenance of genetic variation for longer periods than selection on single alleles predicts; whereas MWU can be shown to maximize, for each gene, a convex combination of the gene’s cumulative fitness in the population and the entropy of the allele distribution, an insight that may be pertinent to the maintenance of variation in evolution.",2014-10-27,https://www.semanticscholar.org/paper/7081b6498e21d92e0574bddde2ef453e912de7ee,Proceedings of the National Academy of Sciences of the United States of America
3669,Adding classes to the C language: An exercise in language evolution,"The C language is a fine tool for writing compact and efficient programs. It is relatively easy to produce good compilers for, and the number of tools available for supporting program‐ ming in C is large, especially in its ‘home environment’, the UNM system. However, C'S facilities for structuring programs were, until recently, rather limited. To remedy this situation, a data abstraction facility, called classes, was added. The class concept described here has benefitted from the experience gained through a year's use. It is now in use at close to a hundred installations.",1983-02-01,https://www.semanticscholar.org/paper/97a4c9ff0f1c01e1d17141af984773ef9522f1ff,"Software, Practice & Experience"
3553,Software Development for Infrastructure,"Infrastructure software needs more stringent correctness, reliability, efficiency, and maintainability requirements than non- essential applications. This implies greater emphasis on up-front design, static structure enforced by a type system, compact data structures, simplified code structure, and improved tool support. Education for infrastructure and application developers should differ to reflect that emphasis. This Web extra video features Bjarne Stroustrup of Texas A&M University discussing how C++ can help improve the reliability, maintainability, and performance of infrastructure software. He also describes features that are part of the latest versions of the C++ language.",,https://www.semanticscholar.org/paper/eb24f14677fab1ae65c3f558c899c2c256c147ae,Computer
970,Primary Open-angle Glaucoma and Increased Risk of Chronic Kidney Disease,"Supplemental Digital Content is available in the text. Precis: The association between primary open-angle glaucoma (POAG) and subsequent development of chronic kidney disease (CKD) was investigated using a nationwide, population-based, retrospective cohort in South Korea. POAG increases the risk of subsequent CKD development. Purpose: The purpose of this study was to investigate the risk of subsequent CKD development in patients with POAG. Methods: In this nationwide, population-based longitudinal cohort, 1,025,340 beneficiaries in the 2002-2013 Korean National Health Insurance database were included. We identified patients with incident POAG and evaluated the risk of subsequent CKD development using diagnostic codes from the database after 2-year wash-out periods. We applied time-varying covariate Cox regression analyses to determine the effect of POAG on the development of CKD: Model 1 included only POAG as a time-varying covariate; Model 2 included Model 1 and demographic information; and Model 3 included Model 2, comorbidity, comedication, and the Charlson Comorbidity Index score. Results: The fixed cohort included 478,303 eligible subjects, and of these subjects, 1749 suffered incident POAG, and 3157 developed CKD. POAG was associated with an increased risk of CKD development [hazard ratio (HR)=7.63; 95% confidence interval (CI), 5.89-9.87] in Model 1; HR=3.54 (95% CI, 2.73-4.58) in Model 2; and HR=2.90 (95% CI, 2.24-3.76) in Model 3]. Conclusion: POAG increased the risk of subsequent CKD in the general population, suggesting that POAG and CKD might share a common pathogenic mechanism.",2019-10-17,https://www.semanticscholar.org/paper/0f498f31899832a3189f86380bbaf7e1cc11554e,Journal of glaucoma
341,Global Synchronization in Sensornets,,2004-04-05,https://www.semanticscholar.org/paper/0c334fd5dd371a36390117910372b189045fdc77,Latin American Symposium on Theoretical Informatics
2440,Augmented reality task guidance for international space station stowage operations,"Built at NASA Johnson Space Center (JSC) and Columbia University and tested in JSC's full-scale mockup of the International Space Station (ISS), StowageApp is a prototype for the future of conducting cargo operations in space. StowageApp dynamically guides astronauts as they complete stowage tasks, packing and unpacking cargo.",2018-08-12,https://www.semanticscholar.org/paper/a4138c11c6c68ebdaf64b47416196bf1784ff6ab,"ACM SIGGRAPH 2018 Virtual, Augmented, and Mixed Reality"
1706,Bayesian Nonnegative Matrix Factorization with Stochastic Variational Inference,,2014-11-06,https://www.semanticscholar.org/paper/87224164eef14589b137547a3fa81f06eef9bbf4,Handbook of Mixed Membership Models and Their Applications
607,Worst-Case and Probabilistic Analysis of a Geometric Location Problem,"We consider the problem of choosing K “medians” among n points on the Euclidean plane such that the sum of the distances from each of the n points to its closest median is minimized. We show that this problem is NP-complete. We also present two heuristics that produce arbitrarily good solutions with probability going to 1. One is a partition heuristic, and works when K grows linearly—or almost so—with n. The other is the “honeycomb” heuristic, and is applicable to rates of growth of K of the form $K \sim n^\varepsilon $, $0 < \varepsilon < 1$.",1981-08-01,https://www.semanticscholar.org/paper/8f38db0d58fc7f0a694b337a888e3b2ecb9df0a4,SIAM journal on computing (Print)
1920,Embedding ant system in genetic algorithm for re-entrant hybrid flow shop scheduling problems with time window constraints,,2017-12-01,https://www.semanticscholar.org/paper/9cd191446b5c3ab80d0a636450d7ddadf7b3db86,Journal of Intelligent Manufacturing
1587,The Blessings of Multiple Causes: Rejoinder,"We thank all the discussants for taking the time and energy to build on this work; and we thank the editors for putting together an engaging and thought-provoking collection of discussions. After reading these contributions, we were struck that these are not mere discussions—indeed, each is an article in itself. This collection pushes forward “The Blessings of Multiple Causes” in significant ways, offering new theory, new criticism, and new application. After highlighting some of the themes of these articles, we will turn to each individually. “The Blessings of Multiple Causes” provide assumptions, theory, and algorithms for multiple causal inference. The deconfounder algorithm involves modeling the causes, using the model to infer a substitute confounder, and then using the substitute confounder in a downstream causal inference. The deconfounder is not a black-box solution to causal inference. Rather, it is a way to use careful domain-specific modeling in the service of causal inference. Causal inference with the deconfounder involves a number of assumptions and trade-offs, and many of the discussants highlighted these. Among them are the following. (1) There can be no unobserved single-cause confounders. (2) When we apply the deconfounder, we trade an increase in estimation variance for a reduction in confounding bias; there is no free lunch. (3) We do not recommend using the deconfounder with causally dependent causes, such as a time series; finding a substitute confounder may be too difficult in these scenarios. There are many directions for further research, and the discussants have pointed out several of the most important ones. We need a more complete picture of identification; D’Amour (2019) and the discussions here make good progress (see Table1). We need to understand the finite-sample properties of the deconfounder, and how to estimate uncertainty about causal inferences when using a substitute multi-cause confounder. We need rigorous methods of model criticism for assessing the validity of the substitute confounder. Deconfounder-like methods have already been used for genome-wide association studies (e.g., Pritchard et al. 2000) and estimating peer effects in networks (Shalizi and McFowland III 2016). More broadly, the deconfounder strategy points to many applications, including in genetics, psychology, education, and marketing, where factor models are routinely fit to largescale data. We hope that statisticians and machine learners will",2019-10-02,https://www.semanticscholar.org/paper/faf9108b8698f34fe80abb5c996b4394611bbc53,Journal of the American Statistical Association
1692,A Probabilistic Model for Using Social Networks in Personalized Item Recommendation,"Preference-based recommendation systems have transformed how we consume media. By analyzing usage data, these methods uncover our latent preferences for items (such as articles or movies) and form recommendations based on the behavior of others with similar tastes. But traditional preference-based recommendations do not account for the social aspect of consumption, where a trusted friend might point us to an interesting item that does not match our typical preferences. In this work, we aim to bridge the gap between preference- and social-based recommendations. We develop social Poisson factorization (SPF), a probabilistic model that incorporates social network information into a traditional factorization method; SPF introduces the social aspect to algorithmic recommendation. We develop a scalable algorithm for analyzing data with SPF, and demonstrate that it outperforms competing methods on six real-world datasets; data sources include a social reader and Etsy.",2015-09-16,https://www.semanticscholar.org/paper/f4a5660fc68339b69289fda1dc4c546d9084748c,ACM Conference on Recommender Systems
1792,Variational Inference for Adaptor Grammars,"Adaptor grammars extend probabilistic context-free grammars to define prior distributions over trees with ""rich get richer"" dynamics. Inference for adaptor grammars seeks to find parse trees for raw text. This paper describes a variational inference algorithm for adaptor grammars, providing an alternative to Markov chain Monte Carlo methods. To derive this method, we develop a stick-breaking representation of adaptor grammars, a representation that enables us to define adaptor grammars with recursion. We report experimental results on a word segmentation task, showing that variational inference performs comparably to MCMC. Further, we show a significant speed-up when parallelizing the algorithm. Finally, we report promising results for a new application for adaptor grammars, dependency grammar induction.",2010-06-02,https://www.semanticscholar.org/paper/ebcd8b40a3d1d08bade75a30a5adddea423dd073,North American Chapter of the Association for Computational Linguistics
3068,Proceedings of the eleventh international joint conference on Measurement and modeling of computer systems,"It is our great pleasure to welcome you to SIGMETRICS/Performance 2009. SIGMETRICS is the flagship conference of the ACM special interest group for the computer systems performance evaluation community. Performance is the flagship conference of the IFIP working group on performance modeling and analysis. Every three years, the two conferences are held jointly, and this is the eleventh joint conference. 
 
This year, we will continue with several of the innovations introduced at last year's SIGMETRICS program. The main conference will again be a full three days, featuring 27 papers, 21 posters, and three invited talks from computer science luminaries, both academic and industrial. We will also reprise the demo competition and student thesis panel that were so well received last year. We are introducing an industrial information seminar, to provide an opportunity for students and academics to hear from representatives in industrial research about performance-related projects and their impact on deployed products and services. 
 
Supplementing the main conference are four interesting workshops, ranging from the venerable to the avant-garde. The workshop on MAthematical performance Modeling and Analysis (MAMA) continues its eleventh year as a forum for talks on early research in the more mathematical areas of computer performance analysis. The workshop on Hot Topics in Metrics (HotMetrics), which had a stellar inauguration last year, will reprise its role in helping to identify ""big"" and ""hard"" problems in performance evaluation and to develop innovative approaches to solving them. We also introduce two new workshops this year: GreenMetrics will explore how improvements to or new uses of Information and Communication Technology (ICT) can contribute towards efforts to minimize global climate change, a problem of increasing importance in modern society. The Learning for Networking workshop will investigate the use of machine learning techniques to tackle the increasingly complex architecture and control features in telecommunications and computer networks. 
 
This year also features a splendid series of tutorials, on topics ranging from social networks to data-center networks, from data-flow programming to packet-flow configuration, and from internet measurement to internet service construction.",2009-06-15,https://www.semanticscholar.org/paper/16d38d2d0ceeb3f909fea66e514fb397232888a5,Measurement and Modeling of Computer Systems
266,When the Players Are Not Expectation Maximizers,,2010-10-18,https://www.semanticscholar.org/paper/16d26b11881e7a563ccf2d6d4cf4e866abca46f3,Algorithmic Game Theory
2033,An efficient computational procedure for determining the container-loading pattern,,2009-04-01,https://www.semanticscholar.org/paper/89b0cfdfc87f2451c286927f646153b05be04468,Computers & industrial engineering
666,Solving Linear Algebra by Program Synthesis,"We solve MIT's Linear Algebra 18.06 course and Columbia University's Computational Linear Algebra COMS3251 courses with perfect accuracy by interactive program synthesis. This surprisingly strong result is achieved by turning the course questions into programming tasks and then running the programs to produce the correct answers. We use OpenAI Codex with zero-shot learning, without providing any examples in the prompts, to synthesize code from questions. We quantify the difference between the original question text and the transformed question text that yields a correct answer. Since all COMS3251 questions are not available online the model is not overfitting. We go beyond just generating code for questions with numerical answers by interactively generating code that also results visually pleasing plots as output. Finally, we automatically generate new questions given a few sample questions which may be used as new course content. This work is a significant step forward in solving quantitative math problems and opens the door for solving many university level STEM courses by machine.",2021-11-16,https://www.semanticscholar.org/paper/4e40595d6ecba027cebb4f2e3b43ae44bcf51daf,arXiv.org
2694,MAGIC: an experimental system for generating multimedia briefings about post-bypass patient status.,"We describe MAGIC, an experimental system for generating multimedia briefings about the clinical status of post-bypass patients entering a cardiac ICU. MAGIC is a distributed system whose components use knowledge-based techniques for planning and generating briefings in text, speech, and graphics. These briefings are coordinated together by reasoning with dynamically generated temporal and spatial constraints. Formative evaluation using system mock-ups with ICU nurses and residents have been used to determine the general format and content of these briefings. We present an overview of MAGIC's architecture and show what it can currently generate.",,https://www.semanticscholar.org/paper/74a43408011fbf9144ebf7dfc68a4b1a1c6024fd,Proceedings : a conference of the American Medical Informatics Association. AMIA Fall Symposium
739,"Market equilibrium under separable, piecewise-linear, concave utilities","We consider Fisher and Arrow--Debreu markets under additively separable, piecewise-linear, concave utility functions and obtain the following results. For both market models, if an equilibrium exists, there is one that is rational and can be written using polynomially many bits. There is no simple necessary and sufficient condition for the existence of an equilibrium: The problem of checking for existence of an equilibrium is NP-complete for both market models; the same holds for existence of an ε-approximate equilibrium, for ε = O(n−5). Under standard (mild) sufficient conditions, the problem of finding an exact equilibrium is in PPAD for both market models. Finally, building on the techniques of Chen et al. [2009a] we prove that under these sufficient conditions, finding an equilibrium for Fisher markets is PPAD-hard.",2011-05-01,https://www.semanticscholar.org/paper/8b1f7c0426499591885930fea1b6ec519a0761d8,JACM
1283,Search for scalar neutrino superpartners in e+mu final states in pp collisions at sqrt[s]=1.96 TeV.,"We report a search for R-parity-violating production and decay of sneutrino particles in the emu final state with 1.04+/-0.06 fb-1 of data collected with the D0 detector at the Fermilab Tevatron Collider in 2002-2006. Good agreement between the data and the standard model prediction is observed. With no evidence for new physics, we set limits on the R-parity-violating couplings lambda'311 and lambda312 as a function of the sneutrino mass.",2007-11-01,https://www.semanticscholar.org/paper/895a6500e064a5afe6f5772af548aa8ba5fbafb1,Physical Review Letters
3710,Full-Body Visual Self-Modeling of Robot Morphologies,"Internal computational models of physical bodies are fundamental to the ability of robots and animals alike to plan and control their actions. These""self-models""allow robots to consider outcomes of multiple possible future actions, without trying them out in physical reality. Recent progress in fully data-driven self-modeling has enabled machines to learn their own forward kinematics directly from task-agnostic interaction data. However, forward-kinema\-tics models can only predict limited aspects of the morphology, such as the position of end effectors or velocity of joints and masses. A key challenge is to model the entire morphology and kinematics, without prior knowledge of what aspects of the morphology will be relevant to future tasks. Here, we propose that instead of directly modeling forward-kinematics, a more useful form of self-modeling is one that could answer space occupancy queries, conditioned on the robot's state. Such query-driven self models are continuous in the spatial domain, memory efficient, fully differentiable and kinematic aware. In physical experiments, we demonstrate how a visual self-model is accurate to about one percent of the workspace, enabling the robot to perform various motion planning and control tasks. Visual self-modeling can also allow the robot to detect, localize and recover from real-world damage, leading to improved machine resiliency. Our project website is at: https://robot-morphology.cs.columbia.edu/",2021-11-11,https://www.semanticscholar.org/paper/42908b31098fd6760fc35e23e77c9c6d3b03e4f9,arXiv.org
470,Default Theories that Always Have Extensions,,1994-09-01,https://www.semanticscholar.org/paper/7fc9e8ffe34853602bf53fe8db81fb39cbec32da,Artificial Intelligence
577,The Traveling Salesman Problem with Many Visits to Few Cities,"We study the version of the traveling salesman problem in which a relatively small number of cities—say, six—must be visited a huge number of times—e.g., several hundred times each. (It costs to go from one city to itself.) We develop an algorithm for this problem whose running time is exponentialin the number of cities, but logarithmic in the number of visits. Our algorithm is a practical approach to the problem for instances of size in the range indicated above. The implementation and analysis of our algorithm give rise to a number of interesting graph-theoretic and counting problems.",1984-02-01,https://www.semanticscholar.org/paper/bd4458ef949c671a42cd17ca12a65e4a5eb63b53,SIAM journal on computing (Print)
3299,Conservation planning on a budget: a “resource light” method for mapping priorities at a landscape scale?,,2009-01-20,https://www.semanticscholar.org/paper/7a7066214cdcc1c30533cab7030bfec83bf9d8ca,Biodiversity and Conservation
2754,Coordinating Text and Graphics in Explanation Generation,"To generate multimedia explanations, a system must be able to coordinate the use of different media in a single explanation. In this paper, we present the architecture that we have developed for COMET (COordinated Multimedia Explanation Testbed), a system that generates directions for equipment maintenance and repair, and we show how it addresses the coordination problem. COMET includes a single content planner that produces a common content description used by multiple media-specific generators, and a media coordinator that performs a fine-grained division of information among media. Bidirectional interaction between media-specific generators allows influence across media. We describe COMET's current capabilities and provide an overview of our plans for extending the system.",1989-10-15,https://www.semanticscholar.org/paper/6ec283b2928292ba8175346e489accded8e758ac,Human Language Technology - The Baltic Perspectiv
2821,Galectin-3 preserves renal tubules and modulates extracellular matrix remodeling in progressive fibrosis.,"Renal tubular cell apoptosis is a critical detrimental event that leads to chronic kidney injury in association with renal fibrosis. The present study was designed to investigate the role of galectin-3 (Gal-3), an important regulator of multiple apoptotic pathways, in chronic kidney disease induced by unilateral ureteral obstruction (UUO). After UUO, Gal-3 expression significantly increased compared with basal levels reaching a peak increase of 95-fold by day 7. Upregulated Gal-3 is predominantly tubular at early time points after UUO but shifts to interstitial cells as the injury progresses. On day 14, there was a significant increase in TdT-mediated dUTP nick end labeling-positive cells (129%) and cytochrome c release (29%), and a decrease in BrdU-positive cells (62%) in Gal-3-deficient compared with wild-type mice. The degree of renal damage was more extensive in Gal-3-deficient mice at days 14 and 21, 35 and 21% increase in total collagen, respectively. Despite more severe fibrosis, myofibroblasts were significantly decreased by 58% on day 14 in the Gal-3-deficient compared with wild-type mice. There was also a corresponding 80% decrease in extracellular matrix synthesis in Gal-3-deficient compared with wild-type mice. Endo180 is a recently recognized receptor for intracellular collagen degradation that is expressed by interstitial cells during renal fibrogenesis. Endo180 expression was significantly decreased by greater than 50% in Gal-3-deficient compared with wild-type mice. Taken together, these results suggested that Gal-3 not only protects renal tubules from chronic injury by limiting apoptosis but that it may lead to enhanced matrix remodeling and fibrosis attenuation.",,https://www.semanticscholar.org/paper/756bfc6688fc0663efbffb152bda0c586886638e,AJP - Renal Physiology
87,Approximate String Joins in a Database (Almost) for Free,"String data is ubiquitous, and its management has taken on particular importance in the past few years. Approximate queries are very important on string data especially for more complex queries involving joins. This is due, for example, to the prevalence of typographical errors in data, and multiple conventions for recording attributes such as name and address. Commercial databases do not support approximate string joins directly, and it is a challenge to implement this functionality efficiently with user-defined functions (UDFs). In this paper, we develop a technique for building approximate string join capabilities on top of commercial databases by exploiting facilities already available in them. At the core, our technique relies on matching short substrings of length , called -grams, and taking into account both positions of individual matches and the total number of such matches. Our approach applies to both approximate full string matching and approximate substring matching, with a variety of possible edit distance functions. The approximate string match predicate, with a suitable edit distance threshold, can be mapped into a vanilla relational expression and optimized by conventional relational optimizers. We demonstrate experimentally the benefits of our technique over the direct use of UDFs, using commercial database systems and real data. To study the I/O and CPU behavior of approximate string join algorithms with variations in edit distance and -gram length, we also describe detailed experiments based on a prototype implementation.",2001-09-11,https://www.semanticscholar.org/paper/5737a1f6fd8d928b88726ada916d7874afdfe0d7,Very Large Data Bases Conference
1918,Strategic capacity planning for smart production: Decision modeling under demand uncertainty,,2017-06-06,https://www.semanticscholar.org/paper/2626f5e6f8592c4baee9a17edc74105ba85bb654,Applied Soft Computing
2453,"A framework to facilitate reusable, modular widget design for real-time interactive systems","Game engines have become popular development platforms for real-time interactive systems. Contemporary game engines, such as Unity and Unreal, feature component-based architectures, in which an object's appearance and behavior is determined by a collection of component scripts added to that object. This design pattern allows common functionality to be contained within component scripts and shared among different types of objects. In this paper, we describe a flexible framework that enables programmers to design modular, reusable widgets for real-time interactive systems using a collection of component scripts. We provide a reference implementation written in C# for the Unity game engine. Making an object, or a group of objects, part of our managed widget framework can be accomplished with just a few drag-and-drop operations in the Unity Editor. While our framework provides hooks and default implementations for common widget behavior (e.g., initialization, refresh, and toggling visibility), programmers can also define custom behavior for a particular widget or combine simple widgets into a hierarchy and build arbitrarily rich ones. Finally, we provide an overview of an accompanying library of scripts that support functionality for testing and networking.",2016-03-20,https://www.semanticscholar.org/paper/2496b98e3d275b6d1d6c689ced55c144fe2a4123,Workshop on Software Engineering and Architectures for Realtime Interactive Systems
2009,Measuring relative performance of wafer fabrication operations: a case study,,2011-06-01,https://www.semanticscholar.org/paper/076cc57437eb11c919af45b7dc3a4c68e25dfb0a,Journal of Intelligent Manufacturing
447,On the Difficulty of Designing Good Classifiers,"It is a very interesting and well-studied problem, given two point sets W, B\(\subseteq\)ℜn, to design a linear decision tree that classifies them —that is, no leaf subdivision contains points from both B and W — and is as simple as possible, either in terms of the total number of nodes, or in terms of its depth. We show that, unless ZPP=NP, the depth of a classifier cannot be approximated within a factor smaller than 6/5, and that the total number of nodes cannot be approximated within a factor smaller than n1/5. Our proof relies on a simple connection between this problem and graph coloring, and uses recent nonapproximability results for graph coloring. We also study the problem of designing a classifier with a single inequality that involves as few variables as possible, and point out certain aspects of the difficulty of this problem.",1996-06-17,https://www.semanticscholar.org/paper/72964363b75234ccfba32010ac8c1e8966ea13e5,SIAM journal on computing (Print)
2931,Variational Variance: Simple and Reliable Predictive Variance Parameterization,"An often overlooked sleight of hand performed with variational autoencoders (VAEs), which has proliferated the literature, is to misrepresent the posterior predictive (decoder) distribution's expectation as a sample from that distribution. Jointly modeling the mean and variance for a normal predictive distribution can result in fragile optimization where the ultimately learned parameters can be ineffective at generating realistic samples. The two most common principled methods to avoid this problem are to either fix the variance or use the single-parameter Bernoulli distribution--both have drawbacks, however. Unfortunately, the problem of jointly optimizing mean and variance networks affects not only unsupervised modeling of continuous data (a taxonomy for many VAE applications) but also regression tasks. To date, only a handful of papers have attempted to resolve these difficulties. In this article, we propose an alternative and attractively simple solution: treat predictive variance variationally. Our approach synergizes with existing VAE-specific theoretical results and, being probabilistically principled, provides access to Empirical Bayes and other such techniques that utilize the observed data to construct well-informed priors. We extend the VAMP prior, which assumes a uniform mixture, by inferring mixture proportions and assignments. This extension amplifies our ability to accurately capture heteroscedastic variance. Notably, our methods experimentally outperform existing techniques on supervised and unsupervised modeling of continuous data.",2020-06-08,https://www.semanticscholar.org/paper/ff8bae876668407fcd9ffc4e042dfc9104821a86,arXiv.org
2494,Subtle cueing for visual search in augmented reality,"Visual search in augmented reality environments is an important task that can be facilitated through different cueing methods. Current cueing methods rely on explicit cueing, which can potentially reduce visual search performance. In comparison, this paper proposes a subtle cueing method that improves visual search performance while being clutter-neutral. Two empirical user studies were conducted to evaluate our subtle cueing method in outdoor scenes. The results show that subtle cueing functions well within a narrow Feature Congestion range, and could be a feasible alternative to explicit cueing.",2012-11-05,https://www.semanticscholar.org/paper/4c2b484e5723b015ec7c20c9b7b3d16bb5c7cc67,International Symposium on Mixed and Augmented Reality
693,The Smoothed Complexity of Policy Iteration for Markov Decision Processes,"We show subexponential lower bounds (i.e., 2Ω (nc)) on the smoothed complexity of the classical Howard’s Policy Iteration algorithm for Markov Decision Processes. The bounds hold for the total reward and the average reward criteria. The constructions are robust in the sense that the subexponential bound holds not only on the average for independent random perturbations of the MDP parameters (transition probabilities and rewards), but for all arbitrary perturbations within an inverse polynomial range. We show also an exponential lower bound on the worst-case complexity for the simple reachability objective.",2022-11-30,https://www.semanticscholar.org/paper/183e447b3101ac996c020d5d49cc1589d48fbde3,Symposium on the Theory of Computing
3264,Water Use Patterns of Sympatric Przewalski’s Horse and Khulan: Interspecific Comparison Reveals Niche Differences,"Acquiring water is essential for all animals, but doing so is most challenging for desert-living animals. Recently Przewalski’s horse has been reintroduced to the desert area in China where the last wild surviving member of the species was seen before it vanished from China in the1960s. Its reintroduction placed it within the range of a close evolutionary relative, the con-generic Khulan. Determining whether or not these two species experience competition and whether or not such competition was responsible for the extinction of Przewalski’s horses in the wild over 50 years ago, requires identifying the fundamental and realized niches of both species. We remotely monitored the presence of both species at a variety of water points during the dry season in Kalamaili Nature Reserve, Xinjiang, China. Przewalski’s horses drank twice per day mostly during daylight hours at low salinity water sources while Khulans drank mostly at night usually at high salinity water points or those far from human residences. Spatial and temporal differences in water use enables coexistence, but suggest that Przewalski’s horses also restrict the actions of Khulan. Such differences in both the fundamental and realized niches were associated with differences in physiological tolerances for saline water and human activity as well as differences in aggression and dominance.",2015-07-10,https://www.semanticscholar.org/paper/f59f18091bcd9b1ede379b90d1cfadcd44aaf07c,PLoS ONE
821,Principles and methods of testing finite state machines-a survey,"With advanced computer technology, systems are getting larger to fulfill more complicated tasks: however, they are also becoming less reliable. Consequently, testing is an indispensable part of system design and implementation; yet it has proved to be a formidable task for complex systems. This motivates the study of testing finite stare machines to ensure the correct functioning of systems and to discover aspects of their behavior. A finite state machine contains a finite number of states and produces outputs on state transitions after receiving inputs. Finite state machines are widely used to model systems in diverse areas, including sequential circuits, certain types of programs, and, more recently, communication protocols. In a testing problem we have a machine about which we lack some information; we would like to deduce this information by providing a sequence of inputs to the machine and observing the outputs produced. Because of its practical importance and theoretical interest, the problem of testing finite state machines has been studied in different areas and at various times. The earliest published literature on this topic dates back to the 1950's. Activities in the 1960's mid early 1970's were motivated mainly by automata theory and sequential circuit testing. The area seemed to have mostly died down until a few years ago when the testing problem was resurrected and is now being studied anew due to its applications to conformance testing of communication protocols. While some old problems which had been open for decades were resolved recently, new concepts and more intriguing problems from new applications emerge. We review the fundamental problems in testing finite state machines and techniques for solving these problems, tracing progress in the area from its inception to the present and the stare of the art. In addition, we discuss extensions of finite state machines and some other topics related to testing.",1996-08-01,https://www.semanticscholar.org/paper/d7790181ae4a6af8c0418f3bc0f4850ddffbc3f8,Proceedings of the IEEE
1811,Multilingual Topic Models for Unaligned Text,"We develop the multilingual topic model for unaligned text (MuTo), a probabilistic model of text that is designed to analyze corpora composed of documents in two languages. From these documents, MuTo uses stochastic EM to simultaneously discover both a matching between the languages and multilingual latent topics. We demonstrate that MuTo is able to find shared topics on real-world multilingual corpora, successfully pairing related documents across languages. MuTo provides a new framework for creating multilingual topic models without needing carefully curated parallel corpora and allows applications built using the topic model formalism to be applied to a much wider class of corpora.",2009-06-18,https://www.semanticscholar.org/paper/b737b228865eefb4a5627d77a0c0984c64fb98ed,Conference on Uncertainty in Artificial Intelligence
1206,Measurement of the forward-backward charge asymmetry and extraction of sin2thetaWeff in pp-->Z/gamma* + X-->e+e- +X events produced at sqrt[s] = 1.96 TeV.,"We present a measurement of the forward-backward charge asymmetry (A FB) in pp-->Z/gamma* + X-->e+e(-) + X events at a center-of-mass energy of 1.96 TeV using 1.1 fb(-1) of data collected with the D0 detector at the Fermilab Tevatron collider. A FB is measured as a function of the invariant mass of the electron-positron pair, and found to be consistent with the standard model prediction. We use the A FB measurement to extract the effective weak mixing angle sin2thetaWeff = 0.2326+/-0.0018(stat)+/-0.0006(syst).",,https://www.semanticscholar.org/paper/3d92c77d75d51ff3c2eefb44327fec7d23b066ae,Physical Review Letters
53,XML & Data Streams,,,https://www.semanticscholar.org/paper/9c72ac1af5630f2a27eff0e5d7726722424f1be9,Stream Data Management
955,Nitric oxide attenuated transforming growth factor-β induced myofibroblast differentiation of human keratocytes,,2021-04-14,https://www.semanticscholar.org/paper/b137928bffe575226ef5b98f6c8da75b933ee24d,Scientific Reports
2116,A DEA Study to Evaluate the Relative Efficiency and Investigate the District Reorganization of the Taiwan Power Company,In this study data envelopment analysis (DEA) models were applied to evaluate the relative efficiencies of twenty-two electricity distribution districts of the Taiwan Power Company (TPC) in Taiwan. The empirical study showed that the TPC districts have good overall efficiency. We found that eleven districts were inefficient. Most of the inefficient districts suffer from scale inefficiency to a greater degree than technical inefficiency. We suggested the specific improvement directions for the corresponding inefficient districts. This study also investigated district reorganization to increase the efficiency. The proposed district reorganization alternatives have higher efficiency scores than the current one.,2001-02-01,https://www.semanticscholar.org/paper/1a8694264d047279aa0b0ab577935fa38518d1b0,IEEE Power Engineering Review
3687,SHIFT3D: Synthesizing Hard Inputs For Tricking 3D Detectors,"We present SHIFT3D, a differentiable pipeline for generating 3D shapes that are structurally plausible yet challenging to 3D object detectors. In safety-critical applications like autonomous driving, discovering such novel challenging objects can offer insight into unknown vulnerabilities of 3D detectors. By representing objects with a signed distanced function (SDF), we show that gradient error signals allow us to smoothly deform the shape or pose of a 3D object in order to confuse a downstream 3D detector. Importantly, the objects generated by SHIFT3D physically differ from the baseline object yet retain a semantically recognizable shape. Our approach provides interpretable failure modes for modern 3D object detectors, and can aid in preemptive discovery of potential safety risks within 3D perception systems before these risks become critical failures.",2023-09-11,https://www.semanticscholar.org/paper/dcd0410e49db0aeac786a4a8700ab27b91f50d52,arXiv.org
2466,"Wearable Computing, 3D Aug* Reality, Photographic/Videographic Gesture Sensing, and Veillance","Wearable computers and Generation-5 Digital Eye Glass easily recognize a user's own gestures, forming the basis for shared AR (Augmediated Reality). This Studio-workshop presents the latest in wearable AR, plus an historical perspective with new insights. Participants will sculpt 3D objects using hand gestures and create Unity 3D art+game objects using computational lightpainting. Participants will also learn how to use 3D gesture-based AR to visualize and understand real-world phenomena, including being able to see sound waves, see radio waves, and see sight itself, through abakographic user-interfaces that interact with ""sightfields"" (time-reversed lightfields). Participants will also surveilluminescent devices that change color when watched by a camera. Long exposure photographs made with such devices generate ""sightpaintings"" that show what a camera can ""see"".",2015-01-15,https://www.semanticscholar.org/paper/43128f8578f8641e70fe967347befa561f191768,"International Conference on Tangible, Embedded, and Embodied Interaction"
2537,Evaluating the benefits of augmented reality for task localization in maintenance of an armored personnel carrier turret,"We present the design, implementation, and user testing of a prototype augmented reality application to support military mechanics conducting routine maintenance tasks inside an armored vehicle turret. Our prototype uses a tracked head-worn display to augment a mechanic's natural view with text, labels, arrows, and animated sequences designed to facilitate task comprehension, location, and execution. A within-subject controlled user study examined professional military mechanics using our system to complete 18 common tasks under field conditions. These tasks included installing and removing fasteners and indicator lights, and connecting cables, all within the cramped interior of an armored personnel carrier turret. An augmented reality condition was tested against two baseline conditions: an untracked headworn display with text and graphics and a fixed flat panel display representing an improved version of the laptop-based documentation currently employed in practice. The augmented reality condition allowed mechanics to locate tasks more quickly than when using either baseline, and in some instances, resulted in less overall head movement. A qualitative survey showed mechanics found the augmented reality condition intuitive and satisfying for the tested sequence of tasks.",2009-10-19,https://www.semanticscholar.org/paper/c0e9ba5a47d70049e9feae8b033c1d8e711c0f28,2009 8th IEEE International Symposium on Mixed and Augmented Reality
2200,Killing of Escherichia coli by Crohn's Disease Monocyte-derived Macrophages and Its Enhancement by Hydroxychloroquine and Vitamin D,"Background:Crohn's disease (CD) is associated with defective innate immunity, including impaired neutrophil chemotaxis, and mucosal invasion by bacteria, particularly adherent and invasive Escherichia coli that replicate inside macrophage phagolysosomes. We compared CD and healthy control (HC) macrophages for their abilities to kill E. coli and generate neutrophil chemoattractants and also assessed the effects of hydroxychloroquine (HCQ) and vitamin D on killing of phagocytosed E. coli. Methods:Peripheral blood monocyte-derived macrophages from CD and HC were compared for bacterial killing and generation of neutrophil chemoattractants in response to CD-derived E. coli. Escherichia coli replication was also assessed in the presence and absence of HCQ, alone and with antibiotics, and vitamin D. Results:Monocyte-derived macrophages from patients with CD were similar to HC in allowing replication of phagocytosed CD-derived E. coli: HM605 {CD: N = 10, mean fold replication in 3 hr = 1.08 (95% confidence interval [CI], 0.39–1.78); HC: N = 9, 1.50 (95% CI, 1.02–1.97); P = 0.15} and also in generation of neutrophil chemoattractants in response to E. coli (mean fold chemotaxis relative to control: CD = 2.55 [95% CI, 2.31–2.80]; HC = 2.65 [95% CI, 2.46–2.85], P = 0.42). HCQ and 1,25 OH2-vitamin D3 both caused dose-dependent inhibition of intramacrophage E. coli replication 3-hour postinfection; HCQ: 73.9% inhibition (P < 0.001) at 1 &mgr;g/mL, accompanied by raised intraphagosomal pH, and 1,25 OH2-vitamin D3: 80.7% inhibition (P < 0.05) at 80 nM. HCQ had synergistic effects with doxycycline and ciprofloxacin. Conclusions:CD and HC macrophages perform similarly in allowing replication of phagocytosed E. coli and generating neutrophil chemoattractants. Replication of phagocytosed E. coli was substantially decreased by HCQ and vitamin D. These warrant further therapeutic trials in CD in combination with relevant antibiotics.",2015-04-01,https://www.semanticscholar.org/paper/c3e79f1a38abb41b202dd156da5abc11dc85a81f,Inflammatory Bowel Diseases
3500,On the existence of schedules that are near-optimal for both makespan and total weighted completion time,,1997-10-01,https://www.semanticscholar.org/paper/c21b96e3cafc1ab2817a4b40c55a00d3523f319f,Operations Research Letters
1148,Search for neutral Higgs Bosons at high tanbeta in the b(h/H/A)-->btau;{+}tau;{-} channel.,"The first search in pp[over ] collisions at sqrt[s]=1.96 TeV for the production of neutral Higgs bosons in association with bottom quarks and decaying in two tau leptons is presented. The cross section for this process is enhanced in many extensions of the standard model, such as its minimal supersymmetric extension (MSSM) at large tanbeta. The data, corresponding to an integrated luminosity of 328 pb;{-1}, were collected with the D0 detector at the Fermilab Tevatron Collider. An upper limit is set on the production cross section of neutral Higgs bosons in the mass range of 90 to 150 GeV, and this limit is used to exclude part of the MSSM parameter space.",,https://www.semanticscholar.org/paper/2d2009e0f6024bc9da7cb8716322e827a46fe666,Physical Review Letters
2753,Near real-time shadow generation using BSP trees,"This paper describes an object-space shadow generation algorithm for static polygonal environments illuminated by movable point light sources. The algorithm can be easily implemented on any graphics system that provides fast polygon scan-conversion and achieves near real-time performance for environments of modest size. It combines elements of two kinds of current shadow generation algorithms: two-pass object-space approaches and shadow volume approaches. For each light source a Binary Space Partitioning (BSP) tree is constructed that represents the shadow volume of the polygons facing it. As each polygon's contribution to a light source's shadow volume is determined, the polygon's shadowed and lit fragments are computed by filtering it down the shadow volume BSP tree. The polygonal scene with its computed shadows can be rendered with any polygon-based visible-surface algorithm. Since the shadow volumes and shadows are computed in object space, they can be used for further analysis of the scene. Pseudocode is provided, along with pictures and timings from an interactive implementation.",1989-07-01,https://www.semanticscholar.org/paper/5a915172497f568e9e94071966cbccffe90439d2,International Conference on Computer Graphics and Interactive Techniques
412,Map graphs,"We consider a modified notion of planarity, in which two nations of a map are considered adjacent when they share any point of their boundaries (not necessarily an edge, as planarity requires). Such adjacencies define a map graph. We give an NP characterization for such graphs, derive some consequences regarding sparsity and coloring, and survey some algorithmic results.",1999-10-13,https://www.semanticscholar.org/paper/c9d2e979c18ffda37215607e915bdd0d5f523e47,JACM
1940,Extended priority-based hybrid genetic algorithm for the less-than-container loading problem,,2016-06-01,https://www.semanticscholar.org/paper/dbba17786159666850e2c3539b12d2c342601a12,Computers & industrial engineering
2519,Opportunistic Tangible User Interfaces for Augmented Reality,"Opportunistic Controls are a class of user interaction techniques that we have developed for augmented reality (AR) applications to support gesturing on, and receiving feedback from, otherwise unused affordances already present in the domain environment. By leveraging characteristics of these affordances to provide passive haptics that ease gesture input, Opportunistic Controls simplify gesture recognition, and provide tangible feedback to the user. In this approach, 3D widgets are tightly coupled with affordances to provide visual feedback and hints about the functionality of the control. For example, a set of buttons can be mapped to existing tactile features on domain objects. We describe examples of Opportunistic Controls that we have designed and implemented using optical marker tracking, combined with appearance-based gesture recognition. We present the results of two user studies. In the first, participants performed a simulated maintenance inspection of an aircraft engine using a set of virtual buttons implemented both as Opportunistic Controls and using simpler passive haptics. Opportunistic Controls allowed participants to complete their tasks significantly faster and were preferred over the baseline technique. In the second, participants proposed and demonstrated user interfaces incorporating Opportunistic Controls for two domains, allowing us to gain additional insights into how user interfaces featuring Opportunistic Controls might be designed.",,https://www.semanticscholar.org/paper/522ffc35a5dba800e055275d4242e2e51f8a2a51,IEEE Transactions on Visualization and Computer Graphics
1659,Reweighted Data for Robust Probabilistic Models,"Probabilistic models analyze data by relying on a set of assumptions. When a model performs poorly, we challenge its assumptions. This approach has led to myriad hand-crafted robust models; they offer protection against small deviations from their assumptions. We propose a simple way to systematically mitigate mismatch of a large class of probabilistic models. The idea is to raise the likelihood of each observation to a weight. Inferring these weights allows a model to identify observations that match its assumptions; down-weighting others enables robust inference and improved predictive accuracy. We study four different forms of model mismatch, ranging from missing latent groups to structure misspecification. A Poisson factorization analysis of the Movielens dataset shows the benefits of reweighting in a real data scenario.",2016-06-13,https://www.semanticscholar.org/paper/9574b281c8d4879aceec428e3cc3d6714efc989a,arXiv.org
1903,A hybrid multi-subpopulation genetic algorithm for textile batch dyeing scheduling and an empirical study,,2018-11-01,https://www.semanticscholar.org/paper/55f277a3d5b6254de108a7812943c0320bacbf29,Computers & industrial engineering
893,"Addendum: Simple Linear-Time Algorithms to Test Chordality of Graphs, Test Acyclicity of Hypergraphs, and Selectively Reduce Acyclic Hypergraphs",,,https://www.semanticscholar.org/paper/27c19e1522fd1acb03402086961161f8e1498f89,SIAM journal on computing (Print)
1964,Special Issue on Artificial Intelligence & Industrial Engineering,,2014-07-22,https://www.semanticscholar.org/paper/83f3a272af717b932a5aba35c18b815e543e5ad2,International Journal of Computational Intelligence Systems
636,The Euclidean Traveling Salesman Problem is NP-Complete,,,https://www.semanticscholar.org/paper/b151ce5e30980fdeda2fc2413250d6502cdddcc5,Theoretical Computer Science
1387,Search for narrow tt resonances in pp collisions at square root of (s)=1.8 TeV.,"A search for narrow resonances that decay into tt pairs has been performed using 130 pb(-1) of data in the lepton + jets channel collected by the Dphi detector in pp collisions at square root of (s)=1.8 TeV. There is no significant deviation observed from the standard-model predictions at a top-quark mass of 175 GeV/c2. We therefore present upper limits at the 95% confidence level on the product of the production cross section and branching fraction to tt for narrow resonances as a function of the resonance mass MX. These limits are used to exclude the existence of a leptophobic top-color particle with mass MX<560 GeV/c2, using a theoretical cross section for a width GammaX=0.012MX.",2003-07-29,https://www.semanticscholar.org/paper/85e0932f4bc6ba2fbde8f3b454ffd137d712e98e,Physical Review Letters
3696,It's Time for Artistic Correspondence in Music and Video,"We present an approach for recommending a music track for a given video, and vice versa, based on both their temporal alignment and their correspondence at an artistic level. We propose a self-supervised approach that learns this correspondence directly from data, without any need of human annotations. In order to capture the high-level concepts that are required to solve the task, we propose modeling the long-term temporal context of both the video and the music signals, using Transformer networks for each modality. Experiments show that this approach strongly outperforms alternatives that do not exploit the temporal context. The combination of our contributions improve retrieval accuracy up to 10× over prior state of the art. This strong improvement allows us to introduce a wide range of analyses and applications. For instance, we can condition music retrieval based on visually defined attributes.",2022-06-01,https://www.semanticscholar.org/paper/4b57f6eb0c1a69349dd3f446d114f2e8301bfcbe,Computer Vision and Pattern Recognition
962,Case report: what gives the myopic tilted disc an oval appearance?,,2020-01-09,https://www.semanticscholar.org/paper/0e4cdddec3cba9176f6719b2560b5d09cc05b1ef,BMC Ophthalmology
2534,Interference avoidance in multi-user hand-held augmented reality,"In a multi-user augmented reality application for a shared physical environment, it is possible for users to interfere with each other. For example, in a multi-player game in which each player holds a display whose tracked position and orientation affect the outcome, one player may physically block another player's view or physically contact another player. We explore software techniques intended to avoid such interference. These techniques modify what a user sees or hears, and what interaction capabilities they have, when their display gets too close to another user's display. We present Redirected Motion, an effective, yet nondistracting, interference avoidance technique for hand-held AR, which transforms the 3D space in which the user moves their display, to direct the display away from other displays. We conducted a within-subject, formal user study to evaluate the effectiveness and distraction level of Redirected Motion compared to other interference avoidance techniques. The study is based on an instrumented, two-player, first-person-shooter, augmented reality game, in which each player holds a 6DOF-tracked ultra-mobile computer. Comparison conditions include an unmanipulated control condition and three other software techniques for avoiding interference: dimming the display, playing disturbing sounds, and disabling interaction capabilities. Subjective evaluation indicates that Redirected Motion was unnoticeable, and quantitative analysis shows that the mean distance between users during Redirected Motion was significantly larger than for the comparison conditions.",2009-10-19,https://www.semanticscholar.org/paper/628c1ade689ced145d22e262cf832d1882dce392,2009 8th IEEE International Symposium on Mixed and Augmented Reality
831,Some Open Problems in Approximation,,1994-03-01,https://www.semanticscholar.org/paper/5776470d2096eae53e8abcffefcb84c9c04afa33,International/Italian Conference on Algorithms and Complexity
39,Classification-aware hidden-web text database selection,"Many valuable text databases on the web have noncrawlable contents that are “hidden” behind search interfaces. Metasearchers are helpful tools for searching over multiple such “hidden-web” text databases at once through a unified query interface. An important step in the metasearching process is database selection, or determining which databases are the most relevant for a given user query. The state-of-the-art database selection techniques rely on statistical summaries of the database contents, generally including the database vocabulary and associated word frequencies. Unfortunately, hidden-web text databases typically do not export such summaries, so previous research has developed algorithms for constructing approximate content summaries from document samples extracted from the databases via querying. We present a novel “focused-probing” sampling algorithm that detects the topics covered in a database and adaptively extracts documents that are representative of the topic coverage of the database. Our algorithm is the first to construct content summaries that include the frequencies of the words in the database. Unfortunately, Zipf's law practically guarantees that for any relatively large database, content summaries built from moderately sized document samples will fail to cover many low-frequency words; in turn, incomplete content summaries might negatively affect the database selection process, especially for short queries with infrequent words. To enhance the sparse document samples and improve the database selection decisions, we exploit the fact that topically similar databases tend to have similar vocabularies, so samples extracted from databases with a similar topical focus can complement each other. We have developed two database selection algorithms that exploit this observation. The first algorithm proceeds hierarchically and selects the best categories for a query, and then sends the query to the appropriate databases in the chosen categories. The second algorithm uses “shrinkage,” a statistical technique for improving parameter estimation in the face of sparse data, to enhance the database content summaries with category-specific words. We describe how to modify existing database selection algorithms to adaptively decide (at runtime) whether shrinkage is beneficial for a query. A thorough evaluation over a variety of databases, including 315 real web databases as well as TREC data, suggests that the proposed sampling methods generate high-quality content summaries and that the database selection algorithms produce significantly more relevant database selection decisions and overall search results than existing algorithms.",2008-03-01,https://www.semanticscholar.org/paper/8e568809e0d8b46b9a0ab47de3996633c2035620,TOIS
222,On the Computational Complexity of Limit Cycles in Dynamical Systems,"We study the Poincare-Bendixson theorem for two-dimensional continuous dynamical systems in compact domains from the point of view of computation, seeking algorithms for finding the limit cycle promised by this classical result. We start by considering a discrete analogue of this theorem and show that both finding a point on a limit cycle, and determining if a given point is on one, are PSPACE-complete. For the continuous version, we show that both problems are uncomputable in the real complexity sense; i.e., their complexity is arbitrarily high. Subsequently, we introduce a notion of an approximate cycle and prove an approximate Poincare-Bendixson theorem guaranteeing that some orbits come very close to forming a cycle in the absence of approximate fixpoints; surprisingly, it holds for all dimensions. The corresponding computational problem defined in terms of arithmetic circuits is PSPACE-complete.",2015-11-24,https://www.semanticscholar.org/paper/f05328b33d4b04161ef866194e923eeb61d2bcf1,Information Technology Convergence and Services
2644,Research Paper: Generation and Evaluation of Intraoperative Inferences for Automated Health Care Briefings on Patient Status After Bypass Surgery,"OBJECTIVE
The authors present a system that scans electronic records from cardiac surgery and uses inference rules to identify and classify abnormal events (e.g., hypertension) that may occur during critical surgical points (e.g., start of bypass). This vital information is used as the content of automatically generated briefings designed by MAGIC, a multimedia system that they are developing to brief intensive care unit clinicians on patient status after cardiac surgery. By recognizing patterns in the patient record, inferences concisely summarize detailed patient data.


DESIGN
The authors present the development of inference rules that identify important information about patient status and describe their implementation and an experiment they carried out to validate their correctness. The data for a set of 24 patients were analyzed independently by the system and by 46 physicians.


MEASUREMENTS
The authors measured accuracy, specificity, and sensitivity by comparing system inferences against physician judgments, in cases where all three physicians agreed and against the majority opinion in all cases.


RESULTS
For laboratory inferences, evaluation shows that the system has an average accuracy of 98 percent (full agreement) and 96 percent (majority model). An analysis of interrater agreement, however, showed that physicians do not agree on abnormal hemodynamic events and could not serve as a gold standard for evaluating hemodynamic events. Analysis of discrepancies reveals possibilities for system improvement and causes of physician disagreement.


CONCLUSIONS
This evaluation shows that the laboratory inferences of the system have high accuracy. The lack of agreement among physicians highlights the need for an objective quality-assurance tool for hemodynamic inferences. The system provides such a tool by implementing inferencing procedures established in the literature.",2001-05-01,https://www.semanticscholar.org/paper/6370d5d5f16310a46285977e6a95e9a153393184,J. Am. Medical Informatics Assoc.
201,A Model for Structured Information Representation in Neural Networks of the Brain,"Abstract Humans can reason at an abstract level and structure information into abstract categories, but the underlying neural processes have remained unknown. Recent experimental data provide the hint that this is likely to involve specific subareas of the brain from which structural information can be decoded. Based on this data, we introduce the concept of assembly projections, a general principle for attaching structural information to content in generic networks of spiking neurons. According to the assembly projections principle, structure-encoding assemblies emerge and are dynamically attached to content representations through Hebbian plasticity mechanisms. This model provides the basis for explaining a number of experimental data and provides a basis for modeling abstract computational operations of the brain.",2016-11-11,https://www.semanticscholar.org/paper/6d4e8da48fca855edb6381cccf81e2ff3dc25dd8,eNeuro
3691,Representing Spatial Trajectories as Distributions,"We introduce a representation learning framework for spatial trajectories. We represent partial observations of trajectories as probability distributions in a learned latent space, which characterize the uncertainty about unobserved parts of the trajectory. Our framework allows us to obtain samples from a trajectory for any continuous point in time, both interpolating and extrapolating. Our flexible approach supports directly modifying specific attributes of a trajectory, such as its pace, as well as combining different partial observations into single representations. Experiments show our method's advantage over baselines in prediction tasks.",2022-10-04,https://www.semanticscholar.org/paper/1c2a3eec0d09ff66266c4484e19fe279aedba3c0,Neural Information Processing Systems
3700,Robust Perception through Equivariance,"Deep networks for computer vision are not reliable when they encounter adversarial examples. In this paper, we introduce a framework that uses the dense intrinsic constraints in natural images to robustify inference. By introducing constraints at inference time, we can shift the burden of robustness from training to the inference algorithm, thereby allowing the model to adjust dynamically to each individual image's unique and potentially novel characteristics at inference time. Among different constraints, we find that equivariance-based constraints are most effective, because they allow dense constraints in the feature space without overly constraining the representation at a fine-grained level. Our theoretical results validate the importance of having such dense constraints at inference time. Our empirical experiments show that restoring feature equivariance at inference time defends against worst-case adversarial perturbations. The method obtains improved adversarial robustness on four datasets (ImageNet, Cityscapes, PASCAL VOC, and MS-COCO) on image recognition, semantic segmentation, and instance segmentation tasks. Project page is available at equi4robust.cs.columbia.edu.",2022-12-12,https://www.semanticscholar.org/paper/82d19ceba300875f108a91539ca555dfad142a99,International Conference on Machine Learning
12,Predicting the impact of scientific concepts using full‐text features,"New scientific concepts, interpreted broadly, are continuously introduced in the literature, but relatively few concepts have a long‐term impact on society. The identification of such concepts is a challenging prediction task that would help multiple parties—including researchers and the general public—focus their attention within the vast scientific literature. In this paper we present a system that predicts the future impact of a scientific concept, represented as a technical term, based on the information available from recently published research articles. We analyze the usefulness of rich features derived from the full text of the articles through a variety of approaches, including rhetorical sentence analysis, information extraction, and time‐series analysis. The results from two large‐scale experiments with 3.8 million full‐text articles and 48 million metadata records support the conclusion that full‐text features are significantly more useful for prediction than metadata‐only features and that the most accurate predictions result from combining the metadata and full‐text features. Surprisingly, these results hold even when the metadata features are available for a much larger number of documents than are available for the full‐text features.",2016-11-01,https://www.semanticscholar.org/paper/52b58e71ca56ae725fd40b88c411a58a8322b872,J. Assoc. Inf. Sci. Technol.
1536,Identifiable Deep Generative Models via Sparse Decoding,"We develop the sparse VAE for unsupervised representation learning on high-dimensional data. The sparse VAE learns a set of latent factors (representations) which summarize the associations in the observed data features. The underlying model is sparse in that each observed feature (i.e. each dimension of the data) depends on a small subset of the latent factors. As examples, in ratings data each movie is only described by a few genres; in text data each word is only applicable to a few topics; in genomics, each gene is active in only a few biological processes. We prove such sparse deep generative models are identifiable: with infinite data, the true model parameters can be learned. (In contrast, most deep generative models are not identifiable.) We empirically study the sparse VAE with both simulated and real data. We find that it recovers meaningful latent factors and has smaller heldout reconstruction error than related methods.",2021-10-20,https://www.semanticscholar.org/paper/0bf88192d02c08661b9185b2b16399306694c4a4,Trans. Mach. Learn. Res.
975,Longitudinal Changes of Optic Nerve Head and Peripapillary Structure during Childhood Myopia Progression on OCT: Boramae Myopia Cohort Study Report 1.,,2018-03-14,https://www.semanticscholar.org/paper/15ec49356dff8471394993ff05671fd630ff72d0,"Ophthalmology (Rochester, Minn.)"
707,REACT to Cyber-Physical Attacks on Power grids (Extended Abstract),"We study cyber attacks on power grids that affect both the physical infrastructure and the data at the control center? which therefore are cyber-physical in nature. In particular, we assume that an adversary attacks an area by: (i) remotely disconnecting some lines within the attacked area, and (ii) modifying the information received from the attacked area to mask the line failures and hide the attacked area from the control center. For the latter, we consider two types of attacks: (i) data distortion: which distorts the data by adding powerful noise to the actual data, and (ii) data replay: which replays a locally consistent old data instead of the actual data. We use the DC power flow model and prove that the problem of finding the set of line failures given the phase angles of the nodes outside of the attacked area is strongly NP-hard, even when the attacked area is known. However, we introduce the polynomial time REcurrent Attack Containment and deTection (REACT) Algorithm to approximately detect the attacked area and line failures after a cyber-physical attack.",2019-01-18,https://www.semanticscholar.org/paper/af0abdcff7ee588260931ad75b52ad80eb9e3c87,PERV
2356,Stimulation of protein synthesis in human neutrophils by gamma-interferon.,"Treatment of human, peripheral blood neutrophils with gamma-interferon both ""primed"" their ability to generate reactive oxidants and increased their rate of protein synthesis. This increased rate of protein synthesis was greatest 60 min after the addition of 100 U/ml gamma-interferon and was not due to an increased intracellular pool of radiolabelled amino acid. Analysis of the newly-synthesized polypeptides by two-dimensional polyacrylamide gel electrophoresis (PAGE) revealed two classes of proteins which were regulated by this agent. The first of these represented proteins whose rate of labelling increased very little (1-2-fold) whereas the rate of biosynthesis of a second group of proteins increased more markedly (10-20-fold). We propose that these newly-synthesized, gamma-interferon regulated proteins play an important role in the function of these cells during an acute inflammatory response.",,https://www.semanticscholar.org/paper/617bac5882bde4e1a01dff53ff3f11e4e8ec3c28,Biochemical Pharmacology
2262,Secretion of oncostatin M by neutrophils in rheumatoid arthritis.,"OBJECTIVE
Neutrophils are known to express and release a large number of proinflammatory cytokines when they are stimulated by inflammatory stimuli. The objective of this study was to determine whether neutrophils express oncostatin M (OSM), a member of the interleukin-6 family of cytokines that has been implicated in the pathogenesis of inflammatory joint disease.


METHODS
Neutrophils were isolated from the blood of healthy volunteer donors and from the blood and synovial fluid of patients with rheumatoid arthritis (RA). OSM levels were measured in cell extracts and in culture supernatants by Western blotting. Total RNA was isolated from control and granulocyte-macrophage colony-stimulating factor (GM-CSF)-treated neutrophils, and OSM messenger RNA levels were quantified by hybridization of a radiolabeled probe.


RESULTS
GM-CSF stimulated a rapid and transient expression and release of OSM from blood neutrophils, which was more rapid than the expression and release from blood monocytes. A 28-kd protein was identified in cell extracts, but an additional 25-kd isoform was detected in culture supernatants. Synovial fluid neutrophils could not be stimulated to express OSM, but this cytokine was detected in cell-free supernatants at various levels.


CONCLUSION
Blood neutrophils can be stimulated to express and rapidly release large quantities of OSM. We propose that this important cytokine is released from neutrophils as they infiltrate rheumatoid joints and, thus, contribute to the complex cytokine network that characterizes RA.",2004-05-01,https://www.semanticscholar.org/paper/26f4e061fcdb98a2a24a652778a583b1215e0826,Arthritis & Rheumatism
3711,Real-Time Neural Voice Camouflage,"Automatic speech recognition systems have created exciting possibilities for applications, however they also enable opportunities for systematic eavesdropping. We propose a method to camouflage a person's voice over-the-air from these systems without inconveniencing the conversation between people in the room. Standard adversarial attacks are not effective in real-time streaming situations because the characteristics of the signal will have changed by the time the attack is executed. We introduce predictive attacks, which achieve real-time performance by forecasting the attack that will be the most effective in the future. Under real-time constraints, our method jams the established speech recognition system DeepSpeech 3.9x more than baselines as measured through word error rate, and 6.6x more as measured through character error rate. We furthermore demonstrate our approach is practically effective in realistic environments over physical distances.",2021-12-14,https://www.semanticscholar.org/paper/4e2dfe2b54bcd5d5c8178aca868959568298f0c8,International Conference on Learning Representations
3670,An experiment with the interchangeability of processes and monitors,"Two styles of operating system implementation based on the use of monitors and processes, respectively, are identified, and arguments for a basic equivalence of these systems despite large stylistic differences are presented. The ‘Lauer‐Needham Duality Hypothesis’ states that the two styles are equivalent, both in terms of ease of programming and in efficiency of the resulting systems. A domain for which the first part of this claim holds is outlined, and data affirming the essential equivalence of performance within that domain are presented. An operating system based on the Cambridge CAP system, called SIMOS, was simulated for a wide range of hardware configurations and job loads. SIMOS is written using a module concept that allows an individual module to be interpreted as a monitor in one run and as a process in another. Runs using a monitor to control access to some data can be compared with runs using a process to control access to the same data. The throughput and response time for the two styles of system were found to be identical in most cases. However, a degradation in response time occurred in a process‐based system when the job load and the low level scheduling policy were poorly matched.",1982-11-01,https://www.semanticscholar.org/paper/251b04c0a7aea5655f7cc4f868d3e2e7ac8edc7e,"Software, Practice & Experience"
1735,Deep Learning with Hierarchical Convolutional Factor Analysis,"Unsupervised multilayered (“deep”) models are considered for imagery. The model is represented using a hierarchical convolutional factor-analysis construction, with sparse factor loadings and scores. The computation of layer-dependent model parameters is implemented within a Bayesian setting, employing a Gibbs sampler and variational Bayesian (VB) analysis that explicitly exploit the convolutional nature of the expansion. To address large-scale and streaming data, an online version of VB is also developed. The number of dictionary elements at each layer is inferred from the data, based on a beta-Bernoulli implementation of the Indian buffet process. Example results are presented for several image-processing applications, with comparisons to related models in the literature.",2013-08-01,https://www.semanticscholar.org/paper/8d6227e26a4bfc5482c12b8f072496ac6e97ed21,IEEE Transactions on Pattern Analysis and Machine Intelligence
938,The Effect of a Connectivity Requirement on the Complexity of Maximum Subgraph Problems,"If Ir IS a property on graphs (or digraphs), the corresponding maximum subgraph problem is Given a graph G find a maximum (induced) subgraph of G satisfying property ~r The author has previously shown this problem to be NP-hard for a large class of properties (the class of properties that are hereditary on induced subgraphs) The effect of adding a connectwtty requirement to ~r is now considered It is shown that for the same class of properties the connected maximum subgraph problem is also NP-hard, moreover, for a certain important subclass of properties, even approximating the node-deletion version of it in any ""reasonable"" way is NP-hard A related set of problems is shown to testify to A~ # NP LI co-NP, In the case that NP # co-NP.",1979-10-01,https://www.semanticscholar.org/paper/48936d2b6d8686260876bafb6a47ca19e0f26f43,JACM
3242,Lingering effects of contraception management on feral mare (Equus caballus) fertility and social behavior,Extended subfertility can cause changes in feral horse reproductive physiology and behavior. Here we examine long-term effects of contraception treatment on females even after cessation of treatment. We identify treatment schedules that have more/less pronounced effects and suggest management recommendations for future populations.,2017-03-18,https://www.semanticscholar.org/paper/915df9310a7b78d660daae2c81ab9ac79b0c471d,Conservation Physiology
403,The Complexity of Optimal Queuing Network Control,"We show that several well-known optimization problems related to the optimal control of queues are provably intractable-independently of any unproven conjecture such as P â NP. In particular, we show that several versions of the problem of optimally controlling a simple network of queues with simple arrival and service distributions and multiple customer classes is complete for exponential time. This is perhaps the first such intractability result for a well-known optimization problem. We also show that the restless bandit problem the generalization of the multi-armed bandit problem to the case in which the unselected processes are not quiescent is complete for polynomial space.",1999-05-01,https://www.semanticscholar.org/paper/3808e63d89b6a251f37cb19d7761e3e037e4897b,Mathematics of Operations Research
2465,Collaboration in Mediated and Augmented Reality (CiMAR) Summary,"Summary form only given. The world is becoming more complex everyday, so problem solving often requires global teams of experts working together. To do this effectively there is a need for collaborative tools, and so a variety of teleconferencing and telepresence technologies have been developed. However, most of them involve some variation of traditional video conferencing, which has limitations, such as not being able to effectively convey spatial cues. This workshop focuses on how Augmented Reality (AR) [1] and Mediated Reality (MR) [2] technology can be used to overcome these limitations and develop radically new types of collaborative experiences. In combination, AR and MR technologies could be used to merge the shared perceived realities of different users, as well as enrich each user's own individual experience in a collaborative task. Several studies have explored the effectiveness of using AR and MR for complex tasks. For example, AR has been shown to improve the effectiveness of individual manual assembly tasks [3], while MR systems have been shown to improve the performance time and mental effort in collaborative design tasks [4]. Recent research on using MR for collaboration among crime scene investigators indicates that using shared visual feedback promotes mutual understanding, leads to consensus, and supports hypothesis testing [5]. There are many examples of collaborative AR applications. The Studierstube system [6] targets face-to-face presentations and allows users to walk around virtual 3D scientific data. WearCom [7] enables users to see remote users as virtual avatars in real space. Höllerer et al. [8] allow indoor AR users to visualize the locations and paths of outdoor AR users, and all users to create shared annotations. Poelman et al. [5] present an AR system that allows remote experts to collaborate with local investigators on a crime scene investigation in order to secure evidence. Datcu et al. [9] present an AR system that supports the distributed situational awareness of cross-organisational teams in the security domain. Gauglitz et al. [10] introduce a tablet-based system that incorporates a touchscreen interface through which a remote user can navigate a physical environment and create world-aligned annotations for supporting remote maintenance. In summary, MR and AR technology is becoming mature enough to support a variety of collaboration scenarios. However, there are still a number of open issues that need further research. One major issue concerns the presence of remote users and how they can interact with the system and each other. Partial answers can be found in the areas of human-computer interaction, social interaction, affective computing, and task domain analysis. This workshop brings together researchers who are developing or interested in creating collaborative systems using AR and MR technologies. Starting with a keynote from Prof. Tetsuro Ogi from Keio University, Japan, workshop participants will discuss open research issues in relation to: Case studies using MR/AR for collaboration; Tools for building collaborative MR/AR systems; Effects of MR/AR on trust, presence, and coordination; Interaction models for collaboration in MR/AR; Tools for collaboration in MR/AR; Collaboration awareness in MR/AR. The goal is to build a picture of current and prior research on collaboration in AR and MR, as well as set up a common research agenda for work going forward. This, in turn, can be used to grow the research community.",2015-09-29,https://www.semanticscholar.org/paper/3ab469a41adb5399a34057bca304197a2214b335,2015 IEEE International Symposium on Mixed and Augmented Reality Workshops
2725,Cutaways and ghosting: satisfying visibility constraints in dynamic 3D illustrations,,1992-09-01,https://www.semanticscholar.org/paper/21d3e6dcfb3ac93c1124f09118bb636e959728af,The Visual Computer
172,A Calculus for Brain Computation,"Do brains compute? How do brains learn? How are intelligence and language achieved in the human brain? In this pursuit, we develop a formal calculus and associated programming language for brain computation, based on the assembly hypothesis, first proposed by Hebb: the basic unit of memory and computation in the brain is an assembly, a sparse distribution over neurons. We show that assemblies can be realized efficiently and neuroplausibly by using random projection, inhibition, and plasticity. Repeated applications of this RP&C primitive (random projection and cap) lead to (1) stable assembly creation through projection; (2) association and pattern completion; and finally (3) merge, where two assemblies form a higher-level assembly and, eventually, hierarchies. Further, these operations are composable, allowing the creation of stable computational circuits and structures. We argue that this functionality, in the presence of merge in particular, might underlie language and syntax in humans.",,https://www.semanticscholar.org/paper/7f72f54b447d7da093789c22bf815f050bdabe61,2019 Conference on Cognitive Computational Neuroscience
131,"Requirements for deadlock-free, adaptive packet routing","This paper studies the problem of deadlock-free packet routing in parallel and distributed architectures. We present three main results. First, we show that the standard technique of ordering the queues so that every packet always has the possibility of moving to a higher ordered queue is not necessary for deadlock-freedom. Second, we show that every deadlock-free, adaptive packet routing algorithm can be restricted, by limiting the adaptivity available, to obtain an oblivious algorithm which is also deadlock-free. Third, we show that any packet routing algorithm for a cycle or torus network which is free of deadlock and which uses only minimal length paths must require at least three queues in some node. This matches the known upper bound of three queues per node for deadlock-free, minimal packet routing on cycle and torus networks.",1992-10-01,https://www.semanticscholar.org/paper/9813b72e279a203cd2aea483c451b887481fc471,ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing
264,Optimal deterministic auctions with correlated priors,,2010-11-04,https://www.semanticscholar.org/paper/1265ed2bb9c4a4b1c69f0a709aa029fbf7aa1004,Games Econ. Behav.
2529,"Session details: Interacting with hands, eyes, and images",,2009-07-27,https://www.semanticscholar.org/paper/1a8ed39de75d588455b6a1df1fe8da9f2bebb6f6,ACM SIGGRAPH 2009 papers
737,Polynomial time algorithms for multi-type branching processesand stochastic context-free grammars,"We show that one can approximate the least fixed point solution for a multivariate system of monotone probabilistic polynomial equations in time polynomial in both the encoding size of the system of equations and in log(1/ε), where ε>0 is the desired additive error bound of the solution. (The model of computation is the standard Turing machine model.)
 We use this result to resolve several open problems regarding the computational complexity of computing key quantities associated with some classic and heavily studied stochastic processes, including multi-type branching processes and stochastic context-free grammars.",2012-01-11,https://www.semanticscholar.org/paper/ecf67f34bf98245d89785a0f392f44c30de14acb,Symposium on the Theory of Computing
3467,"Existence theorems, lower bounds and algorithms for scheduling to meet two objectives","We give general results about the existence of schedules which simultaneously minimize two criteria. Our results are general in that (i) they apply to any scheduling environment and (ii) they apply to all pairs of metrics in which the first metric is one of maximum flow time, makespan, or maximum lateness and the second metric is one of average flow time, average completion time, average lateness, or number of on-time jobs. For most of the pairs of metrics we consider, we show the existence of near-optimal schedules for both metrics as well as some lower bound results. For some pairs of metrics such as (maximum flow time, average weighted flow time) and (maximum flow time, number of on-time jobs), we prove negative results on the ability to approximate both criteria within a constant factor of optimal. For many other criteria we present lower bounds that match or approach our bicriterion existence results.",2002-01-06,https://www.semanticscholar.org/paper/2ee853778d42ee7e14f89fc776a4fe581596f9e2,ACM-SIAM Symposium on Discrete Algorithms
3495,Optimal Time-Critical Scheduling via Resource Augmentation (Extended Abstract),,1997-05-04,https://www.semanticscholar.org/paper/4cf689437066266bc3129fd0667c3418d6bfece3,Symposium on the Theory of Computing
2187,Opisthorchiasis-Induced Cholangiocarcinoma: How Innate Immunity May Cause Cancer.,,,https://www.semanticscholar.org/paper/a6b5f26f1dfde47449921fd3d9ac16343288ffc0,Advances in Parasitology
434,On the analysis of indexing schemes,"We consider the problem of indexing general database workloads (combinations of data sets and sets of potential queries). We define a framework for measuring the efficiency of an indexing scheme for a workload based on two characterizations: storage redundancy (how many times each item in the data set is stored), and access overhead (how many times more blocks than necessary does a query retrieve). Using this framework we present some initial results, showing upper and lower bounds and trade-offs between them in the case of multi-dimensional range queries and set queries.",1997-05-01,https://www.semanticscholar.org/paper/5291d116e72d0ca96f654bd8893c01564fa1603a,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems
305,Computing Equilibria in Anonymous Games,"We present efficient approximation algorithms for finding Nash equilibria in anonymous games, that is, games in which the players utilities, though different, do not differentiate between other players. Our results pertain to such games with many players but few strategies. We show that any such game has an approximate pure Nash equilibrium, computable in polynomial time, with approximation O(s2lambda), where s is the number of strategies and lambda is the Lipschitz constant of the utilities. Finally, we show that there is a PTAS for finding an isin-approximate Nash equilibrium when the number of strategies is two.",2007-10-21,https://www.semanticscholar.org/paper/40da8359c2000f9feafc79b418ff1cfd0216f47f,IEEE Annual Symposium on Foundations of Computer Science
2902,Integrative transcriptomic analysis of the amyotrophic lateral sclerosis spinal cord implicates glial activation and suggests new risk genes,,2022-12-08,https://www.semanticscholar.org/paper/4c6a2fd88342a9083a19044f5c9fb7e77ab545e0,Nature Neuroscience
672,Meta-Learning to Cluster,"Clustering is one of the most fundamental and wide-spread techniques in exploratory data analysis. Yet, the basic approach to clustering has not really changed: a practitioner hand-picks a task-specific clustering loss to optimize and fit the given data to reveal the underlying cluster structure. Some types of losses---such as k-means, or its non-linear version: kernelized k-means (centroid based), and DBSCAN (density based)---are popular choices due to their good empirical performance on a range of applications. Although every so often the clustering output using these standard losses fails to reveal the underlying structure, and the practitioner has to custom-design their own variation. In this work we take an intrinsically different approach to clustering: rather than fitting a dataset to a specific clustering loss, we train a recurrent model that learns how to cluster. The model uses as training pairs examples of datasets (as input) and its corresponding cluster identities (as output). By providing multiple types of training datasets as inputs, our model has the ability to generalize well on unseen datasets (new clustering tasks). Our experiments reveal that by training on simple synthetically generated datasets or on existing real datasets, we can achieve better clustering performance on unseen real-world datasets when compared with standard benchmark clustering techniques. Our meta clustering model works well even for small datasets where the usual deep learning models tend to perform worse.",2019-10-30,https://www.semanticscholar.org/paper/77868d73295d92fa21d64b424ff4359efa0e68ba,arXiv.org
214,Approximate Nash equilibria in anonymous games,,2015-03-01,https://www.semanticscholar.org/paper/06e4ff025e8c0979f715eebb1e0b808857101593,Journal of Economics Theory
1673,Dynamic Poisson Factorization,"Models for recommender systems use latent factors to explain the preferences and behaviors of users with respect to a set of items (e.g., movies, books, academic papers). Typically, the latent factors are assumed to be static and, given these factors, the observed pref- erences and behaviors of users are assumed to be generated without order. These assumptions limit the explorative and predictive capabilities of such models, since users' interests and item popularity may evolve over time. To address this, we propose dPF, a dynamic matrix factorization model based on the recent Poisson factorization model for recommendations. dPF models the time evolving latent factors with a Kalman filter and the actions with Poisson distributions. We derive a scalable variational inference algorithm to infer the latent factors. Finally, we demonstrate dPF on 10 years of user click data from arXiv.org, one of the largest repository of scientific papers and a formidable source of information about the behavior of scientists. Empirically we show performance improvement over both static and, more recently proposed, dynamic recommendation models. We also provide a thorough exploration of the inferred posteriors over the latent variables.",2015-09-15,https://www.semanticscholar.org/paper/39f249d4095fcb875f0b23655d1e734bd8c71b71,ACM Conference on Recommender Systems
2951,Small RNA Sequencing in Cells and Exosomes Identifies eQTLs and 14q32 as a Region of Active Export,"Exosomes are small extracellular vesicles that carry heterogeneous cargo, including RNA, between cells. Increasing evidence suggests that exosomes are important mediators of intercellular communication and biomarkers of disease. Despite this, the variability of exosomal RNA between individuals has not been well quantified. To assess this variability, we sequenced the small RNA of cells and exosomes from a 17-member family. Across individuals, we show that selective export of miRNAs occurs not only at the level of specific transcripts, but that a cluster of 74 mature miRNAs on chromosome 14q32 is massively exported in exosomes while mostly absent from cells. We also observe more interindividual variability between exosomal samples than between cellular ones and identify four miRNA expression quantitative trait loci shared between cells and exosomes. Our findings indicate that genomically colocated miRNAs can be exported together and highlight the variability in exosomal miRNA levels between individuals as relevant for exosome use as diagnostics.",2016-10-31,https://www.semanticscholar.org/paper/c83b96bac22d0c89857e80e38f69aa8bdad370c8,"G3: Genes, Genomes, Genetics"
2310,Modulation of neutrophil apoptosis by pharmacological agents.,"The human neutrophil is known to have a short half-life in circulation, estimated to be between 8 and 16 h. This lifespan is so short because circulating neutrophils constituitively undergo apoptosis. As neutrophils become apoptotic the chromatin becomes condensed and fragmented, expression of the low affinity receptor for IgG (CD16) on the cell surface falls and functional activities such as the ability to produce superoxide are diminished [ 11. Apoptotic circulating neutrophils are quickly cleared from the circulation by phagwytic cells, which recognise markers of apoptosis on the cell surface [2]. As the mature neutrophil has such a short lifespan, the production of neutrophils from the bone marrow must be maintained at -5 x 10"" every day to keep numbers of circulating neutrophils at levels that are able to protect against infections. If this production of neutrophils is disrupted, neutropaenia quickly develops, resulting in a decreased ability to fight off bacterial or fungal infections. Patients undergoing chemoor radio-therapy for the treatment of certain leukaemia's and solid tumours are known to suffer from neutropaenia during the periods of treatment due to damage to the bone marrow. The period and severity of the neutropeania vary greatly, but in all cases there is an increased risk of infection in these patients. This often results in a suspension of therapy and increased hospitalisation times. This therapyinduced neutropaenia may be managed by the use of colony stimulating factors such as Granulocyte-Colony Stimulating Factor (GM-CSF) to increase the production of neutrophils from the bone marrow. These treatments show a reduction in the period and severity of the neutropeania but side-effects such as increased adhesion of neutrophils to capillary walls and pulmonary accumulation, due to up-regulation of certain surface proteins, may be seen. These side-effects are a consequence of neutrophil priming. As an alternative to the use of colony stimulating factors to increase neutrophil production we have been studying the effects of three pharmacological agents (5azacytidine, hexarnethylene-bis-acetamide [HMBA] and sodium butyrate) on neutrophil apoptosis, with the aim of increasing the functional lifespan of the circulating neutrophil. Each of these drugs have previously been shown not to prime neutrophils [3] and so should not elicit the side effects that result from CSFinduced increases in adhesion. Neutrophils were isolated from venous blood of healthy volunteers to >98% purity, and suspended in RPMI 1640 medium supplemented with 0.25% (v/v) foetal calf serum and 2 mM L-glutamine as in [4]. Suspensions of 5 x lo6 celldml were incubated with no additions (controls), or were supplemented with 50 U/ml GM-CSF, 50 ph4 5-azacytidine, 1 mM hexamethylene-bis-acetamide, 0.4 mM sodium butyrate or a combination of GM-CSF with each of the other drugs. After incubation at 37 ""C for 22 h neutrophil aliquots were used for morphological assessment of apoptosis, ability to produce superoxide, and measurement of DNA fragmentation. Cytospins were prepared from 10' cells and stained with May-Griinwald solution; at least 500 cells were assessed per slide [4]. Superoxide production was measured from 5 x Id cells with 10 @I luminol using either 1 @I f-Met-Leu-Phe or 100 ng/d PMA [5]. DNA fragmentation was measured by separating fragmented and non-fragmented DNA by cenbifugation and determining DNA content colorimetrically using diphenylamine reagent [4]. Table 1. shows that sodium butyrate protects neutrophils from apoptosis more effectively than GM-CSF, confirming our previous results, and that this effect is enhanced when these two agents are used together. Sodium butyrate [6], 5-azacyt1dine [7] and HMBA [8] have all been shown to increase gene transcription in other cell types, and to stimulate protein biosynthesis in human neutrophils [3]. However neither HMBA or 5-azacyt1dine gave significant protection from apoptosis Table 1. &",1996-08-01,https://www.semanticscholar.org/paper/c81af580f7bc891821d337b322377800b9b7cf04,Biochemical Society Transactions
3481,Clustering Data without Prior Knowledge,,2000-09-05,https://www.semanticscholar.org/paper/c47a285efa5ff60f5957e029412f3d9ba2db57a4,Workshop on Algorithm Engineering
830,The Complexity of Multiterminal Cuts,"In the multiterminal cut problem one is given an edge-weighted graph and a subset of the vertices called terminals, and is asked for a minimum weight set of edges that separates each terminal from all the others. When the number $k$ of terminals is two, this is simply the mincut, max-flow problem, and can be solved in polynomial time. It is shown that the problem becomes NP-hard as soon as $k=3$, but can be solved in polynomial time for planar graphs for any fixed $k$. The planar problem is NP-hard, however, if $k$ is not fixed. A simple approximation algorithm for arbitrary graphs that is guaranteed to come within a factor of $2-2/k$ of the optimal cut weight is also described.",1994-08-01,https://www.semanticscholar.org/paper/1cf64c2bdd4f1c384a55910606a64c8d831a96ba,SIAM journal on computing (Print)
318,The Connectivity of Boolean Satisfiability: Computational and Structural Dichotomies,"Given a Boolean formula, do its solutions form a connected subgraph of the hypercube? This and other related connectivity considerations underlie recent work on random Boolean satisfiability. We study connectivity properties of the space of solutions of Boolean formulas, and establish computational and structural dichotomies. Specifically, we first establish a dichotomy theorem for the complexity of the st-connectivity problem for Boolean formulas in Schaefer's framework. Our result asserts that the tractable side is more generous than the tractable side in Schaefer's dichotomy theorem for satisfiability, while the intractable side is PSPACE-complete. For the connectivity problem, we establish a dichotomy along the same boundary between membership in coNP and PSPACE-completeness. Furthermore, we establish a structural dichotomy theorem for the diameter of the connected components of the solution space: for the PSPACE-complete cases, the diameter can be exponential, but in all other cases it is linear. Thus, small diameter and tractability of the st-connectivity problem are remarkably aligned.",2006-07-10,https://www.semanticscholar.org/paper/57530a32d50c3b890507f7017ffbc1668aaf8478,SIAM journal on computing (Print)
2448,Papers Reviewers,Presents a listing of the papers' reviewers from the 2017 IEEE Virtual Reality Conference (IEEE VR 2017).,,https://www.semanticscholar.org/paper/c504602de222a7c88b3c0c0c5f021edf5d75b4c7,IEEE Transactions on Visualization and Computer Graphics
3105,Transparent Checkpoint-Restart of Distributed Applications on Commodity Clusters,"We have created ZapC, a novel system for transparent coordinated checkpoint-restart of distributed network applications on commodity clusters. ZapC provides a thin visualization layer on top of the operating system that decouples a distributed application from dependencies on the cluster nodes on which it is executing. This decoupling enables ZapC to checkpoint an entire distributed application across all nodes in a coordinated manner such that it can he restarted from the checkpoint on a different set of cluster nodes at a later time. ZapC checkpoint-restart operations execute in parallel across different cluster nodes, providing faster checkpoint-restart performance. ZapC uniquely supports network state in a transport protocol independent manner, including correctly saving and restoring socket and protocol state for both TCP and UDP connections. We have implemented a ZapC Linux prototype and demonstrate that it provides low visualization overhead and fast checkpoint-restart times for distributed network applications without any application, library, kernel, or network protocol modifications",2005-09-01,https://www.semanticscholar.org/paper/359928dfca1eb50ea392a266c6425bca9a9874db,IEEE International Conference on Cluster Computing
961,Sustained nitric oxide-providing small molecule and precise release behavior study for glaucoma treatment.,"Incidence of glaucoma, a severe disease leading to irreversible loss of vision, is increasing with global aging populations. Lowering intraocular pressure (IOP) is the only proven treatment method for glaucoma. Nitric oxide (NO) is an emerging material targeting the conventional outflow pathway by relaxing the trabecular meshwork (TM). However, there is little understanding on NO level effective to IOP lowering without toxicity. Here we report a novel long-term NO-releasing polydiazeniumdiolate (NOP) that enables lowering IOP via the conventional outflow pathway. NOP is composed of carbon-bound polydiazeniumdiolate, stable NO donor moiety. NO release was monitored with accurate parameters by detection of real-time gas and accumulated form. Based on the NO release information, the selected safe level of NOP exhibited effective TM relaxation and potential an IOP lowering effect in vivo without side effects. This work provides new insights to nitric oxide release behavior that should be considered for glaucoma treatment.",2020-01-08,https://www.semanticscholar.org/paper/0ad730e75df49b5a842938b83897ec88b74974fa,Molecular Pharmaceutics
2409,"Mitochondrial adenosine triphosphatase of the fission yeast, Schizosaccharomyces pombe 972h-. Changes in activity and oligomycin-sensitivity during the cell cycle of catabolite-repressed and -de-repressed cells.","1. Changes in activity of ATPase (adenosine triphosphatase) during the cell cycle of Schizosaccharomyces pombe were analysed in cell-free extracts of cells harvested from different stages of growth of synchronous cultures and also after cell-cycle fractionation. 2. Oligomycin-sensitive ATPase oscillates in both glucose-repressed synchronous cultures and shows four maxima of activity approximately equally spaced through the cell cycle. The amplitude of the oscillations accounts for between 13 and 80% of the total activity at different times in the cell cycle. 3. Oligomycin sensitivity varies over a fourfold range at different stages of the cell cycle. 4. The periodicity of maximum oligomycin sensitivity is one-quarter of a cell cycle. 5. These results were confirmed for the first three-quarters of the cell cycle by cell-cycle fractionation. 6. In cells growing synchronously with glycerol, ATPase activity increases in a stepwise pattern, with two steps per cell cycle; the first of these occurs at 0.54 of the cell cycle and the second at 0.95. 7. These results are discussed in relation to previously obtained data on the development of mitochondrial activities during the cell cycle.",1977-01-15,https://www.semanticscholar.org/paper/a139aa6da9360dac63ee6aa14eb83b0aaeab50cb,Biochemical Journal
2335,The effects of GM-CSF on myeloperoxidase release in normal and myelodysplastic neutrophils.,,1993-12-01,https://www.semanticscholar.org/paper/2a7a46abc3d36d5d048e7d2d0ee8014cd89181df,Leukemia research : a Forum for Studies on Leukemia and Normal Hemopoiesis
2564,On Beyond GUI,,2007-01-30,https://www.semanticscholar.org/paper/fdaead0fed23072980ed13eca3fa80dd2649c391,Australasian User Interface Conference
3399,Scheduling When You Don't Know the Number of Machines,"Often in a scheduling problem, there is uncertainty about the jobs to be processed. The issue of uncertainty regarding the machines has been much less studied. In this paper, we study a scheduling environment in which jobs first need to be grouped into some sets before the number of machines is known, and then the sets need to be scheduled on machines without being separated. In order to evaluate algorithms in such an environment, we introduce the idea of an α-robust algorithm, one which is guaranteed to return a schedule on any number m of machines that is within an α factor of the optimal schedule on m machine, where the optimum is not subject to the restriction that the sets cannot be separated. Under such environment, we give a [EQUATION]-robust algorithm for scheduling on parallel machines to minimize makespan, and show a lower bound [EQUATION]. For the special case when the jobs are infinitesimal, we give a 1.233-robust algorithm with an asymptotic lower bound of 1.207. We also study a case of fair allocation, where the objective is to minimize the difference between the maximum and minimum machine load.",2018-01-07,https://www.semanticscholar.org/paper/9f61d2ed6d80685f12e0909a175b5bb8cea54ae9,ACM-SIAM Symposium on Discrete Algorithms
768,"Recursive Markov chains, stochastic grammars, and monotone systems of nonlinear equations","We define Recursive Markov Chains (RMCs), a class of finitely presented denumerable Markov chains, and we study algorithms for their analysis. Informally, an RMC consists of a collection of finite-state Markov chains with the ability to invoke each other in a potentially recursive manner. RMCs offer a natural abstract model for probabilistic programs with procedures. They generalize, in a precise sense, a number of well-studied stochastic models, including Stochastic Context-Free Grammars (SCFG) and Multi-Type Branching Processes (MT-BP).
 We focus on algorithms for reachability and termination analysis for RMCs: what is the probability that an RMC started from a given state reaches another target state, or that it terminates? These probabilities are in general irrational, and they arise as (least) fixed point solutions to certain (monotone) systems of nonlinear equations associated with RMCs. We address both the qualitative problem of determining whether the probabilities are 0, 1 or in-between, and the quantitative problems of comparing the probabilities with a given bound, or approximating them to desired precision.
 We show that all these problems can be solved in PSPACE using a decision procedure for the Existential Theory of Reals. We provide a more practical algorithm, based on a decomposed version of multi-variate Newton's method, and prove that it always converges monotonically to the desired probabilities. We show this method applies more generally to any monotone polynomial system. We obtain polynomial-time algorithms for various special subclasses of RMCs. Among these: for SCFGs and MT-BPs (equivalently, for 1-exit RMCs) the qualitative problem can be solved in P-time; for linearly recursive RMCs the probabilities are rational and can be computed exactly in P-time.
 We show that our PSPACE upper bounds cannot be substantially improved without a breakthrough on long standing open problems: the square-root sum problem and an arithmetic circuit decision problem that captures P-time on the unit-cost rational arithmetic RAM model. We show that these problems reduce to the qualitative problem and to the approximation problem (to within any nontrivial error) for termination probabilities of general RMCs, and to the quantitative decision problem for termination (extinction) of SCFGs (MT-BPs).",2005-02-24,https://www.semanticscholar.org/paper/3939ab4498e35f8404fddbe42ae5bb1d4e62258f,JACM
3343,Alternative reproductive tactics in the spider Meta segmentata,,1987-04-01,https://www.semanticscholar.org/paper/f7b6464ba032af3204ac987f2f7d505b74ca6f66,Behavioral Ecology and Sociobiology
3247,Between-gender differences in vigilance do not necessarily lead to differences in foraging-vigilance tradeoffs,,2016-03-26,https://www.semanticscholar.org/paper/3554cf5f7f71ce5b972bcae7d2ccc20b69e181e0,Oecologia
718,REACT to Cyber Attacks on Power Grids,"Motivated by the recent cyber attack on the Ukrainian power grid, we study cyber attacks on power grids that affect both the physical infrastructure and the data at the control center–which therefore are cyber-physical in nature. In particular, we assume that an adversary attacks an area by: (i) remotely disconnecting some lines within the attacked area, and (ii) modifying the information received from the attacked area to mask the line failures and hide the attacked area from the control center. For the latter, we consider two types of attacks: (i) data distortion: which distorts the data by adding powerful noise to the actual data, and (ii) data replay: which replays a locally consistent old data instead of the actual data. We use the DC power flow model and prove that the problem of finding the set of line failures given the phase angles of the nodes outside of the attacked area is strongly NP-hard, even when the attacked area is known. However, we introduce the polynomial time REcurrent Attack Containment and deTection (REACT) Algorithm to approximately detect the attacked area and line failures after a cyber-physical attack. We numerically show that it performs well in detecting the attacked area, and detecting single, double, and triple line failures in small and large attacked areas.",2017-09-20,https://www.semanticscholar.org/paper/df2d7000ff4292d81df29f5b1f3efcb6c5921e8e,IEEE Transactions on Network Science and Engineering
3384,Distributed Algorithms for Matching in Hypergraphs,,2020-09-21,https://www.semanticscholar.org/paper/71a42ecba04a76c376e767adbe8443afcf47bd62,Workshop on Approximation and Online Algorithms
2159,Neutrophil function following treatment of psoriatic arthritis patients with secukinumab: altered cytokine signalling but no impairment of host defence.,"OBJECTIVE
Identifying that dysfunction of the IL-23/17 axis underlies psoriatic arthritis (PsA) has led to the development of effective targeted therapies, such as the IL-17A inhibitor, secukinumab. As IL-17A stimulates the secretion of neutrophil chemoattractants, such as CXCL8 (IL-8), we examined the effect of secukinumab on neutrophil function in PsA.


METHODS
Nineteen patients with active PsA were treated with secukinumab. Clinical response (PsARC and PASI) and peripheral blood neutrophil function (apoptosis, receptor expression, phagocytosis/killing, chemotaxis and RNA expression) were measured at 12 week intervals for 48 weeks and compared with age- and sex-matched healthy controls.


RESULTS
At 12 weeks 12/16 (75%) had a PsARC response (100% at 36 weeks) and 10/14 (71%) achieved a PASI90. At baseline, there were no differences in PsA neutrophil ROS generation, constitutive or cytokine-delayed apoptosis, chemotaxis or phagocytosis of opsonised Staphylococcus aureus, compared to healthy controls. Similarly, there were no differences in these functions from baseline to 12-weeks of therapy. However, surface levels of CD11b/CD18 and CD63 increased and expression of CD16 decreased during therapy. In addition, in a sub-group of early (12 week) responders to secukinumab, RNA-seq revealed transcriptome changes predicting down-regulation of cytokine signalling and chemotaxis pathways and up-regulation of de novo gene expression pathways, including translation initiation, mRNA catabolism and translation.


CONCLUSION
Complex changes in the properties of circulating neutrophils occur with secukinumab treatment in PsA that may indicate altered responsiveness to changes in both local and systemic levels of pro-inflammatory cytokines. However, host defence processes of neutrophils were unaltered.",2023-01-06,https://www.semanticscholar.org/paper/bbf61d62abfa4ab9e38cdc954fbc9af095e667e4,Rheumatology
2400,Properties of mitochondria isolated from cyanide-sensitive and cyanide-stimulated cultures of Acanthamoeba castellanii.,"1. Mitochondria isolated from cultures of Acanthamoeba castellanii exhibit respiratory control and oxidize alpha-oxoglutarate, succinate and NADH with ADP:O ratios of about 2.4, 1.4 and 1.25 respectively. 2. Mitochondria from cultures of which the respiration was stimulated up to 50% by 1mm-cyanide (type-A mitochondria) and from cyanide-sensitive cultures (type-B mitochondria) had similar respiratory-control ratios and ADP:O ratios. 3. State-3 rates of respiration were generally more cyanide-sensitive than State-4 rates, and the respiration of type-A mitochondria was more cyanide-resistant than that of type-B mitochondria. 4. Salicylhydroxamic acid alone had little effect on respiratory activities of either type of mitochondria, but when added together with cyanide, irrespective of the order of addition, inhibition was almost complete. 5. Oxidation of externally added NADH by type-A mitochondria was mainly via an oxidase with a low affinity for oxygen (K(m)[unk]15mum), which was largely cyanide-sensitive and partially antimycin A-sensitive; this electron-transport pathway was inhibited by ADP. 6. Cyanide-insensitive but salicylhydroxamic acid-sensitive respiration was stimulated by AMP and ADP, and by ATP after incubation in the presence of MgCl(2). 7. Addition of rotenone to mitochondria oxidizing alpha-oxoglutarate lowered the ADP:O ratios by about one-third and rendered inhibition by cyanide more complete. 8. The results suggest that mitochondria of A. castellanii possess branched pathways of electron transport which terminate in three separate oxidases; the proportions of electron fluxes via these pathways vary at different stages of growth.",1978-07-15,https://www.semanticscholar.org/paper/12a0781a89bb7eaf1f7223b6fb3c202c3c8f2c11,Biochemical Journal
927,Edge-Deletion Problems,"If $\pi $ is a property on graphs or digraphs, the edge-deletion problem can be stated as follows: find the minimum number of edges whose deletion results in a subgraph (or subdigraph) satisfying property $\pi $. Several well-studied graph problems can be formulated as edge-deletion problems.In this paper we show that the edge-deletion problem is NP-complete for the following properties: (1) without cycles of specified length l, or of any length $ \leqq l$, (2) connected and degree-constrained, (3) outerplanar, (4) transitive digraph, (5) line-invertible, (6) bipartite, (7) transitively orientable. For problems (5), (6), (7) we determine the best possible bounds on the node-degrees for which the problems remain NP-complete.",1981-05-01,https://www.semanticscholar.org/paper/b59a2ae98053a756b08b67815c3a5afa2d5ac7f3,SIAM journal on computing (Print)
3098,Understanding the management of client perceived response time,"Understanding and managing the response time of web services is of key importance as dependence on the World Wide Web continues to grow. We present Remote Latency-based Management (RLM), a novel server-side approach for managing pageview response times as perceived by remote clients, in real-time. RLM passively monitors server-side network traffic, accurately tracks the progress of page downloads and their response times in real-time, and dynamically adapts connection setup behavior and web page content as needed to meet response time goals. To manage client perceived pageview response times, RLM builds a novel event node model to guide the use of several techniques for manipulating the packet traffic in and out of a web server complex, including fast SYN and SYN/ACK retransmission, and embedded object removal and rewrite. RLM operates as a stand-alone appliance that simply sits in front of a web server complex, without any changes to existing web clients, servers, or applications. We have implemented RLM on an inexpensive, commodity, Linux-based PC and present experimental results that demonstrate its effectiveness in managing client perceived pageview response times on transactional e-commerce web workloads.",2006-06-26,https://www.semanticscholar.org/paper/9bd7e19452c102069042971617e0b7a10aeef8ce,SIGMETRICS '06/Performance '06
2801,Galectin-3 regulates the innate immune response of human monocytes.,"Galectin-3 is a β-galactoside-binding lectin widely expressed on epithelial and hematopoietic cells, and its expression is frequently associated with a poor prognosis in cancer. Because it has not been well-studied in human infectious disease, we examined galectin-3 expression in mycobacterial infection by studying leprosy, an intracellular infection caused by Mycobacterium leprae. Galectin-3 was highly expressed on macrophages in lesions of patients with the clinically progressive lepromatous form of leprosy; in contrast, galectin-3 was almost undetectable in self-limited tuberculoid lesions. We investigated the potential function of galectin-3 in cell-mediated immunity using peripheral blood monocytes. Galectin-3 enhanced monocyte interleukin 10 production to a TLR2/1 ligand, whereas interleukin 12p40 secretion was unaffected. Furthermore, galectin-3 diminished monocyte to dendritic cell differentiation and T-cell antigen presentation. These data demonstrate an association of galectin-3 with unfavorable host response in leprosy and a potential mechanism for impaired host defense in humans.",2013-03-15,https://www.semanticscholar.org/paper/fdc5353b1922e431b29e046306566be83077d820,Journal of Infectious Diseases
399,On the approximability of the traveling salesman problem (extended abstract),"We show that the traveling salesman problem with triangle inequality cannot be approximated within ~0 when the edge lengths are allowed to be asymmetric and within 12_~9 z2s when the edge lengths are symmetric. The best previous 2s05 _ j 53s1 respectively. The reduction lower bounds were ~ ~ttt is from H£stad's maximum satisfiability of linear equations modulo 2, and is nonconstructive.",2000-05-01,https://www.semanticscholar.org/paper/b1f427b2c3a7bf7e548172c6f26fde1ee0605fa6,Symposium on the Theory of Computing
3549,The demacrofier,"C++ programs can be rejuvenated by replacing error-prone usage of the C Preprocessor macros with type safe C++11 declarations. We have developed a classification of macros that directly maps to corresponding C++11 expressions, statements, and declarations. We have built a set of tools that replaces macros with equivalent C++ declarations and iteratively introduces the refactorings into the software build.",2012-09-23,https://www.semanticscholar.org/paper/48db89589ab1d387b6522abd6bc21c4cc0704804,International Conference on Smart Multimedia
1894,Big Data Analytics for Tool Health Monitoring in Panel Industry,"Tool health monitoring and maintenance has become a more challenge issue for big data recently and is extremely essential in today's highly competitive environment in industry. Good monitoring approach and right maintenance strategy will be a great benefit to the company for reducing the cost and prolong the useful life of machines. Related researches have been studied based on different methods and approaches for investigating the health status of machines in many fields. In this study, we provide a data mining framework for monitoring the tool heal status under the partial least squares approach as we as the control chart construction. A real data from panel industry is applied to demonstrate our proposed research.",2019-04-01,https://www.semanticscholar.org/paper/558c25fd8798211737f776dfdffc1c2e78b71fb7,3D Structure from Multiple Images of Large-Scale Environments
2853,Galectin 3 is required for MLD-STZ induced diabetes in mice (130.38),"
 Galectin-3, member of ancient lectin family characterized by specific binding of β galactosides, has an antiapoptotic function in T cells, macrophages and islet cells. It was therefore of interest to evaluate the susceptibility to multiple low dose induced diabetes (MLD-STZ) in galectin (gal-3) deficient C57BL/6 mice. Gal-3 −/− and gal-3+/+ mice were treated with 5 daily injections of 40 mg/kg STZ and diabetes development evaluated by glycemia and immunohistochemistry of the pancreas. Gal-3+/+ mice developed delayed sustained and progressive hyperglucemia and mononuclear cell infiltrates in the islets. Gal-3 −/− demonstrated only mild glycemia with minimal islet pathology as evaluated by the number of infiltrating cells and insulin content. There was higher number of apoptotic cells in the islets of galectin-3 knock out than in the control gal-3+/+ mice. RT PCR analysis of pancreatic lymph node cells 17 days after diabetes induction revealed the presence of IL-23 and IL-17 in gal-3+/+ but not in Gal-3 −/− mice. TNF-α and INF-γ INOS expression was also attenuated in Gal-3 −/− mice. We concluded that antiapoptotic effect of gal-3 in diabetogenic cells favors the induction of disease. The data are also compatible with our previous finding that IL-23-Th17 axis plays a role in diabetogenesis (Eur J Immunol, 36: 216–23, 2006).
 (Supported by Sheikh Hamdan Awards for Medical Research).",2007-04-01,https://www.semanticscholar.org/paper/bc92a3b420613ddb24681755e36bf754eb9b3318,Journal of Immunology
553,The Complexity of Recognizing Polyhedral Scenes (Extended Abstract),"Given a dr,awi ngof straight lines on the plane, we wish to decide whether it is the projection of the visible part of a set of opaque polyhedra. Although there is an extensive literature and reports on e~iri~11y succesful algorithms for this problem, there has been no definite result concerning its complex; ty. In thi s paper we show that, ratfle~ . surprisingly, this problem is NP-complete. TfllS 1S true even in the relatively simple case of trihedral scenes (no four planes share a point) without shadows or crack.s. Desp . jte this negative result, weoresent a fast algorlthm for the 1mportant spec1a~ case.of·orthohedral scenes (all planes are perpendicular to'one of the three axes) with a fixed number of ""possible"" objects.",,https://www.semanticscholar.org/paper/030d419e03cb2bcb0b37074c70ae59da36f98443,IEEE Annual Symposium on Foundations of Computer Science
532,Why not negation by fixpoint?,"There is a fixpoint semantics for DATALOG programs with negation that is a natural generalization of the standard semantics for DATALOG programs without negation. We show that, unfortunately, several compelling complexity-theoretic obstacles rule out its efficient implementation. As an alternative, we propose Inflationary DATALOG, an efficiently implementable semantics for negation, based on inflationary fixpoints",1988-03-01,https://www.semanticscholar.org/paper/d0e7b3ebabb926f50846c75b9af7d25433c5a41e,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems
270,Inapproximability for VCG-based combinatorial auctions,"The existence of incentive-compatible, computationally-efficient mechanisms for combinatorial auctions with good approximation ratios is the paradigmatic problem in algorithmic mechanism design. It is believed that, in many cases, good approximations for combinatorial auctions may be unattainable due to an inherent clash between truthfulness and computational efficiency. In this paper, we prove the first computational-complexity in-approximability results for incentive-compatible mechanisms for combinatorial auctions. Our results are tight, hold for the important class of VCG-based mechanisms, and are based on the complexity assumption that NP has no polynomial-size circuits. We show two different techniques to obtain such lower bounds: one for deterministic mechanisms that attains optimal dependence on the number of players and number of items, and one that also applies to a class of randomized mechanisms and attains optimal dependence on the number of players. Both techniques are based on novel VC dimension machinery.",2010-01-17,https://www.semanticscholar.org/paper/c64c1db9296224c95957d40882df1cbaffab63e3,ACM-SIAM Symposium on Discrete Algorithms
2193,Neutrophil biomarkers predict response to therapy with tumor necrosis factor inhibitors in rheumatoid arthritis,"Neutrophils are implicated in the pathology of rheumatoid arthritis (RA), but the mechanisms regulating their activation are largely unknown. RA is a heterogeneous disease, and whereas many patients show clinical improvement during TNF inhibitor (TNFi) therapy, a significant proportion fails to respond. In vitro activation of neutrophils with agents, including TNF, results in rapid and selective changes in gene expression, but how neutrophils contribute to TNF signaling in RA and whether TNFi sensitivity involves differential neutrophil responses are unknown. With the use of RNA sequencing (RNA‐Seq), we analyzed blood neutrophils from 20 RA patients, pre‐TNFi therapy, to identify biomarkers of response, measured by a decrease in disease activity score based on 28 joint count (DAS28), 12 wk post‐therapy. Biomarkers were validated by quantitative PCR (qPCR) of blood neutrophils from 2 further independent cohorts of RA patients: 16 pre‐TNFi and 16 predisease‐modifying anti‐rheumatic drugs (DMARDs). Twenty‐three neutrophil transcripts predicted a 12‐wk response to TNFi: 10 (IFN‐regulated) genes predicting a European League against Rheumatism (EULAR) good response and 13 different genes [neutrophil granule protein (NGP) genes] predicting a nonresponse. Statistical analysis indicated a predictive sensitivity and specificity of each gene in the panel of >80%, with some 100% specific. A combination of 3 genes [cytidine monophosphate kinase 2 (CMPK2), IFN‐induced protein with tetratricopeptide repeats 1B (IFIT1B), and RNASE3] had the greatest predictive power [area under the curve (AUC) 0.94]. No correlation was found for a response to DMARDs. We conclude that this panel of genes is selective for predicting a response to TNFi and is not a surrogate marker for disease improvement. We also show that in RA, there is great plasticity in neutrophil phenotype, with circulating cells expressing genes normally only expressed in more immature cells.",2017-03-01,https://www.semanticscholar.org/paper/d8cf40856abe734ac803bb3b0020619a7a13ad17,Journal of Leukocyte Biology
85,SDLIP + STARTS = SDARTS a protocol and toolkit for metasearching,"In this paper we describe how we combined SDLIP and STARTS, two comple mentary protocols for searching over distributed document collections. The resulting protocol, which we call SDARTS, is simple yet expressible enough to enable building sophisticated metasearch engines. SDARTS can be viewed as an instantiation of SDLIP with metasearch-specific elements from STARTS. We also report on our experience building three SDARTS-compliant wrappers: for locally available plain-text document collections, for locally available XML document collections, and for external web-accessible collections. These wrappers were developed to be easily customizable for new collections. Our work was developed as part of Columbia University's Digital Libraries Initiative--Phase 2 (DLI2) project, which involves the departments of Computer Science, Medical Informatics, and Electrical Engineering, the Columbia University libraries, and a large number of industrial partners. The main goal of the project is to provide personalized access to a distributed patient-care digital library.",,https://www.semanticscholar.org/paper/46a2ceb19aae1cef1e2bb2432d09cac08aeaf8ce,ACM/IEEE Joint Conference on Digital Libraries
472,Motion planning on a graph,"We are given a connected, undirected graph G on n vertices. There is a mobile robot on one of the vertices; this vertex is labeled s. Each of several other vertices contains a single movable obstacle. The robot and the obstacles may only reside at vertices, although they may be moved across edges. A vertex may never contain more than one object (robot/obstacle). In one step, we may move either the robot or one of the obstacles from its current position /spl upsi/ to a vacant vertex adjacent to v. Our goal is to move the robot to a designated vertex t using the smallest number of steps possible. The problem is a simple abstraction of a robot motion planning problem, with the geometry replaced by the adjacencies in the graph. We point out its connections to robot motion planning. We study its complexity, giving exact and approximate algorithms for several cases.<<ETX>>",1994-11-20,https://www.semanticscholar.org/paper/acae14ccf64df5bff042db74b4bbbb56471d8f58,Proceedings 35th Annual Symposium on Foundations of Computer Science
892,On a Class of Totally Unimodular Matrices,"We examine the class of totally unimodular matrices-that contain no odd cycles, which we call restricted totally unimodular RTUM. We show that a matrix is RTUM if and only if it can be decomposed in a very simple way into the incidence matrices or their transposes of bipartite graphs or directed graphs, and give a linear time algorithm to perform this task. Based on this decomposition, we show that the 0,1 Integer Programming Problem with an RTUM matrix of constraints has the same time complexity as the b-matching and the max flow problems.",1985-05-01,https://www.semanticscholar.org/paper/22bf0037f688b2d68a3bed9be3f03e1b11b8a1b7,Mathematics of Operations Research
1349,Measurement of inclusive differential cross sections for pp collisions at (square root)s = 1.96 TeV.,"We present measurements of the inclusive production cross sections of the Gamma(1S) bottomonium state in pp collisions at (square root)s = 1.96 TeV. Using the Gamma(1S) --> mu(+)mu(-) decay mode for a data sample of 159 +/- 10 pb(-1) collected by the D0 detector at the Fermilab Tevatron collider, we determine the differential cross sections as a function of the Gamma(1S) transverse momentum for three ranges of the Gamma(1S) rapidity: 0 < y(Gamma) < or = 0.6, 0.6 < y(Gamma) < or = 1.2, and 1.2 < y(Gamma) < or = 1.8.",2005-06-13,https://www.semanticscholar.org/paper/8a1441071ccbe569e43e01b120832a41a3e8fd54,Physical Review Letters
21,Selecting Quality Twitter Content for Events,"
 
 Social media sites such as Twitter contain large amounts of user contributed messages for a wide variety of real-world events. While some of these ""event messages"" might contain interesting and useful information (e.g., event time, location, participants, opinions), others might provide little value (e.g., using heavy slang, incomprehensible language) to people interested in learning about an event. Techniques for effective selection of quality event content may therefore help improve applications such as event browsing and search.In this paper, we explore approaches for finding representative messages among a set of Twitter messages that correspond to the same event, with the goal of identifying high quality, relevant messages that provide useful event information. We evaluate our approaches using a large-scale dataset of Twitter messages, and show that we can automatically select event messages that are both relevant and useful.
 
",2011-07-05,https://www.semanticscholar.org/paper/24dab89bcd046a9cbe4a7a2e8eb9e9478d9a7c56,International Conference on Web and Social Media
3036,Cider: native execution of iOS apps on android,"We present Cider, an operating system compatibility architecture that can run applications built for different mobile ecosystems, iOS or Android, together on the same smartphone or tablet. Cider enhances the domestic operating system, Android, of a device with kernel-managed, per-thread personas to mimic the application binary interface of a foreign operating system, iOS, enabling it to run unmodified foreign binaries. This is accomplished using a novel combination of binary compatibility techniques including two new mechanisms: compile-time code adaptation, and diplomatic functions. Compile-time code adaptation enables existing unmodified foreign source code to be reused in the domestic kernel, reducing implementation effort required to support multiple binary interfaces for executing domestic and foreign applications. Diplomatic functions leverage per-thread personas, and allow foreign applications to use domestic libraries to access proprietary software and hardware interfaces. We have built a Cider prototype, and demonstrate that it imposes modest performance overhead and runs unmodified iOS and Android applications together on a Google Nexus tablet running the latest version of Android.",2014-02-24,https://www.semanticscholar.org/paper/24b59c9addd0262228259b1b30e5448632756caa,International Conference on Architectural Support for Programming Languages and Operating Systems
1607,Empirical Risk Minimization and Stochastic Gradient Descent for Relational Data,"Empirical risk minimization is the main tool for prediction problems, but its extension to relational data remains unsolved. We solve this problem using recent ideas from graph sampling theory to (i) define an empirical risk for relational data and (ii) obtain stochastic gradients for this empirical risk that are automatically unbiased. This is achieved by considering the method by which data is sampled from a graph as an explicit component of model design. By integrating fast implementations of graph sampling schemes with standard automatic differentiation tools, we provide an efficient turnkey solver for the risk minimization problem. We establish basic theoretical properties of the procedure. Finally, we demonstrate relational ERM with application to two non-standard problems: one-stage training for semi-supervised node classification, and learning embedding vectors for vertex attributes. Experiments confirm that the turnkey inference procedure is effective in practice, and that the sampling scheme used for model specification has a strong effect on model performance. Code is available at this https URL.",2018-06-27,https://www.semanticscholar.org/paper/a6b62f412e74edb5daccfbb1c46db887ad9b5ea4,International Conference on Artificial Intelligence and Statistics
3672,‘Long return’: A technique for improving the efficiency of inter‐module communication,"This paper describes a concept called ‘long return’ for use in inter‐module communication systems. First an implementation which implies a simplification of—rather than an extension of—traditional inter‐module communication systems is outlined. This implementation allows long returns to be used as an optimization technique without violating the commonly accepted principles of system structuring. Thereafter an experiment that provides an estimate of the improvements in efficiency of a particular operating system is described. In the SIMOS operating systems (based on the Cambridge CAP operating system) about 20 per cent of all inter‐process communication operations can be avoided by using long returns, and as a consequence the amount of useful work done in the system is typically increased by about 2.0 per cent.",1981-02-01,https://www.semanticscholar.org/paper/165514669f7afeb93b3f2b9abfc141c3f82fe625,"Software, Practice & Experience"
362,An approximate truthful mechanism for combinatorial auctions with single parameter agents,"Mechanism design seeks algorithms whose inputs are provided by selfish agents who would lie if advantageous. Incentive compatible mechanisms compel the agents to tell the truth by making it in their self-interest to do so. Often, as in combinatorial auctions, such mechanisms involve the solution of NP-hard problems. Unfortunately, approximation algorithms typically destroy incentive compatibility. Randomized rounding is a commonly used technique for designing approximation algorithms. We devise a version of randomized rounding that is incentive compatible, giving a truthful mechanism for combinatorial auctions with single parameter agents (e.g., ""single minded bidders"") that approximately maximizes the social value of the auction. We discuss two orthogonal notions of truthfulness for a randomized mechanism, truthfulness with high probability and in expectation, and give a mechanism that achieves both simultaneously.We consider combinatorial auctions where multiple copies of many different items are on sale, and each bidder i desires a subset Si. Given a set of bids, the problem of finding the allocation of items that maximizes total valuation is the well-known SETPACKING problem. This problem is NP-hard, but for the case of items with many identical copies the optimum can be approximated very well. To turn this approximation algorithm into a truthful auction mechanism we overcome two problems: we show how to make the allocation algorithm monotone, and give a method to compute the appropriate payments efficiently.",2003-01-12,https://www.semanticscholar.org/paper/e4558be04e6655b5a68bb736f0d35a4952d80a58,ACM-SIAM Symposium on Discrete Algorithms
749,"Quasi-Birth-Death Processes, Tree-Like QBDs, Probabilistic 1-Counter Automata, and Pushdown Systems","We begin by observing that (discrete-time) Quasi-Birth-Death Processes (QBDs) are equivalent, in a precise sense, to (discrete-time) probabilistic 1-Counter Automata (p1CAs), and both Tree-Like QBDs (TL-QBDs) and Tree-Structured QBDs (TS-QBDs) are equivalent to both probabilistic Pushdown Systems (pPDSs) and Recursive Markov Chains (RMCs). We then proceed to exploit these connections to obtain a number of new algorithmic upper and lower bounds for central computational problems about these models. Our main result is this: for an arbitrary QBD (even a null-recurrent one), we can approximate its termination probabilities (i.e., its G matrix) to within i bits of precision (i.e., within additive error 1/2i), in time polynomial in both the encoding size of the QBD and in i, in the unit-cost rational arithmetic RAM model of computation. Specifically, we show that a decomposed Newton's method can be used to achieve this. We emphasize that this bound is very different from the well-known ""linear/quadratic convergence"" of numerical analysis, known for QBDs and TL-QBDs, which typically gives no constructive bound in terms of the encoding size of the system being solved. In fact, we observe (based on recent results for pPDSs) that for the more general TL-QBDs this bound fails badly. Specifically, in the worst case Newton's method ""converges linearly"" to the termination probabilities for TL-QBDs, but requires exponentially many iterations in the encoding size of the TL-QBD to approximate these probabilities within any non-trivial constant error c < 1. Our upper bound proof for QBDs combines several ingredients: a detailed analysis of the structure of 1-counter automata, an iterative application of a classic condition number bound for errors in linear systems,and a very recent constructive bound on the performance of Newton's method for monotone systems of polynomial equations.",2008-09-14,https://www.semanticscholar.org/paper/039d956bd8b95138cece5dc0d4d1f84a9f1336bf,2008 Fifth International Conference on Quantitative Evaluation of Systems
2165,Internalization of Neutrophil-Derived Microvesicles Modulates TNFα-Stimulated Proinflammatory Cytokine Production in Human Fibroblast-Like Synoviocytes,"Neutrophil-derived microvesicles (NDMVs) have the potential to exert anti-inflammatory effects. Our study aimed to explore the effects of NDMVs on proinflammatory cytokines expressed by tumor necrosis factor α (TNFα)-stimulated fibroblast-like synoviocytes (FLS). FLS were isolated from the synovium of knee osteoarthritis (OA) patients undergoing surgery. NDMVs, isolated from TNFα-stimulated healthy neutrophils, were characterized by electron microscopy and nanoparticle tracking analysis. MTT and scratch wound healing assays were used to measure FLS viability and migration after treatment with NDMVs, while internalization of fluorescently labeled NDMVs was appraised by flow cytometry and confocal microscopy. Levels of proinflammatory cytokines in supernatants were quantified by the Bio-Plex system. Incubation of FLS with NDMVs at a vesicle/cell ratio of 100 resulted in a time-dependent uptake, with 35% of synoviocytes containing microvesicles over a 6–24 h time period, with no significant change in cell viability. TNFα stimulated the cytokine expression in FLS, and NDMVs down-regulated TNFα-induced expression of IL-5, IL-6, IL-8, MCP-1, IFNγ and MIP-1β. However, this down-regulation was selective, as NDMVs had no significant effects on TNFα-stimulated expression of IL-2 or IL-4. NDMVs were internalized by FLS to inhibit TNFα-stimulated broad-spectrum proinflammatory cytokine secretion. NDMVs, therefore, may exhibit an anti-inflammatory role in the regulation of the FLS function.",2021-07-01,https://www.semanticscholar.org/paper/73c5e6ff619e1037b4ee4fd4eac75932119d26d3,International Journal of Molecular Sciences
2757,Scope: automated generation of graphical interfaces,"We describe the design and prototype implementation of Scope, a system that generates graphical user interfaces for applications programmed in C++. The programmer chooses application data objects and functions that define the capabilities of the interface. At runtime, an interface design component, implemented as a set of production system rules, transforms this semantic specification into an interface built using a window system, an associated user interface toolkit, and the hardware input devices available on the system. The rules match application requirements against a semantic description of the toolkit, selecting virtual devices for input, output, and layout. Thus, Scope uses design rules to create interfaces from high-level programming semantics that are customized both for the application and the run-time environment.",1989-11-13,https://www.semanticscholar.org/paper/b93bb0aa7ca65714261ae254e012b8f6360fa44a,ACM Symposium on User Interface Software and Technology
2868,Galectin-12 Is Required for Adipogenic Signaling and Adipocyte Differentiation*,"Galectin-12 is a member of the galectin family consisting of β-galactoside-binding proteins with conserved carbohydrate recognition domains. This protein is preferentially expressed in peripheral blood leukocytes and adipocytes. We previously showed that galectin-12 is induced by cell cycle block at the G1 phase and causes G1 arrest when overexpressed (Yang, R.-Y., Hsu, D. K., Yu, L., Ni, J., and Liu, F.-T. (2001) J. Biol. Chem. 276, 20252-20260). Here, we show that the galectin-12 gene is expressed in mouse preadipocytes and is up-regulated when preadipocytes undergo cell cycle arrest, concomitant with acquisition of the competence to undergo differentiation in response to adipogenic hormone stimulation. Following a brief down-regulation 1 day after adipogenic treatment, its expression was once again markedly elevated when cells underwent terminal differentiation. Down-regulation of endogenous galectin-12 expression by RNA interference greatly reduced the expression of the adipogenic transcription factors CCAAT/enhancer-binding protein-β and -α and peroxisome proliferator-activated receptor-γ and severely suppressed adipocyte differentiation as a result of defective adipogenic signaling. We conclude that galectin-12 is required for signal transduction that conveys hormone stimulation to the induction of adipogenic factors essential for adipocyte differentiation. The findings suggest that galectin-12 is a major regulator of adipose tissue development.",2004-07-09,https://www.semanticscholar.org/paper/fd6e8ef17fd96067c98c26ccfea9cbb81b0d667d,Journal of Biological Chemistry
827,Perspectives on database theory,"Database management systems address the need to store, retrieve, and manipulate large amounts of data in an organized fashion. The database held has grown tremendously in the last 25 years. It is reported that the database industry generated $7 billion in revenue in 1994 and is growing at a rate of 35% per year. Industrial and academic research have been instrumental to this growth. Theory has played an important role in defining the right abstractions and concepts, and providing a firm foundation for the field. In order to access effectively a large volume of data, one needs an abstract logical view of the data, which must be separate from the physical storage of data. The important first component of a database is therefore an abstract view of data (called the data model) and the accompanying specialized high-level language that is used to access the data. The second important component is the data structures that are used to store the data along with the algorithms to support the efficient translation from the logical to the physical world. The third important component is the mechanisms that allow the database to be accessed concurrently by many users, without violating its integrity. Theory has contributed to all three fronts, starting with what is undoubtedly the cornerstone of the area, the introduction and formal definition of the relational model by F.P. Codd (1970). It is a highly unusual compliment for theory when the major commercial products in the field have at their core a mathematically rigorous, formal model. Our primary aims in this paper will be to give a flavor of the types of problems that database theory addresses, and to review how research in the area has evolved over the years. At the end we will try to point to some topics that may be of interest to people in the FOCS community tempted to work in database theory.",1995-10-23,https://www.semanticscholar.org/paper/b956b1e2eb41936462933913ebd7cf104b52d6d0,Proceedings of IEEE 36th Annual Foundations of Computer Science
2721,AutoVisual: rule-based design of interactive multivariate visualizations,"An extension to the n-Vision visualization system, which provides users with a 3D virtual world within which they can visualize and manipulate representations of multivariate relations is discussed. The extension, AutoVisual, is rule based system that eliminates the difficulty in choosing among the many alternative when designing visualizations. AutoVisual designs interactive virtual worlds for visualizing and exploring multivariate relations. It is guided by user-specified visualization tasks and a rule base of design principles. AutoVisual's visualization techniques and the visualization tasks it handles are described. Example visualizations AutoVisual has generated for two problem domains are discussed.<<ETX>>",1993-07-01,https://www.semanticscholar.org/paper/cbed07b76b3387946ab76c3bce77cab10103bf61,IEEE Computer Graphics and Applications
2718,Inferring constraints from multiple snapshots,"Many graphic tasks, such as the manipulation of graphical objects and the construction of user-interface widgets, can be facilitated by geometric constraints. However, the difficulty of specifying constraints by traditional methods forms a barrier to their widespread use. In order to make constraints easier to declare, we have developed a method of specifying constraints implicitly, through multiple examples. Snapshots are taken of an initial scene configuration, and one or more additional snapshots are taken after the scene has been edited into other valid configurations. The constraints that are satisfied in all of the snapshots are then applied to the scene objects. We discuss an efficient algorithm for inferring constraints from multiple snapshots. The algorithm has been incorporated into the Chimera editor, and several examples of its use are discussed.",1993-10-01,https://www.semanticscholar.org/paper/89f81d0e3ea7cd192cf94615169d21738bb47a6e,TOGS
1722,Profile Predictive Inference,"Predictive inference uses a model to analyze a dataset and make predictions about new observations. When a model does not match the data, predictive accuracy suffers. To mitigate this effect, we develop the profile predictive, a predictive density that incorporates the population distribution of data into Bayesian inference. This leads to a practical method for reducing the effect of model mismatch. We extend this method into variational inference and propose a stochastic optimization algorithm, called bumping variational inference (bump-vi). We demonstrate improved predictive accuracy over classical variational inference in two models: a Bayesian mixture model of image histograms and a latent Dirichlet allocation topic model of a text corpus.",2014-11-02,https://www.semanticscholar.org/paper/ef2911c4a5b458ac57c2a30c2e2446ed0af7e762,arXiv.org
2209,Effects of IL-6 and IL-6 blockade on neutrophil function in vitro and in vivo.,"OBJECTIVES
Reports on the regulation of neutrophil function by IL-6 are often conflicting. Therapeutic inhibition of IL-6 in RA is associated with occasional neutropenia, but the mechanisms underlying this observation are poorly understood. This study investigated interactions between IL-6, the anti-IL-6 receptor agent tocilizumab (TCZ) and neutrophils in vitro and in vivo.


METHODS
Neutrophils were isolated from healthy controls and incubated in vitro with pharmacologically relevant concentrations of IL-6 or TCZ. Neutrophils were also isolated from RA patients, including a cohort following TCZ therapy. Apoptosis was measured by annexin V/propidium iodide (PI) flow cytometry; phagocytosis was measured by incubating apoptotic neutrophils with THP-1-derived macrophages; chemotaxis was measured using cell migration through hanging-cell inserts towards IL-8 and cell surface proteins, including adhesion molecules CD11b (αMβ2 integrin) and CD62L (L-selectin) were measured by flow cytometry.


RESULTS
IL-6 (10-100 ng/ml) did not affect the rate of neutrophil apoptosis, priming of the respiratory burst or adhesion molecule expression nor act as a neutrophil chemoattractant. However, IL-6 enhanced signal transducer and activator of transcription 3 (STAT3) activation and neutrophil migration towards IL-8. TCZ in vitro did not induce apoptosis or phagocytosis of neutrophils, nor did it have a significant effect upon apoptosis or cell surface molecule expression. Neutrophil functions in ex vivo neutrophils from RA patients receiving TCZ treatment were unaffected.


CONCLUSION
Therapeutic blockade of IL-6, while inducing a transient neutropenia, does not directly affect neutrophil functions associated with host defence. TCZ-associated neutropenia cannot be explained by direct induction of apoptosis by TCZ, induction of apoptosis following depletion of IL-6, nor increased phagocytosis of neutrophils.",2014-07-01,https://www.semanticscholar.org/paper/84e3e1d49b6ff5a2137763a28c65406149db5709,Rheumatology
3769,Predicting Motivations of Actions by Leveraging Text,"Understanding human actions is a key problem in computer vision. However, recognizing actions is only the first step of understanding what a person is doing. In this paper, we introduce the problem of predicting why a person has performed an action in images. This problem has many applications in human activity understanding, such as anticipating or explaining an action. To study this problem, we introduce a new dataset of people performing actions annotated with likely motivations. However, the information in an image alone may not be sufficient to automatically solve this task. Since humans can rely on their lifetime of experiences to infer motivation, we propose to give computer vision systems access to some of these experiences by using recently developed natural language models to mine knowledge stored in massive amounts of text. While we are still far away from fully understanding motivation, our results suggest that transferring knowledge from language into vision can help machines understand why people in images might be performing an action.",2014-06-20,https://www.semanticscholar.org/paper/03c48850373b40f32b2bc0b1fbf7c13ccf0c8063,Computer Vision and Pattern Recognition
1211,Simultaneous measurement of the ratio R=B(t --> Wb)/B(t --> Wq) and the top-quark pair production cross section with the D0 detector at sqrt(s) = 1.96 TeV.,"We present the first simultaneous measurement of the ratio of branching fractions, R=B(t --> Wb)/B(t --> Wq), with q being a d, s, or b quark, and the top-quark pair production cross section sigma(tt[over]) in the lepton plus jets channel using 0.9 fb(-1) of pp[over] collision data at sqrt(s)=1.96 TeV collected with the D0 detector. We extract R and sigma(tt[over]) by analyzing samples of events with 0, 1, and > or =2 identified b jets. We measure R=0.97(+0.09)/(-0.08)(stat+syst) and sigma(tt[over])=8.18(+0.09)(-0.84)(stat+syst) +/- 0.50(lumi) pb, in agreement with the standard model prediction.",2008-01-08,https://www.semanticscholar.org/paper/46b09acf2ebed02ef3261ee33491138c83dbcd87,Physical Review Letters
1708,Posterior predictive checks to quantify lack-of-fit in admixture models of latent population structure,"Significance Bayesian models, including admixture models, are a powerful framework for articulating complex assumptions about large-scale genetic data; such models are widely used to explore data or to study population-level statistics of interest. However, we assume that a Bayesian model does not oversimplify the complexities in the data, to the point of invalidating our analyses. Here, we develop and study procedures for quantitatively evaluating admixture models of genetic data. Using four large genetic studies, we demonstrate that model checking should be an important part of the modern genetic data analysis pipeline. Our methods help to support inferences drawn from recovered population structure, to protect scientists from being misled by a misspecified model class, and to point scientists toward useful model extensions. Admixture models are a ubiquitous approach to capture latent population structure in genetic samples. Despite the widespread application of admixture models, little thought has been devoted to the quality of the model fit or the accuracy of the estimates of parameters of interest for a particular study. Here we develop methods for validating admixture models based on posterior predictive checks (PPCs), a Bayesian method for assessing the quality of fit of a statistical model to a specific dataset. We develop PPCs for five population-level statistics of interest: within-population genetic variation, background linkage disequilibrium, number of ancestral populations, between-population genetic variation, and the downstream use of admixture parameters to correct for population structure in association studies. Using PPCs, we evaluate the quality of the admixture model fit to four qualitatively different population genetic datasets: the population reference sample (POPRES) European individuals, the HapMap phase 3 individuals, continental Indians, and African American individuals. We found that the same model fitted to different genomic studies resulted in highly study-specific results when evaluated using PPCs, illustrating the utility of PPCs for model-based analyses in large genomic studies.",2014-06-30,https://www.semanticscholar.org/paper/9df7ebe156cc341beb75b48e78b06d96ad86572c,Proceedings of the National Academy of Sciences of the United States of America
1606,De novo gene signature identification from single‐cell RNA‐seq with hierarchical Poisson factorization,"Common approaches to gene signature discovery in single cell RNA-sequencing (scRNA-seq) depend upon predefined structures like clusters or pseudo-temporal order, require prior normalization, or do not account for the sparsity of single cell data. We present single cell Hierarchical Poisson Factorization (scHPF), a Bayesian factorization method that adapts Hierarchical Poisson Factorization [1] for de novo discovery of both continuous and discrete expression patterns from scRNA-seq. scHPF does not require prior normalization and captures statistical properties of single cell data better than other methods in benchmark datasets. Applied to scRNA-seq of the core and margin of a high-grade glioma, scHPF uncovers marked differences in the abundance of glioma subpopulations across tumor regions and subtle, regionally-associated expression biases within glioma subpopulations. scHFP revealed an expression signature that was spatially biased towards the glioma-infiltrated margins and associated with inferior survival in glioblastoma.",2018-07-11,https://www.semanticscholar.org/paper/9db5b92337a361a68207e3560704b541c23622cf,bioRxiv
522,On the Convergence of Query Evaluation,,1989-04-01,https://www.semanticscholar.org/paper/04ca8ad0b0fdc81bc11c9fea2c6d9247ed1b6c21,Journal of computer and system sciences (Print)
269,On Learning Algorithms for Nash Equilibria,,2010-10-18,https://www.semanticscholar.org/paper/929988cff413e36c8adec611676cfee0c812fc5f,Algorithmic Game Theory
3186,Vaccination-hesitancy and global warming: distinct social challenges with similar behavioural solutions,"Although the COVID-19 vaccine has dramatically changed the fight against the pandemic, many exhibit vaccination-hesitancy. At the same time, continued human-induced emissions of greenhouse gases pose an alarming threat to humanity. Based on the theory of Subjective Expected Relative Similarity (SERS) and a recent international study that drastically modified COVID-19 health-related attitudes, we explain why a similar approach and a corresponding public policy are expected to help resolve both behavioural issues: reduce vaccination hesitancy and motivate climate actions.",2022-06-01,https://www.semanticscholar.org/paper/fdd9672af55d0d9e161f4c0cf97dd51138211e29,Royal Society Open Science
296,Market equilibrium via a primal--dual algorithm for a convex program,We give the first polynomial time algorithm for exactly computing an equilibrium for the linear utilities case of the market model defined by Fisher. Our algorithm uses the primal--dual paradigm in the enhanced setting of KKT conditions and convex programs. We pinpoint the added difficulty raised by this setting and the manner in which our algorithm circumvents it.,2008-10-01,https://www.semanticscholar.org/paper/ceb12d7358557d362101960501d1cc96b96de786,JACM
2575,Content-aware scrolling,"Scrolling is used to navigate large information spaces on small screens, but is often too restrictive or cumbersome to use for particular types of content, such as multi-page, multi-column documents. To address this problem, we introduce content-aware scrolling (CAS), an approach that takes into account various characteristics of document content to determine scrolling direction, speed, and zoom. We also present the CAS widget, which supports scrolling through a content-aware path using traditional scrolling methods, demonstrating the advantages of making a traditional technique content-aware.",2006-10-15,https://www.semanticscholar.org/paper/9c39799b5a4364905fdb66caa6c06b3bb5efee44,ACM Symposium on User Interface Software and Technology
992,The effect of Vaccinium uliginosum extract on tablet computer-induced asthenopia: randomized placebo-controlled study,,2016-08-18,https://www.semanticscholar.org/paper/7f3eb8acc1828c7104577971c14106f2234bdef1,BMC Complementary and Alternative Medicine
3474,Simultaneously optimizing two scheduling objectives,"Scheduling algorithms are designed to optimize many optimality criteria in a wide variety of scheduling models. To do so is well-motivated and justified, as an algorithm that works well for one scheduling problem/objective may perform poorly on a different scheduling problem/objective. In this abstract, we give very general results about the existence of schedules which simultaneously minimize two criteria. Our results are general in that they apply to almost any scheduling environment, and that they apply to all pairs of metrics in which the first metric is one of maximum flow time, makespan, or maximum lateness and the second metric is one of average flow time, average completion time, average lateness, number of on-time jobs. We will show that for almost all such pairs of metrics there exist schedules which are simultaneously close to optimal for both metrics.",2001-04-23,https://www.semanticscholar.org/paper/13f92c2b45ec0b30451a2f482d169d389037b81d,"Proceedings, International Parallel and Distributed Processing Symposium (IPDPS)"
2181,014 APPA inhibits neutrophil pro-inflammatory functions without impairing host defence: is this a potential new therapy for arthritis?,,2019-04-01,https://www.semanticscholar.org/paper/75d7d783500aa3071fa332f5c43a65fc9371850e,Rheumatology
860,Minimum and maximum delay problems in real-time systems,,1991-07-01,https://www.semanticscholar.org/paper/5e3fdbe80861e7bbb2bd8c6c3a3c348075f6276c,Formal Methods Syst. Des.
1324,Measurement of the Bs(0) lifetime using semileptonic decays.,"We report a measurement of the Bs(0) lifetime in the semileptonic decay channel Bs(0) --> Ds- mu+ nuX (and its charge conjugate), using approximately 0.4 fb(-1) of data collected with the D0 detector during 2002-2004. Using 5176 reconstructed Ds- mu+ signal events, we have measured the Bs(0) lifetime to be tau(Bs(0))=1.398+/-0.044(stat)(-0.025)(+0.028)(syst) ps. This is the most precise measurement of the Bs(0) lifetime to date.",2006-04-24,https://www.semanticscholar.org/paper/aa9b1fdfce9f08aa56263c9f0736d7fe7043d70b,Physical Review Letters
14,Ranking Deep Web Text Collections for Scalable Information Extraction,"Information extraction (IE) systems discover structured information from natural language text, to enable much richer querying and data mining than possible directly over the unstructured text. Unfortunately, IE is generally a computationally expensive process, and hence improving its efficiency, so that it scales over large volumes of text, is of critical importance. State-of-the-art approaches for scaling the IE process focus on one text collection at a time. These approaches prioritize the extraction effort by learning keyword queries to identify the ""useful"" documents for the IE task at hand, namely, those that lead to the extraction of structured ""tuples."" These approaches, however, do not attempt to predict which text collections are useful for the IE task---and hence merit further processing---and which ones will not contribute any useful output---and hence should be ignored altogether, for efficiency. In this paper, we focus on an especially valuable family of text sources, the so-called deep web collections, whose (remote) contents are only accessible via querying. Specifically, we introduce and study techniques for ranking deep web collections for an IE task, to prioritize the extraction effort by focusing on collections with substantial numbers of useful documents for the task. We study both (adaptations of) state-of-the-art resource selection strategies for distributed information retrieval, and IE-specific approaches. Our extensive experimental evaluation over realistic deep web collections, and for several different IE tasks, shows the merits and limitations of the alternative families of approaches, and provides a roadmap for addressing this critically important building block for efficient, scalable information extraction.",2015-10-17,https://www.semanticscholar.org/paper/767dcce161a6a2a2bb8c404e08b2e169b36e6c1c,International Conference on Information and Knowledge Management
243,The New Faces of Combinatorial Optimization,,2012-04-19,https://www.semanticscholar.org/paper/3627b9251afb693b9d143213ee910b7392f217ea,International Symposium on Combinatorial Optimization
2040,Modeling semiconductor testing job scheduling and dynamic testing machine configuration,,2008-07-01,https://www.semanticscholar.org/paper/291dffa4ee142582c4f332be019d0c6787d98261,Expert systems with applications
3250,Social networks predict selective observation and information spread in ravens,"Animals are predicted to selectively observe and learn from the conspecifics with whom they share social connections. Yet, hardly anything is known about the role of different connections in observation and learning. To address the relationships between social connections, observation and learning, we investigated transmission of information in two raven (Corvus corax) groups. First, we quantified social connections in each group by constructing networks on affiliative interactions, aggressive interactions and proximity. We then seeded novel information by training one group member on a novel task and allowing others to observe. In each group, an observation network based on who observed whose task-solving behaviour was strongly correlated with networks based on affiliative interactions and proximity. Ravens with high social centrality (strength, eigenvector, information centrality) in the affiliative interaction network were also central in the observation network, possibly as a result of solving the task sooner. Network-based diffusion analysis revealed that the order that ravens first solved the task was best predicted by connections in the affiliative interaction network in a group of subadult ravens, and by social rank and kinship (which influenced affiliative interactions) in a group of juvenile ravens. Our results demonstrate that not all social connections are equally effective at predicting the patterns of selective observation and information transmission.",2016-07-01,https://www.semanticscholar.org/paper/e1ea08992af39011570b4fec3c6a95e13239dfb5,Royal Society Open Science
3409,An Empirical Study of Online Packet Scheduling Algorithms,,2016-03-25,https://www.semanticscholar.org/paper/7b3eadd9a04e44321983f840be56e930a1aae171,The Sea
536,On Stochastic Scheduling with In-Tree Precedence Constraints,"We consider the problem of optimal scheduling of a set of jobs obeying in-tree precedence constraints, when a number M of processors is available. It is assumed that the service times of different jobs are independent identically distributed random variables. Subject to a minor assumption on the service time distribution, we show that policies of the ""Highest Level First"" type are optimal asymptotically, as the number of jobs tends to infinity.",1987-02-01,https://www.semanticscholar.org/paper/06245c745eb73db6bfa78c90f45a957e4f9be828,SIAM journal on computing (Print)
2899,"Vector Embeddings by Sequence Similarity and Context for Improved Compression, Similarity Search, Clustering, Organization, and Manipulation of cDNA Libraries","This paper demonstrates the utility of organized numerical representations of genes in research involving flat string gene formats (i.e., FASTA/FASTQ5). FASTA/FASTQ files have several current limitations, such as their large file sizes, slow processing speeds for mapping and alignment, and contextual dependencies. These challenges significantly hinder investigations and tasks that involve finding similar sequences. The solution lies in transforming sequences into an alternative representation that facilitates easier clustering into similar groups compared to the raw sequences themselves. By assigning a unique vector embedding to each short sequence, it is possible to more efficiently cluster and improve upon compression performance for the string representations of cDNA libraries. Furthermore, through learning alternative coordinate vector embeddings based on the contexts of codon triplets, we can demonstrate clustering based on amino acid properties. Finally, using this sequence embedding method to encode barcodes and cDNA sequences, we can improve the time complexity of the similarity search by coupling vector embeddings with an algorithm that determines the proximity of vectors in Euclidean space; this allows us to perform sequence similarity searches in a quicker and more modular fashion.",2023-08-08,https://www.semanticscholar.org/paper/e9e31939ec1f183cf2e512112f6768decf8fb7b9,arXiv.org
1600,Noisin: Unbiased Regularization for Recurrent Neural Networks,"Recurrent neural networks (RNNs) are powerful models of sequential data. They have been successfully used in domains such as text and speech. However, RNNs are susceptible to overfitting; regularization is important. In this paper we develop Noisin, a new method for regularizing RNNs. Noisin injects random noise into the hidden states of the RNN and then maximizes the corresponding marginal likelihood of the data. We show how Noisin applies to any RNN and we study many different types of noise. Noisin is unbiased--it preserves the underlying RNN on average. We characterize how Noisin regularizes its RNN both theoretically and empirically. On language modeling benchmarks, Noisin improves over dropout by as much as 12.2% on the Penn Treebank and 9.4% on the Wikitext-2 dataset. We also compared the state-of-the-art language model of Yang et al. 2017, both with and without Noisin. On the Penn Treebank, the method with Noisin more quickly reaches state-of-the-art performance.",2018-05-03,https://www.semanticscholar.org/paper/672f28e2772b7d7895c5ce08ccd07eac3e60219e,International Conference on Machine Learning
772,Efficiently computing succinct trade-off curves,,2005-12-08,https://www.semanticscholar.org/paper/9fef816c3eebd04071bd8a9ab3524ea507eab8b1,Theoretical Computer Science
1550,A general linear-time inference method for Gaussian Processes on one dimension,"Gaussian Processes (GPs) provide powerful probabilistic frameworks for interpolation, forecasting, and smoothing, but have been hampered by computational scaling issues. Here we investigate data sampled on one dimension (e.g., a scalar or vector time series sampled at arbitrarily-spaced intervals), for which state-space models are popular due to their linearly-scaling computational costs. It has long been conjectured that state-space models are general, able to approximate any one-dimensional GP. We provide the first general proof of this conjecture, showing that any stationary GP on one dimension with vector-valued observations governed by a Lebesgue-integrable continuous kernel can be approximated to any desired precision using a specifically-chosen state-space model: the Latent Exponentially Generated (LEG) family. This new family offers several advantages compared to the general state-space model: it is always stable (no unbounded growth), the covariance can be computed in closed form, and its parameter space is unconstrained (allowing straightforward estimation via gradient descent). The theorem’s proof also draws connections to Spectral Mixture Kernels, providing insight about this popular family of kernels. We develop parallelized algorithms for performing inference and learning in the c ©2021 Jackson Loper, David Blei, John P. Cunningham, Liam Paninski. License: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at http://jmlr.org/papers/v22/21-0072.html. Loper, Blei, Cunningham, and Paninski LEG model, test the algorithm on real and synthetic data, and demonstrate scaling to datasets with billions of samples.",,https://www.semanticscholar.org/paper/bede99a33904742db2abe4f3b93cae70f4fdbe91,Journal of machine learning research
3763,Anticipating the future by watching unlabeled video,"In many computer vision applications, machines will need to reason beyond the present, and predict the future. This task is challenging because it requires leveraging extensive commonsense knowledge of the world that is difficult to write down. We believe that a promising resource for efficiently obtaining this knowledge is through the massive amounts of readily available unlabeled video. In this paper, we present a large scale framework that capitalizes on temporal structure in unlabeled video to learn to anticipate both actions and objects in the future. The key idea behind our approach is that we can train deep networks to predict the visual representation of images in the future. We experimentally validate this idea on two challenging ""in the wild"" video datasets, and our results suggest that learning with unlabeled videos significantly helps forecast actions and anticipate objects.",2015-04-29,https://www.semanticscholar.org/paper/0fb3b63090f95af97723efe565893eb25ea9188c,arXiv.org
2517,ARmonica: a collaborative sonic environment,"ARmonica is a 3D audiovisual augmented reality environment in which players can position and edit virtual bars that play sounds when struck by virtual balls launched under the influence of physics. Players experience ARmonica through head-tracked head-worn displays and tracked hand-held ultramobile personal computers, and interact through tracked Wii remotes and touch-screen taps. The goal is for players to collaborate in the creation and editing of an evolving sonic environment. Research challenges include supporting walk-up usability without sacrificing deeper functionality.",2010-10-03,https://www.semanticscholar.org/paper/15a680b23e34c28ffb21098a40e5becf440c4645,ACM Symposium on User Interface Software and Technology
3398,Parallel Graph Connectivity in Log Diameter Rounds,"Many modern parallel systems, such as MapReduce, Hadoop and Spark, can be modeled well by the MPC model. The MPC model captures well coarse-grained computation on large data — data is distributed to processors, each of which has a sublinear (in the input data) amount of memory and we alternate between rounds of computation and rounds of communication, where each machine can communicate an amount of data as large as the size of its memory. This model is stronger than the classical PRAM model, and it is an intriguing question to design algorithms whose running time is smaller than in the PRAM model. One fundamental graph problem is connectivity. On an undirected graph with n nodes and m edges, O(log n) round connectivity algorithms have been known for over 35 years. However, no algorithms with better complexity bounds were known. In this work, we give fully scalable, faster algorithms for the connectivity problem, by parameterizing the time complexity as a function of the diameter of the graph. Our main result is a O(log D log log_m/n n) time connectivity algorithm for diameter-d graphs, using Θ(m) total memory. If our algorithm can use more memory, it can terminate in fewer rounds, and there is no lower bound on the memory per processor. We extend our results to related graph problems such as spanning forest, finding a DFS sequence, exact/approximate minimum spanning forest, and bottleneck spanning forest. We also show that achieving similar bounds for reachability in directed graphs would imply faster boolean matrix multiplication algorithms. We introduce several new algorithmic ideas. We describe a general technique called double exponential speed problem size reduction which roughly means that if we can use total memory n to reduce a problem from size n to n/k, for k=(N/n)^Θ(1) in one phase, then we can solve the problem in O(loglog_N/n n) phases. In order to achieve this fast reduction for graph connectivity, we use a multistep algorithm. One key step is a carefully constructed truncated broadcasting scheme where each node broadcasts neighbor sets to its neighbors in a way that limits the size of the resulting neighbor sets. Another key step is random leader contraction, where we choose a smaller set of leaders than many previous works do.",2018-05-08,https://www.semanticscholar.org/paper/8b7733d62a1ccde31a309349b1a868a7dd4e3a58,IEEE Annual Symposium on Foundations of Computer Science
3408,Max-min Fair Rate Allocation and Routing in Energy Harvesting Networks: Algorithmic Analysis,,2016-06-08,https://www.semanticscholar.org/paper/7035e705130ba4fab8e93941add4d9555415b26d,Algorithmica
2459,Personalized Compass: A Demonstration of a Compact Visualization for Direction and Location,"Maps on mobile/wearable devices often make it difficult to determine the location of a point of interest (POI). For example, a POI may exist outside the map or on a background with no meaningful cues. To address this issue, we present Personalized Compass, a self-contained compact graphical location indicator. Personalized Compass uses personal a priori POIs to establish a reference frame, within which a POI in question can then be localized. Graphically, a personalized compass combines a multi-needle compass with an abstract overview map. We analyze the characteristics of Personalized Compass and the existing Wedge technique, and report on a user study comparing them. Personalized Compass performs better for four inference tasks, while Wedge is better for a locating task. Based on our analysis and study results, we suggest the two techniques are complementary and offer design recommendations. In this demonstration, we present an iOS application comparing Personalized Compass with Wedge for map-based location and direction tasks.",2016-05-07,https://www.semanticscholar.org/paper/98713199ed1966d08437b0586eb478dbeb76366e,CHI Extended Abstracts
197,Understanding evolution through algorithms,Why is evolution so successful? What is the role of sex (recombination)? Why is there so much diversity in populations? How do novel traits arise? Are mutations random? And is evolution optimizing something? This talk will review recent work by the speaker and collaborators aiming at understanding the many persistent mysteries of evolution through computational ideas.,2016-10-03,https://www.semanticscholar.org/paper/48cfd81af14af7c7b478f51bcf293b5487f1747e,Formal Methods in Computer-Aided Design
680,Distance Preserving Embeddings for General n-Dimensional Manifolds,"Low dimensional embeddings of manifold data have gained popularity in the last decade. However, a systematic finite sample analysis of manifold embedding algorithms largely eludes researchers. Here we present two algorithms that embed a general n-dimensionalmanifold into Rd (where d only depends on some key manifold properties such as its intrinsic dimension, volume and curvature) that guarantee to approximately preserve all interpoint geodesic distances.",2012-06-16,https://www.semanticscholar.org/paper/68762c51b68ef689e9809bbc42f1b9abf141fa5e,Annual Conference Computational Learning Theory
3235,Pastoralist societies in flux: A conceptual framework analysis of herding and land use among the Mukugodo Maasai of Kenya,,2017-07-04,https://www.semanticscholar.org/paper/022cb5f4d7c6edb67cbbebfc2b8ad87a02b6d9cd,Pastoralism
1380,Measurement of the ratio of inclusive cross sections sigma(pp --> Z + b jet)/sigma(pp --> Z + jet) at square root(s) = 1.96 TeV.,"Using the data collected with the D0 detector at square root(s) = 1.96 TeV, for integrated luminosities of about 180 pb(-1), we have measured the ratio of inclusive cross sections for pp --> Z + b jet to pp --> Z + jet production. The inclusive Z + b-jet reaction is an important background to searches for the Higgs boson in associated ZH production at the Fermilab Tevatron collider. Our measurement is the first of its kind, and relies on the Z --> e+ e- and Z --> mu+ mu- modes. The combined measurement of the ratio yields 0.021+/-0.005 for hadronic jets with transverse momenta pT > 20 GeV/c and pseudorapidities absolute value(eta) < 2.5, consistent with next-to-leading-order predictions of the standard model.",2004-10-26,https://www.semanticscholar.org/paper/eed6b53f0466bd84d68be086012707c6e125d865,Physical Review Letters
812,On the complexity of protein folding (abstract),". Proceedings of the 29th Annual ACM Sym- posium on the Theory of Computing, 1997, pp.21-29. [13] R Unger, J. Moult. Finding the lowest free energy con- formation of a protein is an NP-hard problem: proof and implications. Bulletin of Mathematical Biology 55 (1993), pp. 1163-119s.",1998-03-01,https://www.semanticscholar.org/paper/ef1ce9addc245bbb0591961586d6f5c300a60c87,Annual International Conference on Research in Computational Molecular Biology
1690,Variational Gaussian Process,"Abstract: Representations offered by deep generative models are fundamentally tied to their inference method from data. Variational inference methods require a rich family of approximating distributions. We construct the variational Gaussian process (VGP), a Bayesian nonparametric model which adapts its shape to match complex posterior distributions. The VGP generates approximate posterior samples by generating latent inputs and warping them through random non-linear mappings; the distribution over random mappings is learned during inference, enabling the transformed outputs to adapt to varying complexity. We prove a universal approximation theorem for the VGP, demonstrating its representative power for learning any model. For inference we present a variational objective inspired by autoencoders and perform black box inference over a wide class of models. The VGP achieves new state-of-the-art results for unsupervised learning, inferring models such as the deep latent Gaussian model and the recently proposed DRAW.",2015-11-20,https://www.semanticscholar.org/paper/ed032736652ac7e1f36ea17bd253cd1bfdcc3864,International Conference on Learning Representations
1980,Special issue on recent advance in Intelligent Manufacturing Systems,,2013-05-01,https://www.semanticscholar.org/paper/8e09a0536e3a5a6dcb78bccc17db2dc4e911d956,Computers & industrial engineering
3448,Budget optimization in search-based advertising auctions,"Internet search companies sell advertisement slots based on users' search queries via an auction. While there has been previous work onthe auction process and its game-theoretic aspects, most of it focuses on the Internet company. In this work, we focus on the advertisers, who must solve a complex optimization problem to decide how to place bids on keywords to maximize their return (the number of user clicks on their ads) for a given budget. We model the entire process and study this budget optimization problem. While most variants are NP-hard, we show, perhaps surprisingly, that simply randomizing between two uniform strategies that bid equally on all the keywordsworks well. More precisely, this strategy gets at least a 1-1/ε fraction of the maximum clicks possible. As our preliminary experiments show, such uniform strategies are likely to be practical. We also present inapproximability results, and optimal algorithms for variants of the budget optimization problem.",2006-12-08,https://www.semanticscholar.org/paper/2a800e0862a5c7cb1c4f1238e93d4bde44c46f0a,ACM Conference on Economics and Computation
2477,"2014 IEEE International Symposium on Mixed and Augmented Reality - Media, Art, Social Science, Humanities and Design, ISMAR-MASH'D 2014, Munich, Germany, September 10-12, 2014",,,https://www.semanticscholar.org/paper/745546473038ea67b12a45997fc2d6d0116f3ddc,ISMAR-MASH'D
2495,Proceedings of the 15th International Conference on the Foundations of Digital Games,"On behalf of the organizing committee, we welcome you to the 2012 Foundation of Digital Games Conference (FDG 2012) held in Raleigh, North Carolina from May 29-June 1, 2012. FDG is at the forefront, looking at the exciting emerging field of game research from an academic and scientific perspective and providing novel directions and theoretical foundations for emerging advances. We are pleased to welcome you to our growing community and are providing your contributions to feed the development for the future of the field of game research. The diversity and strength of the submissions this year continues to demonstrate that game research is continuously growing and maturing.",2012-05-29,https://www.semanticscholar.org/paper/5d8548480966626832de536a58a08b9e48a86c7b,International Conference on Foundations of Digital Games
2687,UIST'007 (panel): where will we be ten years from now?,"The conference this year is the tenth anniversary of UIST. The keynote talk discusses the history of UIST over the last ten years; this panel looks into the future of the field over the next ten. Each of the panelists will describe a scenario for what life will be like when we meet for UIST’O’I, ten years from now. They will also have a chance to challenge or question each others’ scenarios and to participate in open discussion with the audience.",1997-10-01,https://www.semanticscholar.org/paper/42e4e1fca978ed983694978da3e154abca1ef7dd,ACM Symposium on User Interface Software and Technology
2695,Data characterization for automatically visualizing heterogeneous information,"Automated graphical generation systems should be able to design effective presentations for heterogeneous (quantitative and qualitative) information in static or interactive environments. When building such a system, it is important to thoroughly understand the presentation-related characteristics of domain-specific information. We define a data-analysis taxonomy that can be used to characterize heterogeneous information. In addition to capturing the presentation-related properties of data, our characterization takes into account the user's information-seeking goals and visual-interpretation preferences. We use automatically-generated examples from two different application domains to demonstrate the coverage of the proposed taxonomy and its utility for selecting effective graphical techniques.",1996-10-28,https://www.semanticscholar.org/paper/c430e684e5f99874cb8abb49cc89812193ab4bfa,Proceedings IEEE Symposium on Information Visualization '96
1854,Integrating Topics and Syntax,"Statistical approaches to language learning typically focus on either short-range syntactic dependencies or long-range semantic dependencies between words. We present a generative model that uses both kinds of dependencies, and can be used to simultaneously find syntactic classes and semantic topics despite having no representation of syntax or semantics beyond statistical dependency. This model is competitive on tasks like part-of-speech tagging and document classification with models that exclusively use short- and long-range dependencies respectively.",2004-12-01,https://www.semanticscholar.org/paper/0ecc5ffeae38689dd2fe6ed4c32a6745744d7641,Neural Information Processing Systems
934,On a class of totally unimodular matrices,"We examine the class of matrices that satisfy Commoner's sufficient condition for total unimodularity [C], which we call restricted totally unimodular (RTUM). We show that a matrix is RTUM if and only if it can be decomposed in a very simple way into the incidence matrices (or their transposes) of bipartite graphs or directed graphs, and give a linear time algorithm to perform this task. Based on this decomposition, we show that the 0,1 Integer Programming Problem with an RTUM matrix of constraints has the same time complexity as the b-matching and the max flow problems.",1980-10-13,https://www.semanticscholar.org/paper/a5c4cf30dcbbc130f693dd1b75e0032f42b741bd,21st Annual Symposium on Foundations of Computer Science (sfcs 1980)
1542,Unsupervised Representation Learning via Neural Activation Coding,"We present neural activation coding (NAC) as a novel approach for learning deep representations from unlabeled data for downstream applications. We argue that the deep encoder should maximize its nonlinear expressivity on the data for downstream predictors to take full advantage of its representation power. To this end, NAC maximizes the mutual information between activation patterns of the encoder and the data over a noisy communication channel. We show that learning for a noise-robust activation code increases the number of distinct linear regions of ReLU encoders, hence the maximum nonlinear expressivity. More interestingly, NAC learns both continuous and discrete representations of data, which we respectively evaluate on two downstream tasks: (i) linear classification on CIFAR-10 and ImageNet-1K and (ii) nearest neighbor retrieval on CIFAR-10 and FLICKR-25K. Empirical results show that NAC attains better or comparable performance on both tasks over recent baselines including SimCLR and DistillHash. In addition, NAC pretraining provides significant benefits to the training of deep generative models. Our code is available at https://github.com/yookoon/nac.",2021-12-07,https://www.semanticscholar.org/paper/5cdfc4fa6cf6a297599182a80f2460027070183b,International Conference on Machine Learning
2414,Precueing Object Placement and Orientation for Manual Tasks in Augmented Reality,"When a user is performing a manual task, AR or VR can provide information about the current subtask (cueing) and upcoming subtasks (precueing) that makes them easier and faster to complete. Previous research on cueing and precueing in AR and VR has focused on path-following tasks requiring simple actions at each of a series of locations, such as pushing a button or just visiting. We consider a more complex task, whose subtasks involve moving to and picking up an item, moving that item to a designated place while rotating it to a specific angle, and depositing it. We conducted two user studies to examine how people accomplish this task while wearing an AR headset, guided by different visualizations that cue and precue movement and rotation. Participants performed best when given movement information for two successive subtasks and rotation information for a single subtask. In addition, participants performed best when the rotation visualization was split across the manipulated object and its destination.",2022-09-01,https://www.semanticscholar.org/paper/73e52edbb12665f689ab04807e686d37bb384543,IEEE Transactions on Visualization and Computer Graphics
2955,Erratum: Characterization of functional methylomes by next-generation capture sequencing identifies novel disease-associated variants,,2015-07-29,https://www.semanticscholar.org/paper/22f927c037e8d05a685684b8cdab1b4328e57fd4,Nature Communications
237,Sparse covers for sums of indicators,,2013-06-05,https://www.semanticscholar.org/paper/04e8bf2e697790906bce721301fe22eebd5c3f24,arXiv.org
3039,KVM/ARM: the design and implementation of the linux ARM hypervisor,"As ARM CPUs become increasingly common in mobile devices and servers, there is a growing demand for providing the benefits of virtualization for ARM-based devices. We present our experiences building the Linux ARM hypervisor, KVM/ARM, the first full system ARM virtualization solution that can run unmodified guest operating systems on ARM multicore hardware. KVM/ARM introduces split-mode virtualization, allowing a hypervisor to split its execution across CPU modes and be integrated into the Linux kernel. This allows KVM/ARM to leverage existing Linux hardware support and functionality to simplify hypervisor development and maintainability while utilizing recent ARM hardware virtualization extensions to run virtual machines with comparable performance to native execution. KVM/ARM has been successfully merged into the mainline Linux kernel, ensuring that it will gain wide adoption as the virtualization platform of choice for ARM. We provide the first measurements on real hardware of a complete hypervisor using ARM hardware virtualization support. Our results demonstrate that KVM/ARM has modest virtualization performance and power costs, and can achieve lower performance and power costs compared to x86-based Linux virtualization on multicore hardware.",2014-02-24,https://www.semanticscholar.org/paper/ba9115479d13f06bb448b34310c026237b65a051,International Conference on Architectural Support for Programming Languages and Operating Systems
925,Worst-case ration for planar graphs and the method of induction on faces,"The fact that several inlportant combinatorial optimiz.ation problems are NP-colnplete has motivated research on the worst-case analysis of approximation heuristics for these problems [Jol, GIl, Ch). TIlcse inve.stigations have produced some very interesting results"" and considerable insight has been gained by no\v into the power and limitations of existing techniques. The most inlportnnt paradigm in this area is bin packing [JDGGU, J02, Yao, GJ2, GJ3] and its generalizations [GOJY, CGJT). '[his is so because of the elegance and depth of the cOlnbinatorial arguments employed in the proofs of the upper bounds, and the intricate constnlctions of exanlples that achieve them. Despite the presence of the unifying cotlcept of a weighting [unction, the arguments are usually ingenious yet ad hoc, and the construction of worst-case exmnp)es is largely decoupled from the upper bounding process. In this paper we present a fanlily of results concerning certain extrenlal properties of planar graphs. In particular we show the follo\ving: (1) The greedy heuristic (i.e., repeatedly pick the node with smallest degree and delete its neighborhood) applied to a planar graph with n nodes yields an independent set of size at least 4n/21. (2) l'he greedy heuristic yields an independent set at )etlst 23/63 times the optimum. (3) A planar graph with n nodes and minimum degree 3 has ahvays a nzatching with fewer than n/3 free nodes.",1981-10-28,https://www.semanticscholar.org/paper/a00698715ac9afeb85a8e1992bcd4231f809467b,22nd Annual Symposium on Foundations of Computer Science (sfcs 1981)
2888,Expression and function of an IgE-binding animal lectin (epsilon BP) in mast cells.,"epsilon BP (IgE-binding protein) is a 31,000 M(r) protein originally identified in rat basophilic leukemia (RBL) cells. The protein is composed of two domains with the amino-terminal domain containing a highly conserved repetitive sequence and the carboxyl-terminal domain containing consensus sequences shared by other beta-galactoside-binding soluble lectins. The protein has wide tissue distribution, is found on cell surfaces and in extracellular milieu. By combined efforts from several research groups including ours a multifunctional nature of this lectin began to emerge. This review emphasizes the following characteristics of epsilon BP: (i) epsilon BP is secreted by cells such as macrophages; (ii) like many other lectins, epsilon BP functions at least bivalently; (iii) epsilon BP has specificity for distinct oligosaccharide structures that have a terminal galactose not masked by sialic acids; and (iv) in addition to binding IgE, epsilon BP binds to surfaces of various cell types via lectin-carbohydrate interaction. Importantly, epsilon BP binds to the IgE receptor on mast cells. We propose that epsilon BP can function as a modulatory protein on various cells by cross-linking critical cell surface glycoproteins. The proposed action of epsilon BP on mast cells is presented as a model.",,https://www.semanticscholar.org/paper/cf735c1c2f5c501edfc3525f982c8039ebf709b7,Immunopharmacology
3147,Virtual-Time Round-Robin: An O(1) Proportional Share Scheduler,"Proportional share resource management provides a flexible and useful abstraction for multiplexing timeshared resources. However, previous proportional share mechanisms have either weak proportional sharing accuracy or high scheduling overhead. We present VirtualTime Round-Robin (VTRR), a proportional share scheduler that can provide good proportional sharing accuracy with O(1) scheduling overhead. VTRR achieves this by combining the benefits of fair queueing algorithms with a round-robin scheduling mechanism. Unlike many other schedulers, VTRR is simple to implement. We have implemented a VTRR CPU scheduler in Linux in less than 100 lines of code. Our performance results demonstrate that VTRR provides accurate proportional share allocation with constant, sub-microsecond scheduling overhead. The scheduling overhead using VTRR is two orders of magnitude less than the standard Linux scheduler for large numbers of clients.",2001-06-25,https://www.semanticscholar.org/paper/9d5705cfaec7002218c09c808a6ffc26d34bc192,"USENIX Annual Technical Conference, General Track"
255,Economies with non-convex production and complexity equilibria,"The convexity assumptions required for the Arrow-Debreu theorem are reasonable and realistic for preferences; however, they are highly problematic for production because they rule out economies of scale. We take a complexity-theoretic look at economies with non-convex production. It is known that in such markets equilibrium prices may not exist; we show that it is an intractable problem to achieve Pareto efficiency, the fundamental objective achieved by equilibrium prices. The same is true for core efficiency or any one of an array of concepts of stability, with the degree of intractability ranging from F Δ2P-completeness to PSPACE-hardness. We also identify a novel phenomenon that we call complexity equilibrium in which agents quiesce, not because there is no way for any one of group of them to improve their situation, but because discovering the changes necessary for (individual or group) improvement is intractable. In fact, we exhibit a somewhat natural distribution of economies that has an average-case hard complexity equilibrium.",2011-06-05,https://www.semanticscholar.org/paper/88748063316e36a8443f2415588adde2367dea99,ACM Conference on Economics and Computation
1261,Observation of the doubly strange b baryon Omegab-.,"We report the observation of the doubly strange b baryon Omegab- in the decay channel Omegab(-)-->J/psiOmega-, with J/psi-->mu+mu(-) and Omega(-)-->LambdaK(-)-->(ppi-)K-, in pp collisions at sqrt[s]=1.96 TeV. Using approximately 1.3 fb(-1) of data collected with the D0 detector at the Fermilab Tevatron Collider, we observe 17.8+/-4.9(stat)+/-0.8(syst) Omegab- signal events at a mass of 6.165+/-0.010(stat)+/-0.013(syst) GeV. The significance of the observed signal is 5.4sigma, corresponding to a probability of 6.7 x 10(-8) of it arising from a background fluctuation.",2008-08-29,https://www.semanticscholar.org/paper/dca4159b180a91e820986c72251934613e5b757f,Physical Review Letters
874,On recognizing integer polyhedra,,1990-03-01,https://www.semanticscholar.org/paper/d90b135f5e58fb656b958c3732b318d559e97129,Comb.
567,Convergence of sideways query evaluation,"Sets of Horn clauses with no function symbols and negation can be considered as a query language (sometimes called DATALOG) that generalizes relational algebra. For example, the following clauses define the transitive closure relation of a binary relation A (not definable in relational algebra): Rule 1: T(z, y) : -A(z, y). Rule 2: T(z, y) : -A(z, z),T(z, y). Here A is a database relation, (it does not appear on the left-hand side of any rule), whereas T is a non-database relation. A typical query would be T(5, CC)?, asking for all items reachable from item 5.",1985-06-01,https://www.semanticscholar.org/paper/ea2b03020bfb04e80444354293346f0f4ab30ea0,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems
889,Linear and Book Embeddings of Graphs,,1986-08-01,https://www.semanticscholar.org/paper/08a27b0336173d0844ce365345b0f8b6d6b5a13f,Aegean Workshop on Computing
1713,The Inverse Regression Topic Model,"Taddy (2013) proposed multinomial inverse regression (MNIR) as a new model of annotated text based on the influence of metadata and response variables on the distribution of words in a document. While effective, MNIR has no way to exploit structure in the corpus to improve its predictions or facilitate exploratory data analysis. On the other hand, traditional probabilistic topic models (like latent Dirichlet allocation) capture natural heterogeneity in a collection but do not account for external variables. In this paper, we introduce the inverse regression topic model (IRTM), a mixed-membership extension of MNIR that combines the strengths of both methodologies. We present two inference algorithms for the IRTM: an efficient batch estimation algorithm and an online variant, which is suitable for large corpora. We apply these methods to a corpus of 73K Congressional press releases and another of 150K Yelp reviews, demonstrating that the IRTM outperforms both MNIR and supervised topic models on the prediction task. Further, we give examples showing that the IRTM enables systematic discovery of in-topic lexical variation, which is not possible with previous supervised topic models.",2014-06-21,https://www.semanticscholar.org/paper/bc0d7df38dd99d19ea586ecea1e478c4305993fc,International Conference on Machine Learning
2557,Balloon Selection: A Multi-Finger Technique for Accurate Low-Fatigue 3D Selection,"Balloon selection is a 3D interaction technique that is modeled after the real world metaphor of manipulating a helium balloon attached to a string. Balloon selection allows for precise 3D selection in the volume above a tabletop surface by using multiple fingers on a multi-touch-sensitive surface. The 3DOF selection tasks is decomposed in part into a 2DOF positioning task performed by one finger on the tabletop in an absolute 2D Cartesian coordinate system and a 1DOF positioning task performed by another finger on the tabletop in a relative 2D polar coordinate system. We have evaluated balloon selection in a formal user study that compared it to two well-known interaction techniques for selecting a static 3D target: a 3DOF tracked wand and keyboard cursor keys. We found that balloon selection was significantly faster than using cursor keys and had a significantly lower error rate than the wand. The lower error rate appeared to result from the user's hands being supported by the tabletop surface, resulting in significantly reduced hand tremor and arm fatigue.",2007-03-10,https://www.semanticscholar.org/paper/6caec67a2c4320d89aabda06be93617c4bfa7d41,IEEE Symposium on 3D User Interfaces
148,TopBot: automated network topology detection with a mobile robot,"We have demonstrated that a properly-equipped mobile robot can easily construct a detailed map of the wireless coverage of an urban environment. The Autonomous Vehicle for Exploration and Navigation of Urban Environments (AVENUE) mobile robot was successfully used to generate such maps in both manual and autonomous modes of operation. The resulting database contained a wealth of information for many different positions in the region, with a list of all access points viewable from each location together with a quality measure (the signal-to-noise ratio) of every detected signal. At a later time, the AVENUE system effectively used the data in this map to determine the approximate position of the robot as it traveled through the urban area.",2003-11-10,https://www.semanticscholar.org/paper/dfe0bac75cd0ca1c5c3d04c91f4e006cc9eca41f,IEEE International Conference on Robotics and Automation
3478,Implementation of a PTAS for Scheduling with Release Dates,,2001-01-05,https://www.semanticscholar.org/paper/b04cce450d832fef98e4d1321dd646434ec7f081,Workshop on Algorithm Engineering and Experimentation
930,Algorithms for Acyclic Database Schemes,"Many real-world situations can be captured by a set of functional dependencies and a single join dependency of a particular form called acyclic [B..]. The join dependency corresponds to a natural decomposition into meaningfull objects (an acyclic database scheme). Our purpose in this paper is to describe efficient algorithms in this setting for various problems, such as computing projections, minimizing joins, inferring dependencies, and testing for dependency satisfaction.",1981-09-09,https://www.semanticscholar.org/paper/fe0b45175713c4637486956f63f9234685a88c1c,Very Large Data Bases Conference
235,The Intractability of Dynamic Mechanism Design,"We introduce a dynamic mechanism design problem in which the designer wants to offer for sale an item to an agent, and another item to the same agent at some point in the future. The agent’s joint distribution of valuations for the two items is known, and the agent knows the valuation for the current item (but not for the one in the future). The designer seeks to maximize expected revenue, and the auction must be deterministic, truthful, and ex post individually rational. The optimum mechanism involves a protocol whereby the seller elicits the buyer’s current valuation, and based on the bid makes two take-it-or-leave-it offers, one for now and one for the future. We show that finding the optimum mechanism — arguably the simplest meaningful dynamic mechanism design problem imaginable — is NP-hard. We also prove several positive results, among them a polynomial linear programming-based algorithm for the optimum randomized auction, and we show strong separations in revenue between non-adaptive, adaptive, and randomized auctions. University of California, Berkeley, Computer Science Department. Email: christos@cs.berkeley.edu University of California, Berkeley, Computer Science Department. Email: geopier@gmail.com University of California, Berkeley, Computer Science Department. Email: alexpsomi@cs.berkeley.edu University of California, Berkeley, Computer Science Department. Email: aviad@cs.berkeley.edu 1",2014-07-21,https://www.semanticscholar.org/paper/c48c2bdae11409533fa700a2fe8cfff9aff20272,arXiv.org
663,Contrastive Loss is All You Need to Recover Analogies as Parallel Lines,"While static word embedding models are known to represent linguistic analogies as parallel lines in high-dimensional space, the underlying mechanism as to why they result in such geometric structures remains obscure. We find that an elementary contrastive-style method employed over distributional information performs competitively with popular word embedding models on analogy recovery tasks, while achieving dramatic speedups in training time. Further, we demonstrate that a contrastive loss is sufficient to create these parallel structures in word embeddings, and establish a precise relationship between the co-occurrence statistics and the geometric structure of the resulting word embeddings.",2023-06-14,https://www.semanticscholar.org/paper/abdc56442fdf09a08b8495d57a1e5a22840c3f8c,Workshop on Representation Learning for NLP
1905,Constructing a Metrology Sampling Framework for In-line Inspection in Semiconductor Fabrication,,2018-08-26,https://www.semanticscholar.org/paper/62b2e35b655de5f173bf85c3a8cd4db56c69fd57,Advances in Production Management Systems
1127,Combination of Tevatron searches for the standard model Higgs boson in the W+W- decay mode.,"We combine searches by the CDF and D0 Collaborations for a Higgs boson decaying to W+W-. The data correspond to an integrated total luminosity of 4.8 (CDF) and 5.4 (D0) fb(-1) of pp collisions at square root(s) = 1.96 TeV at the Fermilab Tevatron collider. No excess is observed above background expectation, and resulting limits on Higgs boson production exclude a standard model Higgs boson in the mass range 162-166 GeV at the 95% C.L.",2010-01-25,https://www.semanticscholar.org/paper/4a07d407cba3352bf12ebf95dc17345e6ba7cca1,Physical Review Letters
2946,Annotation-free quantification of RNA splicing using LeafCutter,,2017-12-11,https://www.semanticscholar.org/paper/d1c2faa14fb8b8091ad9cf3393711c44353aac7e,Nature Genetics
377,On the Eigenvalue Power Law,,2002-09-13,https://www.semanticscholar.org/paper/ca6432cc2b8a50c953f049d900cb219a84e53336,International Workshop Randomization and Approximation Techniques in Computer Science
849,The complexity of multiway cuts (extended abstract),"In the Multiway Cut problem we are given an edge-weighted graph and a subset of the vertices called terminals, and asked for a minimum weight set of edges that separates each terminal from all the others. When the number <italic>k</italic> of terminals is two, this is simply the min-cut, max-flow problem, and can be solved in polynomial time. We show that the problem becomes NP-hard as soon as <italic>k</italic> = 3, but can be solved in polynomial time for planar graphs for any fixed <italic>k</italic>. The planar problem is NP-hard, however, if <italic>k</italic> is not fixed. We also describe a simple approximation algorithm for arbitrary graphs that is guaranteed to come within a factor of 2–2/<italic>k</italic> of the optimal cut weight.",1992-07-01,https://www.semanticscholar.org/paper/523b9604863d9dd9282723d927b6efb7bad1bcb0,Symposium on the Theory of Computing
494,On the Optimal Bisection of a Polygon,"We show that bisecting a polygon into two equal (possibly disconnected) parts with the smallest possible total perimeter is NP-complete, and it is in fact NP-hard to approximate within any ratio. In contrast, we give a dynamic programming algorithm which finds a subdivision into two parts with total perimeter at most that of the optimum bisection, such that the two parts have areas within e of each other; the time is polynomial in the number of sides of the polygon, and 1/e. When the polygon is convex, or if the parts are required to be connected, then the exact problem can be solved in quadratic time. INFORMS Journal on Computing, ISSN 1091-9856, was published as ORSA Journal on Computing from 1989 to 1995 under ISSN 0899-1499.",1992-11-01,https://www.semanticscholar.org/paper/dcdf8f62ad3f9b7861c288ce0a12d4f9307cee27,INFORMS journal on computing
1931,Manufacturing intelligence and smart production for industry 3.5 and empirical study of decision-based virtual metrology for controlling overlay errors,"Focusing on the challenges of wafer fabs with multiple tools and high-mix products, this study aims to propose a decision-based virtual metrology framework to reduce the metrology cost and enhance productivity. An empirical study was conducted in a leading semiconductor company in Taiwan to validate the effectiveness of proposed approach for controlling overlay errors in lithography processes while ensuring the quality level. The results have shown practical viability of the proposed approach for reducing sampling rate and metrology cost.",2016-04-25,https://www.semanticscholar.org/paper/2c4c8916291778c271f2cf7851ed5bcfd2995363,"International Symposium on VLSI Design, Automation and Test"
2639,Guest Editors' Introduction,"The term mixed reality (MR) was first used in the mid 1990s (Milgram & Kishino, 1994), after the popularization of virtual reality (VR). VR refers to the experience of users immersed in a virtual computer-created world. Thus, the world of VR exists within the computer. In contrast, MR attempts to correlate the virtual world with the real world. At one end of the spectrum of ways in which virtual and real worlds can be combined is augmented reality (AR), a term that was first used in the early 1990s (Caudell & Mizell, 1992). AR supplements the real world with information obtained from a virtual world; for example, a seethrough head-mounted display (HMD) may be used to superimpose a computer-generated image on the user’s view of the real world. Augmented virtuality (AV), in contrast, refers to augmenting a virtual world with information obtained from the real world. Using AV, a more realistic virtual world than that of VR can be realized; for example, complex shapes and actual images of naturally occurring objects can be incorporated into a virtual world. In 1997, Mixed Reality Systems Laboratory Inc. was inaugurated in Japan as the founding body for a four-year joint project between the Ministry of International Trade and Industry (MITI) and Canon Inc. Three universities, the University of Tokyo, Hokkaido University and the University of Tsukuba, joined the collaboration, and an MR research group comprising industry, government and academia was established. The editors of this special issue, Ohta and Hirose, are collaborative researchers involved in the project, and Feiner participated as an international advisor. As part of this project, the International Symposium on Mixed Reality (ISMR) was held twice, in 1999 and 2001, to encourage MR research worldwide. To compile this special issue, we selected some of the best papers presented at ISMR 2001, and asked their authors to submit revised versions for review. The following is a brief summary of the articles included in this special issue. Sawada and colleagues report on one of the core AR technologies: head tracking. Sawada’s group describes a hardware solution based on miniature high-precision gyroscopes. Hedley and colleagues and Walairacht and colleagues present studies of user interface devices for freely manipulating the AR environment. Lee and colleagues introduce a system that allows the user to model physical objects within AR. Mann and Fung describe an approach to obscure visually undesirable objects to create diminished reality. Ohta and colleagues, MacIntyre and colleagues, and Sakagawa and colleagues report on methods for fusing the real world and virtual worlds. Ohta’s group describes an augmented reality system that uses an interactively computed depth map of the real world. MacIntyre’s group reports on software for embedding 2D video actors in 3D augmented reality, while Sakagawa’s group presents hardware for incorporating 3D, real world, ray-space models into virtual reality. When MR was first introduced, it dealt with the fusion of real and virtual worlds, accomplished, in principle, with VR technology. However, as MR research has progressed, it has become clear that a broader perspective is required to better take into account the real world. In particular, this has meant that MR researchers have begun to address the real world beyond the laboratory, exploring mobile and wearable computing technology. The MR field is progressing rapidly, and we hope that this special issue helps convey its vitality. Finally, we express our sincere thanks to Prof. Hirota of the University of Tokyo, who handled the administration work for the submitted manuscripts. We also thank Dr. Tamura of MR Systems Laboratory, and the members of the ISMR Committee.",,https://www.semanticscholar.org/paper/f04f65e6244a8af66b4449bdd2d6efb0267a560a,Presence: Teleoperators & Virtual Environments
2391,"The mitochondrial adenosine triphosphatase of Acanthamoeba castellanii. Oscillatory accumulation of enzyme activity, enzyme protein and F1-inhibitor during the cell cycle.","1. The mitochondrial ATPase of Acanthamoeba castellanii accumulated discontinuously in synchronous cultures prepared by a minimally perturbing size-selection technique. 2. Enzyme activity per ml of culture doubled overall during one cell cycle time of 8 h, but oscillated to give seven maxima during this period. Similar oscillations were observed in the specific activities of ATPase and of the naturally occurring inhibitor protein. 3. These variations in enzyme activity reflected changes in amount of enzyme protein as assayed by an immunological technique. 4. Large variations in I50 values (micrograms of inhibitor/mg of protein necessary for 50% inhibition of inhibitor-sensitive activity) for inhibition of ATPase activity by seven different inhibitors of energy conservation were observed. Activity was more sensitive to inhibition by oligomycin, efrapeptin, citreoviridin and quercetin when values were highest. 5. The results are discussed in relation to the phased organization of biosynthesis and degradation of cellular components known to occur during the cell cycle of this organization.",1982-02-15,https://www.semanticscholar.org/paper/e9d25e518d55e3f2ebb03ec658935561b2a1b167,Biochemical Journal
262,On optimal single-item auctions,"We revisit the problem of designing the profit-maximizing single-item auction, solved by Myerson in his seminal paper for the case in which bidder valuations are independently distributed. We focus on general joint distributions, seeking the optimal deterministic incentive compatible auction. We give a geometric characterization of the optimal auction through a duality theorem, resulting in an efficient algorithm for finding the optimal deterministic auction in the two-bidder case and an inapproximability result for three or more bidders.",2010-11-04,https://www.semanticscholar.org/paper/10e89c74c3565ac20b94454a1051279047a0ad84,Symposium on the Theory of Computing
616,A Worst-Case Analysis of Nearest Neighbor Searching by Projection,,1980-07-14,https://www.semanticscholar.org/paper/9f896738b9d622dd694777ef0ef4d3aea0b500cb,"International Colloquium on Automata, Languages and Programming"
907,Cutting and Partitioning a Graph aifter a Fixed Pattern (Extended Abstract),,1983-07-18,https://www.semanticscholar.org/paper/a5010453d836642967c95cbb1e710f5c95fd651d,"International Colloquium on Automata, Languages and Programming"
806,Protocol Feature Interactions,,1998-11-03,https://www.semanticscholar.org/paper/2df6ed60549c46121cbc0ce7af6f846a59bc1251,Formal Techniques for (Networked and) Distributed Systems
2997,"Algorithmic Foundations of Robotics XV - Proceedings of the Fifteenth Workshop on the Algorithmic Foundations of Robotics, WAFR 2022, College Park, MD, USA, 22-24 June, 2022",,,https://www.semanticscholar.org/paper/912c77c0a6fff1119073347c90a6d06b3885f096,Workshop on the Algorithmic Foundations of Robotics
986,Safety of Nonporous Silica Nanoparticles in Human Corneal Endothelial Cells,,2017-11-06,https://www.semanticscholar.org/paper/574d89f32f6d089c702b775973da63ba5c45bc50,Scientific Reports
3114,WebPod: persistent Web browsing sessions with pocketable storage devices,"We present WebPod, a portable system that enables mobile users to use the same persistent, personalized web browsing session on any Internet-enabled device. No matter what computer is being used, WebPod provides a consistent browsing session, maintaining all of a user's plugins, bookmarks, browser web content, open browser windows, and browser configuration options and preferences. This is achieved by leveraging rapid improvements in capacity, cost, and size of portable storage devices. WebPod provides a virtualization and checkpoint/restart mechanism that decouples the browsing environment from the host, enabling web browsing sessions to be suspended to portable storage, carried around, and resumed from the storage device on another computer. WebPod virtualization also isolates web browsing sessions from the host, protecting the browsing privacy of the user and preventing malicious web content from damaging the host. We have implemented a Linux WebPod prototype and demonstrate its ability to quickly suspend and resume web browsing sessions, enabling a seamless web browsing experience for mobile users as they move among computers.",2005-05-10,https://www.semanticscholar.org/paper/f522222a16bddbe7a9ec0029f93e589e89712597,The Web Conference
2412,Precueing Sequential Rotation Tasks in Augmented Reality,"Augmented reality has been used to improve sequential-task performance by cueing information about a current task step and precueing information about future steps. Existing work has shown the benefits of precueing movement (translation) information. However, rotation is also a major component in many real-life tasks, such as turning knobs to adjust parameters on a console. We developed an AR testbed to investigate whether and how much precued rotation information can improve user performance. We consider two unimanual tasks: one requires a user to make sequential rotations of a single object, and the other requires the user to move their hand between multiple objects to rotate them in sequence. We conducted a user study to explore these two tasks using circular arrows to communicate rotation. In the single-object task, we examined the impact of number of precues and visualization style on user performance. Results show that precues improved performance and that arrows with highlighted heads and tails, with each destination aligned with the next origin, yielded the shortest completion time on average. In the multiple-object task, we explored whether rotation precues can be helpful in conjunction with movement precues. Here, using a rotation cue without rotation precues in conjunction with a movement cue and movement precues performed the best, implying that rotation precues were not helpful when movement was also required.",2022-11-29,https://www.semanticscholar.org/paper/2b28e82d29a3f825a25c4a92349a56aa813e1b60,Virtual Reality Software and Technology
75,Top-k selection queries over relational databases: Mapping strategies and performance evaluation,"In many applications, users specify target values for certain attributes, without requiring exact matches to these values in return. Instead, the result to such queries is typically a rank of the ""top k"" tuples that best match the given attribute values. In this paper, we study the advantages and limitations of processing a top-k query by translating it into a single range query that a traditional relational database management system (RDBMS) can process efficiently. In particular, we study how to determine a range query to evaluate a top-k query by exploiting the statistics available to an RDBMS, and the impact of the quality of these statistics on the retrieval efficiency of the resulting scheme. We also report the first experimental evaluation of the mapping strategies over a real RDBMS, namely over Microsoft's SQL Server 7.0. The experiments show that our new techniques are robust and significantly more efficient than previously known strategies requiring at least one sequential scan of the data sets.",2002-06-01,https://www.semanticscholar.org/paper/10927ae8a906b482330a95b411285e2e4e407ca6,TODS
3585,Verification and semantic parallelization of goal-driven autonomous software,"Future space missions such as the Mars Science Laboratory demand the engineering of some of the most complex man-rated autonomous software systems. According to some recent estimates, the certification cost for mission-critical software exceeds its development cost. The current process-oriented methodologies do not reach the level of detail of providing guidelines for the development and validation of concurrent software. Time and concurrency are the most critical notions in an autonomous space system. In this work we present the design and implementation of a first concurrency and time centered framework for verification and semantic parallelization of real-time C++ within the JPL Mission Data System Framework (MDS). The end goal of the industrial project that motivated our work is to provide certification artifacts and accelerated testing of the complex software interactions in autonomous flight systems. As a case study we demonstrate the verification and semantic parallelization of the MDS Goal Networks.",2008-09-23,https://www.semanticscholar.org/paper/e8cfacb4856a73be75118525fa394177b669bae6,Autonomic Computing and Communication Systems
2161,Anti-Inflammatory Effects and Decreased Formation of Neutrophil Extracellular Traps by Enoxaparin in COVID-19 Patients,"Neutrophil Extracellular Traps (NETs) are a contributing factor of vascular thrombosis and alveolar damage in COVID-19 patients. As enoxaparin is currently used to inhibit vascular thrombosis, this study aimed to investigate whether enoxaparin also reduced inflammation and NETs in COVID-19 patients. Patients with COVID-19 infection were classified into three groups: mild, moderate, and severe (n = 10 for all groups). Plasma was collected from patients and healthy donors (n = 10). Neutrophils isolated from healthy controls were incubated with COVID-19 or healthy plasma, and with or without enoxaparin pretreatment in vitro. Neutrophils and plasma isolated from patients treated with enoxaparin were also investigated. The levels of inflammatory cytokines and NET products such as dsDNA, NE, MPO–DNA and Histone–DNA complexes in plasma and supernatants were measured using immunofluorescence staining and ELISA kits. The expression of inflammatory signaling genes by neutrophils (RELA, SYK, ERK and PKC) was measured using real-time qPCR. The levels of NET products were elevated in the plasma of COVID-19 patients, particularly in the severe group (p < 0.01). Moreover, plasma from the severe group enhanced NET formation (p < 0.01) from neutrophils in vitro. Enoxaparin pretreatment in vitro decreased plasma-induced NETs in a dose-dependent manner and down-regulated the expression of inflammatory genes (p < 0.05). Patients treated with prophylactic enoxaparin showed lower inflammatory cytokine levels and expression of inflammatory genes (p < 0.05). Increased NETs were associated with the severity of COVID-19 infection, particularly in patients with severe pneumonia, and could be used as biomarkers to assess disease severity. Enoxaparin pretreatment inhibited NETs and reduced the expression of inflammatory cytokines, and these effects mostly persisted in patients treated with prophylactic enoxaparin.",2022-04-27,https://www.semanticscholar.org/paper/03e2abef3d2b1e5b236ad7c3cd86bade4910e85b,International Journal of Molecular Sciences
502,Optimal coteries,"In a network processors may faiL It is desirable that the surviving nodes continue to operate, but no two disjoint partitions operate independently. One SOIUtion for this problem is based on the notion of coteries. A coterie is a family of sets of nodes such that any two sets in the family intersect. in the protocol implied by a coterie, a connected subgraph of the network functions if its set of nodes contains one of the sets in the family. One simple way to implement coteries is by voting. We study the following problem: Given a network with weigh ts, find the coterie that maximizes the expected number of operating nodes. We show that this problem, although hopelessly intractable in general, can be solved when the network is a cactus (all biconnected components are either edges or cycles). Even the case of a cycle with equal probabilities and weights is quite nontrivial and interesting.",1991-07-01,https://www.semanticscholar.org/paper/a52ee366a2ffd7d26af1d8ad9b438d3512917934,ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing
3288,Biometric animal databases from field photographs: identification of individual zebra in the wild,"We describe an algorithmic and experimental approach to a fundamental problem in field ecology: computer-assisted individual animal identification. We use a database of noisy photographs taken in the wild to build a biometric database of individual animals differentiated by their coat markings. A new image of an unknown animal can then be queried by its coat markings against the database to determine if the animal has been observed and identified before. Our algorithm, called StripeCodes, efficiently extracts simple image features and uses a dynamic programming algorithm to compare images. We test its accuracy against two different classes of methods: Eigenface, which is based on algebraic techniques, and matching multi-scale histograms of differential image features, an approach from signal processing. StripeCodes performs better than all competing methods for our dataset, and scales well with database size.",2011-04-18,https://www.semanticscholar.org/paper/c1886c3dd3c12afb079efa2d0f14efa5324fb721,International Conference on Multimedia Retrieval
2890,"Biochemical and biophysical characterization of human recombinant IgE-binding protein, an S-type animal lectin.",,1992-07-15,https://www.semanticscholar.org/paper/3dfcc1d5c1465baecf415f85a5d42d0ce1977707,Journal of Biological Chemistry
1751,Probabilistic topic models,Surveying a suite of algorithms that offer a solution to managing large document archives.,2012-04-01,https://www.semanticscholar.org/paper/7314be5cd836c8f06bd1ecab565b00b65259eac6,Communications of the ACM
1852,A latent mixed membership model for relational data,"Modeling relational data is an important problem for modern data analysis and machine learning. In this paper we propose a Bayesian model that uses a hierarchy of probabilistic assumptions about the way objects interact with one another in order to learn latent groups, their typical interaction patterns, and the degree of membership of objects to groups. Our model explains the data using a small set of parameters that can be reliably estimated with an efficient inference algorithm. In our approach, the set of probabilistic assumptions may be tailored to a specific application domain in order to incorporate intuitions and/or semantics of interest. We demonstrate our methods on simulated data and we successfully apply our model to a data set of protein-to-protein interactions.",2005-08-21,https://www.semanticscholar.org/paper/48291923daa6a5d3174c3150444d2996141f78b2,LinkKDD '05
5,Leveraging Just a Few Keywords for Fine-Grained Aspect Detection Through Weakly Supervised Co-Training,"User-generated reviews can be decomposed into fine-grained segments (e.g., sentences, clauses), each evaluating a different aspect of the principal entity (e.g., price, quality, appearance). Automatically detecting these aspects can be useful for both users and downstream opinion mining applications. Current supervised approaches for learning aspect classifiers require many fine-grained aspect labels, which are labor-intensive to obtain. And, unfortunately, unsupervised topic models often fail to capture the aspects of interest. In this work, we consider weakly supervised approaches for training aspect classifiers that only require the user to provide a small set of seed words (i.e., weakly positive indicators) for the aspects of interest. First, we show that current weakly supervised approaches fail to leverage the predictive power of seed words for aspect detection. Next, we propose a student-teacher approach that effectively leverages seed words in a bag-of-words classifier (teacher); in turn, we use the teacher to train a second model (student) that is potentially more powerful (e.g., a neural network that uses pre-trained word embeddings). Finally, we show that iterative co-training can be used to cope with noisy seed words, leading to both improved teacher and student models. Our proposed approach consistently outperforms previous weakly supervised approaches (by 14.1 absolute F1 points on average) in six different domains of product reviews and six multilingual datasets of restaurant reviews.",2019-09-01,https://www.semanticscholar.org/paper/7ab1f41d7bdfd9133167d92bca787fac888103cd,Conference on Empirical Methods in Natural Language Processing
1099,Silicon detector dark matter results from the final exposure of CDMS II.,"We report results of a search for weakly interacting massive particles (WIMPS) with the silicon detectors of the CDMS II experiment. This blind analysis of 140.2 kg day of data taken between July 2007 and September 2008 revealed three WIMP-candidate events with a surface-event background estimate of 0.41(-0.08)(+0.20)(stat)(-0.24)(+0.28)(syst). Other known backgrounds from neutrons and 206Pb are limited to <0.13 and <0.08 events at the 90% confidence level, respectively. The exposure of this analysis is equivalent to 23.4 kg day for a recoil energy range of 7-100 keV for a WIMP of mass 10  GeV/c2. The probability that the known backgrounds would produce three or more events in the signal region is 5.4%. A profile likelihood ratio test of the three events that includes the measured recoil energies gives a 0.19% probability for the known-background-only hypothesis when tested against the alternative WIMP+background hypothesis. The highest likelihood occurs for a WIMP mass of 8.6  GeV/c2 and WIMP-nucleon cross section of 1.9×10(-41)  cm2.",2013-04-15,https://www.semanticscholar.org/paper/17034b264acb33aa5e0c33738685fe456cba7193,Physical Review Letters
1668,Bayesian Poisson Tucker Decomposition for Learning the Structure of International Relations,"We introduce Bayesian Poisson Tucker decomposition (BPTD) for modeling country--country interaction event data. These data consist of interaction events of the form ""country $i$ took action $a$ toward country $j$ at time $t$."" BPTD discovers overlapping country--community memberships, including the number of latent communities. In addition, it discovers directed community--community interaction networks that are specific to ""topics"" of action types and temporal ""regimes."" We show that BPTD yields an efficient MCMC inference algorithm and achieves better predictive performance than related models. We also demonstrate that it discovers interpretable latent structure that agrees with our knowledge of international relations.",2016-06-06,https://www.semanticscholar.org/paper/f2485ee93ef70bc307a418ec904271ebe54af76d,International Conference on Machine Learning
1576,Using Embeddings to Correct for Unobserved Confounding,"We consider causal inference in the presence of unobserved confounding. In particular, we study the case where a proxy is available for the confounder but the proxy has non-iid structure. As one example, the link structure of a social network carries information about its members. As another, the text of a document collection carries information about their meanings. In both these settings, we show how to effectively use the proxy to do causal inference. The main idea is to reduce the causal estimation problem to a semi-supervised prediction of both the treatments and outcomes. Networks and text both admit high-quality embedding models that can be used for this semi-supervised prediction. Our method yields valid inferences under suitable (weak) conditions on the quality of the predictive model. We validate the method with experiments on a semi-synthetic social network dataset. We demonstrate the method by estimating the causal effect of properties of computer science submissions on whether they are accepted at a conference.",2019-02-11,https://www.semanticscholar.org/paper/9c3ab9aa3de816e37778f9514b89355b02e09fad,arXiv.org
414,Topological Queries,,1999-07-20,https://www.semanticscholar.org/paper/f8d66dc1509e2da793b6e6df43587537dec00422,"International Multi-Conference on Systems, Signals & Devices"
440,Towards an analysis of indexing schemes,,,https://www.semanticscholar.org/paper/ed4b7703b0ec9fb8edad37ca22055b6c3e5f838a,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems
668,Automated Symbolic Law Discovery: A Computer Vision Approach,"One of the most exciting applications of modern artificial intelligence is to automatically discover scientific laws from experimental data. This is not a trivial problem as it involves searching for a complex mathematical relationship over a large set of explanatory variables and operators that can be combined in an infinite number of ways.

Inspired by the incredible success of deep learning in computer vision, we tackle this problem by adapting various successful network architectures into the symbolic law discovery pipeline. The novelty of our approach is in (1) encoding the input data as an image with super-resolution, (2) developing an appropriate deep network pipeline, and (3) predicting the importance of each mathematical operator from the relationship image. This allows us to prior the exponentially large search with the predicted importance of the symbolic operators, which can significantly accelerate the discovery process.

We apply our model to a variety of plausible relationships---both simulated and from physics and mathematics domains---involving different dimensions and constituents. We show that our model is able to identify the underlying operators from data, achieving a high accuracy and AUC (91% and 0.96 on average resp.) for systems with as many as ten independent variables. Our method significantly outperforms the current state of the art in terms of data fitting (R^2), discovery rate (recovering the true relationship), and succinctness (output formula complexity). The discovered equations can be seen as first drafts of scientific laws that can be helpful to the scientists for (1) hypothesis building, and (2) understanding the complex underlying structure of the studied phenomena. Our approach holds a real promise to help speed up the rate of scientific discovery.",2021-05-18,https://www.semanticscholar.org/paper/8951402483e58f6279266ac836a88ad1e445eda6,AAAI Conference on Artificial Intelligence
2196,Differential changes in gene expression in human neutrophils following TNF‐α stimulation: Up‐regulation of anti‐apoptotic proteins and down‐regulation of proteins involved in death receptor signaling,"Responses of human neutrophils to TNF‐α are complex and multifactorial. Exposure of human neutrophils to TNF‐α in vitro primes the respiratory burst, delays apoptosis and induces the expression of several genes including chemokines, and TNF‐α itself. This study aimed to determine the impact of TNF‐α exposure on the expression of neutrophil genes and proteins that regulate apoptosis. Quantitative PCR and RNA‐Seq, identified changes in expression of several apoptosis regulating genes in response to TNF‐α exposure. Up‐regulated genes included TNF‐α itself, and several anti‐apoptotic genes, including BCL2A1, CFLAR (cFLIP) and TNFAIP3, whose mRNA levels increased above control values by between 4‐20 fold (n = 3, P < 0.05). In contrast, the expression of pro‐apoptotic genes, including CASP8, FADD and TNFRSF1A and TNFRSF1B, were significantly down‐regulated following TNF‐α treatment. These changes in mRNA levels were paralleled by decreases in protein levels of caspases 8 and 10, TRADD, FADD, TNFRSF1A and TNFRSF1B, and increased cFLIP protein levels, as detected by western blotting. These data indicate that when neutrophils are triggered by TNF‐α exposure, they undergo molecular changes in transcriptional expression to up‐regulate expression of specific anti‐apoptotic proteins and concomitantly decrease expression of specific proteins involved in death receptor signaling which will alter their function in TNF‐α rich environments.",2015-12-02,https://www.semanticscholar.org/paper/2cd0e96a43285a5fce422ebed195f67d8833f8e4,"Immunity, Inflammation and Disease"
2366,"The relationship between superoxide generation, cytochrome β and oxygen in activated neutrophils",,,https://www.semanticscholar.org/paper/f4e1f5bb54916856ac3d34f66ca8fc3caf8788a0,FEBS Letters
2479,Patient-centered tools for medication information search,"Recent research focused on online health information seeking highlights a heavy reliance on general-purpose search engines. However, current general-purpose search interfaces do not necessarily provide adequate support for non-experts in identifying suitable sources of health information. Popular search engines have recently introduced search tools in their user interfaces for a range of topics. In this work, we explore how such tools can support non-expert, patient-centered health information search. Scoping the current work to medication-related search, we report on findings from a formative study focused on the design of patient-centered, medication-information search tools. Our study included qualitative interviews with patients, family members, and domain experts, as well as observations of their use of Remedy, a technology probe embodying a set of search tools. Post-operative cardio-thoracic surgery patients and their visiting family members used the tools to find information about their hospital medications and were interviewed before and after their use. Domain experts conducted similar search tasks and provided qualitative feedback on their preferences and recommendations for designing these tools. Findings from our study suggest the importance of four valuation principles underlying our tools: credibility, readability, consumer perspective, and topical relevance.",2014-05-20,https://www.semanticscholar.org/paper/979531b9232895705c45f4b151f945678b953545,International Conference on Pervasive Computing Technologies for Healthcare
2506,Enabling large-scale outdoor Mixed Reality and Augmented Reality,"While there is significant recent progress in technologies supporting augmented reality for small indoor environments, there is still much work to be done for large outdoor environments. This workshop focuses primarily on research that enables high-quality outdoor Mixed Reality (MR) and Augmented Reality (AR) applications. These research topics include, but are not restricted to: — 3D geo-referenced data (images, point clouds, and models) — Algorithms for object recognition from large databases of geo-referenced data — Algorithms for object tracking in outdoor environment — Multi-cue fusion to achieve improved performance of object detection and tracking — Novel representation schemes to facilitate large-scale content distribution — 3D reasoning to support intelligent augmentation — Novel and improved mobile capabilities for data capture (device sensors), processing, and display — Applications, experiences, and user interface techniques. The workshop will also showcase existing prototypes of applications enabled by these technologies: mirror worlds, high-fidelity virtual environments, applications of panoramic imagery, and user studies relating to these media types. This workshop aims to bring together academic and industrial researchers and to foster discussion amongst participants on the current state of the art and future directions for technologies that enable large-scale outdoor MR and AR applications. The workshop will start with a session in which position statements and overviews of the state of the art are presented. In the afternoon, we will follow up with discussion sessions and a short closing session.",2011-10-26,https://www.semanticscholar.org/paper/458898fc29d3c57d62c98de5218947a66326a57d,"2011 IEEE International Symposium on Mixed and Augmented Reality - Arts, Media, and Humanities"
182,From Nash Equilibria to Chain Recurrent Sets: An Algorithmic Solution Concept for Game Theory,"In 1950, Nash proposed a natural equilibrium solution concept for games hence called Nash equilibrium, and proved that all finite games have at least one. The proof is through a simple yet ingenious application of Brouwer’s (or, in another version Kakutani’s) fixed point theorem, the most sophisticated result in his era’s topology—in fact, recent algorithmic work has established that Nash equilibria are computationally equivalent to fixed points. In this paper, we propose a new class of universal non-equilibrium solution concepts arising from an important theorem in the topology of dynamical systems that was unavailable to Nash. This approach starts with both a game and a learning dynamics, defined over mixed strategies. The Nash equilibria are fixpoints of the dynamics, but the system behavior is captured by an object far more general than the Nash equilibrium that is known in dynamical systems theory as chain recurrent set. Informally, once we focus on this solution concept—this notion of “the outcome of the game”—every game behaves like a potential game with the dynamics converging to these states. In other words, unlike Nash equilibria, this solution concept is algorithmic in the sense that it has a constructive proof of existence. We characterize this solution for simple benchmark games under replicator dynamics, arguably the best known evolutionary dynamics in game theory. For (weighted) potential games, the new concept coincides with the fixpoints/equilibria of the dynamics. However, in (variants of) zero-sum games with fully mixed (i.e., interior) Nash equilibria, it covers the whole state space, as the dynamics satisfy specific information theoretic constants of motion. We discuss numerous novel computational, as well as structural, combinatorial questions raised by this chain recurrence conception of games.",2018-10-01,https://www.semanticscholar.org/paper/91b1271aa1f675d8ab6a51b581d722a7e7b69382,Entropy
3781,Efficiently Scaling Up Video Annotation with Crowdsourced Marketplaces,,2010-09-05,https://www.semanticscholar.org/paper/09dd01e19b247a33162d71f07491781bdf4bfd00,European Conference on Computer Vision
1643,Reparameterization Gradients through Acceptance-Rejection Sampling Algorithms,"Variational inference using the reparameterization trick has enabled large-scale approximate Bayesian inference in complex probabilistic models, leveraging stochastic optimization to sidestep intractable expectations. The reparameterization trick is applicable when we can simulate a random variable by applying a differentiable deterministic function on an auxiliary random variable whose distribution is fixed. For many distributions of interest (such as the gamma or Dirichlet), simulation of random variables relies on acceptance-rejection sampling. The discontinuity introduced by the accept-reject step means that standard reparameterization tricks are not applicable. We propose a new method that lets us leverage reparameterization gradients even when variables are outputs of a acceptance-rejection sampling algorithm. Our approach enables reparameterization on a larger class of variational distributions. In several studies of real and synthetic data, we show that the variance of the estimator of the gradient is significantly lower than other state-of-the-art methods. This leads to faster convergence of stochastic gradient variational inference.",2016-10-18,https://www.semanticscholar.org/paper/0998c939e00af09b49ae04fc78aaca7625a0c895,International Conference on Artificial Intelligence and Statistics
514,ON GRAPH-THEORETIC LEMMATA AND COMPLEXITY CLASSES (Extended Abstract ),"We define several new complexity classes of search problems, ""between"" the classes FP and FNP. These classes are contained (along with factoring, and (JPY)'s class PLS) in the class TFNP of search problems that always have a solu- tion. A problem in each of these new classes is de- fined in terms of an implicitly given, exponentially large graph, very much like PLS. The existence of the solution sought is established via a simple graph- theoretic lemma with an inefficiently constructive proof; for example, PLS can be thought of as cor- responding to the lemma ""every dag has a sink."" The new classes are based on lemmata such as ""ev- ery graph has an even number of odd-degree nodes."" We show several class containments and collapses, resulting in the two new classes PDLF C PLF; the relation of either class to PLS is open. PLF contains several important problems for which no polynom'al time algorithm is presently known, including the lin- ear complementarity problem for P-matrices, finding a mixed equilibrium in a non-zero sum game, find- ing a second Hamilton circuit in a Hamiltonian cubic graph, finding a second Hamiltonian decomposition in a quartic graph, and the computational versions of Sperner's Lemma and Brouwer's Fixpoint Theo- rem. In fact, the latter two problems are shown to be PDLF-complete.",,https://www.semanticscholar.org/paper/86d60cb8f1bf79de6593bc5f53a6a0feedd85dd9,IEEE Annual Symposium on Foundations of Computer Science
3739,DeepBase: Deep Inspection of Neural Networks,"Although deep learning models perform remarkably well across a range of tasks such as language translation and object recognition, it remains unclear what high-level logic, if any, they follow. Understanding this logic may lead to more transparency, better model design, and faster experimentation. Recent machine learning research has leveraged statistical methods to identify hidden units that behave (e.g., activate) similarly to human understandable logic, but those analyses require considerable manual effort. Our insight is that many of those studies follow a common analysis pattern, and therefore there is opportunity to provide a declarative abstraction to easily express, execute and optimize them. This paper describes DeepBase, a system to inspect neural network behaviors through a unified interface. We model logic with user-provided hypothesis functions that annotate the data with high-level labels (e.g., part-of-speech tags, image captions). DeepBase lets users quickly identify individual or groups of units that have strong statistical dependencies with desired hypotheses. We discuss how DeepBase can express existing analyses, propose a set of simple and effective optimizations to speed up a standard Python implementation by up to 72x, and reproduce recent studies from the NLP literature.",2018-08-13,https://www.semanticscholar.org/paper/1d913f05044382ddc1fea170a739fffa189c9c88,SIGMOD Conference
2350,"Inhibition of neutrophil superoxide secretion by the preservative, methylhydroxybenzoate: effects mediated by perturbation of intracellular Ca2+?","The preservative, methylhydroxybenzoate inhibited O2- secretion from human neutrophils activated by both the chemotactic peptide fMet-Leu-Phe and phorbol myristate acetate (PMA): the low level of oxidant secretion activated by the ionophore A23187 was similarly reduced in preservative-treated suspensions. Oxidant secretion was similarly reduced in fMet-Leu-Phe and A23187 treated suspensions in which intracellular Ca2+ was buffered by loading with Quin-2, indicating that methylhydroxybenzoate may exert its effects by perturbation of intracellular Ca2(+)-dependent processes. Methylhydroxybenzoate could mimic EGTA in preventing the Ca2+ dependent enhancement of trypsin activity and could also bind this cation in experiments using a Ca2+ electrode, although the preservative bound Ca2+ more slowly and had a lower affinity than EGTA. These data indicate that methylhydroxybenzoate may exert its effects on neutrophils by perturbation of Ca2(+)-dependent activation pathways and this phenomenon may also explain its other known pharmacological effects. Furthermore, these observations provide an insight into the mechanisms by which intracellular Ca2+ may regulate oxidant secretion.",,https://www.semanticscholar.org/paper/351a75130a19904360fde89a3114a8edffc98204,Free Radical Research Communications
3501,Experimental study of minimum cut algorithms,"Recently, several new algorithms have been developed for the minimum cut problem that substantially improve worst-case time bounds for the problem. These algorithms are very different from the earlier ones and from each other. We conduct an experimental evaluation of the relative performance of these algorithms. In the process, we develop heuristics and data structures that substantially improve practical performance of the algorithms. We also develop problem families for testing minimum cut algorithms. Our work leads to a better understanding of practical performance of the minimum cut algorithms and produces very efficient codes for the problem.",1997-01-05,https://www.semanticscholar.org/paper/dddf141709c2d07d9ce77c3a2ff79cbe2434b47d,ACM-SIAM Symposium on Discrete Algorithms
2249,Haemophilus influenzae induces neutrophil necrosis: a role in chronic obstructive pulmonary disease?,"Noncapsulate Haemophilus influenzae is commonly found in the airways of patients with chronic obstructive pulmonary disease (COPD), both during stable disease and during exacerbations. Neutrophils are also found in large numbers in sputum from patients with COPD, which also contains released neutrophil products such as elastase. Why H. influenzae colonizes the lungs of patients with COPD in the presence of such large numbers of infiltrating neutrophils is not known. We set out to determine if abnormal interactions between H. influenzae and neutrophils could impact on COPD pathology. Noncapsulate H. influenzae clinical isolates were incubated in vitro with neutrophils from healthy volunteers, and respiratory burst activity, cytokine and chemokine production, phagocytosis and killing of bacteria, and neutrophil apoptosis and necrosis were measured. Neutrophil morphology was determined in sputum samples. H. influenzae were phagocytosed by neutrophils, thereby activating a respiratory burst and the secretion of the neutrophil chemoattractant IL-8. However, rather than kill the bacteria, the neutrophils themselves were killed (largely via necrosis) and released their granule contents into the extracellular environment. Neutrophil-derived IL-8, generated after the interaction of H. influenzae with neutrophils, may result in the further infiltration of neutrophils into the lungs, thereby amplifying the inflammatory response. However, the infiltrating neutrophils fail to kill the bacteria and instead release tissue-damaging products into the lung as they undergo necrosis. These results may help to explain the clinical picture in COPD.",2007-03-15,https://www.semanticscholar.org/paper/b2a3bcd2f50ebf6b72905099a5d41e0915e44ad0,American Journal of Respiratory Cell and Molecular Biology
805,Black Box Checking,"Two main approaches are used for increasing the quality of systems: in model checking , one checks properties of a known design of a system; in testing, one usually checks whether a given implementation, whose internal structure is often unknown, conforms with an abstract design. We are interested in the combination of these techniques. Namely, we would like to be able to test whether an implementation with unknown structure satisfies some given properties. We propose and formalize this problem of black box checking and suggest several algorithms. Since the input to black box checking is not given initially, as is the case in the classical model of computation, but is learned through experiments, we propose a computational model based on games with incomplete information. We use this model to analyze the complexity of the problem. We also address the more practical question of finding an approach that can detect errors in the implementation before completing an exhaustive search.",1999-10-05,https://www.semanticscholar.org/paper/8ab91a1d00b1e4bda4c800672fb196541622a998,Formal Techniques for (Networked and) Distributed Systems
2686,Top-down hierarchical planning of coherent visual discourse,"A visual discourse is a series of connected visual displays. A coherent visual discourse requires smooth transitions between displays, consistent design within and across displays, and successful integration of new information into existing displays. We present an approach for automatically designing a coherent visual discourse. A top-down, hierarchical-decomposition partial-order planner is used to efficiently plan the visual discourse. Visual representations are modelled as visual objects, graphical techniques are employed as planning operators, and design policies are encoded as constraints. This approach not only improves the computational efficiency compared to search-based approaches, but also facilitates knowledge encoding, and ensures global coherency.",1997-01-06,https://www.semanticscholar.org/paper/3836a4548a1ea682c54c5a425b112c19a946924d,International Conference on Intelligent User Interfaces
157,Assemblies of neurons learn to classify well-separated distributions,"An assembly is a large population of neurons whose synchronous firing is hypothesized to represent a memory, concept, word, and other cognitive categories. Assemblies are believed to provide a bridge between high-level cognitive phenomena and low-level neural activity. Recently, a computational system called the Assembly Calculus (AC), with a repertoire of biologically plausible operations on assemblies, has been shown capable of simulating arbitrary space-bounded computation, but also of simulating complex cognitive phenomena such as language, reasoning, and planning. However, the mechanism whereby assemblies can mediate learning has not been known. Here we present such a mechanism, and prove rigorously that, for simple classification problems defined on distributions of labeled assemblies, a new assembly representing each class can be reliably formed in response to a few stimuli from the class; this assembly is henceforth reliably recalled in response to new stimuli from the same class. Furthermore, such class assemblies will be distinguishable as long as the respective classes are reasonably separated -- for example, when they are clusters of similar assemblies. To prove these results, we draw on random graph theory with dynamic edge weights to estimate sequences of activated vertices, yielding strong generalizations of previous calculations and theorems in this field over the past five years. These theorems are backed up by experiments demonstrating the successful formation of assemblies which represent concept classes on synthetic data drawn from such distributions, and also on MNIST, which lends itself to classification through one assembly per digit. Seen as a learning algorithm, this mechanism is entirely online, generalizes from very few samples, and requires only mild supervision -- all key attributes of learning in a model of the brain.",2021-10-07,https://www.semanticscholar.org/paper/295e71794653a36385680be74eca63b971bbe258,Annual Conference Computational Learning Theory
542,Optimum Grip of a Polygon,"It has been shown by Baker, Fortune and Grosse that any two-dimensional polygonal object can be prehended stably with three fingers, so that its weight (along the third dimension) is balanced. Besides, in this paper we show that form closure of a polygon object can be achieved by four fingers (previous proofs were not complete). We formulate and solve the problem of finding the optimum stable grip or form closure of any given polygon. For stable grip it is most natural to minimize the forces needed to balance through friction the object''s weight along the third dimension. For form closure, we minimize the worst-case forces needed to balance any unit force acting on the center of gravity of the object. The mathematical techniques used in the two instances are an interesting mix of Optimization and Euclidean geometry. Our results lead to algorithms for the efficient computation of the optimum grip in each case.",1987-04-01,https://www.semanticscholar.org/paper/f05f41e1c53a670ecfab1340ac8f54a357cb9da7,Int. J. Robotics Res.
1466,Production of four-prong final states in photon-photon collisions.,"Results are presented on the exclusive production of four-prong final states in photon-photon collisions from the TPC/Two-Gamma detector at the SLAC e/sup +/e/sup -/ storage ring PEP. Measurement of dE/dx and momentum in the time-projection chamber (TPC) provides identification of the final states 2..pi../sup +/2..pi../sup -/, K/sup +/K/sup -/..pi../sup +/..pi../sup -/, and 2K/sup +/2K/sup -/. For two quasireal incident photons, both the 2..pi../sup +/2..pi../sup -/ and K/sup +/K/sup -/..pi../sup +/..pi../sup -/ cross sections show a steep rise from threshold to a peak value, followed by a decrease at higher mass. Cross sections for the production of the final states rho/sup 0/rho/sup 0/, rho/sup 0/..pi../sup +/..pi../sup -/, and phi..pi../sup +/..pi../sup -/ are presented, together with upper limits for phirho/sup 0/, phiphi, and K/sup *//sup 0/K-bar /sup *//sup 0/. The rho/sup 0/rho/sup 0/ contribution dominates the four-pion cross section at low masses, but falls to nearly zero above 2 GeV. Such behavior is inconsistent with expectations from vector dominance but can be accommodated by four-quark resonance models or by t-channel factorization. Angular distributions for the part of the data dominated by rho/sup 0/rho/sup 0/ final states are consistent with the production of J/sup P/ = 2/sup +/ or 0/sup +/ resonances butmore » also with isotropic (nonresonant) production. When one of the virtual photons has mass (m/sub ..gamma..//sup 2/ = -Q/sup 2/not =0), the four-pion cross section is still dominated by rho/sup 0/rho/sup 0/ at low final-state masses W/sub ..gamma..//sub ..gamma../ and by 2..pi../sup +/2..pi../sup -/ at higher mass. Further, the dependence of the cross section on Q/sup 2/ becomes increasingly flat as W/sub ..gamma..//sub ..gamma../ increases.« less",,https://www.semanticscholar.org/paper/741bad4e4321699d7b5fa5f84acfe4e6eca6d466,"Physical Review D, Particles and fields"
1,Quantifying the Effects of COVID-19 on Restaurant Reviews,"The COVID-19 pandemic has implications beyond physical health, affecting society and economies. Government efforts to slow down the spread of the virus have had a severe impact on many businesses, including restaurants. Mandatory policies such as restaurant closures, bans on social gatherings, and social distancing restrictions have affected restaurant operations as well as customer preferences (e.g., prompting a demand of stricter hygiene standards). As of now, however, it is not clear how and to what extent the pandemic has affected restaurant reviews, an analysis of which could potentially inform policies for addressing this ongoing situation. In this work, we present our efforts to understand the effects of COVID-19 on restaurant reviews, with a focus on Yelp reviews produced during the pandemic for New York City and Los Angeles County restaurants. Overall, we make the following contributions. First, we assemble a dataset of 600 reviews with manual annotations of fine-grained COVID-19 aspects related to restaurants (e.g., hygiene practices, service changes, sympathy and support for local businesses). Second, we address COVID-19 aspect detection using supervised classifiers, weakly-supervised approaches based on keywords, and unsupervised topic modeling approaches, and experimentally show that classifiers based on pre-trained BERT representations achieve the best performance (F1=0.79). Third, we analyze the number and evolution of COVID-related aspects over time and show that the resulting time series have substantial correlation (Spearman’s \rho=0.84) with critical statistics related to the COVID-19 pandemic, including the number of new COVID-19 cases. To our knowledge, this is the first work analyzing the effects of COVID-19 on Yelp restaurant reviews and could potentially inform policies by public health departments, for example, to cover resource utilization.",2021-06-01,https://www.semanticscholar.org/paper/9fafc9896d2b81d1328791e4c2fd7ab096f155f3,International Workshop on Natural Language Processing for Social Media
2620,An extended menu navigation interface using multiple pressure-sensitive strips,"We present extensions and modifications that we have made to a cursorless menu navigation interface that is controlled by multiple pressure-sensitive linear strips. Our approach is based on detecting pressure thresholds and dual-finger motions, allowing us to overcome the physical limitations of the prototype input device and virtually more than triple the number of linear strips that can be used for menu navigation. This allows the user to directly access and navigate in up to fourteen independent multiple-depth menu trees. Our system allows this breadth and depth of navigation without the need for an on-screen cursor, nor the need to navigate a 2D input device, thereby reducing the need for visual feedback. We have tried to design the on-screen graphical interface so that it can be easily used with very small field-of-view display devices, such as eyeglass displays. Additionally, since our input device is physically compact and can be easily positioned on different places on the body, the menu navigation system is especially appropriate for wearable computing systems.",2003-10-21,https://www.semanticscholar.org/paper/8378ff677f0c07b3931d506610a2aeedb89d737d,"Seventh IEEE International Symposium on Wearable Computers, 2003. Proceedings."
3725,Globetrotter: Connecting Languages by Connecting Images,"Machine translation between many languages at once is highly challenging, since training with ground truth re-quires supervision between all language pairs, which is dif-ficult to obtain. Our key insight is that, while languages may vary drastically, the underlying visual appearance of the world remains consistent. We introduce a method that uses visual observations to bridge the gap between languages, rather than relying on parallel corpora or topo-logical properties of the representations. We train a model that aligns segments of text from different languages if and only if the images associated with them are similar and each image in turn is well-aligned with its textual description. We train our model from scratch on a new dataset of text in over fifty languages with accompanying images. Experiments show that our method outperforms previous work on unsupervised word and sentence translation using retrieval. Code, models and data are available on globetrotter.cs.columbia.edu",2020-12-08,https://www.semanticscholar.org/paper/ec19b41534219677864c473a379067d18b3c0908,Computer Vision and Pattern Recognition
3209,On Multifaceted Definitions of Multilevel Societies: Response to Papageorgiou and Farine.,,2020-11-11,https://www.semanticscholar.org/paper/bda325186cbc2d958334ecff1255a06bcbd96168,Trends in Ecology & Evolution
897,The complexity of reliable concurrency control,"We define what it means for a schedule to be reliable, that is, correct in the face of possible transaction failures (assuming that aborting a transaction to restore correctness is not allowed). It turns out that the right definition is recursive, and surprisingly involved. We show that, in fact, testing a schedule for reliability is PSPACE-complete, and thus in some sense even harder than the ordinary NP-complete notions of correctness examined in the past. However, we also prove that all conflict serializable schedules are always reliable, and thus reliability should not be an extra complication for practical concurrency control systems. Finally, we examine two other notions of reliability, related to multiple versions and aborts.",1985-03-25,https://www.semanticscholar.org/paper/a9133f4f0fa0663d33d819fd0f031bf42248924c,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems
801,On the Complexity of Database Queries,"We revisit the issue of the complexity of database queries, in the light of the recent parametric refinement of complexity theory. We show that, if the query size (or the number of variables in the query) is considered as a parameter, then the relational calculus and its fragments (conjunctive queries, positive queries) are classified at appropriate levels of the so-called W hierarchy of Downey and Fellows. These results strongly suggest that the query size is inherently in the exponent of the data complexity of any query evaluation algorithm, with the implication becoming stronger as the expressibility of the query language increases. On the positive side, we show that this exponential dependence can be avoided for the extension of acyclic queries with ? (but not <) inequalities.",1999-06-01,https://www.semanticscholar.org/paper/4f077ff4b673cd6695a2ad5e60fa5fc46f6f843f,Journal of computer and system sciences (Print)
829,On complexity as bounded rationality (extended abstract),"It has been hoped that computational approaches can help resolve some well-known paradoxes in game theory. We prove that tf the repeated prisoner’s dilemma M played by finite automata with less than exponentially (in the number of rounds) many states, then cooperation can be achieved an equilibrium (while with ezponentiaily many states, defection is the only equtlibrzum). We furthermore prove a generalization to arbitrary games and Pareto optimai points. Finally, we present a general model of poiynomially computable games, and characterize in terms of fami!iar complexity classes ranging from NP to NEXP the natural problems that arise in relation with such games. 1. GAMES AND BOUNDED RATIONALITY The theory of games [AH], founded by von Neumann and Morgenstern, is supposed to model the behavior of rational economic agents. Very often, however, game theory predicts behavior that can be criticized as unnatural and nonrational —we describe a famous instance below. It has been hoped that the situation can be remedied if the model is modified to take into account appropriate notions of complexity. This work is a contribution in this line of research. To describe and motivate the results, we shall first need some definitions and",1994-05-23,https://www.semanticscholar.org/paper/0620514c01c0aa861af19e9142864f4088af0535,Symposium on the Theory of Computing
2566,Virtual Vouchers: Prototyping a Mobile Augmented Reality User Interface for Botanical Species Identification,"the tools that botanists require for field-work must evolve and take on new forms. Of particular importance is the ability to identify existing and new species in the field. Mobile augmented reality systems can make it possible to access, view, and inspect a large database of virtual species examples side-by-side with physical specimens. In this paper, we present prototypes of a mobile augmented reality electronic field guide and techniques for displaying and inspecting computer vision-based visual search results in the form of virtual vouchers. Our work addresses head-movement controlled augmented reality for hands-free interaction and tangible augmented reality. We describe results from our design and investigation process and discuss observations and feedback from lab trials by botanists.",2006-03-25,https://www.semanticscholar.org/paper/069d63cd6d9a8d6a68df2949e1904bdea48c88c6,IEEE Symposium on 3D User Interfaces
44,Efficient Keyword Search Across Heterogeneous Relational Databases,"Keyword search is a familiar and potentially effective way to find information of interest that is ""locked"" inside relational databases. Current work has generally assumed that answers for a keyword query reside within a single database. Many practical settings, however, require that we combine tuples from multiple databases to obtain the desired answers. Such databases are often autonomous and heterogeneous in their schemas and data. This paper describes Kite, a solution to the keyword-search problem over heterogeneous relational databases. Kite combines schema matching and structure discovery techniques to find approximate foreign-key joins across heterogeneous databases. Such joins are critical for producing query results that span multiple databases and relations. Kite then exploits the joins - discovered automatically across the databases - to enable fast and effective querying over the distributed data. Our extensive experiments over real-world data sets show that (1) our query processing algorithms are efficient and (2) our approach manages to produce high-quality query results spanning multiple heterogeneous databases, with no need for human reconciliation of the different databases.",2007-04-15,https://www.semanticscholar.org/paper/430d8e89c6466987f637d9a2f3642679de3844ba,IEEE International Conference on Data Engineering
2984,Message Passing Algorithms for the Dirichlet Diffusion Tree,"We demonstrate efficient approximate inference for the Dirichlet Diffusion Tree (Neal, 2003), a Bayesian nonparametric prior over tree structures. Although DDTs provide a powerful and elegant approach for modeling hierarchies they haven’t seen much use to date. One problem is the computational cost of MCMC inference. We provide the first deterministic approximate inference methods for DDT models and show excellent performance compared to the MCMC alternative. We present message passing algorithms to approximate the Bayesian model evidence for a specific tree. This is used to drive sequential tree building and greedy search to find optimal tree structures, corresponding to hierarchical clusterings of the data. We demonstrate appropriate observation models for continuous and binary data. The empirical performance of our method is very close to the computationally expensive MCMC alternative on a density estimation problem, and significantly outperforms kernel density estimators.",,https://www.semanticscholar.org/paper/cf38d26d22c56ceda4779212ee11282716f4ef5f,International Conference on Machine Learning
3187,Stepping Up: A U.S. Perspective on the Ten Steps to Responsible Inland Fisheries,,2021-12-22,https://www.semanticscholar.org/paper/0386c5c438f41ed726c6160df759a532d5f55832,Fisheries
549,Shortest-Path Motion,,1986-12-18,https://www.semanticscholar.org/paper/4ebe2a7b86327b5ccf85ba7d9f093ba8a9bb83aa,Foundations of Software Technology and Theoretical Computer Science
393,Latent Semantic Indexing: A Probabilistic Analysis,"Latent semantic indexing (LSI) is an information retrieval technique based on the spectral analysis of the term-document matrix, whose empirical success had heretofore been without rigorous prediction and explanation. We prove that, under certain conditions, LSI does succeed in capturing the underlying semantics of the corpus and achieves improved retrieval performance. We propose the technique of random projection as a way of speeding up LSI. We complement our theorems with encouraging experimental results. We also argue that our results may be viewed in a more general framework, as a theoretical basis for the use of spectral methods in a wider class of applications such as collaborative filtering.",2000-10-01,https://www.semanticscholar.org/paper/408f9a6585db6f34f9c724b0f5c3600f0e9dab24,Journal of computer and system sciences (Print)
790,Model checking of hierarchical state machines,"Model checking is emerging as a practical tool for detecting logical errors in early stages of system design. We investigate the model checking of hierarchical (nested) systems, i.e. finite state machines whose states themselves can be other machines. This nesting ability is common in various software design methodologies and is available in several commercial modeling tools. The straightforward way to analyze a hierarchical machine is to flatten it (thus, incurring an exponential blow up) and apply a model checking tool on the resulting ordinary FSM. We show that this flattening can be avoided. We develop algorithms for verifying linear time requirements whose complexity is polynomial in the size of the hierarchical machine. We address also the verification of branching time requirements and provide efficient algorithms and matching lower bounds.",2001-05-01,https://www.semanticscholar.org/paper/339b38e9e70ed9a0b55cc3c38385fc3c6f606080,SIGSOFT '98/FSE-6
2349,Neutrophils isolated from the synovial fluid of patients with rheumatoid arthritis: priming and activation in vivo.,"The oxidative metabolism of neutrophils isolated from the bloodstream and synovial fluid of 16 patients with rheumatoid arthritis was compared by measuring the ability of neutrophils to generate luminol dependent chemiluminescence and to secrete O2-. Measurements of receptor mediated--that is, N-formyl-methionyl-leucyl-phenylalanine stimulated--activation or receptor and second message independent--that is phorbol myristate acetate stimulated--activation showed that synovial fluid neutrophils had biochemical characteristics to suggest that they had been either up-regulated (primed) or down-regulated (activated) in vivo. These conclusions were confirmed by comparison of these responses with the changes in oxidative metabolism observed during in vitro priming and activation of control neutrophils: synovial fluid neutrophils possessed lower levels of myeloperoxidase than paired bloodstream cells, and unlike bloodstream cells could not be primed in vitro. These data thus suggest that synovial fluid neutrophils have been exposed to both priming and activating agents within rheumatoid joints.",1991-03-01,https://www.semanticscholar.org/paper/fddd4788538e89229954578529db0c43fe9f459c,Annals of the Rheumatic Diseases
1669,Overdispersed Black-Box Variational Inference,"We introduce overdispersed black-box variational inference, a method to reduce the variance of the Monte Carlo estimator of the gradient in black-box variational inference. Instead of taking samples from the variational distribution, we use importance sampling to take samples from an overdispersed distribution in the same exponential family as the variational approximation. Our approach is general since it can be readily applied to any exponential family distribution, which is the typical choice for the variational approximation. We run experiments on two non-conjugate probabilistic models to show that our method effectively reduces the variance, and the overhead introduced by the computation of the proposal parameters and the importance weights is negligible. We find that our overdispersed importance sampling scheme provides lower variance than black-box variational inference, even when the latter uses twice the number of samples. This results in faster convergence of the black-box inference procedure.",2016-03-03,https://www.semanticscholar.org/paper/fccf3f8fea450a32a5c1f85e113127d70ed9ad7c,Conference on Uncertainty in Artificial Intelligence
2054,An evolutionary approach to rehabilitation patient scheduling: A case study,,2008-09-16,https://www.semanticscholar.org/paper/86e6415148a5703252086032c11dcead3d9ad713,European Journal of Operational Research
3730,Learning to Learn Words from Visual Scenes,,2019-11-25,https://www.semanticscholar.org/paper/007ca8ca7a68451c32da034c72a06238434843c1,European Conference on Computer Vision
845,The Approximation of Maximum Subgraph Problems,,1993-07-05,https://www.semanticscholar.org/paper/fc79e62fdb1f31f04b2590fad5391eed3cd7cd97,"International Colloquium on Automata, Languages and Programming"
2339,Effect of cytotoxic drugs on mature neutrophil function in the presence and absence of granulocyte‐macrophage colony‐stimulating factor,"Summary. The effects of the cytotoxic drugs, adriamycin, cyclophosphamide, daunomycin (daunorubicin), prednisolone, actinomycin D. azacytidine and vincristine at concentrations of 1 μM on mature neutrophil function were examined. Up to 5 h incubation with adriamycin, azacytidine, cyclophosphamide, daunomycin and prednisolone had no effect on either luminol chemiluminescence or superoxide secretion. However, after 15 min or 1 h (but not 5 h) incubation vincristine enhanced fMet‐Leu‐Phe stimulated chemiluminescence, whilst after 5 h incubation with actinomycin D the ability of neutrophils to generate reactive oxidants in response to all stimuli tested was impaired: after 5 h incubation with adriamycin reactive oxidant production was also impaired, but only when fMet‐Leu‐Phe was used as stimulant. All of the drugs tested except azacytidine inhibited neutrophil oxidant production after 5 h incubation in the presence of granulocyte‐macrophage colony‐stimulating factor (GM‐CSF). Actinomycin D and cyclophosphamide also inhibited GM‐CSF stimulated protein biosynthesis. These data indicate that cytotoxic drugs may compromise the potentially beneficial effects of CSFs on mature neutrophil function during therapy.",1993-06-01,https://www.semanticscholar.org/paper/fdc39f9e3b8197890d905e1f38b86b76800167d9,British Journal of Haematology
61,"Proceedings of the Seventh International Workshop on the Web and Databases, WebDB 2004, June 17-18, 2004, Maison de la Chimie, Paris, France, Colocated with ACM SIGMOD/PODS 2004",,,https://www.semanticscholar.org/paper/c88d258d8820bd2c2d227794382b07fa4b933919,International Workshop on the Web and Databases
233,Unsupervised Learning through Prediction in a Model of Cortex,"We propose a primitive called PJOIN, for ""predictive join,"" which combines and extends the operations JOIN and LINK, which Valiant proposed as the basis of a computational theory of cortex. We show that PJOIN can be implemented in Valiant's model. We also show that, using PJOIN, certain reasonably complex learning and pattern matching tasks can be performed, in a way that involves phenomena which have been observed in cognition and the brain, namely memory-based prediction and downward traffic in the cortical hierarchy.",2014-12-26,https://www.semanticscholar.org/paper/b4f00525d7fca5c1cfe2e698683e38de42c56925,arXiv.org
1259,Measurement of the electron charge asymmetry in pp[over ]-->W+X-->enu+X events at sqrt[s]=1.96 TeV.,"We present a measurement of the electron charge asymmetry in pp[over ]-->W+X-->enu+X events at a center of mass energy of 1.96 TeV using 0.75 fb(-1) of data collected with the D0 detector at the Fermilab Tevatron Collider. The asymmetry is measured as a function of the electron transverse momentum and pseudorapidity in the interval (-3.2, 3.2) and is compared with expectations from next-to-leading order calculations in perturbative quantum chromodynamics. These measurements will allow more accurate determinations of the proton parton distribution functions.",,https://www.semanticscholar.org/paper/dc29a175b073c6efb2265886d1bc3a7121c1c2c9,Physical Review Letters
224,On Neural Networks and Paul Spirakis,,,https://www.semanticscholar.org/paper/f65d5b27c5b104781dea6121f3e224088cae1ca1,"Algorithms, Probability, Networks, and Games"
2860,"Galectin-3 Induces Death of Candida Species Expressing Specific β-1,2-Linked Mannans1","Lectins play a critical role in host protection against infection. The galectin family of lectins recognizes saccharide ligands on a variety of microbial pathogens, including viruses, bacteria, and parasites. Galectin-3, a galectin expressed by macrophages, dendritic cells, and epithelial cells, binds bacterial and parasitic pathogens including Leishmania major, Trypanosoma cruzi, and Neisseria gonorrhoeae. However, there have been no reports of galectins having direct effects on microbial viability. We found that galectin-3 bound only to Candida albicans species that bear β-1,2-linked oligomannans on the cell surface, but did not bind Saccharomyces cerevisiae that lacks β-1,2-linked oligomannans. Surprisingly, binding directly induced death of Candida species containing specific β-1,2-linked oligomannosides. Thus, galectin-3 can act as a pattern recognition receptor that recognizes a unique pathogen-specific oligosaccharide sequence. This is the first description of antimicrobial activity for a member of the galectin family of mammalian lectins; unlike other lectins of the innate immune system that promote opsonization and phagocytosis, galectin-3 has direct fungicidal activity against opportunistic fungal pathogens.",2006-10-01,https://www.semanticscholar.org/paper/f4d0dea0115146ad68f373ba78a4c2db6be2fb95,Journal of Immunology
2172,Defective Neutrophil Function in Patients with Sepsis Is Mostly Restored by ex vivo Ascorbate Incubation,"Background Neutrophil function is essential for effective defence against bacterial infections but is defective in patients with sepsis. Ascorbate or vitamin C, which is low in the plasma of patients with sepsis, is stored inside human neutrophils and is essential for their normal function. Objective This study aimed to determine if ascorbate treatment ex vivo improved neutrophil function in patients with sepsis. Patients and Methods Human blood neutrophils were isolated from 20 patients with sepsis and 20 healthy age-matched controls. Neutrophils were incubated with or without ascorbate (1, 5, 10, 20 and 40 mM) for periods up to 2h. Chemotaxis was evaluated using a chemotactic chamber in response to the chemoattractant, fMLP. Phagocytosis (uptake of pHrodo red stained S. aureus) and apoptosis (annexin-V/propidium iodide staining) were measured by flow cytometry. Neutrophil extracellular trap (NET) formation was detected and quantified using DAPI, anti-myeloperoxidase and anti-neutrophil elastase immuno-fluorescence staining. Quantifluor detected the amount of dsDNA in NET supernatants, while quantitative PCR identified changes in expression of PADI4 gene. Results Chemotactic and phagocytic activities were decreased in patients with sepsis but increased after treatment with the high concentrations of ascorbate. Apoptosis was increased in the sepsis patients but not altered by ascorbate treatment. Spontaneous NET formation was observed in patients with sepsis. A quantity of 1mM ascorbate decreased spontaneous NETosis to that of normal, healthy neutrophils, while high concentrations of ascorbate (>10mM) further promoted NET formation. Conclusion Dysregulated neutrophil function was observed in patients with sepsis which could contribute to disease pathology and outcomes. Exposure to ascorbate could reverse some of these changes in function. These novel discoveries raise the possibility that ascorbate treatment could be used as an adjunctive therapy that could result in improved neutrophil function during sepsis.",2020-06-01,https://www.semanticscholar.org/paper/6dbce917dd546b770fd985e8aec1edf17051020e,Journal of Inflammation Research
2418,Using Multi-Level Precueing to Improve Performance in Path-Following Tasks in Virtual Reality,"Work on VR and AR task interaction and visualization paradigms has typically focused on providing information about the current step (a cue) immediately before or during its performance. Some research has also shown benefits to simultaneously providing information about the next step (a precue). We explore whether it would be possible to improve efficiency by precueing information about multiple upcoming steps before completing the current step. To accomplish this, we developed a remote VR user study comparing task completion time and subjective metrics for different levels and styles of precueing in a path-following task. Our visualizations vary the precueing level (number of steps precued in advance) and style (whether the path to a target is communicated through a line to the target, and whether the place of a target is communicated through graphics at the target). Participants in our study performed best when given two to three precues for visualizations using lines to show the path to targets. However, performance degraded when four precues were used. On the other hand, participants performed best with only one precue for visualizations without lines, showing only the places of targets, and performance degraded when a second precue was given. In addition, participants performed better using visualizations with lines than ones without line",2021-08-27,https://www.semanticscholar.org/paper/1be8fef777f6499ca5b9f44eca322b5628acaa73,IEEE Transactions on Visualization and Computer Graphics
1800,Dirichlet Process Mixtures of Generalized Linear Models,"We propose Dirichlet Process mixtures of Generalized Linear Models (DP-GLM), a new class of methods for nonparametric regression. Given a data set of input-response pairs, the DP-GLM produces a global model of the joint distribution through a mixture of local generalized linear models. DP-GLMs allow both continuous and categorical inputs, and can model the same class of responses that can be modeled with a generalized linear model. We study the properties of the DP-GLM, and show why it provides better predictions and density estimates than existing Dirichlet process mixture regression models. We give conditions for weak consistency of the joint distribution and pointwise consistency of the regression estimate.",2009-09-28,https://www.semanticscholar.org/paper/49be9a3bd3d59a1ff2399213594af003eec82d45,Journal of machine learning research
177,Random Projection in the Brain and Computation with Assemblies of Neurons,"9 It has been recently shown via simulations [7] that random projection followed by a cap operation 10 (setting to one the k largest elements of a vector and everything else to zero), a map believed 11 to be an important part of the insect olfactory system, has strong locality sensitivity properties. 12 We calculate the asymptotic law whereby the overlap in the input vectors is conserved, verify13 ing mathematically this empirical finding. We then focus on the far more complex homologous 14 operation in the mammalian brain, the creation through successive projections and caps of an 15 assembly (roughly, a set of excitatory neurons representing a memory or concept) in the presence 16 of recurrent synapses and plasticity. After providing a careful definition of assemblies, we prove 17 that the operation of assembly projection converges with high probability, over the randomness 18 of synaptic connectivity, even if plasticity is relatively small (previous proofs relied on high plas19 ticity). We also show that assembly projection has itself some locality preservation properties. 20 Finally, we propose a large repertoire of assembly operations, including associate, merge, recip21 rocal project, and append, each of them both biologically plausible and consistent with what we 22 know from experiments, and show that this computational system is capable of simulating, again 23 with high probability, arbitrary computation in a quite natural way. We hope that this novel way 24 of looking at brain computation, open-ended and based on reasonably mainstream ideas in neu25 roscience, may prove an attractive entry point for computer scientists to work on understanding 26 the brain. 27 2012 ACM Subject Classification Dummy classification 28",,https://www.semanticscholar.org/paper/fc9ade6f7ebe9afb9884ca4830b4d6cbc79145fc,Information Technology Convergence and Services
1712,Smoothed Gradients for Stochastic Variational Inference,"Stochastic variational inference (SVI) lets us scale up Bayesian computation to massive data. It uses stochastic optimization to fit a variational distribution, following easy-to-compute noisy natural gradients. As with most traditional stochastic optimization methods, SVI takes precautions to use unbiased stochastic gradients whose expectations are equal to the true gradients. In this paper, we explore the idea of following biased stochastic gradients in SVI. Our method replaces the natural gradient with a similarly constructed vector that uses a fixed-window moving average of some of its previous terms. We will demonstrate the many advantages of this technique. First, its computational cost is the same as for SVI and storage requirements only multiply by a constant factor. Second, it enjoys significant variance reduction over the unbiased estimates, smaller bias than averaged gradients, and leads to smaller mean-squared error against the full gradient. We test our method on latent Dirichlet allocation with three large corpora.",2014-06-13,https://www.semanticscholar.org/paper/b21fd95f438792add77d1d35132a01b895648c17,Neural Information Processing Systems
370,Special Issue on PODS 1999 - Guest Editors' Foreword,,,https://www.semanticscholar.org/paper/5c29b127932fc3d6ed0ebafda5490444149bb293,Journal of computer and system sciences (Print)
654,Acute angiographic and clinical results of long balloon percutaneous transluminal coronary angioplasty and adjuvant stenting for long narrowings.,,1994-04-01,https://www.semanticscholar.org/paper/e9cd4fc91b9d1285744574d07167b3af0581c601,American Journal of Cardiology
1785,The IBP Compound Dirichlet Process and its Application to Focused Topic Modeling,"The hierarchical Dirichlet process (HDP) is a Bayesian nonparametric mixed membership model—each data point is modeled with a collection of components of different proportions. Though powerful, the HDP makes an assumption that the probability of a component being exhibited by a data point is positively correlated with its proportion within that data point. This might be an undesirable assumption. For example, in topic modeling, a topic (component) might be rare throughout the corpus but dominant within those documents (data points) where it occurs. We develop the IBP compound Dirichlet process (ICD), a Bayesian nonparametric prior that decouples across-data prevalence and within-data proportion in a mixed membership model. The ICD combines properties from the HDP and the Indian buffet process (IBP), a Bayesian nonparametric prior on binary matrices. The ICD assigns a subset of the shared mixture components to each data point. This subset, the data point's ""focus"", is determined independently from the amount that each of its components contribute. We develop an ICD mixture model for text, the focused topic model (FTM), and show superior performance over the HDP-based topic model.",2010-06-21,https://www.semanticscholar.org/paper/a0bcdf26fcd382dd54ac38be07e6b33179bd52f4,International Conference on Machine Learning
97,An investigation of linguistic features and clustering algorithms for topical document clustering,"We investigate four hierarchical clustering methods (single-link, complete-link, groupwise-average, and single-pass) and two linguistically motivated text features (noun phrase heads and proper names) in the context of document clustering. A statistical model for combining similarity information from multiple sources is described and applied to DARPA's Topic Detection and Tracking phase 2 (TDT2) data. This model, based on log-linear regression, alleviates the need for extensive search in order to determine optimal weights for combining input features. Through an extensive series of experiments with more than 40,000 documents from multiple news sources and modalities, we establish that both the choice of clustering algorithm and the introduction of the additional features have an impact on clustering performance. We apply our optimal combination of features to the TDT2 test data, obtaining partitions of the documents that compare favorably with the results obtained by participants in the official TDT2 competition.",2000-07-01,https://www.semanticscholar.org/paper/36609db38881b34a896f0cca58f9942f451e4815,Annual International ACM SIGIR Conference on Research and Development in Information Retrieval
969,Effect of Ubiquinol on Glaucomatous Neurodegeneration and Oxidative Stress: Studies for Retinal Ganglion Cell Survival and/or Visual Function,"Oxidative stress is one of major causal factors in glaucomatous neurodegeneration. Ubiquinol promotes retinal ganglion cell (RGC) survival against glaucomatous insults such as oxidative stress. Here we investigated the effect of ubiquinol on RGC survival and/or visual function in mouse models of glaucoma and oxidative stress. DBA/2J and age-matched DBA/2J-Gpnmb+ (D2-Gpnmb+), which do not develop intraocular pressure elevation, or C57BL/6J mice were fed with ubiquinol (1%) or control diet daily for 5 or 2 months. We assessed RGC survival by Brn3a immunohistochemistry and measured expression levels of active and total BAX, peroxisome proliferator-activated receptor-gamma coactivator 1α, transcription factor A (TFAM) and oxidative phosphorylation (OXPHOS) complex protein. Following induction of oxidative stress by paraquat injection, we also assessed visual function. In glaucomatous retina, ubiquinol supplementation significantly promoted RGC survival, blocked BAX activation and increased TFAM and OXPHOS complex II protein expression. Also, ubiquinol supplementation ameliorated oxidative stress-induced visual dysfunction. These findings indicate that ubiquinol promotes RGC survival by increasing TFAM expression and OXPHOS complex II activity in glaucomatous neurodegeneration, and that ubiquinol enhances RGC survival and preserves visual function against oxidative stress. We propose that ubiquinol has a therapeutic potential for treating oxidative stress-associated glaucomatous neurodegeneration.",2020-10-01,https://www.semanticscholar.org/paper/f134e2b5450fbce65c7ce6dd4ae934a774f35b11,Antioxidants
2424,XREye: Simulating Visual Impairments in Eye-Tracked XR,"Many people suffer from visual impairments, which can be difficult for patients to describe and others to visualize. To aid in understanding what people with visual impairments experience, we demonstrate a set of medically informed simulations in eye-tracked XR of several common conditions that affect visual perception: refractive errors (myopia, hyperopia, and presbyopia), cornea disease, and age-related macular degeneration (wet and dry).",2020-03-01,https://www.semanticscholar.org/paper/494357c63cd2a38d503ef6ea2e53216b0ce9ae19,2020 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)
705,Smoothed complexity of local max-cut and binary max-CSP,"We show that the smoothed complexity of the FLIP algorithm for local Max-Cut is at most φ n O(√logn), where n is the number of nodes in the graph and φ is a parameter that measures the magnitude of perturbations applied on its edge weights. This improves the previously best upper bound of φ n O(logn) by Etscheid and Roglin. Our result is based on an analysis of long sequences of flips, which shows that it is very unlikely for every flip in a long sequence to incur a positive but small improvement in the cut weight. We also extend the same upper bound on the smoothed complexity of FLIP to all binary Maximum Constraint Satisfaction Problems.",2019-11-23,https://www.semanticscholar.org/paper/83228979c513f79efea340bb7ae4820faa730276,Symposium on the Theory of Computing
2964,A reversible infinite HMM using normalised random measures,"We present a nonparametric prior over reversible Markov chains. We use completely random measures, specifically gamma processes, to construct a countably infinite graph with weighted edges. By enforcing symmetry to make the edges undirected we define a prior over random walks on graphs that results in a reversible Markov chain. The resulting prior over infinite transition matrices is closely related to the hierarchical Dirichlet process but enforces reversibility. A reinforcement scheme has recently been proposed with similar properties, but the de Finetti measure is not well characterised. We take the alternative approach of explicitly constructing the mixing measure, which allows more straightforward and efficient inference at the cost of no longer having a closed form predictive distribution. We use our process to construct a reversible infinite HMM which we apply to two real datasets, one from epigenomics and one ion channel recording.",2014-03-17,https://www.semanticscholar.org/paper/7cdcc054fa7abbb06e5f5d18faebbce90df95fc0,International Conference on Machine Learning
2698,Architectural Anatomy,"We provide an overview of the early stages of three related research projects whose goals are to exploit augmented reality, virtual worlds, and artificial intelligence to explore relationships between perceived architectural space and the structural systems that support it. In one project, we use a see-through head-mounted display to overlay a graphic representation of a building's structural systems on the user's view of a room within the building. This overlaid virtual world shows the out-lines of the concrete joists, beams, and columns surrounding the room, as well as the reinforcing steel inside them, and includes displays from a commercially available structural analysis program. In a related project, the structural view is exposed by varying the opacity of room finishes and concrete in a 3D model of the room and surrounding structure rendered on a conventional CRT. We also describe a hypermedia database, currently under construction, depicting major, twentieth-century American buildings. The interactive, multidisciplinary elements of the database—including structural and thermal analyses, free body diagrams (which show how forces are resisted by portions of a structure under various loading conditions), facsimiles of construction documents, and critical essays—are bound together and made available over the World-Wide Web. Finally, we discuss the relationships among all these projects, and their potential applications to teaching architecture students and to construction, assembly, and repair of complex structures.",,https://www.semanticscholar.org/paper/3ee68de062826570530cd53d2f727054d7f57a5a,Presence: Teleoperators & Virtual Environments
575,On Two Geometric Problems Related to the Traveling Salesman Problem,,1984-06-01,https://www.semanticscholar.org/paper/6ca8df6bb1f5b54b6889f4c003c0ece405b3d573,J. Algorithms
1737,Efficient Online Inference for Bayesian Nonparametric Relational Models,"Stochastic block models characterize observed network relationships via latent community memberships. In large social networks, we expect entities to participate in multiple communities, and the number of communities to grow with the network size. We introduce a new model for these phenomena, the hierarchical Dirichlet process relational model, which allows nodes to have mixed membership in an unbounded set of communities. To allow scalable learning, we derive an online stochastic variational inference algorithm. Focusing on assortative models of undirected networks, we also propose an efficient structured mean field variational bound, and online methods for automatically pruning unused communities. Compared to state-of-the-art online learning methods for parametric relational models, we show significantly improved perplexity and link prediction accuracy for sparse networks with tens of thousands of nodes. We also showcase an analysis of Little-Sis, a large network of who-knows-who at the heights of business and government.",2013-12-05,https://www.semanticscholar.org/paper/91608e4567959f331f3b231f4f8fb5e33ce71a15,Neural Information Processing Systems
3015,Microservices and Containers,"The articles in this special section focus on microservices and containers. These services allow an application to be comprised of many independently operating and scalable components, have become a common service paradigm. The ability to construct an application by provisioning these interoperating components has various advantages, including the isolation and independent development of tools such as key-value stores, authentication, logging, and many others. Containers are one type of system infrastructure that is commonly used to support microservices. With container management systems like Docker and orchestration systems like Kubernetes to control applications and dynamically provision their resources, cloud services can be extremely scalable, reliable, and reactive. However, other systems beyond containers can be used to support microservices, and many applications other than microservices benefit from containerization.",2019-11-01,https://www.semanticscholar.org/paper/480846bb2752dbdbaa3382c33c82b8affb1e5437,IEEE Internet Computing
911,Algebraic Dependencies,,,https://www.semanticscholar.org/paper/3fd4adf51111cc0b126b2ae5353e66b7cef80330,Journal of computer and system sciences (Print)
1956,Inventory survival analysis for semiconductor memory manufacturing,"The high variety of and intermittent demand for semiconductor memory products frequently limits the use of forecast error normalization in estimating inventory. Inventory turnover is a practical performance indicator that is used to calculate the number of days for which a company retains inventory before selling a product. Although previous studies on inventory level settings have primarily applied information regarding demand variability and forecast error, few studies have investigated the inventory turnover for inventory decisions. Inventory turnover data are time scaled, suited for a small sample, and right censored to fit the input of survival analysis. In this study, a model in which inventory turnover and survival analysis were integrated was developed to estimate the production inventory survival function used to determine inventory level. Data analysis results based on real settings indicated the viability of using inventory survival analysis to determine semiconductor memory inventory level settings.",2014-12-07,https://www.semanticscholar.org/paper/441fa7fb9ed08e278268761d0e6aec4adff23714,Proceedings of the Winter Simulation Conference 2014
1541,Conformal Sensitivity Analysis for Individual Treatment Effects,"Estimating an individual treatment effect (ITE) is essential to personalized decision making. However, existing methods for estimating the ITE often rely on unconfoundedness, an assumption that is fundamentally untestable with observed data. To assess the robustness of individual-level causal conclusion with unconfoundedness, this paper proposes a method for sensitivity analysis of the ITE, a way to estimate a range of the ITE under unobserved confounding. The method we develop quantifies unmeasured confounding through a marginal sensitivity model [Ros2002, Tan2006], and adapts the framework of conformal inference to estimate an ITE interval at a given confounding strength. In particular, we formulate this sensitivity analysis problem as a conformal inference problem under distribution shift, and we extend existing methods of covariate-shifted conformal inference to this more general setting. The result is a predictive interval that has guaranteed nominal coverage of the ITE, a method that provides coverage with distribution-free and nonasymptotic guarantees. We evaluate the method on synthetic data and illustrate its application in an observational study.",2021-12-07,https://www.semanticscholar.org/paper/5645f1cc9c3c86091b1eeb58150a68e4f062de27,Journal of the American Statistical Association
2654,A study of communication in the Cardiac Surgery Intensive Care Unit and its implications for automated briefing,"We present a study of the information transferred among caregivers in the context of cardiac surgery and use the study to evaluate a system, MAGIC, that we are developing for automated generation of briefings. Our framework integrates cognitive and quantitative evaluation methods and features three standards that reflect current practice in the Cardiothoracic Intensive Care Unit (CTICU). Using experimental design to compare human-generated and machine-generated briefings, we show that MAGIC's current level of performance is useful. Moreover, MAGIC could help improve information flow in the CTICU by providing a consistent set of information earlier than in current practice. The separate standards are also consistent in suggesting specific modifications that may be necessary for iterative design and further system development.",,https://www.semanticscholar.org/paper/a43b08dab508794496b0eb12d4275be5bec0d7b5,American Medical Informatics Association Annual Symposium
2025,A conceptual methodology of “industrial engineering” for “the industry as a whole”: Semiconductor industry as illustration,"The discipline of industrial engineering has been declining in many countries in light of the changes of industry structures in developed countries, in which the competition is no longer among individual companies while the collaboration among horizontally specialized value providers are critical for the success of the individual companies as well as the whole supply chain. There should be a systematic methodology of “industrial engineering” that focuses on “industry as a whole” as the subject of study to differentiate our discipline from others such as electrical engineering or chemical engineering. Focusing on semiconductor industry as the specific subject, this study aims to propose a conceptual methodology of industrial engineering to investigate semiconductor industry. In particular, semiconductor industry is one of the most complicated and capital-intensive industries. Driven by Moore's Law, semiconductor industry has a clock speed faster than other industries and thus can provide an important benchmark for other industries. In the presentation, we propose a framework to explain the evolution of the semiconductor industry from the point of view of modularity and integration driven by technical and economical considerations to simplify complex production problems. Several case studies of the companies in different positions of the semiconductor value chain are illustrated to discuss some of the challenges and ongoing changes. TSMC established a pure wafer foundry business model in 1987 that assumes all the costs of capital expenditure and expenses in wafer fabrication never competes with its clients of fabless design houses and IDMs. The pure foundry can easily scale up or down its production capacity according to an individual customer's needs, while maximizing fab utilization with a portfolio of various customers. Such a business model freed fabless and IDM designers from the burden of capital investment for advanced technology capacity. Thus, IC designers can concentrate on chip design for various applications including PC and consumer electronics. Meanwhile, the semiconductor industry is moving to more narrow specialization such as Ardentec that focuses on the middle layer of wafer sort between front-end of IC design and wafer fabrication and backend of IC packaging and final testing. Nevertheless, Global Unichip Corporation that positions itself as a design foundry focusing on SoC (System on a Chip) is trying to virtually integrate the supply chain to deal with technical challenges driven by Moore's Law and technological inseparability involved in SoC and SiP (System in Package). This talk will conclude with discussions of the implications of semiconductor industry evolution to computers and industrial engineering research.",2010-07-25,https://www.semanticscholar.org/paper/5bec55d7da3bc89fdb3c5e6316f4408ea1604a50,The 40th International Conference on Computers & Indutrial Engineering
1679,Bayesian Poisson Tensor Factorization for Inferring Multilateral Relations from Sparse Dyadic Event Counts,"We present a Bayesian tensor factorization model for inferring latent group structures from dynamic pairwise interaction patterns. For decades, political scientists have collected and analyzed records of the form ""country i took action a toward country j at time t"" - known as dyadic events - in order to form and test theories of international relations. We represent these event data as a tensor of counts and develop Bayesian Poisson tensor factorization to infer a low-dimensional, interpretable representation of their salient patterns. We demonstrate that our model's predictive performance is better than that of standard non-negative tensor factorization methods. We also provide a comparison of our variational updates to their maximum likelihood counterparts. In doing so, we identify a better way to form point estimates of the latent factors than that typically used in Bayesian Poisson matrix factorization. Finally, we showcase our model as an exploratory analysis tool for political scientists. We show that the inferred latent factor matrices capture interpretable multilateral relations that both conform to and inform our knowledge of international a airs.",2015-06-10,https://www.semanticscholar.org/paper/4ddb48f5e50ecf3595a0ef2517d36d077f968e25,Knowledge Discovery and Data Mining
2708,Research issues in perception and user interfaces,"Current visualization tools are capable but still require too much visualization knowledge on the user's part. This requirement restricts the user in what is possible. Nor do the tools take account of what is known regarding cognition and perception. The authors focus at on three things: presentation of information to best match human cognitive and perceptual capabilities, interactive tools and systems to facilitate creation and navigation of visualizations, and software system features to improve visualization tools.<<ETX>>",1994-03-01,https://www.semanticscholar.org/paper/bb12239125e436f3d4e4bd4864c4a653cae49c4e,IEEE Computer Graphics and Applications
3425,Feasible and Accurate Algorithms for Covering Semidefinite Programs,,2010-06-21,https://www.semanticscholar.org/paper/b47164586799d8a37b72095c98d4120fa92759d2,Scandinavian Workshop on Algorithm Theory
2845,Galectin-3 protects keratinocytes from UVB-induced apoptosis by enhancing AKT activation and suppressing ERK activation.,,2008-10-01,https://www.semanticscholar.org/paper/fde83c08779971b1b2abe8ceb25a6811d9303c95,Journal of Investigative Dermatology
2583,Multi-monitor mouse,"Multiple-monitor computer configurations significantly increase the distances that users must traverse with the mouse when interacting with existing applications, resulting in increased time and effort. We introduce the Multi-Monitor Mouse (M3) technique, which virtually simulates having one mouse pointer per monitor when using a single physical mouse device. M3 allows for conventional control of the mouse within each monitor's screen, while permitting immediate warping across monitors when desired to increase mouse traversal speed. We report the results of a user study in which we compared three implementations of M3 and two cursor placement strategies. Our results suggest that using M3 significantly increases interaction speed in a multi-monitor environment. All eight study participants strongly preferred M3 to the regular mouse behavior.",2005-04-02,https://www.semanticscholar.org/paper/49127d30b7f62986b675f905f17ed28de0da4a65,CHI Extended Abstracts
66,QProber: A system for automatic classification of hidden-Web databases,"The contents of many valuable Web-accessible databases are only available through search interfaces and are hence invisible to traditional Web ""crawlers."" Recently, commercial Web sites have started to manually organize Web-accessible databases into Yahoo!-like hierarchical classification schemes. Here we introduce QProber, a modular system that automates this classification process by using a small number of query probes, generated by document classifiers. QProber can use a variety of types of classifiers to generate the probes. To classify a database, QProber does not retrieve or inspect any documents or pages from the database, but rather just exploits the number of matches that each query probe generates at the database in question. We have conducted an extensive experimental evaluation of QProber over collections of real documents, experimenting with different types of document classifiers and retrieval models. We have also tested our system with over one hundred Web-accessible databases. Our experiments show that our system has low overhead and achieves high classification accuracy across a variety of databases.",,https://www.semanticscholar.org/paper/5e13887f269b61c14142b6286665579abe5a9f45,TOIS
1873,UNISON decision framework for hybrid optimization of wastewater treatment and recycle for Industry 3.5 and cleaner semiconductor manufacturing,,,https://www.semanticscholar.org/paper/18e16a5158e9c8eedb7c611a41a79496ef976040,"Resources, Conservation and Recycling"
121,dSCAM: finding document copies across multiple databases,"The advent of the Internet has made the illegal dissemination of copyrighted material easy. An important problem is how to automatically detect when a ""new"" digital document is ""suspiciously close"" to existing ones. The SCAM project at Stanford University has addressed this problem when there is a single registered-document database. However, in practice, test documents may appear in many autonomous databases, and one would like to discover copies without having to exhaustively search in all databases. The authors' approach, dSCAM, is a distributed version of SCAM that keeps succinct metainformation about the contents of the available document databases. Given a suspicious document S, dSCAM uses its information to prune all databases that cannot contain any document that is close enough to S, and hence the search can focus on the remaining sites. They also study how to query the remaining databases so as to minimize different querying costs. They empirically study the pruning and searching schemes, using a collection of 50 databases and two sets of test documents.",1996-12-01,https://www.semanticscholar.org/paper/44e832f182dd508587d6abed2e61cad8061c68c0,Fourth International Conference on Parallel and Distributed Information Systems
773,Realizability and verification of MSC graphs,,2005-02-15,https://www.semanticscholar.org/paper/bff6c72f7826ad4831c0b65e6ed322165ee5504d,Theoretical Computer Science
1018,Guided Deep Reinforcement Learning for Articulated Swimming Robots,"Deep reinforcement learning has recently been applied to a variety of robotics applications, but learning locomotion for robots with unconventional configurations is still limited. Prior work has shown that, despite the simple modeling of articulated swimmer robots, such systems struggle to find effective gaits using reinforcement learning due to the heterogeneity of the search space. In this work, we leverage insight from geometric models of these robots in order to focus on promising regions of the space and guide the learning process. We demonstrate that our augmented learning technique is able to produce gaits for different learning goals for swimmer robots in both low and high Reynolds number fluids.",2023-01-30,https://www.semanticscholar.org/paper/798c599b3ff4ac4392189ec162574400733f32af,arXiv.org
23,Hip and trendy: Characterizing emerging trends on Twitter,"Twitter, Facebook, and other related systems that we call social awareness streams are rapidly changing the information and communication dynamics of our society. These systems, where hundreds of millions of users share short messages in real time, expose the aggregate interests and attention of global and local communities. In particular, emerging temporal trends in these systems, especially those related to a single geographic area, are a significant and revealing source of information for, and about, a local community. This study makes two essential contributions for interpreting emerging temporal trends in these information systems. First, based on a large dataset of Twitter messages from one geographic area, we develop a taxonomy of the trends present in the data. Second, we identify important dimensions according to which trends can be categorized, as well as the key distinguishing features of trends that can be derived from their associated messages. We quantitatively examine the computed features for different categories of trends, and establish that significant differences can be detected across categories. Our study advances the understanding of trends on Twitter and other social awareness streams, which will enable powerful applications and activities, including user-driven real-time information services for local communities. © 2011 Wiley Periodicals, Inc.",2011-05-01,https://www.semanticscholar.org/paper/aedca557e79d36d4c3f81f76a247a98f1cfffcc4,J. Assoc. Inf. Sci. Technol.
932,Equivalences Among Relational Expressions with the Union and Difference Operators,"Queries in relational databases can be formulated in terms of relational expressions using the relational operations select, project, join, union, and difference The equivalence problem for these queries is studied with query optimization m mind It ts shown that testmg eqmvalence of relational expressions with the operators select, project, join, and union is complete m the class FIt of the polynomial-time hierarchy A nonprocedural representation for queries formulated by these expressions is proposed This method of query representation can be viewed as a generahzatlon of tableaux or conjunctive queries (which are used to represent expressions with only select, project, and join) Furthermore, this method is extended to queries formulated by relatmnal expressions that also contain the difference operator, provided that the project operator is not applied to subexpresstons with the difference operator A procedure for testing eqmvalence of these queries is given It ts shown that testmg containment of tableaux is a necessary step in testing equivalence of queries with union and difference Three important cases m which containment of tableaux can be tested m polynomial time are described, although the containment problem is shown to be NP-complete even for tableaux that correspond to expressions with only one project and several join operators",1980-10-01,https://www.semanticscholar.org/paper/4e60a72a0b58f62b405ab5eb43b184f5fff77710,Journal of the ACM
722,"Greatest Fixed Points of Probabilistic Min/Max Polynomial Equations, and Reachability for Branching Markov Decision Processes",,2015-02-19,https://www.semanticscholar.org/paper/00de2315d1880833ec33435f3fd919d6b13e44ff,"International Colloquium on Automata, Languages and Programming"
854,Expressing combinatorial optimization problems by linear programs,"Many combinatorial optimization problems call for the optimization of a linear function over a certain polytope. Typically, these polytopes have an exponential number of facets. We explore the problem of finding small linear programming formulations when one may use any new variables and constraints. We show that expressing the matching and the Traveling Salesman Problem by a symmetric linear program requires exponential size. We relate the minimum size needed by a LP to express a polytope to a combinatorial parameter, point out some connections with communication complexity theory, and examine the vertex packing polytope for some classes of graphs.",1991-12-01,https://www.semanticscholar.org/paper/0a6219ca8fef322c6bf7b1ff439635cfcf096282,Symposium on the Theory of Computing
1839,Statistical modeling of biomedical corpora: mining the Caenorhabditis Genetic Center Bibliography for genes related to life span,,,https://www.semanticscholar.org/paper/2205bc21bf3fb324bb96fad5c32c3e2cfd7304ee,BMC Bioinformatics
2724,Fast object-precision shadow generation for area light sources using BSP trees,"This paper introduces an efficient object-precision shadow generation algorithm for static polygonal environments directly illuminated by convex area light sources. Penumbra and umbra regions are calculated analytically and represented as a pair of BSP trees for each light source. As the trees are built, convex scene polygons are filtered down the trees, and split into fragments that are wholly lit, in penumbra, or in umbra. The illumination due to the light source is calculated at selected points within the wholly lit and penumbra regions by contour integration with the visible parts of the light source. We use a fast analytic algorithm to compute the fragments of the area light source visible from a point in penumbra. Rendering is done using hardwaresupported linear interpblated shading on a 3D graphics workstation. Because the scene itself is represented as a BSP tree, visible-surface determination may be performed by using either workstation-supported hardware (e.g., a z-buffer) or software BSP-tree traversal. We provide sample images created by our implementation, including timings and polygon counts. CR",1992-06-01,https://www.semanticscholar.org/paper/04d74e92025d68adc923b084b25bd9f58777d557,ACM Symposium on Interactive 3D Graphics and Games
3678,Humans as Light Bulbs: 3D Human Reconstruction from Thermal Reflection,"The relatively hot temperature of the human body causes people to turn into long-wave infrared light sources. Since this emitted light has a larger wavelength than visible light, many surfaces in typical scenes act as infrared mirrors with strong specular reflections. We exploit the thermal reflections of a person onto objects in order to locate their position and reconstruct their pose, even if they are not visible to a normal camera. We propose an analysis-by-synthesis framework that jointly models the objects, people, and their thermal reflections, which combines generative models with differentiable rendering of reflections. Quantitative and qualitative experiments show our approach works in highly challenging cases, such as with curved mirrors or when the person is completely unseen by a normal camera.",2023-05-02,https://www.semanticscholar.org/paper/16a122b84bab5e61775869967b1822b71dd453a9,Computer Vision and Pattern Recognition
2162,Phenotypic and functional changes of T cell subsets after CoronaVac vaccination,,2022-10-01,https://www.semanticscholar.org/paper/09df568859721ea62b3098174bcb0a6d1b8be6e3,Vaccine
3013,Optimizing Nested Virtualization Performance Using Direct Virtual Hardware,"Nested virtualization, running virtual machines and hypervisors on top of other virtual machines and hypervisors, is increasingly important because of the need to deploy virtual machines running software stacks on top of virtualized cloud infrastructure. However, performance remains a key impediment to further adoption as application workloads can perform many times worse than native execution. To address this problem, we introduce DVH (Direct Virtual Hardware), a new approach that enables a host hypervisor, the hypervisor that runs directly on the hardware, to directly provide virtual hardware to nested virtual machines without the intervention of multiple levels of hypervisors. We introduce four DVH mechanisms, virtual-passthrough, virtual timers, virtual inter-processor interrupts, and virtual idle. DVH provides virtual hardware for these mechanisms that mimics the underlying hardware and in some cases adds new enhancements that leverage the flexibility of software without the need for matching physical hardware support. We have implemented DVH in the Linux KVM hypervisor. Our experimental results show that DVH can provide near native execution speeds and improve KVM performance by more than an order of magnitude on real application workloads.",2020-03-09,https://www.semanticscholar.org/paper/af687ee53dbbfc35eeec2e6e66f37587cad9ed7d,International Conference on Architectural Support for Programming Languages and Operating Systems
1912,An empirical study of demand forecasting of non-volatile memory for smart production of semiconductor manufacturing,"As high-speed computing is crucial to empower intelligent manufacturing for Industry 4.0, non-volatile memory (NVM) is critical semiconductor component of the cloud and data centre for the infrastructures. The NVM manufacturing is capital intensive, in which capacity utilisation significantly affects the capital effectiveness and profitability of semiconductor companies. Since capacity migration and expansion involve long lead times, demand forecasting plays a critical role for smart production of NVM manufacturers for revenue management. However, the shortening product life cycles of integrated circuits (IC), the fluctuations of semiconductor supply chains, and uncertainty involved in demand forecasting make the present problem increasingly difficult in the consumer electronics era. Focusing on the realistic needs of NVM demand forecasting, this study aims to develop a decision framework that integrates an improved technology diffusion model and a proposed adjustment mechanism to incorporate domain insights. An empirical study was conducted in a leading semiconductor company for validation. A comparison of alternative approaches is also provided. The results have shown the practical viability of the proposed approach.",2018-01-09,https://www.semanticscholar.org/paper/810c89a120886d8781bab6788603f35ce622f8be,International Journal of Production Research
2928,Active Learning in CNNs via Expected Improvement Maximization,"Deep learning models such as Convolutional Neural Networks (CNNs) have demonstrated high levels of effectiveness in a variety of domains, including computer vision and more recently, computational biology. However, training effective models often requires assembling and/or labeling large datasets, which may be prohibitively time-consuming or costly. Pool-based active learning techniques have the potential to mitigate these issues, leveraging models trained on limited data to selectively query unlabeled data points from a pool in an attempt to expedite the learning process. Here we present ""Dropout-based Expected IMprOvementS"" (DEIMOS), a flexible and computationally-efficient approach to active learning that queries points that are expected to maximize the model's improvement across a representative sample of points. The proposed framework enables us to maintain a prediction covariance matrix capturing model uncertainty, and to dynamically update this matrix in order to generate diverse batches of points in the batch-mode setting. Our active learning results demonstrate that DEIMOS outperforms several existing baselines across multiple regression and classification tasks taken from computer vision and genomics.",2020-11-27,https://www.semanticscholar.org/paper/5af1dab052d40a938c8a8112844783264f1c2af5,arXiv.org
1972,"User-experience of tablet operating system: An experimental investigation of Windows 8, iOS 6, and Android 4.2",,2014-07-01,https://www.semanticscholar.org/paper/f8451d64eeb352e099a60fae126e84e7fbe813d6,Computers & industrial engineering
381,"Algorithms, games, and the internet","If the Internet is the next great subject for Theoretical Computer Science to model and illuminate mathematically, then Game Theory, and Mathematical Economics more generally, are likely to prove useful tools. In this talk I survey some opportunities and challenges in this important frontier.",2001-07-06,https://www.semanticscholar.org/paper/2434a75d334b10f2eadddb80bb6cbe7af9f19bae,Symposium on the Theory of Computing
2052,Modeling and analysis of semiconductor manufacturing in a shrinking world: Challenges and successes,"A panel session on the role of modeling and analysis in semiconductor manufacturing in a shrinking world is presented. Therefore, two participants are from Asia, two from Europe, and two from US and there are two panel organizers/ moderators (Fowler and Monch). One panelist from each continent is from industry and one from academia. Only initial position statements are included in the proceedings. However, these initial statements form the basis for the panel discussion. The statements of the panelists from industry relate to modeling and analysis problems found in their own companies. The position statements of the panelists from academia describe the role that modeling and analysis is expected to play in their current and ongoing research in semiconductor manufacturing. Furthermore, their views on the challenges and successes of modeling and analysis in a globalized world are also included.",2008-12-01,https://www.semanticscholar.org/paper/73e98a6f781d522b95d536c935fcc8409dbaa413,Online World Conference on Soft Computing in Industrial Applications
3731,Learning to Learn Words from Narrated Video,"When we travel, we often encounter new scenarios we have never experienced before, with new sights and new words that describe them. We can use our language-learning ability to quickly learn these new words and correlate them with the visual world. In contrast, language models often do not robustly generalize to novel words and compositions. We propose a framework that learns how to learn text representations from visual context. Experiments show that our approach significantly outperforms the state-of-the-art in visual language modeling for acquiring new words and predicting new compositions. Model ablations and visualizations suggest that the visual modality helps our approach more robustly generalize at these tasks. Project webpage is available at https://expert.cs.columbia.edu/",2019-11-25,https://www.semanticscholar.org/paper/1178245776864b2c5286992e61822e50bf772220,arXiv.org
3697,Doubly Right Object Recognition: A Why Prompt for Visual Rationales,"Many visual recognition models are evaluated only on their classification accuracy, a metric for which they obtain strong performance. In this paper, we investigate whether computer vision models can also provide correct rationales for their predictions. We propose a “doubly right” object recognition benchmark, where the metric requires the model to simultaneously produce both the right labels as well as the right rationales. We find that state-of-the-art visual models, such as CLIP, often provide incorrect rationales for their categorical predictions. However, by transferring the rationales from language models into visual representations through a tailored dataset, we show that we can learn a “why prompt,” which adapts large visual representations to produce correct rationales. Visualizations and empirical experiments show that our prompts significantly improve performance on doubly right object recognition, in addition to zero-shot transfer to unseen tasks and datasets.",2022-12-12,https://www.semanticscholar.org/paper/4eb5198062f78ecf844ff48bcaefe4c1c0f395cc,Computer Vision and Pattern Recognition
762,A note on broadcast encryption key management with applications to large scale emergency alert systems,"Emergency alerting capability is crucial for the prompt response to natural disasters and terrorist attacks. The emerging network infrastructure and secure broadcast techniques enable prompt and secure delivery of emergency notification messages. With the ubiquitous deployment of alert systems, scalability and heterogeneity pose new challenges for the design of secure broadcast schemes. In this paper, we discuss the key generation problem with the goal of minimizing the total number of keys which need to be generated by the alert center and distributed to the users. Two encryption schemes, zero message scheme and extended header scheme, are modeled formally. For both schemes we show the equivalence of the general optimal key generation (OKG) problem and the bipartite clique cover (BCC) problem, and show that OKG problem is NP-hard. The result is then generalized to the case with resource constraints, and we provide a heuristic algorithm for solving the restricted BCC (and OKG) problem.",2006-04-25,https://www.semanticscholar.org/paper/6d87b107c6290487c32d49574f5563f388cc72e7,Proceedings 20th IEEE International Parallel & Distributed Processing Symposium
468,On the Random Walk Method for Protocol Testing,,1994-06-21,https://www.semanticscholar.org/paper/73bd5c59a5955e5b464a5c54706332953fdfec19,International Conference on Computer Aided Verification
1271,Search for production of single top quarks via tcg and tug flavor-changing-neutral-current couplings.,"We search for the production of single top quarks via flavor-changing-neutral-current couplings of a gluon to the top quark and a charm (c) or up (u) quark. We analyze 230 pb{-1} of lepton+jets data from pp[over] collisions at a center of mass energy of 1.96 TeV collected by the D0 detector at the Fermilab Tevatron Collider. We observe no significant deviation from standard model predictions, and hence set upper limits on the anomalous coupling parameters kappa{g}{c}/Lambda and kappa{g}{u}/Lambda, where kappa{g} define the strength of tcg and tug couplings, and Lambda defines the scale of new physics. The limits at 95% C.L. are kappa{g}{c}/Lambda<0.15 TeV-1 and kappa{g}{u}/Lambda<0.037 TeV-1.",2007-02-01,https://www.semanticscholar.org/paper/132fbf5732862878d37e61b258845f791b01aaed,Physical Review Letters
3760,Generating Videos with Scene Dynamics,"We capitalize on large amounts of unlabeled video in order to learn a model of scene dynamics for both video recognition tasks (e.g. action classification) and video generation tasks (e.g. future prediction). We propose a generative adversarial network for video with a spatio-temporal convolutional architecture that untangles the scene's foreground from the background. Experiments suggest this model can generate tiny videos up to a second at full frame rate better than simple baselines, and we show its utility at predicting plausible futures of static images. Moreover, experiments and visualizations show the model internally learns useful features for recognizing actions with minimal supervision, suggesting scene dynamics are a promising signal for representation learning. We believe generative video models can impact many applications in video understanding and simulation.",2016-09-08,https://www.semanticscholar.org/paper/ee091ccf24c4f053c5c3dfbefe4a7975ed3447c1,Neural Information Processing Systems
129,The Efficacy of GlOSS for the Text Database Discovery Problem,"The popularity of information retrieval has led users to a new problem: finding which text databases (out of thousands of candidate choices) are the most relevant to a user. Answering a given query with a list of relevant databases is the text database discovery problem. The first part of this paper presents a practical method for attacking this problem based on estimating the result size of a query and a database. The method is termed GlOSS--Glossary of Servers Server. The second part of this paper evaluates GlOSS using four different semantics to answer a user''s queries. Real users'' queries were used in the experiments. We also describe several variations of GlOSS and compare their efficacy. In addition, we analyze the storage cost of our approach to the problem.",1993-12-01,https://www.semanticscholar.org/paper/8fc1b50c90069f2d1720aecf76ed03f0d402b36b,ACM SIGMOD Conference
3040,Transparent mutable replay for multicore debugging and patch validation,"We present Dora, a mutable record-replay system which allows a recorded execution of an application to be replayed with a modified version of the application. This feature, not available in previous record-replay systems, enables powerful new functionality. In particular, Dora can help reproduce, diagnose, and fix software bugs by replaying a version of a recorded application that is recompiled with debugging information, reconfigured to produce verbose log output, modified to include additional print statements, or patched to fix a bug.
 Dora uses lightweight operating system mechanisms to record an application execution by capturing nondeterministic events to a log without imposing unnecessary timing and ordering constraints. It replays the log using a modified version of the application even in the presence of added, deleted, or modified operations that do not match events in the log. Dora searches for a replay that minimizes differences between the log and the replayed execution of the modified program. If there are no modifications, Dora provides deterministic replay of the unmodified program.
 We have implemented a Linux prototype which provides transparent mutable replay without recompiling or relinking applications. We show that Dora is useful for reproducing, diagnosing, and fixing software bugs in real-world applications, including Apache and MySQL. Our results show that Dora (1) captures bugs and replays them with applications modified or reconfigured to produce additional debugging output for root cause diagnosis, (2) captures exploits and replays them with patched applications to validate that the patches successfully eliminate vulnerabilities, (3) records production workloads and replays them with patched applications to validate patches with realistic workloads, and (4) maintains low recording overhead on commodity multicore hardware, making it suitable for production systems.",2013-03-16,https://www.semanticscholar.org/paper/33623a9fec52e01e92c6ba1ae4d67b01f0c76fe5,International Conference on Architectural Support for Programming Languages and Operating Systems
64,Efficient IR-Style Keyword Search over Relational Databases,,2003-09-09,https://www.semanticscholar.org/paper/15c71e9d0b467796ecc0519fe64203a4a79f76e5,Very Large Data Bases Conference
1926,Analysing semiconductor manufacturing big data for root cause detection of excursion for yield enhancement,"With the shrinking feature size of integrated circuits driven by continuous technology migrations for wafer fabrication, the control of tightening critical dimensions is critical for yield enhancement, while physical failure analysis is increasingly difficult. In particular, the yield ramp up stage for implementing new technology node involves new production processes, unstable machine configurations, big data with multiple co-linearity and high dimensionality that can hardly rely on previous experience for detecting root causes. This research aims to propose a novel data-driven approach for Analysing semiconductor manufacturing big data for low yield (namely, excursions) diagnosis to detect process root causes for yield enhancement. The proposed approach has shown practical viability to efficiently detect possible root causes of excursion to reduce the trouble shooting time and improve the production yield effectively.",2017-09-02,https://www.semanticscholar.org/paper/da60cf9577d3d52575f38fe3879bab63d066267d,International Journal of Production Research
1022,Locomotion of a multi-link non-holonomic snake robot with passive joints,"Conventional approaches in prescribing controls for locomoting robots assume control over all input degrees of freedom (DOFs). Many robots, such as those with non-holonomic constraints, may not require or even allow for direct command over all DOFs. In particular, a snake robot with more than three links with non-holonomic constraints cannot achieve arbitrary configurations in all of its joints while simultaneously locomoting. For such a system, we assume partial command over a subset of the joints, and allow the rest to evolve according to kinematic chained and dynamic models. Different combinations of actuated and passive joints, as well as joints with dynamic elements such as torsional springs, can drastically change the coupling interactions and stable oscillations of joints. We use tools from nonlinear analysis to understand emergent oscillation modes of various robot configurations and connect them to overall locomotion using geometric mechanics and feedback control for robots that may not fully utilize all available inputs. We also experimentally verify observations and motion planning results on a physical non-holonomic snake robot.",2020-01-27,https://www.semanticscholar.org/paper/e641fa9ee43b4f2f83e76293772cc6eefa2eab38,Int. J. Robotics Res.
3284,Reciprocal insurance among Kenyan pastoralists,,2012-08-25,https://www.semanticscholar.org/paper/ce77a6e9ec27eb78d47ce415ddeaf7c5767070e3,Theoretical Ecology
3450,Proceedings of the Seventeenth Annual ACM-SIAM Symposium on Discrete Algorithms: Preface,,2006-02-28,https://www.semanticscholar.org/paper/4858290dadeb822edc1cd9a6a16def33a468efe7,ACM-SIAM Symposium on Discrete Algorithms
2624,Taking it to the streets: how virtual reality can change mobile computing,"Virtual reality has long been an indoor affair. Whether constrained by stationary computers ordisplays, or by the limitations of our tracking technologies, researchers typically build virtualenvironments that work within a single physical room or a portion of a room. Even distributedvirtual reality systems usually interconnect two or more such indoor spaces. Meanwhile, ascomputers grow ever smaller and faster, mobile computing is becoming an increasingly importantpart of our daily lives, accompanying us wherever we go, outdoors, as well as indoors.What will it take for virtual reality to move outdoors and finally see the light of day? And, whyshould we care? Within the virtual reality research community, work on augmented reality hasalready begun to explore outdoor environments-tracking using computer vision, gyroscopes,accelerometers, compasses, and GPS; and experimenting with (barely) wearable testbeds. I willdiscuss why virtual reality (especially in the form of augmented reality) and mobile computing are asynergistic combination, and will provide an overview of the research problems that must beaddressed for mobile augmented reality systems to play a major role in our future.Among the issues that I will review are overcoming physical and aesthetic barriers to mobilityand wearability; tracking and registration of heads, hands, bodies, and other objects; renderingvirtual objects in the real world; and developing sufficiently high quality displays. Equallyimportant is the design of head-tracked user interfaces that are well suited to mobility. Wearablesystems will need to support collaboration among mobile users, facile interaction with real andvirtual objects, and coordination across a wide range of heterogeneous displays and devices. Keyhere is the volatile nature of mobile interactions-users continually move into and out of thepresence of other users, devices, and objects, and rapidly change tasks. Furthermore, augmentedreality makes it possible for real and virtual objects to share the same display space, creating thepotential for a variety of visually confusing relationships as objects overlap and occlude each other.Avoiding these problems will require that the virtual world be redesigned and laid out on the fly, tomaintain desired visual relationships between virtual objects and other real and virtual objects.",2003-03-22,https://www.semanticscholar.org/paper/ad0dd2bff4744a367996843689710ba0bcae5da3,"IEEE Virtual Reality, 2003. Proceedings."
2713,Windows on the world: 2D windows for 3D augmented reality,"INTRODUCTION We describe the design and implementation of a prototype When we think of the use of head-mounted displays and 3D heads-up window system intended for use in a 3D environinteraction devices to present virtual worlds, it is often in ment. Our system includes a see-through head-mounted terms of environments populated solely by 3D objects. display that runs a full X server whose image is overlaid on There are many situations, however, in which 2D text and the user’s view of the physical world. The user’s head is graphics of the sort supported by current window systems tracked so that the display indexes into a large X bitmap, can be useful components of these environments. This is effectively placing the user inside a display space that is especially true in the case of the many applications that run mapped onto part of a surrounding virtual sphere. By under an industry standard window system such as X [13]. tracking the user’s body, and interpreting head motion relaWhile we might imagine porting or enhancing a significant tive to it, we create a portable information surround that X application to take advantage of the 3D capabilities of a envelopes the user as they move about. virtual world, the effort and cost may not be worth the return, especially if the application is inherently 2D. We support three kinds of windows implemented on top of Therefore, we have been exploring how we can incorporate the X server: windows fixed to the head-mounted display, an existing 2D window system within a 3D virtual world. windows fixed to the information surround, and windows fixed to locations and objects in the 3D world. Objects can We are building an experimental system that supports a full also be tracked, allowing windows to move with them. To X11 server on a see-through head-mounted display. Our demonstrate the utility of this model, we describe a small display overlays a selected portion of the X bitmap on the hypermedia system that allows links to be made between user’s view of the world, creating an X-based augmented windows and windows to be attached to objects. Thus, our reality. Depending on the situation and application, the hypermedia system can forge links between any combinauser may wish to treat a window as a stand-alone entity or tion of physical objects and virtual windows. to take advantage of the potential relationships that can be made between it and the visible physical world. To make this possible, we have developed facilities that allow X",1993-12-01,https://www.semanticscholar.org/paper/3a6ca1738e7affb77d001c6bc5128c85608cc912,ACM Symposium on User Interface Software and Technology
163,Total Functions in the Polynomial Hierarchy,". We identify several genres of search problems beyond NP for which existence of solutions is guaranteed. One class that seems especially rich in such problems is PEPP (for “polynomial empty pigeonhole principle”), which includes problems related to existence theorems proved through the union bound, such as ﬁnding a bit string that is far from all codewords, ﬁnding an explicit rigid matrix, as well as a problem we call Complexity, capturing Complexity Theory’s quest. When the union bound is generous, in that solutions constitute at least a polynomial fraction of the domain, we have a family of seemingly weaker classes α - PEPP, which are inside FP NP | poly. Higher in the hierarchy, we identify the constructive version of the Sauer-Shelah lemma and the appropriate generalization of PPP that contains it. The resulting total function hierarchy turns out to be more stable than the polynomial hierarchy: it is known that, under oracles, total functions within FNP may be easy, but total functions a level higher may still be harder than FP NP . ,",,https://www.semanticscholar.org/paper/99aa888d13c646985616f4a8b85095b70206a711,Electron. Colloquium Comput. Complex.
1832,Mixed Membership Stochastic Blockmodels,"Observations consisting of measurements on relationships for pairs of objects arise in many settings, such as protein interaction and gene regulatory networks, collections of author-recipient email, and social networks. Analyzing such data with probabilisic models can be delicate because the simple exchangeability assumptions underlying many boilerplate models no longer hold. In this paper, we describe a latent variable model of such data called the mixed membership stochastic blockmodel. This model extends blockmodels for relational data to ones which capture mixed membership latent relational structure, thus providing an object-specific low-dimensional representation. We develop a general variational inference algorithm for fast approximate posterior inference. We explore applications to social and protein interaction networks.",2007-05-30,https://www.semanticscholar.org/paper/d9b9fb207013bf8afb064f23f3dffc7edd005f73,Neural Information Processing Systems
112,Mediating and Metasearching on the Internet,"The Internet emerges as the largest database. Increasingly, users want to issue complex queries across Internet sources to obtain the data they require. However, finding relevant information sources and querying them manually is problematic: there are numerous sources, and they vary in the type of information objects they contain and in the interface they present to their users. Some sources contain text documents and support simple query models where a query is just a list of keywords. Other sources contain more structured data and provide query interfaces in the style of relational query languages. Furthermore, users have to manually fuse the query results by merging information, removing redundancies, ranking the answer objects in the appropriate order, and so on. Since it is tedious to contact several heterogeneous sources, users can benefit from metasearchers and mediators, which are services that provide users with a virtual integrated view of the heterogeneous sources. Users access the view using a unified query interface that offers location, model, and interface transparency, i.e., users have the illusion of a single database and do not have to be aware of the location and interface of the sources. Although users and applications might access data directly through wrappers, mediators and metasearchers offer an integrated view of the world, where information related to the same entity has been fused together, redundancies have been eliminated, and inconsistencies have been removed. The architecture of metasearchers and mediators are virtually identical (Figure 1). Wrappers export a common data model view of each source’s data. Wrappers also provide a common query interface. After receiving a query, a wrapper translates it into a source-specific query or command, hence giving interface transparency to the user. Then, the wrapper translates the query results from the underlying source into the common data model or format. To evaluate a user query over multiple heterogeneous databases, both metasearchers and mediators will typically perform three main tasks:",,https://www.semanticscholar.org/paper/89e4353bf86caa32b2a7deb1b2b3d75fc3d34ab9,IEEE Data Engineering Bulletin
2449,Interactive tools for inpatient medication tracking: a multi-phase study with cardiothoracic surgery patients,"OBJECTIVE
Prior studies of computing applications that support patients' medication knowledge and self-management offer valuable insights into effective application design, but do not address inpatient settings. This study is the first to explore the design and usefulness of patient-facing tools supporting inpatient medication management and tracking.


MATERIALS AND METHODS
We designed myNYP Inpatient, a custom personal health record application, through an iterative, user-centered approach. Medication-tracking tools in myNYP Inpatient include interactive views of home and hospital medication data and features for commenting on these data. In a two-phase pilot study, patients used the tools during cardiothoracic postoperative care at Columbia University Medical Center. In Phase One, we provided 20 patients with the application for 24-48 h and conducted a closing interview after this period. In Phase Two, we conducted semi-structured interviews with 12 patients and 5 clinical pharmacists who evaluated refinements to the tools based on the feedback received during Phase One.


RESULTS
Patients reported that the medication-tracking tools were useful. During Phase One, 14 of the 20 participants used the tools actively, to review medication lists and log comments and questions about their medications. Patients' interview responses and audit logs revealed that they made frequent use of the hospital medications feature and found electronic reporting of questions and comments useful. We also uncovered important considerations for subsequent design of such tools. In Phase Two, the patients and pharmacists participating in the study confirmed the usability and usefulness of the refined tools.


CONCLUSIONS
Inpatient medication-tracking tools, when designed to meet patients' needs, can play an important role in fostering patient participation in their own care and patient-provider communication during a hospital stay.",,https://www.semanticscholar.org/paper/03a7424bf9c971a447120a8edb96040518d76c9d,J. Am. Medical Informatics Assoc.
3496,"Optimal Time-Critical Scheduling via Resource Augmentation
",,1997-04-01,https://www.semanticscholar.org/paper/6778008e23e05b69e0e243b1f0b63db7056336d8,Symposium on the Theory of Computing
3179,Expert range maps of global mammal distributions harmonised to three taxonomic authorities,"Comprehensive, global information on species' occurrences is an essential biodiversity variable and central to a range of applications in ecology, evolution, biogeography and conservation. Expert range maps often represent a species' only available distributional information and play an increasing role in conservation assessments and macroecology. We provide global range maps for the native ranges of all extant mammal species harmonised to the taxonomy of the Mammal Diversity Database (MDD) mobilised from two sources, the Handbook of the Mammals of the World (HMW) and the Illustrated Checklist of the Mammals of the World (CMW).",2022-03-27,https://www.semanticscholar.org/paper/8782ac121cffd1107567c4e0fc8677fa3b53b1ad,Journal of Biogeography
3273,Linking social environment and stress physiology in feral mares (Equus caballus): group transfers elevate fecal cortisol levels.,,2014-01-15,https://www.semanticscholar.org/paper/8d0de1904552631f2f3a505a1fe9d90935d637c7,General and Comparative Endocrinology
2915,The role of grain boundary ferrite evolution and thermal aging on creep cavitation of type 316H austenitic stainless steel,,2021-03-11,https://www.semanticscholar.org/paper/47702d1f5f78ac4751e6d32e617a7d9d13ee1922,Materials Science & Engineering: A
1540,On the Assumptions of Synthetic Control Methods,"Synthetic control (SC) methods have been widely applied to estimate the causal effect of large-scale interventions, e.g., the state-wide effect of a change in policy. The idea of synthetic controls is to approximate one unit's counterfactual outcomes using a weighted combination of some other units' observed outcomes. The motivating question of this paper is: how does the SC strategy lead to valid causal inferences? We address this question by re-formulating the causal inference problem targeted by SC with a more fine-grained model, where we change the unit of the analysis from""large units""(e.g., states) to""small units""(e.g., individuals in states). Under this re-formulation, we derive sufficient conditions for the non-parametric causal identification of the causal effect. We highlight two implications of the reformulation: (1) it clarifies where""linearity""comes from, and how it falls naturally out of the more fine-grained and flexible model, and (2) it suggests new ways of using available data with SC methods for valid causal inference, in particular, new ways of selecting observations from which to estimate the counterfactual.",2021-12-10,https://www.semanticscholar.org/paper/466abebd6519c16f952c4633c10c0b693e240e3e,International Conference on Artificial Intelligence and Statistics
190,TFNP: An Update,,2017-05-24,https://www.semanticscholar.org/paper/b30712d457f99b58c15e1a6a27442bda56b0f100,International/Italian Conference on Algorithms and Complexity
1769,Variational Inference for Stick-Breaking Beta Process Priors,"We present a variational Bayesian inference algorithm for the stick-breaking construction of the beta process. We derive an alternate representation of the beta process that is amenable to variational inference, and present a bound relating the truncated beta process to its infinite counterpart. We assess performance on two matrix factorization problems, using a non-negative factorization model and a linear-Gaussian model.",2011-06-28,https://www.semanticscholar.org/paper/7bbd25e76044e3bb7c3a9dfe4dba5ebeb4e95925,International Conference on Machine Learning
1937,Big data analytics for modeling WAT parameter variation induced by process tool in semiconductor manufacturing and empirical study,"With the feature size shrinkage in advanced technology nodes, the modeling of process variations has become more critical for troubleshooting and yield enhancement. Misalignment among equipment tools or chambers in process stages is a major source of process variations. Because a process flow contains hundreds of stages during semiconductor fabrication, tool/chamber misalignment may more significantly affect the variation of transistor parameters in a wafer acceptance test. This study proposes a big data analytic framework that simultaneously considers the mean difference between tools and wafer-to-wafer variation and identifies possible root causes for yield enhancement. An empirical study was conducted to demonstrate the effectiveness of proposed approach and obtained promising results.",2016-12-11,https://www.semanticscholar.org/paper/8f14c1a2a713309262d3914faa57370e30aa78da,Online World Conference on Soft Computing in Industrial Applications
2751,"Interaction issues in visualization: requirements, techniques, and devices",,1990-10-23,https://www.semanticscholar.org/paper/dd980be7af26fb0336e78c01b4e08329b3b63957,Visual ..
367,Selfish behavior and stability of the internet:: a game-theoretic analysis of TCP,"For years, the conventional wisdom [7, 22] has been that the continued stability of the Internet depends on the widespread deployment of ""socially responsible"" congestion control. In this paper, we seek to answer the following fundamental question: If network end-points behaved in a selfish manner, would the stability of the Internet be endangered?.We evaluate the impact of greedy end-point behavior through a game-theoretic analysis of TCP. In this ""TCP Game"" each flowattempts to maximize the throughput it achieves by modifying its congestion control behavior. We use a combination of analysis and simulation to determine the Nash Equilibrium of this game. Our question then reduces to whether the network operates efficiently at these Nash equilibria.Our findings are twofold. First, in more traditional environments -- where end-points use TCP Reno-style loss recovery and routers use drop-tail queues -- the Nash Equilibria are reasonably efficient. However, when endpoints use more recent variations of TCP (e.g., SACK) and routers employ either RED or drop-tail queues, the Nash equilibria are very inefficient. This suggests that the Internet of the past could remain stable in the face of greedy end-user behavior, but the Internet of today is vulnerable to such behavior. Second, we find that restoring the efficiency of the Nash equilibria in these settings does not require heavy-weight packet scheduling techniques (e.g., Fair Queuing) but instead can be done with a very simple stateless mechanism based on CHOKe [21].",,https://www.semanticscholar.org/paper/10505a43a45d53b1653a8e34653dbfaaa0663343,"Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication"
221,Can Almost Everybody be Almost Happy? PCP for PPAD and the Inapproximability of Nash,"We conjecture that PPAD has a PCP-like complete problem, seeking a near equilibrium in which all but very few players have very little incentive to deviate. We show that, if one assumes that this problem requires exponential time, several open problems in this area are settled. The most important implication, proved via a ""birthday repetition"" reduction, is that the n^O(log n) approximation scheme of [LMM03] for the Nash equilibrium of two-player games is essentially optimum. Two other open problems in the area are resolved once one assumes this conjecture, establishing that certain approximate equilibria are PPAD-complete: Finding a relative approximation of two-player Nash equilibria (without the well-supported restriction of [Das13]), and an approximate competitive equilibrium with equal incomes [Bud11] with small clearing error and near-optimal Gini coefficient.",2015-04-09,https://www.semanticscholar.org/paper/dda357a1e242ee27aaaaa4f0ca35e3e79a20dc32,arXiv.org
1886,An inverse-distance weighting genetic algorithm for optimizing the wafer exposure pattern for enhancing OWE for smart manufacturing,,2020-09-01,https://www.semanticscholar.org/paper/8099215fc5b5d3a94ed2692b18013da10d64c68f,Applied Soft Computing
3671,Classes: an abstract data type facility for the C language,"Language constructs for definition and use of abstract data types ease the design and maintenance of large programs. This paper describes the C class concept, an extension to the C language providing such constructs. A class is defined using standard C data types and functions, and it can itself be used as a building block for new classes. A class provides a way of restricting access to a data structure to a specific set of functions associated with it, without incurring significant overheads at compile time or at run time.The C class concept is introduced by small examples of its use, and familiarity with the C language [2] is assumed. Appendix A is a complete small C program using classes.Classes have been in use for more than a year on a dozen PDP11 and VAX UNIX systems [1], and they are currently used for a diverse set of projects on more than 30 systems. Classes are currently implemented by an intermediate pass of the cc compiler, called the class pre-processor, which is invoked when the directive #class is found in a C source file. The class pre-processor is easily ported to a system with a version of the portable C compiler. A Motorola68000 version is in use.",,https://www.semanticscholar.org/paper/645dcc2de160209d6d547c557348ff9d61b9b619,SIGP
582,Topological Bandwidth,,1983-03-09,https://www.semanticscholar.org/paper/9fb6260ab31ec3e25be68aa8a699a9af3ee0b8cc,Colloquium on Trees in Algebra and Programming
795,From Rule-based to Automata-based Testing,,2000-10-10,https://www.semanticscholar.org/paper/6dcd45fe755d4a8d295e83c4b5a31ba79c54de35,Formal Techniques for (Networked and) Distributed Systems
2912,Deep mendelian randomization: Investigating the causal knowledge of genomic deep learning models,"Multi-task deep learning (DL) models can accurately predict diverse genomic marks from sequence, but whether these models learn the causal relationships between genomic marks is unknown. Here, we describe Deep Mendelian Randomization (DeepMR), a method for estimating causal relationships between genomic marks learned by genomic DL models. By combining Mendelian Randomization with in silico mutagenesis, DeepMR obtains local (locus specific) and global estimates of (an assumed) linear causal relationship between marks. In a simulation designed to test recovery of pairwise causal relations between transcription factors (TFs), DeepMR gives accurate and unbiased estimates of the ‘true’ global causal effect, but its coverage decays in the presence of sequence-dependent confounding. We then apply DeepMR to examine the global relationships learned by a state-of-the-art DL model, BPNet [Avsec et al., 2020], between TFs involved in reprogramming. DeepMR’s causal effect estimates validate previously hypothesized relationships between TFs and suggest new relationships for future investigation.",2022-02-02,https://www.semanticscholar.org/paper/c2818374b762b0d8176d71ef7bc754d0aa9c51a3,bioRxiv
720,Doubly Balanced Connected Graph Partitioning,"We introduce and study the doubly balanced connected graph partitioning problem: Let G=(V, E) be a connected graph with a weight (supply/demand) function p : V → {−1, +1} satisfying p(V)=∑ j&isin V p(j) = 0. The objective is to partition G into (V1,V2) such that G[V1] and G[V2] are connected, ∣p(V1)∣,∣p(V2)∣≤ cp, and max{ ∣V1 / V2∣,∣V2 / V1∣} ≤ cs, for some constants cp and cs. When G is 2-connected, we show that a solution with cp=1 and cs=2 always exists and can be found in randomized polynomial time. Moreover, when G is 3-connected, we show that there is always a “perfect” solution (a partition with p(V1)=p(V2)=0 and ∣V1∣=∣V2∣, if ∣V∣≡ 0 (mod 4)), and it can be found in randomized polynomial time. Our techniques can be extended, with similar results, to the case in which the weights are arbitrary (not necessarily ±1), and to the case that p(V)≠ 0 and the excess supply/demand should be split evenly. They also apply to the problem of partitioning a graph with two types of nodes into two large connected subgraphs that preserve approximately the proportion of the two types.",2016-07-21,https://www.semanticscholar.org/paper/44987e5805b918f8b6e3791b35be85d4828b93b6,ACM-SIAM Symposium on Discrete Algorithms
1429,Measurement of the Lambdab0 lifetime using semileptonic decays.,"We report a measurement of the Lambda(b)(0) lifetime using a sample corresponding to 1.3 fb(-1) of data collected by the D0 experiment in 2002-2006 during run II of the Fermilab Tevatron collider. The Lambda(b)(0) baryon is reconstructed via the decay Lambda(b)(0)-->micronuLambda(c)(+)X. Using 4437+/-329 signal candidates, we measure the Lambda(b)(0) lifetime to be tau(Lambda(b)(0))=1.290(-0.110)(+0.119)(stat)(-0.091)(+0.087)(syst) ps, which is among the most precise measurements in semileptonic Lambda(b)(0) decays. This result is in good agreement with the world average value.",1999-02-01,https://www.semanticscholar.org/paper/36b79f7545730310e054ab0cc97ae5049610e76a,Physical Review Letters
3742,Tracking Emerges by Colorizing Videos,,2018-06-25,https://www.semanticscholar.org/paper/360ef12906a531733b66e7e15c3d51771e7126d3,European Conference on Computer Vision
2441,Mercury: A Messaging Framework for Modular UI Components,"In recent years, the entity--component--system pattern has become a fundamental feature of the software architectures of game-development environments such as Unity and Unreal, which are used extensively in developing 3D user interfaces. In these systems, UI components typically respond to events, requiring programmers to write application-specific callback functions. In some cases, components are organized in a hierarchy that is used to propagate events among vertically connected components. When components need to communicate horizontally, programmers must connect those components manually and register/unregister events as needed. Moreover, events and callback signatures may be incompatible, making modular UIs cumbersome to build and share within or across applications. To address these problems, we introduce a messaging framework, Mercury, to facilitate communication among components. We provide an overview of Mercury, outline its underlying protocol and how it propagates messages to responders using relay nodes, describe a reference implementation in Unity, and present example systems built using Mercury to explain its advantages.",2018-04-21,https://www.semanticscholar.org/paper/cea0e4778f02d0663062b510868747108571c085,International Conference on Human Factors in Computing Systems
3655,Pointers to Class Members in C++,,,https://www.semanticscholar.org/paper/5b4326d758d326c81c3ac84f2a1a155f8b348793,C++ Conference
2535,Interaction and presentation techniques for shake menus in tangible augmented reality,"Menus play an important role in both information presentation and system control. We explore the design space of shake menus, which are intended for use in tangible augmented reality. Shake menus are radial menus displayed centered on a physical object and activated by shaking that object. One important aspect of their design space is the coordinate system used to present menu options. We conducted a within-subjects user study to compare the speed and efficacy of several alternative methods for presenting shake menus in augmented reality (world-referenced, display-referenced, and object-referenced), along with a baseline technique (a linear menu on a clipboard). Our findings suggest tradeoffs amongst speed, efficacy, and flexibility of interaction, and point towards the possible advantages of hybrid approaches that compose together transformations in different coordinate systems. We close by describing qualitative feedback from use and present several illustrative applications of the technique.",2009-10-19,https://www.semanticscholar.org/paper/82ee6442f78cdeeb15410df4ed1ebfe7da8379c8,2009 8th IEEE International Symposium on Mixed and Augmented Reality
674,Noise-tolerant fair classification,"Fairness-aware learning involves designing algorithms that do not discriminate with respect to some sensitive feature (e.g., race or gender). Existing work on the problem operates under the assumption that the sensitive feature available in one's training sample is perfectly reliable. This assumption may be violated in many real-world cases: for example, respondents to a survey may choose to conceal or obfuscate their group identity out of fear of potential discrimination. This poses the question of whether one can still learn fair classifiers given noisy sensitive features. In this paper, we answer the question in the affirmative: we show that if one measures fairness using the mean-difference score, and sensitive features are subject to noise from the mutually contaminated learning model, then owing to a simple identity we only need to change the desired fairness-tolerance. The requisite tolerance can be estimated by leveraging existing noise-rate estimators from the label noise literature. We finally show that our procedure is empirically effective on two case-studies involving sensitive feature censoring.",2019-01-30,https://www.semanticscholar.org/paper/c4ac496bf57410638260196a25d8ae3366ea03c7,Neural Information Processing Systems
375,A deterministic (2-2/(k+1))n algorithm for k-SAT based on local search,,2002-10-23,https://www.semanticscholar.org/paper/a9359f5d9ef7b3484c24ba65467dcb337d341e40,Theoretical Computer Science
402,Software synthesis of variable-length code decoder using a mixture of programmed logic and table lookups,"Implementation of variable-length code (VLC) decoders can involve a tradeoff between the number of decoding steps and memory usage. In this paper, we proposed a novel scheme for optimizing this tradeoff using a machine model abstracted from general purpose processors with hierarchical memories. We formulate the VLC decode problem as an optimization problem where the objective is to minimize the average decoding time. After showing that the problem is NP-complete, we present a Lagrangian algorithm that finds an approximate solution with bounded error. An implementation is automatically synthesized by a code generator. To demonstrate the efficacy of our approach, we conducted experiments of decoding codebooks for a pruned tree-structured vector quantizer and H.263 motion vector that show a performance gain of our proposed algorithm over single table lookup implementation and logic implementation.",1999-03-29,https://www.semanticscholar.org/paper/174ec7f0600c03b9169af7ae89f00cc268b60569,Proceedings DCC'99 Data Compression Conference (Cat. No. PR00096)
1631,Deep Probabilistic Programming,"We propose Edward, a Turing-complete probabilistic programming language. Edward defines two compositional representations---random variables and inference. By treating inference as a first class citizen, on a par with modeling, we show that probabilistic programming can be as flexible and computationally efficient as traditional deep learning. For flexibility, Edward makes it easy to fit the same model using a variety of composable inference methods, ranging from point estimation to variational inference to MCMC. In addition, Edward can reuse the modeling representation as part of inference, facilitating the design of rich variational models and generative adversarial networks. For efficiency, Edward is integrated into TensorFlow, providing significant speedups over existing probabilistic systems. For example, we show on a benchmark logistic regression task that Edward is at least 35x faster than Stan and 6x faster than PyMC3. Further, Edward incurs no runtime overhead: it is as fast as handwritten TensorFlow.",2017-01-13,https://www.semanticscholar.org/paper/8e9532262bb862c62d2d011a93da014da2d97ee9,International Conference on Learning Representations
921,Worst-Case Ratios for Planar Graphs and the Method of Induction on Faces (Extended Abstract),,,https://www.semanticscholar.org/paper/411a2cf2ef4f61cc05c72170d671d4cba1d056cc,IEEE Annual Symposium on Foundations of Computer Science
683,Learning hierarchical similarity metrics,"Categories in multi-class data are often part of an underlying semantic taxonomy. Recent work in object classification has found interesting ways to use this taxonomy structure to develop better recognition algorithms. Here we propose a novel framework to learn similarity metrics using the class taxonomy. We show that a nearest neighbor classifier using the learned metrics gets improved performance over the best discriminative methods. Moreover, by incorporating the taxonomy, our learned metrics can also help in some taxonomy specific applications. We show that the metrics can help determine the correct placement of a new category that was not part of the original taxonomy, and can provide effective classification amongst categories local to specific subtrees of the taxonomy.",2012-06-16,https://www.semanticscholar.org/paper/ed3d28a6015937a282e4f9c273307fb4a732f8ff,2012 IEEE Conference on Computer Vision and Pattern Recognition
3031,ARM Virtualization: Performance and Architectural Implications,"ARM servers are becoming increasingly common, making server technologies such as virtualization for ARM of growing importance. We present the first study of ARM virtualization performance on server hardware, including multi-core measurements of two popular ARM and x86 hypervisors, KVM and Xen. We show how ARM hardware support for virtualization can enable much faster transitions between VMs and the hypervisor, a key hypervisor operation. However, current hypervisor designs, including both Type 1 hypervisors such as Xen and Type 2 hypervisors such as KVM, are not able to leverage this performance benefit for real application workloads. We discuss the reasons why and show that other factors related to hypervisor software design and implementation have a larger role in overall performance. Based on our measurements, we discuss changes to ARM's hardware virtualization support that can potentially bridge the gap to bring its faster VM-to-hypervisor transition mechanism to modern Type 2 hypervisors running real applications. These changes have been incorporated into the latest ARM architecture.",2016-06-18,https://www.semanticscholar.org/paper/ad3058c4e44a2770f4f3ed38693f0787a2ccf2f3,International Symposium on Computer Architecture
2499,Designing inpatient technology to meet the medication information needs of cardiology patients,"As patients are encouraged to become active participants in their own care, recent research has begun to explore the direct sharing of electronic health information with patients during hospital visits. The design of patient-facing views of clinical information is, however, a relatively recent line of inquiry. Research is needed to further understand guidelines for communicating specific types of information to hospital patients. In this work, we focus on cardiology patients' information needs related to their hospital medications. We assessed these needs to inform the design of interactive, electronic views of medication information for cardiology inpatients. We present results of in-situ interviews with 11 inpatients and 6 nurses in a cardiology step-down unit. Our findings suggest that cohesive trends in medication information needs exist across cardiology inpatients. We discuss interview results and their implications for the design of inpatient-facing information technology. We also discuss key ways in which electronic medication information, formatted for inpatient use, differs from that formatted for outpatient or transitional medication-management use.",2012-01-28,https://www.semanticscholar.org/paper/8d774db92e5367a3d15e828fb724a0bc1cb83b90,International Health Informatics Symposium
1997,Hybrid Sampling Strategy-based Multiobjective Evolutionary Algorithm,,,https://www.semanticscholar.org/paper/49abeffaa2087d90ac12bd77434888bced102cb3,Complex Adaptive Systems
2989,Large Scale Nonparametric Bayesian Inference: Data Parallelisation in the Indian Buffet Process,"Nonparametric Bayesian models provide a framework for flexible probabilistic modelling of complex datasets. Unfortunately, the high-dimensional averages required for Bayesian methods can be slow, especially with the unbounded representations used by nonparametric models. We address the challenge of scaling Bayesian inference to the increasingly large datasets found in real-world applications. We focus on parallelisation of inference in the Indian Buffet Process (IBP), which allows data points to have an unbounded number of sparse latent features. Our novel MCMC sampler divides a large data set between multiple processors and uses message passing to compute the global likelihoods and posteriors. This algorithm, the first parallel inference scheme for IBP-based models, scales to datasets orders of magnitude larger than have previously been possible.",2009-12-07,https://www.semanticscholar.org/paper/8e20c700d413278e06affa930e21e1aa04f27aca,Neural Information Processing Systems
3407,Scheduling (Dagstuhl Seminar 16081),"This report documents the program and the outcomes of Dagstuhl Seminar 16081 ""Scheduling"". The seminar was centered around recent new developments, discussion of open problems and exploring future research directions within the broader scheduling community.",,https://www.semanticscholar.org/paper/689cef1db9ee33df5394813b16f7d1fda45117f9,Dagstuhl Reports
1844,Panel Discussion,,,https://www.semanticscholar.org/paper/793a76d1025eed16abf92a22300ec6964227aa9a,SNA@ICML
1595,A Probabilistic Model of Cardiac Physiology and Electrocardiograms,"An electrocardiogram (EKG) is a common, non-invasive test that measures the electrical activity of a patient's heart. EKGs contain useful diagnostic information about patient health that may be absent from other electronic health record (EHR) data. As multi-dimensional waveforms, they could be modeled using generic machine learning tools, such as a linear factor model or a variational autoencoder. We take a different approach:~we specify a model that directly represents the underlying electrophysiology of the heart and the EKG measurement process. We apply our model to two datasets, including a sample of emergency department EKG reports with missing data. We show that our model can more accurately reconstruct missing data (measured by test reconstruction error) than a standard baseline when there is significant missing data. More broadly, this physiological representation of heart function may be useful in a variety of settings, including prediction, causal analysis, and discovery.",2018-12-01,https://www.semanticscholar.org/paper/40da199f9a377b8f041574d2ded87e085975a75b,arXiv.org
3756,SoundNet: Learning Sound Representations from Unlabeled Video,"We learn rich natural sound representations by capitalizing on large amounts of unlabeled sound data collected in the wild. We leverage the natural synchronization between vision and sound to learn an acoustic representation using two-million unlabeled videos. Unlabeled video has the advantage that it can be economically acquired at massive scales, yet contains useful signals about natural sound. We propose a student-teacher training procedure which transfers discriminative visual knowledge from well established visual recognition models into the sound modality using unlabeled video as a bridge. Our sound representation yields significant performance improvements over the state-of-the-art results on standard benchmarks for acoustic scene/object classification. Visualizations suggest some high-level semantics automatically emerge in the sound network, even though it is trained without ground truth labels.",2016-10-27,https://www.semanticscholar.org/paper/7ab8d3af6f78f9c9f64a2f2d38471401ad0988a9,Neural Information Processing Systems
3028,NEVE: Nested Virtualization Extensions for ARM,"Nested virtualization, the ability to run a virtual machine inside another virtual machine, is increasingly important because of the need to deploy virtual machines running software stacks on top of virtualized cloud infrastructure. As ARM servers make inroads in cloud infrastructure deployments, supporting nested virtualization on ARM is a key requirement, which has been met recently with the introduction of nested virtualization support to the ARM architecture. We build the first hypervisor to use ARM nested virtualization support and show that despite similarities between ARM and x86 nested virtualization support, performance on ARM is much worse than on x86. This is due to excessive traps to the hypervisor caused by differences in non-nested virtualization support. To address this problem, we introduce a novel paravirtualization technique to rapidly prototype architectural changes for virtualization and evaluate their performance impact using existing hardware. Using this technique, we propose Nested Virtualization Extensions for ARM (NEVE), a set of simple architectural changes to ARM that can be used by software to coalesce and defer traps by logging the results of hypervisor instructions until the results are actually needed by the hypervisor or virtual machines. We show that NEVE allows hypervisors running real application workloads to provide an order of magnitude better performance than current ARM nested virtualization support and up to three times less overhead than x86 nested virtualization. NEVE will be included in ARMv8.4, the next version of the ARM architecture.",2017-10-14,https://www.semanticscholar.org/paper/8b5916fcf0e655815007588caf2d007e378bc1d0,Symposium on Operating Systems Principles
3442,Non-Preemptive Min-Sum Scheduling with Resource Augmentation,"We give the first O(l)-speed O(l) approximation polynomial-time algorithms for several nonpreemptive min-sum scheduling problems where jobs arrive over time and must be processed on one machine. More precisely, we give the first O(l)-speed O(l)-approximations for the non-preemptive scheduling problems; l|r<sub>j</sub>| Sigmaw<sub>j</sub>F<sub>j</sub> (weighted flow time), l |r<sub>j</sub>| SigmaT<sub>j</sub> (total tardiness), the broadcast version of 1 |r<sub>j</sub>| Sigmaw<sub>j</sub>F<sub>j</sub> , an O(I)-speed, 1-approximation for l |r<sub>j</sub>| Sigma U macr<sub>j</sub> (throughput maximization), and an O(l)-machine, O(l)-speed O(1)-approximation for l |r<sub>j</sub>| Sigmaw<sub>j</sub>T<sub>j</sub> (weighted tardiness). Our main contribution is an integer programming formulation whose relaxation is sufficiently close to the integer optimum, and which can be transformed to a schedule on a faster machine.",2007-10-21,https://www.semanticscholar.org/paper/85e70fb59afa4e9ba693ab0f83959a8419264c5d,IEEE Annual Symposium on Foundations of Computer Science
3681,Zero-1-to-3: Zero-shot One Image to 3D Object,"We introduce Zero-1-to-3, a framework for changing the camera viewpoint of an object given just a single RGB image. To perform novel view synthesis in this under-constrained setting, we capitalize on the geometric priors that large-scale diffusion models learn about natural images. Our conditional diffusion model uses a synthetic dataset to learn controls of the relative camera viewpoint, which allow new images to be generated of the same object under a specified camera transformation. Even though it is trained on a synthetic dataset, our model retains a strong zero-shot generalization ability to out-of-distribution datasets as well as in-the-wild images, including impressionist paintings. Our viewpoint-conditioned diffusion approach can further be used for the task of 3D reconstruction from a single image. Qualitative and quantitative experiments show that our method significantly outperforms state-of-the-art single-view 3D reconstruction and novel view synthesis models by leveraging Internet-scale pre-training.",2023-03-20,https://www.semanticscholar.org/paper/2c70684973bc4d7b6f8404a647b8031c4d3c8383,arXiv.org
2457,Providing Assistance for Orienting 3D Objects Using Monocular Eyewear,"Many tasks require that a user rotate an object to match a specific orientation in an external coordinate system. This includes tasks in which one object must be oriented relative to a second prior to assembly and tasks in which objects must be held in specific ways to inspect them. Research has investigated guidance mechanisms for some 6DOF tasks, using wide--field-of-view, stereoscopic virtual and augmented reality head-worn displays (HWDs). However, there has been relatively little work directed toward smaller field-of-view lightweight monoscopic HWDs, such as Google Glass, which may remain more comfortable and less intrusive than stereoscopic HWDs in the near future. We have designed and implemented a novel visualization approach and three additional visualizations representing different paradigms for guiding unconstrained manual 3DOF rotation, targeting these monoscopic HWDs. We describe our exploration of these paradigms and present the results of a user study evaluating the relative performance of the visualizations and showing the advantages of our new approach.",2016-10-15,https://www.semanticscholar.org/paper/62400a70bbb1bc58b24bfafb78df30a07f697714,Symposium on Spatial User Interaction
890,Deadlock-Freedom (and Safety) of Transactions in a Distributed Database,,1986-10-01,https://www.semanticscholar.org/paper/0a6cb89853157e8328b3adb6b1a79cd1c81b5dde,Journal of computer and system sciences (Print)
132,Adaptive deadlock-free worm-hole routing in hypercubes,"Two new algorithms for worm-hole routing in the hypercube are presented. The first hypercube algorithm is adaptive, but non-minimal in the sense that some derouting is permitted. Then another deadlock-free adaptive worm-hole based routing algorithm for the hypercube interconnection is presented which is minimal. Finally some well-known worm-hole algorithms for the hypercube were evaluated together with the new ones on a hypercube of 2/sup 10/ nodes. One oblivious algorithm, the Dimension-Order, or E-Cube routing algorithm (W. Dally, C. Seitz, 1987) was tried. In addition, three partially adaptive algorithms were considered: the Hanging algorithm (Y. Birk, P. Gibbons, D. Soroker, J. Sanz, 1989 and S. Konstantinidou, 1990), the Zenith algorithm (S. Konstantinidou, 1990), and the Hanging-Order algorithm (G.-M. Chia, S. Chalasani, C.S. Raghavendra, 1991). Finally, a fully adaptive minimal algorithm presented independently by L. Gravano, G. Pifarre, S.A. Felperin and J. Sanz (1991) and J. Duato was tried. This algorithm allows each message to choose adaptively among all the shortest paths from its source to its destination. Only four virtual channels per physical link are needed to achieve this. This technique is referred to as Fully. The results obtained show that the two new algorithms are good candidates as a choice for worm-hole routing in the hypercube network.<<ETX>>",1992-03-01,https://www.semanticscholar.org/paper/cfb408cca6810a0509922ed963b14e6f35dbee05,Proceedings Sixth International Parallel Processing Symposium
1962,A data mining approach for analyzing semiconductor MES and FDC data to enhance overall usage effectiveness (OUE),"AbstractWafer fabrication is a complex and lengthy process that involves hundreds of process steps with monitoring numerous process parameters at the same time for yield enhancement. Big data is automatically collected during manufacturing processes in modern wafer fabrication facility. Thus, potential useful information can be extracted from big data to enhance decision quality and enhance operational effectiveness. This study aims to develop a data mining framework that integrates FDC and MES data to enhance the overall usage effectiveness (OUE) for cost reduction. We validated this approach with an empirical study in a semiconductor company in Taiwan. The results demonstrated the practical viability of this approach. The extracted information and knowledge is helpful to engineers for identifying the major tools factors affecting indirect material usage effectiveness and identify specific periods of time when a functional tool has abnormal usage of material.",2014-07-22,https://www.semanticscholar.org/paper/761fbdba4182210cba43c5a1d6e9d354a87de2ea,International Journal of Computational Intelligence Systems
1883,Dynamic coordinated scheduling for supply chain under uncertain production time to empower smart production for Industry 3.5,,2020-04-01,https://www.semanticscholar.org/paper/44249e25f5050cfd39280d4bdd964f12c810c903,Computers & industrial engineering
279,VC v. VCG: Inapproximability of Combinatorial Auctions via Generalizations of the VC Dimension,"The existence of incentive-compatible computationally-efficient protocols for combinatorial auctions with decent approximation ratios is the paradigmatic problem in computational mechanism design. It is believed that in many cases good approximations for combinatorial auctions may be unattainable due to an inherent clash between truthfulness and computational efficiency. However, to date, researchers lack the machinery to prove such results. In this paper, we present a new approach that we believe holds great promise for making progress on this important problem. We take the first steps towards the development of new technologies for lower bounding the VC-dimension of k-tuples of disjoint sets. We apply this machinery to prove the first computational-complexity inapproximability results for incentive-compatible mechanisms for combinatorial auctions. These results hold for the important class of VCG-based mechanisms, and are based on the complexity assumption that NP has no polynomial-size circuits.",2009-05-12,https://www.semanticscholar.org/paper/acf1c480d31881cbc22b752673d3442813ce0886,arXiv.org
775,Multiway cuts in node weighted graphs,,,https://www.semanticscholar.org/paper/5151d92034ee4da74884be4c6350cbb3928a7eca,J. Algorithms
3738,Moments in Time Dataset: One Million Videos for Event Understanding,"We present the Moments in Time Dataset, a large-scale human-annotated collection of one million short videos corresponding to dynamic events unfolding within three seconds. Modeling the spatial-audio-temporal dynamics even for actions occurring in 3 second videos poses many challenges: meaningful events do not include only people, but also objects, animals, and natural phenomena; visual and auditory events can be symmetrical in time (“opening” is “closing” in reverse), and either transient or sustained. We describe the annotation process of our dataset (each video is tagged with one action or activity label among 339 different classes), analyze its scale and diversity in comparison to other large-scale video datasets for action recognition, and report results of several baseline models addressing separately, and jointly, three modalities: spatial, temporal and auditory. The Moments in Time dataset, designed to have a large coverage and diversity of events in both visual and auditory modalities, can serve as a new challenge to develop models that scale to the level of complexity and abstract reasoning that a human processes on a daily basis.",2018-01-09,https://www.semanticscholar.org/paper/07174c2f209f15cacf9ad3422b48652df286be69,IEEE Transactions on Pattern Analysis and Machine Intelligence
107,Exploiting Geographical Location Information of Web Pages,"Many information resources on the web are relevant primarily to limited geographical communities. For instance, web sites containing information on restaurants, theaters, and apartment rentals are relevant primarily to web users in geographical proximity to these locations. In contrast, other information resources are relevant to a broader geographical community. For instance, an on-line newspaper may be relevant to users across the United States. Unfortunately, the geographical scope of web resources is largely ignored by web search engines. We make the case for identifying and exploiting the geographical location information of web sites so that web search engines can rank resources in a geographically sensitive fashion, in addition to using more traditional information-retrieval strategies. In this paper, we first consider how to compute the geographical location of web pages. Subsequently, we consider how to exploit such information in one specific ""proof-of-concept"" application we implemented in JAVA, and discuss other examples as well.",1999-06-03,https://www.semanticscholar.org/paper/fa27189fb9955538b0590d16b525af1dc3ad5a8b,International Workshop on the Web and Databases
1929,Semiconductor manufacturing intelligence and automation,,2016-09-01,https://www.semanticscholar.org/paper/04712c50c37817fa2f3443365d564c768d306693,Computers & industrial engineering
1958,Overlay Error Compensation Using Advanced Process Control With Dynamically Adjusted Proportional-Integral R2R Controller,"As semiconductor manufacturing reaching nanotechnology, to obtain high resolution and alignment accuracy via minimizing overlay errors within the tolerance is crucial. To address the needs of changing production and process conditions, this study aims to propose a novel dynamically adjusted proportional-integral (DAPI) run-to-run (R2R) controller to adapt equipment parameters to enhance the overlay control performance. This study evaluates the performance of controllers via the variation of each overlay factor and the variation of maximum overlay errors in real settings. To validate the effectiveness of the proposed approach, an empirical study was conducted in a leading semiconductor company in Taiwan and the results showed practical viability of the proposed DAPI controller to reduce overlay errors effectively than conventional exponentially weighted moving average controller used in this company.",2014-04-01,https://www.semanticscholar.org/paper/484c348a458c0fddf7d66baba8ffa01c5f337812,IEEE Transactions on Automation Science and Engineering
3090,REPETE2: A next generation home telemedicine architecture.,"As the availability of home broadband increases, there is an increasing need for a broadband-based home telemedicine architecture. A home tele-medicine architecture supporting broadband and remote training is presented.",2007-10-11,https://www.semanticscholar.org/paper/c6f723b51cf6291288cc4566d6df58ccb7665f85,AMIA ... Annual Symposium proceedings. AMIA Symposium
2789,A monocyte-keratinocyte-derived co-culture assay accurately identifies efficacies of BET inhibitors as therapeutic candidates for psoriasiform dermatitis.,,2020-08-15,https://www.semanticscholar.org/paper/a564638f5a22aadcb253481962933351656294f7,Journal of dermatological science (Amsterdam)
539,Optimal piecewise linear motion of an object among obstacles,,1987-11-01,https://www.semanticscholar.org/paper/cb01578f96bc6046d614d11830aa79d477db6572,Algorithmica
2118,Sampling strategy and model to measure and compensate overlay errors,"Overlay is one of the key designed rules for producing VLSI devices. In order to have a better resolution and alignment accuracy in lithography process, it is important to model the overlay errors and then to compensate them into tolerances. This study aimed to develop a new model that bridges the gap between the existing theoretical models and the data obtained in real settings and to discuss the overlay sampling strategies with empirical data in a wafer fab. In addition, we used simulation to examine the relations between the various factors and the caused overlay errors. This paper concluded with discussions on further research.",2001-08-22,https://www.semanticscholar.org/paper/c0202599d024955251d1ce5c83f1082a6732d422,SPIE Advanced Lithography
2634,The AIL automated interface layout system,"We describe an automated layout system called AIL that generates the user interface for the PERSIVAL digital library project. AIL creates a layout based on a variety of content components and associated meta-data information provided by the PERSIVAL generation and retrieval modules. By leveraging semantic links between the content components, the layout that AIL provides is both context and user-model aware. In addition, AIL is capable of interacting intelligently with the natural language generation components of PERSIVAL to tailor the length of the text content for a given layout.",2002-01-13,https://www.semanticscholar.org/paper/94f39d5e80bbef400ba2acb339afe04a797d4e5f,International Conference on Intelligent User Interfaces
1061,The LUX-ZEPLIN (LZ) experiment,,2019-10-21,https://www.semanticscholar.org/paper/a1d27c9d6784e1372056b39260dcd3710c1dbd19,"Nuclear Instruments and Methods in Physics Research Section A : Accelerators, Spectrometers, Detectors and Associated Equipment"
2947,LeafCutter: annotation-free quantification of RNA splicing,"The excision of introns from pre-mRNA is an essential step in mRNA processing. We developed LeafCutter to study sample and population variation in intron splicing. LeafCutter identifies variable intron splicing events from short-read RNA-seq data and finds alternative splicing events of high complexity. Our approach obviates the need for transcript annotations and circumvents the challenges in estimating relative isoform or exon usage in complex splicing events. LeafCutter can be used both for detecting differential splicing between sample groups, and for mapping splicing quantitative trait loci (sQTLs). Compared to contemporary methods, we find 1.4–2.1 times more sQTLs, many of which help us ascribe molecular effects to disease-associated variants. Strikingly, transcriptome-wide associations between LeafCutter intron quantifications and 40 complex traits increased the number of associated disease genes at 5% FDR by an average of 2.1-fold as compared to using gene expression levels alone. LeafCutter is fast, scalable, easy to use, and available at https://github.com/davidaknowles/leafcutter.",2016-03-16,https://www.semanticscholar.org/paper/30220ed3a861a5164a836f3af5b74b1c0c104d9b,bioRxiv
3092,Secure Isolation of Untrusted Legacy Applications,"Existing applications often contain security holes that are not patched until after the system has already been compromised. Even when software updates are available, applying them often results in system services being unavailable for some time. This can force administrators to leave system services in an insecure state for extended periods. To address these system security issues, we have developed the PeaPod virtualization layer. The PeaPod virtualization layer provides a group of processes and associated users with two virtualization abstractions, pods and peas. A pod provides an isolated virtualized environment that is decoupled from the underlying operating system instance. A pea provides an easy-to-use least privilege model for fine grain isolation amongst application components that need to interact with one another. As a result, the system easily enables the creation of lightweight environments for privileged program execution that can help with intrusion prevention and containment. Our measurements on real world desktop and server applications demonstrate that the PeaPod virtualization layer imposes little overhead and enables secure isolation of untrusted applications.",2007-11-01,https://www.semanticscholar.org/paper/e12037d5aeb225ef51a78cdce69fcd279231411f,LiSA
371,On the complexity of equilibria,"We prove complexity, approximability, and inapproximability results for the problem of finding an exchange equilibrium in markets with indivisible (integer) goods, most notably a polynomial-time algorithm that approximates the market equilibrium arbitrarily closely when the number of goods is bounded and the utilities are linear. We also show a communication complexity lower bound, implying that the ideal informational economy of a market with unique individual optima is unattainable in general.",2002-05-19,https://www.semanticscholar.org/paper/6ae7db97f3ca25fa05af292503cc7b5dc0ea5f5f,Symposium on the Theory of Computing
808,Markov decision processes and regular events,,,https://www.semanticscholar.org/paper/8097db0f844d9663ab5e90ae9a8b1db7192d4a03,IEEE Transactions on Automatic Control
525,Finding Feasible Paths for a Two-Point Body,,1989-03-01,https://www.semanticscholar.org/paper/8de116327b63349c4519fb0ce9afa463b9b5e4c3,J. Algorithms
1569,Using Text Embeddings for Causal Inference,"We address causal inference with text documents. For example, does adding a theorem to a paper affect its chance of acceptance? Does reporting the gender of a forum post author affect the popularity of the post? We estimate these effects from observational data, where they may be confounded by features of the text such as the subject or writing quality. Although the text suffices for causal adjustment, it is prohibitively high-dimensional. The challenge is to find a low-dimensional text representation that can be used in causal inference. A key insight is that causal adjustment requires only the aspects of text that are predictive of both the treatment and outcome. Our proposed method adapts deep language models to learn low-dimensional embeddings from text that predict these values well; these embeddings suffice for causal adjustment. We establish theoretical properties of this method. We study it empirically on semi-simulated and real data on paper acceptance and forum post popularity. Code is available at this https URL.",2019-05-29,https://www.semanticscholar.org/paper/298b72096b8a770b0cdb263dd53cf2463b8a1a1d,arXiv.org
1236,Search for large extra dimensions via single photon plus missing energy final states at sqrt s = 1.96 TeV.,We report on a search for large extra dimensions in a data sample of approximately 1 fb(-1) of pp[over] collisions at sqrt s=1.96 TeV. We investigate Kaluza-Klein graviton production with a photon and missing transverse energy in the final state. At the 95% C.L. we set limits on the fundamental mass scale M(D) from 884 to 778 GeV for two to eight extra dimensions.,,https://www.semanticscholar.org/paper/8947c5688408cc22299b6fab270544839b781cab,Physical Review Letters
43,DeSIGN: an intelligent tutor to teach american sign language,"This paper presents the development of DeSIGN, an educational software application for those deaf students who are taught to communicate using American Sign Language (ASL). The software reinforces English vocabulary and ASL signs by providing two essential components of a tutor, lessons and tests. The current version was designed for 5 th and 6 th graders, whose literacy skills lag by a grade or more on average. In addition, a game that allows the students to be creative has been integrated into the tests. Another feature of DeSIGN is its ability to intelligently adapt its tests to the changing knowledge of the student as determined by a knowledge tracing algorithm. A separate interface for the teacher enables additions and modifications to the content of the tutor and provides progress monitoring. These dynamic aspects help motivate the students to use the software repeatedly. This software prototype aims at a feasible and sustainable approach to increase the participation of deaf people in society. DeSIGN has undergone an iteration of testing and is currently in use at a school for the deaf in Pittsburgh.",,https://www.semanticscholar.org/paper/16d388dc0275c888776859884a309168cd5aac0e,Slate
1877,Solid waste management in emerging economies: opportunities and challenges for reuse and recycling,,2021-09-01,https://www.semanticscholar.org/paper/1c7bd4528f3942e32c8007e9b1ac2654b0540477,"Resources, Conservation and Recycling"
1810,Decoupling Sparsity and Smoothness in the Discrete Hierarchical Dirichlet Process,"We present a nonparametric hierarchical Bayesian model of document collections that decouples sparsity and smoothness in the component distributions (i.e., the ""topics""). In the sparse topic model (sparseTM), each topic is represented by a bank of selector variables that determine which terms appear in the topic. Thus each topic is associated with a subset of the vocabulary, and topic smoothness is modeled on this subset. We develop an efficient Gibbs sampler for the sparseTM that includes a general-purpose method for sampling from a Dirichlet mixture with a combinatorial number of components. We demonstrate the sparseTM on four real-world datasets. Compared to traditional approaches, the empirical results will show that sparseTMs give better predictive performance with simpler inferred models.",2009-12-07,https://www.semanticscholar.org/paper/b5026e3fc62a9d7b244fe9a88d22907aa976a7e4,Neural Information Processing Systems
57,Learning to find answers to questions on the Web,"We introduce a method for learning to find documents on the Web that contain answers to a given natural language question. In our approach, questions are transformed into new queries aimed at maximizing the probability of retrieving answers from existing information retrieval systems. The method involves automatically learning phrase features for classifying questions into different types, automatically generating candidate query transformations from a training set of question/answer pairs, and automatically evaluating the candidate transformations on target information retrieval systems such as real-world general purpose search engines. At run-time, questions are transformed into a set of queries, and reranking is performed on the documents retrieved. We present a prototype search engine, Tritus, that applies the method to Web search engines. Blind evaluation on a set of real queries from a Web search engine log shows that the method significantly outperforms the underlying search engines, and outperforms a commercial search engine specializing in question answering. Our methodology cleanly supports combining documents retrieved from different search engines, resulting in additional improvement with a system that combines search results from multiple Web search engines.",2004-05-01,https://www.semanticscholar.org/paper/4d2d2e3d02ba66be54be5170099ee4ba0d923958,TOIT
2895,Computational models of dopamine release measured by fast scan cyclic voltammetry in vivo,"Abstract Dopamine neurotransmission in the striatum is central to many normal and disease functions. Ventral midbrain dopamine neurons exhibit ongoing tonic firing that produces low extrasynaptic levels of dopamine below the detection of conventional extrasynaptic cyclic voltammetry (∼10–20 nanomolar), with superimposed bursts that can saturate the dopamine uptake transporter and produce transient micromolar concentrations. The bursts are known to lead to marked presynaptic plasticity via multiple mechanisms, but analysis methods for these kinetic parameters are limited. To provide a deeper understanding of the mechanics of the modulation of dopamine neurotransmission by physiological, genetic, and pharmacological means, we present three computational models of dopamine release with different levels of spatiotemporal complexity to analyze in vivo fast-scan cyclic voltammetry recordings from the dorsal striatum of mice. The models accurately fit to cyclic voltammetry data and provide estimates of presynaptic dopamine facilitation/depression kinetics and dopamine transporter reuptake kinetics, and we used the models to analyze the role of synuclein proteins in neurotransmission. The models’ results support recent findings linking the presynaptic protein α-synuclein to the short-term facilitation and long-term depression of dopamine release, as well as reveal a new role for β-synuclein and/or γ-synuclein in the long-term regulation of dopamine reuptake.",2023-02-10,https://www.semanticscholar.org/paper/cc344a3a08a028a8ca9b77a39af138c7922f5617,PNAS Nexus
410,Algorithmic aspects of protein structure similarity,"We show that calculating contact map overlap (a measure of similarity of protein structures) is NP-hard, but can be solved in polynomial time for several interesting and relevant special cases. We identify an important special case of this problem corresponding to self-avoiding walks, and prove a decomposition theorem and a corollary approximation result for this special case. These are the first approximation algorithms with guaranteed error bounds, and NP-completeness results in the literature in the area of protein structure alignment/fold recognition for measures of structure similarity of practical interest.",1999-10-17,https://www.semanticscholar.org/paper/92b2a92790e787b122741ef136d36c3737bdd445,40th Annual Symposium on Foundations of Computer Science (Cat. No.99CB37039)
2197,Whose Gene Is It Anyway? The Effect of Preparation Purity on Neutrophil Transcriptome Studies,"Protocols for the isolation of neutrophils from whole blood often result in neutrophil preparations containing low numbers (~5%) of contaminating leukocytes, and it is possible that these contaminating cells contribute to highly sensitive assays that measure neutrophil gene expression (e.g. qPCR). We investigated the contribution of contaminating leukocytes on the transcriptome profile of human neutrophils following stimulation with inflammatory cytokines (GM-CSF, TNFα), using RNA-Seq. Neutrophils were isolated using Polymorphprep or the StemCell untouched neutrophil isolation kit (negative selection of “highly pure” neutrophils). The level of contamination was assessed by morphology and flow cytometry. The major source of contamination in Polymorphprep neutrophil preparations was from eosinophils and was highly donor dependent. Contaminating cells were largely, but not completely, absent in neutrophil suspensions prepared using negative selection, but the overall yield of neutrophils was decreased by around 50%. RNA-seq analysis identified only 25 genes that were significantly differentially-expressed between Polymorphprep and negatively-selected neutrophils across all three treatment groups (untreated, GM-CSF, TNFα). The expression levels of 34 cytokines/chemokines both before and after GM-CSF or TNFα treatment were not significantly different between neutrophil isolation methods and therefore not affected by contributions from non-neutrophil cell types. This work demonstrates that low numbers (<5%) of contaminating leukocytes in neutrophil preparations contribute very little to the overall gene expression profile of cytokine-stimulated neutrophils, and that protocols for the isolation of highly pure neutrophils result in significantly lower yields of cells which may hinder investigations where large numbers of cells are required or where volumes of blood are limited.",2015-09-24,https://www.semanticscholar.org/paper/8e0a74cd5105e724df473d62cdbc807e96f233c8,PLoS ONE
3017,Heterogeneous Multi-Mobile Computing,"As smartphones and tablets proliferate, there is a growing demand for multi-mobile computing, the ability to combine multiple mobile systems into more capable ones. We present M2, a system for multi-mobile computing that enables existing unmodified mobile apps to share and combine multiple devices, including cameras, displays, speakers, microphones, sensors, GPS, and input. M2 introduces a new data-centric approach that leverages higher-level device abstractions and hardware acceleration to efficiently share device data, not API calls. To support heterogeneous devices, M2 introduces device transformation, a new technique to mix and match different types of devices. Example transformations include combining multiple displays into a single larger display for better viewing, or substituting accelerometer for touchscreen input to provide a Nintendo Wii-like experience with existing mobile gaming apps. We have implemented M2 and show that it (1) operates across heterogeneous systems, including multiple versions of Android and iOS, (2) can enable unmodified Android apps to use multiple mobile devices in new and powerful ways, including supporting users with disabilities and better audio conferencing, and (3) can run apps across mobile systems with modest overhead and qualitative performance indistinguishable from using local device hardware.",2019-06-12,https://www.semanticscholar.org/paper/5beb9ed1dd02bcc0baa26d21a2b7568aec47901b,"ACM SIGMOBILE International Conference on Mobile Systems, Applications, and Services"
3225,Knowledgeable Lemurs Become More Central in Social Networks,,2018-04-23,https://www.semanticscholar.org/paper/61b3cfe5cc085225312adf897fb61014d9339bd1,Current Biology
1612,Augment and Reduce: Stochastic Inference for Large Categorical Distributions,"Categorical distributions are ubiquitous in machine learning, e.g., in classification, language models, and recommendation systems. They are also at the core of discrete choice models. However, when the number of possible outcomes is very large, using categorical distributions becomes computationally expensive, as the complexity scales linearly with the number of outcomes. To address this problem, we propose augment and reduce (A&R), a method to alleviate the computational complexity. A&R uses two ideas: latent variable augmentation and stochastic variational inference. It maximizes a lower bound on the marginal likelihood of the data. Unlike existing methods which are specific to softmax, A&R is more general and is amenable to other categorical models, such as multinomial probit. On several large-scale classification problems, we show that A&R provides a tighter bound on the marginal likelihood and has better predictive performance than existing approaches.",2018-02-12,https://www.semanticscholar.org/paper/d640dffe89d91367577e4e017d69a9c2bb3c8339,International Conference on Machine Learning
619,On linear characterizations of combinatorial optimization problems,We show that there can be no computationally tractable description by linear inequalities of the polyhedron associated with any NP-complete combinatorial optimization problem unless NP = co-NP -- a very unlikely event. We also apply the ellipsoid method for linear programming to show that a combinatorial optimization problem is solvable in polynomial time if and only if it admits a small generator of violated inequalities.,1980-10-13,https://www.semanticscholar.org/paper/da27773e409b3a42676019ac33696342778744a5,21st Annual Symposium on Foundations of Computer Science (sfcs 1980)
793,Multiobjective query optimization,"The optimization of queries in distributed database systems is known to be subject to delicate trade-offs. For example, the Mariposa database system allows users to specify a desired delay-cost tradeoff (that is, to supply a decreasing function u(d), specifying how much the user is willing to pay in order to receive the query results within time d); Mariposa divides a query graph into horizontal “strides,” analyzes each stride, and uses a greedy heuristic to find the “best” plan for all strides. We show that Mariposa's greedy heuristic can be arbitrarily far from the desired optimum. Applying a recent approach in multiobjective optimization algorithms to this problem, we show that the optimum cost-delay trade-off (Pareto) curve in Mariposa's framework can be approximated fast within any desired accuracy. We also present a polynomial algorithm for the general multiobjective query optimization problem, which approximates arbirarily well the optimum cost-delay tradeoff (without the restriction of Mariposa's heuristic stride subdivision).",2001-05-01,https://www.semanticscholar.org/paper/e11dbdb31f93e968f401f9f0786a738153aee5d4,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems
774,Checking LTL properties of recursive Markov chains,"We present algorithms for the qualitative and quantitative model checking of linear temporal logic (LTL) properties for recursive Markov chains (RMCs). Recursive Markov chains are a natural abstract model of procedural probabilistic programs and related systems involving recursion and probability. For the qualitative problem (""given a RMC A and an LTL formula /spl phi/, do the computations of A satisfy /spl phi/ almost surely?) we present an algorithm that runs in polynomial space in A and exponential time in /spl phi/. For several classes of RMCs, including RMCs with one exit (a special case that corresponds to well-studied probabilistic systems, e.g., multi-type branching processes and stochastic context-free grammars) the algorithm runs in polynomial time in A and exponential time in /spl phi/. On the other hand, we also prove that the problem is EXPTIME-hard, and hence it is EXPTlME-complete. For the quantitative problem (""does the probability that a computation of A satisfies /spl phi/ exceed a given threshold p?"", or approximate the probability within a desired precision) we present an algorithm that runs in polynomial space in A and exponential space in /spl phi/. For linearly-recursive RMCs, we can compute the exact probability in time polynomial in A and exponential in /spl phi/. These results improve by one exponential, in both the qualitative and quantitative case, the complexity that one would obtain if one first translated the LTL formula to a Buchi automaton and then applied the model checking algorithm for Buchi automata from K. Etessami and M. Yannakakis (2005). Our results combine techniques developed in A. Pnueli and L. D. Zuck. (1993) for analysts of RMCs and in C. Courcoubetis and M. Yannakakis (1995) for LTL model checking of flat Markov Chains, and extend them with new techniques.",2005-09-19,https://www.semanticscholar.org/paper/f5056b97254aa6926c96ba52667ef5945bb2cdc0,International Conference on Quantitative Evaluation of Systems
2313,Impaired neutrophil phagocytosis in preterm neonates: lack of correlation with expression of immunoglobulin or complement receptors.,"Preterm neonates are vulnerable to infection as a result of a compromised immune system. The function of neutrophils from 'well', 'stressed', and 'maturing' preterm neonates was compared with term neonate and adult neutrophils using a whole-blood phagocytosis assay. Cell surface expression of complement receptors and immunoglobulin G receptors was measured on neutrophils in whole blood from the same samples. Fewer actively phagocytosing neutrophils were found in all preterm neonate samples, especially in maturing neonates. Phagocytic rates were slower, and the number of Escherichia coli ingested was smaller in preterm neonate than in term neonate neutrophils. Expression of immunoglobulin G receptors and complement receptor 3 on neutrophils was not directly related to phagocytic activity.",,https://www.semanticscholar.org/paper/70898756d530b223f16e9378db4f1b8070907379,Biology of the Neonate
232,The complexity of fairness through equilibrium,"Competitive equilibrium with equal incomes (CEEI) is a well-known fair allocation mechanism [Foley67:Resource, Varian74: Equity, Thomson85:Theories]; however, for indivisible resources a CEEI may not exist. It was shown in Budish [2011] that in the case of indivisible resources there is always an allocation, called A-CEEI, that is approximately fair, approximately truthful, and approximately efficient, for some favorable approximation parameters. This approximation is used in practice to assign business school students to classes. In this paper we show that finding the A-CEEI allocation guaranteed to exist by Budish's theorem is PPAD-complete. We further show that finding an approximate equilibrium with better approximation guarantees is even harder: NP-complete.",2014-06-01,https://www.semanticscholar.org/paper/a77fcd72e185271d23140699eebf8c460d793ebd,ACM Conference on Economics and Computation
3211,The behavioural challenge of the COVID-19 pandemic: indirect measurements and personalized attitude changing treatments (IMPACT),"Following the outbreak of COVID-19 pandemic, governments around the globe coerced their citizens to adhere to preventive health behaviours, aiming to reduce the effective reproduction numbers of the virus. Driven by game theoretic considerations and inspired by the work of US National Research Council's Committee on Food Habits (1943) during WWII, and the post-WWII Yale Communication Research Program, the present research shows how to achieve enhanced adherence to health regulations without coercion. To this aim, we combine three elements: (i) indirect measurements, (ii) personalized interventions, and (iii) attitude changing treatments (IMPACT). We find that a cluster of short interventions, such as elaboration on possible consequences, induction of cognitive dissonance, addressing next of kin and similar others and receiving advice following severity judgements, improves individuals' health-preserving attitudes. We propose extending the use of IMPACT under closure periods and during the resumption of social and economic activities under COVID-19 pandemic, since efficient and lasting adherence should rely on personal attitudes rather than on coercion alone. Finally, we point to the opportunity of international cooperation generated by the pandemic.",2020-08-01,https://www.semanticscholar.org/paper/dd07776f1d6cc9b1d0e960058ab4dbeffeffcece,Royal Society Open Science
3778,AVSS 2011 demo session: A large-scale benchmark dataset for event recognition in surveillance video,"Summary form only given. We present a concept for automatic construction site monitoring by taking into account 4D information (3D over time), that is acquired from highly-overlapping digital aerial images. On the one hand today's maturity of flying micro aerial vehicles (MAVs) enables a low-cost and an efficient image acquisition of high-quality data that maps construction sites entirely from many varying viewpoints. On the other hand, due to low-noise sensors and high redundancy in the image data, recent developments in 3D reconstruction workflows have benefited the automatic computation of accurate and dense 3D scene information. Having both an inexpensive high-quality image acquisition and an efficient 3D analysis workflow enables monitoring, documentation and visualization of observed sites over time with short intervals. Relating acquired 4D site observations, composed of color, texture, geometry over time, largely supports automated methods toward full scene understanding, the acquisition of both the change and the construction site's progress.",2011-08-30,https://www.semanticscholar.org/paper/2c305fa65fed336e6be1d15a6567075c6ea6e51b,Advanced Video and Signal Based Surveillance
2048,Demand forecast of semiconductor products based on technology diffusion,"Demand forecast plays a critical role to determine capital investment for capacity planning. Given the involved uncertainties and long lead-time for capacity expansion, semiconductor companies have to predict future demands as a basis for related manufacturing strategic decisions. As semiconductor products in a consumer era become more diversified with shortening life cycle, demand forecast also becomes more complex and difficult. This study aims to develop a demand forecast model based on product life cycle and technology diffusion. While little research has been done to employ diffusion models to forecast the demands of semiconductor products. The proposed model modifies a multi-generation diffusion model incorporating marketing variables into the model for semiconductor product and uses nonlinear least square method to estimate the parameters. An empirical study is conducted to validate the proposed model with real data of semiconductor products. This research concludes with discussion on future research directions.",2008-12-07,https://www.semanticscholar.org/paper/610882d6b882729e9fd9fc99a285c82599b41e3c,Online World Conference on Soft Computing in Industrial Applications
3688,There’s a Time and Place for Reasoning Beyond the Image,"Images are often more significant than only the pixels to human eyes, as we can infer, associate, and reason with contextual information from other sources to establish a more complete picture. For example, in Figure 1, we can find a way to identify the news articles related to the picture through segment-wise understandings of the signs, the buildings, the crowds, and more. This reasoning could provide the time and place the image was taken, which will help us in subsequent tasks, such as automatic storyline construction, correction of image source in intended effect photographs, and upper-stream processing such as image clustering for certain location or time.In this work, we formulate this problem and introduce TARA: a dataset with 16k images with their associated news, time, and location, automatically extracted from New York Times, and an additional 61k examples as distant supervision from WIT. On top of the extractions, we present a crowdsourced subset in which we believe it is possible to find the images’ spatio-temporal information for evaluation purpose. We show that there exists a 70% gap between a state-of-the-art joint model and human performance, which is slightly filled by our proposed model that uses segment-wise reasoning, motivating higher-level vision-language joint models that can conduct open-ended reasoning with world knowledge.The data and code are publicly available at https://github.com/zeyofu/TARA.",2022-03-01,https://www.semanticscholar.org/paper/02fff38be9c6caa03a0bfb0de61090971fe2c072,Annual Meeting of the Association for Computational Linguistics
2505,Exploring the Benefits of Augmented Reality Documentation for Maintenance and Repair,"We explore the development of an experimental augmented reality application that provides benefits to professional mechanics performing maintenance and repair tasks in a field setting. We developed a prototype that supports military mechanics conducting routine maintenance tasks inside an armored vehicle turret, and evaluated it with a user study. Our prototype uses a tracked headworn display to augment a mechanic's natural view with text, labels, arrows, and animated sequences designed to facilitate task comprehension, localization, and execution. A within-subject controlled user study examined professional military mechanics using our system to complete 18 common tasks under field conditions. These tasks included installing and removing fasteners and indicator lights, and connecting cables, all within the cramped interior of an armored personnel carrier turret. An augmented reality condition was tested against two baseline conditions: the same headworn display providing untracked text and graphics and a fixed flat panel display representing an improved version of the laptop-based documentation currently employed in practice. The augmented reality condition allowed mechanics to locate tasks more quickly than when using either baseline, and in some instances, resulted in less overall head movement. A qualitative survey showed that mechanics found the augmented reality condition intuitive and satisfying for the tested sequence of tasks.",2011-10-01,https://www.semanticscholar.org/paper/3244243bd0ab1790dfda1128390fd56674c24389,IEEE Transactions on Visualization and Computer Graphics
2599,Collaborative mixed reality visualization of an archaeological excavation,"We present VITA (visual interaction tool for archaeology), an experimental collaborative mixed reality system for offsite visualization of an archaeological dig. Our system allows multiple users to visualize the dig site in a mixed reality environment in which tracked, see-through, head-worn displays are combined with a multi-user, multi-touch, projected table surface, a large screen display, and tracked hand-held displays. We focus on augmenting existing archaeological analysis methods with new ways to organize, visualize, and combine the standard 2D information available from an excavation (drawings, pictures, and notes) with textured, laser range-scanned 3D models of objects and the site itself. Users can combine speech, touch, and 3D hand gestures to interact multimodally with the environment. Preliminary user tests were conducted with archaeology researchers and students, and their feedback is presented here.",2004-11-02,https://www.semanticscholar.org/paper/4835ffa05831617be3e0c36d306b0e0fc339f703,Third IEEE and ACM International Symposium on Mixed and Augmented Reality
422,Latent semantic indexing: a probabilistic analysis,"Latent semantic indexing (LX) is an information retrieval technique based on the spectral analysis of the term-document matrix, whose empirical success had heretofort been without rigorous prediction and explanation. We prove that, under certain conditions, LSI does succeed in capturing the underlying semantics of the corpus and achieves improved retrieval performance. We also propose the technique of random projection as a way of speeding up LSI. We complement our theorems with encouraging experimental results. We also argue that our results may be viewed in a more general framework, as a theoretical basis for the use of spectral methods in a wider class of applications such as collaborative filtering.",1998-05-01,https://www.semanticscholar.org/paper/5cfac7b32b7f2e279a63c90c8f0798f4d9f24c12,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems
3330,Science and the Pursuit of a Sustainable World.,,1993-11-01,https://www.semanticscholar.org/paper/eab3e6f6c0be505da229bcd878234b5062aa519a,Ecological Applications
2444,Evaluating the effect of positional head-tracking on task performance in 3D modeling user interfaces,,2017-06-01,https://www.semanticscholar.org/paper/25a7748cdf18cd535b97e0528b30ef666778fa2d,Computers & graphics
1346,Measurement of the WW production cross section in pp collisions at square root[s]=1.96 TeV.,"We present a measurement of the W boson pair-production cross section in pp collisions at a center-of-mass energy of sqrt[s]=1.96 TeV. The data, collected with the Run II D0 detector at Fermilab, correspond to an integrated luminosity of 224-252 pb(-1) depending on the final state (ee, emu, or mumu). We observe 25 candidates with a background expectation of 8.1+/-0.6(stat)+/-0.6(syst)+/-0.5(lum) events. The probability for an upward fluctuation of the background to produce the observed signal is 2.3x10(-7), equivalent to 5.2 standard deviations. The measurement yields a cross section of 13.8(+4.3)(-3.8)(stat)+1.2-0.9(syst)+/-0.9(lum) pb, in agreement with predictions from the standard model.",,https://www.semanticscholar.org/paper/84444e25cc8f3e71deef5630a09099fd4760abe3,Physical Review Letters
3184,Collective wisdom in polarized groups,"The potential for groups to outperform the cognitive capabilities of even highly skilled individuals, known as the “wisdom of the crowd”, is crucial to the functioning of democratic institutions. In recent years, increasing polarization has led to concern about its effects on the accuracy of electorates, juries, courts, and congress. While there is empirical evidence of collective wisdom in partisan crowds, a general theory has remained elusive. Central to the challenge is the difficulty of disentangling the effect of limited interaction between opposing groups (homophily) from their tendency to hold opposing viewpoints (partisanship). To overcome this challenge, we develop an agent-based model of collective wisdom parameterized by the experimentally-measured behaviour of participants across the political spectrum. In doing so, we reveal that differences across the political spectrum in how individuals express and respond to knowledge interact with the structure of the network to either promote or undermine wisdom. We verify these findings experimentally and construct a more general theoretical framework. Finally, we provide evidence that incidental, context-specific differences across the political spectrum likely determine the impact of polarization. Overall, our results show that whether polarized groups benefit from collective wisdom is generally predictable but highly context-specific.",2022-08-01,https://www.semanticscholar.org/paper/bdd689efbacc2b6e051932cd0243a3e28a194a2b,Collective Intelligence
3188,Staying Alive: Long-Term Success of Bottlenose Dolphin Interventions in Southwest Florida,"Small cetaceans face persistent threats from fisheries interactions, making effective mitigation a priority for conservation. In southwest Florida, interactions come primarily from small-scale recreational hook and line and trap/pot fisheries, with regional stranding network partners working with federal agency managers to assess and intervene as possible in cases of live animal entanglement. Evaluating success of intervention cases is difficult due to financial and logistical constraints which may preclude detailed follow-up monitoring. Survival over the initial 6 weeks post-release has been used as a marker of short-term success for small-cetacean rescue and/or rehabilitation cases. Early intervention prior to stranding, especially via remote disentanglement or rescue and immediate re-release onsite, can save entangled free-ranging dolphins facing life-threatening anthropogenic injuries. However, given the costs associated with interventions, it is important to understand the benefits of these endeavors not only to save individuals, but also to establish if and how saved individuals contribute to social functioning, survival and reproduction within small, resident populations facing multiple concurrent threats. Here we provide evidence from 27 well-documented common bottlenose dolphin (Tursiops truncatus) intervention cases during 1985–2019 where follow-up monitoring over multiple years sheds light on the longer-term success of these efforts and potential benefits to local populations. Nearly all rescued individuals (92%) survived longer than 6 weeks post-release (mean minimum survival period = 5 years, range 0–35 years), with 13 still observed frequently within their prior resident communities, in good physical health, and engaging in normal behavior. Survivorship rates did not decline substantially between 1 and 5 years post-rescue, meaning survival beyond 1 year may be a useful benchmark of long-term success. Rescued females that reached reproductive maturity (n = 4) have produced 12 post-intervention offspring to date. Social network analysis and demographic modeling applied to cases from the long-term resident community in Sarasota Bay confirmed that animals maintain social connections post-intervention and that interventions result in higher population growth rates. While not every intervention succeeds, this study demonstrates the conservation value of pre-stranding interventions which allow individuals that otherwise would be lost to remain viable and productive members of local populations when prevention of anthropogenic injury is not possible.",2021-01-18,https://www.semanticscholar.org/paper/0905b9d270f3ad470556b4fdbbde43edde7a9741,Frontiers in Marine Science
3011,Formally Verified Memory Protection for a Commodity Multiprocessor Hypervisor,"Hypervisors are widely deployed by cloud computing providers to support virtual machines, but their growing complexity poses a security risk, as large codebases contain many vulnerabilities. We present SeKVM, a layered Linux KVM hypervisor architecture that has been formally verified on multiprocessor hardware. Using layers, we isolate KVM’s trusted computing base into a small core such that only the core needs to be verified to ensure KVM’s security guarantees. Using layers, we model hardware features at different levels of abstraction tailored to each layer of software. Lower hypervisor layers that configure and control hardware are verified using a novel machine model that includes multiprocessor memory management hardware such as multi-level shared page tables, tagged TLBs, and a coherent cache hierarchy with cache bypass support. Higher hypervisor layers that build on the lower layers are then verified using a more abstract and simplified model, taking advantage of layer encapsulation to reduce proof burden. Furthermore, layers provide modularity to reduce verification effort across multiple implementation versions. We have retrofitted and verified multiple versions of KVM on Arm multiprocessor hardware, proving the correctness of the implementations and that they contain no vulnerabilities that can affect KVM’s security guarantees. Our work is the first machine-checked proof for a commodity hypervisor using multiprocessor memory management hardware. SeKVM requires only modest KVM modifications and incurs only modest performance overhead versus unmodified KVM on real application workloads.",,https://www.semanticscholar.org/paper/ad320d4d95c9a0f92271d7adc0babd81735a9c85,USENIX Security Symposium
2716,A virtual world for network management,"Existing network management systems typically use a combination of textual displays and 2-D directed graph representations of network topology. A network management system is being designed that uses a virtual world presented through a 3-D stereo display and manipulated with a 3-D mouse. The goal is to allow the user to better understand and control the structure and behavior of a large, complex network. In the current prototype, the user interacts with a 3-D representation of a network whose topology and behavior are specified by a separate network emulator. The user can choose from among a set of different views of the network. For example, one view shows a selected virtual path as a series of logical links contained within a physical path. The system will serve as a testbed for the knowledge-based design of network visualizations.<<ETX>>",1993-09-18,https://www.semanticscholar.org/paper/69e4ddd32ed71f59779940eca699035d9a9c1c6c,Proceedings of IEEE Virtual Reality Annual International Symposium
540,The Complexity of Markov Decision Processes,"We investigate the complexity of the classical problem of optimal policy computation in Markov decision processes. All three variants of the problem finite horizon, infinite horizon discounted, and infinite horizon average cost were known to be solvable in polynomial time by dynamic programming finite horizon problems, linear programming, or successive approximation techniques infinite horizon. We show that they are complete for P, and therefore most likely cannot be solved by highly parallel algorithms. We also show that, in contrast, the deterministic cases of all three problems can be solved very fast in parallel. The version with partially observed states is shown to be PSPACE-complete, and thus even less likely to be solved in polynomial time than the NP-complete problems; in fact, we show that, most likely, it is not possible to have an efficient on-line implementation involving polynomial time on-line computations and memory of an optimal policy, even if an arbitrary amount of precomputation is allowed. Finally, the variant of the problem in which there are no observations is shown to be NP-complete.",1987-08-01,https://www.semanticscholar.org/paper/d51eb16dfed68bf6e16b8b4516d607370b91189a,Mathematics of Operations Research
1807,A Bayesian Analysis of Dynamics in Free Recall,"We develop a probabilistic model of human memory performance in free recall experiments. In these experiments, a subject first studies a list of words and then tries to recall them. To model these data, we draw on both previous psychological research and statistical topic models of text documents. We assume that memories are formed by assimilating the semantic meaning of studied words (represented as a distribution over topics) into a slowly changing latent context (represented in the same space). During recall, this context is reinstated and used as a cue for retrieving studied words. By conceptualizing memory retrieval as a dynamic latent variable model, we are able to use Bayesian inference to represent uncertainty and reason about the cognitive processes underlying memory. We present a particle filter algorithm for performing approximate posterior inference, and evaluate our model on the prediction of recalled words in experimental data. By specifying the model hierarchically, we are also able to capture inter-subject variability.",2009-12-07,https://www.semanticscholar.org/paper/9dfcaad0c019a67c46e0156bc8da52e32628d446,Neural Information Processing Systems
2371,Protein synthesis is activated in primed neutrophils: a possible role in inflammation,,1987-11-01,https://www.semanticscholar.org/paper/cff0b72c32b69f38df8107f75e7104330ccab58b,Bioscience Reports
2597,Evaluation of visual balance for automated layout,"Layout refers to the process of determining the size and position of the visual objects in an information presentation. We introduce the WeightMap, a bitmap representation of the visual weight of a presentation. In addition, we present algorithms that use WeightMaps to allow an automated layout system to evaluate the effectiveness of its layouts. Our approach is based on the concepts of visual weight and visual balance, which are fundamental to the visual arts. The objects in the layout are each assigned a visual weight, and a WeightMap is created that encodes the visual weight of the layout. Image-processing techniques, including pyramids and edge detection, are then used to efficiently analyze the WeightMap for balance. In addition, derivatives of the sums of the rows and columns are used to generate suggestions for how to improve the layout.",2004-01-13,https://www.semanticscholar.org/paper/3cd2864741cb892194b249de980b799bd6e91d75,International Conference on Intelligent User Interfaces
2617,SenseShapes: using statistical geometry for object selection in a multimodal augmented reality,"We introduce a set of statistical geometric tools designed to identify the objects being manipulated through speech and gesture in a multimodal augmented reality system. SenseShapes are volumetric regions of interest that can be attached to parts of the user's body to provide valuable information about the user's interaction with objects. To assist in object selection, we generate a rich set of statistical data and dynamically choose which data to consider based on the current situation.",2003-10-07,https://www.semanticscholar.org/paper/5bca94396a4f7739eb682413e57a325d5dbaaf84,"The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings."
3449,"On the Complexity of Processing Massive, Unordered, Distributed Data","An existing approach for dealing with massive data sets is to stream over the input in few passes and perform computations with sublinear resources. This method does not work for truly massive data where even making a single pass over the data with a processor is prohibitive. Successful log processing systems in practice such as Google's MapReduce and Apache's Hadoop use multiple machines. They efficiently perform a certain class of highly distributable computations defined by local computations that can be applied in any order to the input. 
Motivated by the success of these systems, we introduce a simple algorithmic model for massive, unordered, distributed (mud) computation. We initiate the study of understanding its computational complexity. Our main result is a positive one: any unordered function that can be computed by a streaming algorithm can also be computed with a mud algorithm, with comparable space and communication complexity. We extend this result to some useful classes of approximate and randomized streaming algorithms. We also give negative results, using communication complexity arguments to prove that extensions to private randomness, promise problems and indeterminate functions are impossible. 
We believe that the line of research we introduce in this paper has the potential for tremendous impact. The distributed systems that motivate our work successfully process data at an unprecedented scale, distributed over hundreds or even thousands of machines, and perform hundreds of such analyses each day. The mud model (and its generalizations) inspire a set of complexity-theoretic questions that lie at their heart.",2006-11-21,https://www.semanticscholar.org/paper/31f27ef6ed49a27ac0ff91bc3f776e72e197009f,arXiv.org
1149,Measurement of the Zgamma --> nunu[over ]gamma production cross section and limits on anomalous ZZgamma and Zgammagamma couplings in pp[over] collisions at sqrt[s] = 1.96 TeV.,"We present the first observation of the Zgamma --> nunu[over ]gamma process at the Fermilab Tevatron at 5.1 standard deviations significance, based on 3.6 fb;{-1} of integrated luminosity collected with the D0 detector at the Fermilab Tevatron pp[over ] Collider at sqrt[s] = 1.96 TeV. The measured Zgamma production cross section multiplied by the branching fraction of Z --> nunu[over] is 32 +/- 9(stat + syst) +/-2 (lumi) fb for the photon E_{T} > 90 GeV. It is in agreement with the standard model prediction of 39 +/- 4 fb. We set limits on anomalous trilinear Zgammagamma and ZZgamma gauge boson couplings, most of which are the most restrictive to date.",,https://www.semanticscholar.org/paper/364da6d89c96e08edc49042e8481e5e70c08dbeb,Physical Review Letters
1381,Observation and properties of the X(3872) decaying to J/psipi(+)pi(-) in pp collisions at sqrt[s]=1.96 TeV.,"We report the observation of the X(3872) in the J/psipi(+)pi(-) channel, with J/psi decaying to mu(+)mu(-), in pp collisions at sqrt[s]=1.96 TeV. Using approximately 230 pb(-1) of data collected with the Run II D0 detector, we observe 522+/-100 X(3872) candidates. The mass difference between the X(3872) state and the J/psi is measured to be 774.9+/-3.1(stat)+/-3.0(syst) MeV/c(2). We have investigated the production and decay characteristics of the X(3872) and find them to be similar to those of the psi(2S) state.",2004-05-04,https://www.semanticscholar.org/paper/f410d876f09095bc64cc696f3b4d748a018a62c8,Physical Review Letters
3257,Caught between two worlds: genes and environment influence behaviour of plains×Grevy's zebra hybrids in central Kenya,,2015-08-01,https://www.semanticscholar.org/paper/6aeacde99edc9790173abdacab7759c037e852f6,Animal Behaviour
883,Verifying temporal properties of finite-state probabilistic programs,"The complexity of testing whether a finite-state (sequential or concurrent) probabilistic program satisfies its specification expressed in linear temporal logic. For sequential programs an exponential-time algorithm is given and it is shown that the problem is in PSPACE; this improves the previous upper bound by two exponentials and matches the known lower bound. For concurrent programs is is shown that the problem is complete in double exponential time, improving the previous upper and lower bounds by one exponential each. These questions are also addressed for specifications described by omega -automata or formulas in extended temporal logic.<<ETX>>",1988-10-24,https://www.semanticscholar.org/paper/5d6b5905c006dd527af13a93fc9ba3704663cb64,[Proceedings 1988] 29th Annual Symposium on Foundations of Computer Science
1734,An Adaptive Learning Rate for Stochastic Variational Inference,"Stochastic variational inference finds good posterior approximations of probabilistic models with very large data sets. It optimizes the variational objective with stochastic optimization, following noisy estimates of the natural gradient. Operationally, stochastic inference iteratively subsamples from the data, analyzes the subsample, and updates parameters with a decreasing learning rate. However, the algorithm is sensitive to that rate, which usually requires hand-tuning to each application. We solve this problem by developing an adaptive learning rate for stochastic variational inference. Our method requires no tuning and is easily implemented with computations already made in the algorithm. We demonstrate our approach with latent Dirichlet allocation applied to three large text corpora. Inference with the adaptive learning rate converges faster and to a better approximation than the best settings of hand-tuned rates.",2013-06-16,https://www.semanticscholar.org/paper/7ce557aa5ee42846061a7ee5344ee56b43775ee0,International Conference on Machine Learning
2170,The Inhibitory Effect of Human Beta-defensin-3 on Candida Glabrata Isolated from Patients with Candidiasis,"ABSTRACT Candida glabrata is a common non-albicans Candida species found in patients with candidiasis and it sometimes develops antifungal resistance. Human beta-defensin-3 (hBD-3) is an antimicrobial peptide of immune system active against various types of microbes including Candida spp. This study investigated antifungal activity of hBD-3 and its synergistic effect with a first-line antifungal agent on C. glabrata clinical isolates. Candida spp. were characterised in patients with candidiasis. The antifungal activities of hBD-3 and fluconazole against C. glabrata were evaluated using Broth microdilution assay. The synergistic activity of these two agents was determined by checkerboard microdilution and time-killing assays. The cytotoxicity of hBD-3 was evaluated using LDH-cytotoxicity colorimetric assay. Of 307 episodes from 254 patients diagnosed with candidiasis, C. glabrata was found in 21 clinical isolates. Antifungal susceptibility tests of C. glabrata were performed, fluconazole demonstrated an inhibitory effect at concentrations of 0.25-8 μg/ml, but one antifungal resistant strain was identified (>64 μg/ml). hBD-3 showed an inhibitory effect against all selected strains at concentrations of 50-75 μg/ml and exhibited a synergistic effect with fluconazole at the fractional inhibitory concentration index (FICI) of 0.25-0.50. A concentration of 25 μg/ml of hBD-3 alone showed no cytotoxicity but synergistic activity was seen with fluconazole. In conclusion, hBD-3 has antifungal activity against C. glabrata and synergistic effects with fluconazole at concentrations that alone, have no cytotoxicity. hBD-3 could be used as an adjunctive therapy with first-line antifungal agents for patients with C. glabrata infection particularly those infected with fluconazole-resistant strains.",2020-04-22,https://www.semanticscholar.org/paper/4aae600730758b948a5a6897b3c5c659baad395e,Immunological Investigations
2802,Galectin-3 binds to CD45 on diffuse large B-cell lymphoma cells to regulate susceptibility to cell death.,"Diffuse large B-cell lymphoma (DLBCL) is the most common non-Hodgkin lymphoma and an aggressive malignancy. Galectin-3 (gal-3), the only antiapoptotic member of the galectin family, is overexpressed in DLBCL. While gal-3 can localize to intracellular sites, gal-3 is secreted by DLBCL cells and binds back to the cell surface in a carbohydrate-dependent manner. The major counterreceptor for gal-3 on DLBCL cells was identified as the transmembrane tyrosine phosphatase CD45. Removal of cell-surface gal-3 from CD45 with the polyvalent glycan inhibitor GCS-100 rendered DLBCL cells susceptible to chemotherapeutic agents. Binding of gal-3 to CD45 modulated tyrosine phosphatase activity; removal of endogenous cell-surface gal-3 from CD45 with GCS-100 increased phosphatase activity, while addition of exogenous gal-3 reduced phosphatase activity. Moreover, the increased susceptibility of DLBCL cells to chemotherapeutic agents after removal of gal-3 by GCS-100 required CD45 phosphatase activity. Gal-3 binding to a subset of highly glycosylated CD45 glycoforms was regulated by the C2GnT-1 glycosyltransferase, indicating that specific glycosylation of CD45 is important for regulation of gal-3-mediated signaling. These data identify a novel role for cell-surface gal-3 and CD45 in DLBCL survival and suggest novel therapeutic targets to sensitize DLBCL cells to death.",2012-11-29,https://www.semanticscholar.org/paper/0399898ac1781ca236655c82513aad81c4356013,Blood
3103,MOVE: An End-to-End Solution to Network Denial of Service,"We present a solution to the denial of service (DoS) problem that does not rely on network infrastructure support, conforming to the end-to-end (e2e) design principle. Our approach is to combine an overlay network, which allows us to treat authorized traffic preferentially, with a lightweight process-migration environment that allows us to move services easily between different parts of a distributed system. Functionality residing on a part of the system that is subjected to a DoS attack migrates to an unaffected location. The overlay network ensures that traffic from legitimate users, who are authenticated before they are allowed to access the service, is routed to the new location. We demonstrate the feasibility and effectiveness of our approach by measuring the performance of an experimental prototype against a series of attacks using PlanetLab, a distributed experimental testbed. Our preliminary results show that the end-toend latency remains at acceptable levels during regular operation, increasing only by a factor of 2 to 3, even for large overlays. When a process migrates due to a DoS attack, the disruption of service for the end user is in the order of a few seconds, depending on the network proximity of the servers involved in the migration.",,https://www.semanticscholar.org/paper/104c67cd413008b4370d4cca320db096dc426339,Network and Distributed System Security Symposium
1973,Hybrid estimation of distribution algorithm with multiple subpopulations for semiconductor manufacturing scheduling problem with limited waiting-time constraint,"This paper considers a semiconductor manufacturing scheduling problem (SMSP), subjected to all the practical constraints such as limited waiting time, machine status, different process time on different machines, setup time and arrival time in wafer fabrication facilities (fabs) of semiconductor manufacturing industry. A hybrid estimation of distribution algorithm with multiple subpopulations (HEDA-MS) is proposed to solve SMSP effectively within several specified minutes for an online scheduling requirement. An empirical study simulates eight scenarios from practical data to compare the performance of HEDA-MS and GA, not only to minimize the makespan, but to make total exceeded of limited waiting time into zero. For all the scenarios, the proposed HEDA-MS obtains a smaller makespan than GA with less total exceeded limited waiting time.",2014-10-30,https://www.semanticscholar.org/paper/fc75b55760ce5a1328834a1af67f28da149c81aa,2014 IEEE International Conference on Automation Science and Engineering (CASE)
2036,A hybrid approach of data mining and genetic algorithms for rehabilitation scheduling,"To enhance the medical care quality and patient satisfaction, the hospital management has received considerable attention. This research aims to develop an intelligent approach that integrates Genetic Algorithm (GA) and Data Mining (DM) approaches to resolve the physical therapy scheduling problems to reduce patient waiting time and thus enhance service quality. In particular, this approach employed the attribute-oriented induction method to extract the patterns of the solutions generated from the GA approach. Thus, the decision rules derived from the patterns can be applied to resolve similar therapy scheduling problems with much lesser computational effort. The results of an empirical study conducted in a general hospital validated the practical viability of this approach.",,https://www.semanticscholar.org/paper/bdc12e94e3a01272ac3f28f632735af2ae8eb6fd,International Journal of Manufacturing Technology and Management (IJMTM)
2897,Smoother: A Unified and Modular Framework for Incorporating Structural Dependency in Spatial Omics Data,"Spatial omics technologies can help identify spatially organized biological processes, but existing computational approaches often overlook structural dependencies in the data. Here, we introduce Smoother, a unified framework that integrates positional information into non-spatial models via modular priors and losses. In simulated and real datasets, Smoother enables accurate data imputation, cell-type deconvolution, and dimensionality reduction with remarkable efficiency. In colorectal cancer, Smoother-guided deconvolution revealed plasma cell and fibroblast subtype localizations linked to tumor microenvironment restructuring. Additionally, joint modeling of spatial and single-cell human prostate data with Smoother allowed for spatial mapping of reference populations with significantly reduced ambiguity.",2023-08-07,https://www.semanticscholar.org/paper/d9df5b93fc19eafb90a6809b6af4169676707b00,bioRxiv
478,The parallel complexity of simple logic programs,"We consider logic programs with a single recursive rules, whose right-hand side consists of binary relations forming a chain. We give a complete characterization of all programs of this form that are computable in NC (assuming that <italic>P</italic> <inline-equation> <f>≠</f> </inline-equation>). Our proof uses ideas from automata and language theory, and the combinatorics of strings.",1993-09-01,https://www.semanticscholar.org/paper/440b760f09081084667b1c6caf431381a55dcd08,JACM
3193,"Land use influence on distribution and abundance of herbivores in Samburu-Laikipia, Kenya","The distribution and abundance of different wildlife herbivores was studied in Samburu-Laikipia landscape. The study sites included; Mpala and Oljogi, both commercial ranches in Laikipia district; Oldonyiro and Kipsing community areas in Isiolo district; West Gate Conservancy, Ngaroni Community area, Kalama Community area and Sessia-Barsalinga Community area in Samburu district; and Buffalo Spring National Reserve and Samburu National Reserve both protected areas in the landscape. The objectives of the study were: 1) Determine the influence of different land use on seasonal abundances and distribution wildlife species and 2) Examine the influence of livestock, human settlements and water on wildlife species in Samburu-Laikipia landscape. Distance sampling was used to estimate wildlife, livestock and bomas densities. Distance to nearest water was projected from GPS coordinates for both wildlife and livestock sighting using ARCGIS. Our analysis showed non-uniform distributions of wildlife groups across the Samburu-Laikipia ecosystem largely driven by seasonal rainfall patterns and land use types. Like predicted, most wildlife groups occurred in higher abundances on protected areas, Laikipia commercial ranches and community conservancies unlike in community grazing areas in both dry and wet season. However, large grazers increased substantially in community grazing areas over the wet season when livestock grazing was heavy, stimulating growth of short annuals plants of high-quality nutrients. Human activities had negative influences on all wildlife groups. Our findings indicate that the type of land -use influenced herbivore distribution and abundance in Samburu-Laikipia landscape. This suggests that human activities, including pastoralism, in conjunction with season rainfall patterns and land-use shape herbivore distribution and abundance in the area. Conservation strategies for successfully increasing survival of wildlife therefore, requires maintenance of a mixture of land-use types with well controlled and sustainable development.",2021-07-12,https://www.semanticscholar.org/paper/5cea8da9d9c2cdf4ecbcbd9d38b031c1778781b8,"Journal of sustainability, environment and peace"
2943,Interactions between genetic variation and cellular environment in skeletal muscle gene expression,"From whole organisms to individual cells, responses to environmental conditions are influenced by genetic makeup, where the effect of genetic variation on a trait depends on the environmental context. RNA-sequencing quantifies gene expression as a molecular trait, and is capable of capturing both genetic and environmental effects. In this study, we explore opportunities of using allele-specific expression (ASE) to discover cis acting genotype-environment interactions (GxE) - genetic effects on gene expression that depend on an environmental condition. Treating 17 common, clinical traits as approximations of the cellular environment of 267 skeletal muscle biopsies, we identify 10 candidate interaction quantitative trait loci (iQTLs) across 6 traits (12 unique gene-environment trait pairs; 10% FDR per trait) including sex, systolic blood pressure, and low-density lipoprotein cholesterol. Although using ASE is in principle a promising approach to detect GxE effects, replication of such signals can be challenging as validation requires harmonization of environmental traits across cohorts and a sufficient sampling of heterozygotes for a transcribed SNP. Comprehensive discovery and replication will require large human transcriptome datasets, or the integration of multiple transcribed SNPs, coupled with standardized clinical phenotyping.",2017-02-03,https://www.semanticscholar.org/paper/7ece4ee052fe88f8f535b60c82d19a70065f91d4,bioRxiv
609,The complexity of searching a graph,"T. Parsons proposed and partially analyzed the following pursuit-evasion problem on graphs: A team of searchers traverse the edges of a graph G in pursuit of a fugitive, who moves along the edges of the graph with complete knowledge of the locations of the pursuers. What is the smallest number s(G) of searchers that will suffice for guaranteeing capture of the fugitive? We show that determining whether s(G) ≤ K, for a given integer K, is NP-hard for general graphs but can be solved in linear time for trees. We also provide a structural characterization of those graphs with s(G) ≤ K for K = 1,2,3.",,https://www.semanticscholar.org/paper/bc5cc32ae4e10e2aca8e40170f2ec5a33c84bf7a,22nd Annual Symposium on Foundations of Computer Science (sfcs 1981)
838,Computing the Throughput of a Network with Dedicated Lines,,1993-04-27,https://www.semanticscholar.org/paper/4f79b70053c844b2db434a17dacb81001fb7728c,Discrete Applied Mathematics
1561,A Digital Field Experiment Reveals Large Effects of Friend-to-Friend Texting on Voter Turnout,"Two decades of field experiments on get-out-the-vote tactics suggest that impersonal tactics, like mass emails, have only a modest or negligible effect on voter turnout, while more personal tactics, like door-to-door canvassing, are more effective. However, the COVID-19 pandemic threatens to upend the vast face-to-face voter mobilization efforts that have figured prominently in recent presidential election campaigns. If campaigns can no longer send canvassers to voters' doors, what tactics can they turn to in order to mobilize their supporters? This paper evaluates a promising alternative to face-to-face get-out-the-vote tactics: mobile app technology that enables millions of people to message their friends to urge them to vote. Prior to the most recent US midterm elections in 2018, the mobile app Outvote randomized an aspect of their system, hoping to unobtrusively assess the causal effect of their users' messages on voter turnout. We develop a statistical methodology to address the challenges of such data, and then analyze the Outvote study. Our analysis reveals evidence of very large and statistically significant treatment effects from friend-to-friend mobilization efforts ($\widehat{\textrm{CACE}}$= 8.3, $\textrm{CI}$ = (1.2, 15.3)). Further, the statistical methodology can be used to study other friend-to-friend messaging efforts. These results suggest that friend-to-friend texting, which is a personal voter mobilization effort that does not require face-to-face contact, is an effective alternative to conventional voter mobilization tactics.",2020-09-21,https://www.semanticscholar.org/paper/a75c9e46d5d5d84fb5b089186af123a20e22d8e6,Social Science Research Network
1020,The Geometric Structure of Externally Actuated Planar Locomoting Systems in Ambient Media,"Robots often interact with the world via attached parts such as wheels, joints, or appendages. In many systems, these interactions, and the manner in which they lead to locomotion, can be understood using the machinery of geometric mechanics, explaining how inputs in the shape space of a robot affect motion in its configuration space and the configuration space of its environment. In this paper we consider an opposite type of locomotion, wherein robots are influenced actively by interactions with an externally forced ambient medium. We investigate two examples of externally actuated systems; one for which locomotion is governed by a principal connection, and is usually considered to possess no drift dynamics, and another for which no such connection exists, with drift inherent in its locomotion. For the driftless system, we develop geometric tools based on previously understood internally actuated versions of the system and demonstrate their use for motion planning under external actuation. For the system possessing drift, we employ nonholonomic reduction to obtain a reduced representation of the system dynamics, illustrate geometric features conducive to studying locomotion, and derive strategies for external actuation.",2021-08-14,https://www.semanticscholar.org/paper/e38c13e5e83b34494c27a4d73477b1bc1e9baf0f,arXiv.org
2651,Guest Editors' Introduction: Virtual Reality,,,https://www.semanticscholar.org/paper/475aa90ce96d93c726805b5c4e04cafe35c0cc8e,IEEE Computer Graphics and Applications
2319,Role of Fc gamma receptors in the activation of neutrophils by soluble and insoluble immunoglobulin aggregates isolated from the synovial fluid of patients with rheumatoid arthritis.,"OBJECTIVES--Synovial fluid from patients with rheumatoid arthritis contains both soluble and insoluble immunoglobulin aggregates which activate reactive oxidant production in human neutrophils. The objectives were to determine the roles played by Fc gamma receptors in activation of neutrophils by these complexes. METHODS--Pronase treatment was used to remove Fc gamma RIII from the neutrophil surface and blocking monoclonal antibodies were used to prevent the binding of complexes to Fc gamma RII and Fc gamma RIII. RESULTS--When Fc gamma RIII was removed from the cell surface by pronase treatment, activation by the soluble aggregates did not occur [mean (SD) inhibition 89 (16)%, n = 6] whereas activation via the insoluble aggregates was less affected [34 (16)%, n = 6]. Blocking the binding to Fc gamma RIII with antibodies decreased activation in response to the soluble aggregates [mean (SD) inhibition 71 (22)%, n = 8] but again had a lower effect on activation by the insoluble aggregates [40 (17)%, n = 9]. When binding to Fc gamma RII was blocked, activation via the soluble aggregates was substantially inhibited [mean (SD) 93 (13)%, n = 8] whereas that via the insoluble aggregates was inhibited to a much lesser extent [28 (38)%, n = 9]. When Fc gamma RII and III were simultaneously blocked, activation by the insoluble aggregates was only inhibited by 45% [(19), n = 5]. CONCLUSION--These data thus indicate that activation of human neutrophils by soluble immunoglobulin aggregates from rheumatoid synovial fluid occurs via cooperative occupancy of both Fc gamma RII and III: perturbation of binding to either of these receptor classes will abrogate activation.",1994-08-01,https://www.semanticscholar.org/paper/2358b0357a1c04a33b634fa26a2f545a7fb3ac33,Annals of the Rheumatic Diseases
3222,Resolving a conservation dilemma: Vulnerable lions eating endangered zebras,"When predators are removed or suppressed for generations, prey populations tend to increase and when predators are re-introduced, prey densities should fall back to pre-control levels. In cases of apparent competition where there are alternate abundant and rare prey species, rare species may decline further than expected or disappear altogether. Recently, concern about the impact of recovering predator populations on wildlife in Laikipia County, Kenya, has led to questions of whether lions (Panthera leo, IUCN Red List Vulnerable) exert top-down pressure on Grevy’s zebra (Equus grevyi, IUCN Red List Endangered). We examined effects of lion predation on Plain’s zebra (E. quagga, IUCN Red List Near Threatened) and Grevy’s zebra populations in a 2,105 km2 area defined by lion movements. We used line transect surveys to estimate density of Grevy’s (0.71/km2) and Plain’s (15.9/km2) zebras, and satellite telemetry to measure movements for lions and both zebras. We tracked lions to potential feeding sites to estimate predation rates on zebras. We compared field-based estimates of predation rates on both zebras to random gas models of encounters that result in predation to ask if lions prey preferentially on Grevy’s zebra at a sufficient rate to drive population declines. Lions preyed on Grevy’s zebra significantly less than expected in 15 of 16 (94%) scenarios considered and lions preyed on Plain’s zebras as expected or significantly less than expected in 15 of 16 scenarios. Population trend of Grevy’s zebra indicates that the Kenya population may be stabilizing. Recruitment rate to the population has tripled since 2004, making it unlikely that lions are having an impact on Grevy’s zebras. In Laikipia County, competitive displacement by livestock (Livestock: Grevy’s zebra ratio = 864:1) and interference competition for grass with Plain’s zebra (Plain’s zebra:Grevy’s zebra ratio = 22:1) are most likely the predominant threats to Grevy’s Zebra recovery.",2018-08-29,https://www.semanticscholar.org/paper/34484d636900b210d49daceb0b6edb5033f6a558,PLoS ONE
733,Computation of Least Fixed Points,,2012-08-27,https://www.semanticscholar.org/paper/5b40d4e4ecea846c70f5bd6c307188825d661b88,International Symposium on Mathematical Foundations of Computer Science
3766,Where are they looking?,"Humans have the remarkable ability to follow the gaze of other people to identify what they are looking at. Following eye gaze, or gaze-following, is an important ability that allows us to understand what other people are thinking, the actions they are performing, and even predict what they might do next. Despite the importance of this topic, this problem has only been studied in limited scenarios within the computer vision community. In this paper, we propose a deep neural network-based approach for gaze-following and a new benchmark dataset, GazeFollow, for thorough evaluation. Given an image and the location of a head, our approach follows the gaze of the person and identifies the object being looked at. Our deep network is able to discover how to extract head pose and gaze orientation, and to select objects in the scene that are in the predicted line of sight and likely to be looked at (such as televisions, balls and food). The quantitative evaluation shows that our approach produces reliable results, even when viewing only the back of the head. While our method outperforms several baseline approaches, we are still far from reaching human performance on this task. Overall, we believe that gaze-following is a challenging and important problem that deserves more attention from the community.",2015-12-07,https://www.semanticscholar.org/paper/7294d3ac0001e4b36c67aeb5c31d1db8ba1da23a,Neural Information Processing Systems
710,Passive Static Equilibrium with Frictional Contacts and Application to Grasp Stability Analysis,"This paper studies the problem of passive grasp stability under an external disturbance, that is, the ability of a grasp to resist a disturbance through passive responses at the contacts. To obtain physically consistent results, such a model must account for friction phenomena at each contact; the difficulty is that friction forces depend in non-linear fashion on contact behavior (stick or slip). We develop the first polynomial-time algorithm which either solves such complex equilibrium constraints for two-dimensional grasps, or otherwise concludes that no solution exists. To achieve this, we show that the number of possible `slip states' (where each contact is labeled as either sticking or slipping) that must be considered is polynomial (in fact quadratic) in the number of contacts, and not exponential as previously thought. Our algorithm captures passive response behaviors at each contact, while accounting for constraints on friction forces such as the maximum dissipation principle.",2018-06-04,https://www.semanticscholar.org/paper/6594b81a9d2f9cfb94abe816ca5489c18d657926,Robotics: Science and Systems
3779,Video Annotation and Tracking with Active Learning,"We introduce a novel active learning framework for video annotation. By judiciously choosing which frames a user should annotate, we can obtain highly accurate tracks with minimal user effort. We cast this problem as one of active learning, and show that we can obtain excellent performance by querying frames that, if annotated, would produce a large expected change in the estimated object track. We implement a constrained tracker and compute the expected change for putative annotations with efficient dynamic programming algorithms. We demonstrate our framework on four datasets, including two benchmark datasets constructed with key frame annotations obtained by Amazon Mechanical Turk. Our results indicate that we could obtain equivalent labels for a small fraction of the original cost.",2011-12-12,https://www.semanticscholar.org/paper/5f5d107016990cb297c26fbd7bee083c6df3aa62,Neural Information Processing Systems
359,On the complexity of price equilibria,,2003-09-01,https://www.semanticscholar.org/paper/aab30ce2f074d94c47b0a8782214fd64bf360ded,Journal of computer and system sciences (Print)
556,The complexity of recognizing polyhedral scenes,,1985-10-21,https://www.semanticscholar.org/paper/5ab528cf8712117494715e57c5f4c7626aaf4a3a,26th Annual Symposium on Foundations of Computer Science (sfcs 1985)
3698,Shadows Shed Light on 3D Objects,"3D reconstruction is a fundamental problem in computer vision, and the task is especially challenging when the object to reconstruct is partially or fully occluded. We introduce a method that uses the shadows cast by an unobserved object in order to infer the possible 3D volumes behind the occlusion. We create a differentiable image formation model that allows us to jointly infer the 3D shape of an object, its pose, and the position of a light source. Since the approach is end-to-end differentiable, we are able to integrate learned priors of object geometry in order to generate realistic 3D shapes of different object categories. Experiments and visualizations show that the method is able to generate multiple possible solutions that are consistent with the observation of the shadow. Our approach works even when the position of the light source and object pose are both unknown. Our approach is also robust to real-world images where ground-truth shadow mask is unknown.",2022-06-17,https://www.semanticscholar.org/paper/571cfbddba05cd997c27dff0b0ae9b1e02f7eb17,arXiv.org
1305,Limits on spin-independent interactions of weakly interacting massive particles with nucleons from the two-tower run of the cryogenic dark matter search.,"We report new results from the Cryogenic Dark Matter Search (CDMS II) at the Soudan Underground Laboratory. Two towers, each consisting of six detectors, were operated for 74.5 live days, giving spectrum-weighted exposures of 34 (12) kg d for the Ge (Si) targets after cuts, averaged over recoil energies 10-100 keV for a weakly interacting massive particle (WIMP) mass of 60 GeV/c2. A blind analysis was conducted, incorporating improved techniques for rejecting surface events. No WIMP signal exceeding expected backgrounds was observed. When combined with our previous results from Soudan, the 90% C.L. upper limit on the spin-independent WIMP-nucleon cross section is 1.6 x 10(-43) cm2 from Ge and 3 x 10(-42) cm2 from Si, for a WIMP mass of 60 GeV/c2. The combined limit from Ge (Si) is a factor of 2.5 (10) lower than our previous results and constrains predictions of supersymmetric models.",2006-01-13,https://www.semanticscholar.org/paper/256ce0f0a9c0f3ef0176fa8a7f3e435f2a344ead,Physical Review Letters
2385,Complement-mediated lysis of pigeon erythrocyte ghosts analysed by flow cytometry. Evidence for the involvement of a 'threshold' phenomenon.,"Flow-cytometric analysis of complement-mediated lysis of antibody-coated pigeon erythrocyte ghosts containing fluorescein was carried out to determine whether lysis involved a gradual release of fluorescein or a 'threshold' release from individual cells. Antibody-coated ghosts were comprised of three subpopulations identified by fluorescence and scatter (size). These were: (a) highly fluorescent, medium scatter, (b) medium fluorescence, high scatter, and (c) low (or zero) fluorescence, low scatter. Lysed ghosts and isolated nuclei were identified by fluorescence microscopy and scanning electron microscopy. Fluorescence distributions analysed by flow cytometry indicated that, after complement attack, those ghosts remaining intact retained all their fluorescent label. A time course of changes in ratios of the three subpopulations indicated that once lysis of an individual ghost was initiated, release of label was complete within 1 min; no stages of intermediary fluorescence appeared, and those ghosts remaining at the end of the experiment retained the same fluorescence intensity as control ghosts. The results supported the hypothesis that complement-mediated cell lysis is a 'threshold' phenomenon; a submaximal response by a cell population representing a complete response by only some of the cells rather than a partial response by all of the cells.",1983-10-15,https://www.semanticscholar.org/paper/9e592fd58c84f802ad7d134b39f835ba12b2ee22,Biochemical Journal
698,Epinoia: Intent Checker for Stateful Networks,"Intent-Based Networking (IBN) has been increasingly deployed in production enterprise networks. Automated network configuration in IBN lets operators focus on intents- i.e., the end to end business objectives-rather than spelling out details of the configurations that implement these objectives. Automation brings its own concerns as the administrators cannot rely on traditional network troubleshooting tools. This situation is further exacerbated in the case of stateful Network Functions (NFs) whose packet processing behavior depends on previously observed traffic patterns. To ensure that the network configuration and state derived from network automation matches the administrator’s specified intent, we propose, Epinoia, a network intent checker for stateful networks. Epinoia relies on a unified model for NFs by leveraging the causal precedence relationships that exist between NF packet I/Os and states. Scalability of Epinoia is achieved by decomposing intents into sub-checking tasks and maintaining a causality graph between checked invariants. Epinoia checks for network-wide intent violations incrementally to reduce overhead in the event of network changes. Our evaluation results using real-world network topologies show that Epinoia can perform comprehensive checking within a few seconds per network with intent updates.",2021-07-01,https://www.semanticscholar.org/paper/93adbd0fcd79f4d1efe86f0b5fa9c0cd0cade3d9,International Conference on Computer Communications and Networks
454,Topological Inference,"Geographical database systems deal with certain basic topological relations such as ""A overlaps B"" and ""B contains C"" between simply connected regions in the plane. It is of great interest to make sound inferences from elementary statements of this form. This problem has been identified extensively in the recent literature, but very limited progress has been made towards addressing the considerable technical difficulties involved. In this paper we study the computational problems involved in developing such an inference system. We point out that the problem has two distinct components that interact in rather complex ways: relational consistency, and planarity. We develop polynomial-time algorithms for several important special cases, and prove almost all the others to be NP-hard.",1995-08-20,https://www.semanticscholar.org/paper/77c8f39a59e6082115fc97898f131a5da4cd4d15,International Joint Conference on Artificial Intelligence
3764,Do We Need More Training Data?,,2015-03-01,https://www.semanticscholar.org/paper/36961e595e31dea127bfa49124b5cc4f2100710f,International Journal of Computer Vision
2974,Fixed-Form Variational Posterior Approximation through Stochastic Linear Regression,"textabstractWe propose a general algorithm for approximating nonstandard Bayesian posterior distributions. The algorithm minimizes the Kullback-Leibler divergence of an approximating distribution to the intractable posterior distribu- tion. Our method can be used to approximate any posterior distribution, provided that it is given in closed form up to the proportionality constant. The approxi- mation can be any distribution in the exponential family or any mixture of such distributions, which means that it can be made arbitrarily precise. Several exam- ples illustrate the speed and accuracy of our approximation method in practice.",2012-06-28,https://www.semanticscholar.org/paper/51336c08ea621d9c1c724607daf86991b2931fa3,arXiv.org
31,Building query optimizers for information extraction: the SQoUT project,"Text documents often embed data that is structured in nature. This structured data is increasingly exposed using information extraction systems, which generate structured relations from documents, introducing an opportunity to process expressive, structured queries over text databases. This paper discusses our SQoUT1 project, which focuses on processing structured queries over relations extracted from text databases. We show how, in our extraction-based scenario, query processing can be decomposed into a sequence of basic steps: retrieving relevant text documents, extracting relations from the documents, and joining extracted relations for queries involving multiple relations. Each of these steps presents different alternatives and together they form a rich space of possible query execution strategies. We identify execution efficiency and output quality as the two critical properties of a query execution, and argue that an optimization approach needs to consider both properties. To this end, we take into account the userspecified requirements for execution efficiency and output quality, and choose an execution strategy for each query based on a principled, cost-based comparison of the alternative execution strategies.",2009-03-20,https://www.semanticscholar.org/paper/7acdfa6c26662376079d924e731fa795597122f0,SGMD
3108,Experiences teaching operating systems using virtual platforms and Linux,"Operating system courses teach students much more when they provide hands-on kernel-level project experience with a real operating system. However, enabling a large class of students to do kernel development can be difficult. To address this problem, we created a virtual kernel development environment in which operating systems can be developed, debugged, and rebooted in a shared computer facility without affecting other users. Using virtual machines and remote display technology, our virtual kernel development laboratory enables even distance learning students at remote locations to participate in kernel development projects with on-campus students. We have successfully deployed and used our virtual kernel development environment together with the open-source Linux kernel to provide kernel-level project experiences for over nine hundred students in the introductory operating system course at Columbia University.",2005-02-23,https://www.semanticscholar.org/paper/967a4a1d8bf1ef13051c6aa75d4c2348fe60b7d2,OPSR
316,The complexity of computing a Nash equilibrium,"We resolve the question of the complexity of Nash equilibrium by showing that the problem of computing a Nash equilibrium in a game with 4 or more players is complete for the complexity class PPAD. Our proof uses ideas from the recently-established equivalence between polynomial time solvability of normal form games and graphical games, establishing that these kinds of games can simulate a PPAD-complete class of Brouwer functions.",2006-05-21,https://www.semanticscholar.org/paper/2982faa144efe77492e13e64917a7337d0b96008,Symposium on the Theory of Computing
983,Safety of Nonporous Silica Nanoparticles in Human Corneal Endothelial Cells,,2017-11-06,https://www.semanticscholar.org/paper/049ace421ade4c45c779e29c6923e33ef8de02da,Scientific Reports
846,Online minimization of transition systems (extended abstract),"We are given a transition system implicitly through a compact representation and wish to perform simultaneously reachability analysis and minimization without constructing first the whole system graph. We present an algorithm for this problem that applies to general systems, provided we have appropriate primitive operations for manipulating blocks of states and we can determine termination; the number of operations needed to construct the minimal reachable graph is quadratic in the size of this graph. We specialize the method to obtain efficient algorithms for extended finite state machines that apply separable affine transformations on the variables.",1992-07-01,https://www.semanticscholar.org/paper/0383138611eb997fe52ba8a3b96d727580a627e6,Symposium on the Theory of Computing
2627,Mutual disambiguation of 3D multimodal interaction in augmented and virtual reality,"We describe an approach to 3D multimodal interaction in immersive augmented and virtual reality environments that accounts for the uncertain nature of the information sources. The resulting multimodal system fuses symbolic and statistical information from a set of 3D gesture, spoken language, and referential agents. The referential agents employ visible or invisible volumes that can be attached to 3D trackers in the environment, and which use a time-stamped history of the objects that intersect them to derive statistics for ranking potential referents. We discuss the means by which the system supports mutual disambiguation of these modalities and information sources, and show through a user study how mutual disambiguation accounts for over 45% of the successful 3D multimodal interpretations. An accompanying video demonstrates the system in action.",2003-11-05,https://www.semanticscholar.org/paper/ed54d91b97f134437f18706462d957bde46ad0c3,International Conference on Multimodal Interaction
13,Learning to Rank Adaptively for Scalable Information Extraction,"Information extraction systems extract structured data from natural language text, to support richer querying and analysis of the data than would be possible over the unstructured text. Unfortunately, information extraction is a computationally expensive task, so exhaustively processing all documents of a large collection might be prohibitive. Such exhaustive processing is generally unnecessary, though, because many times only a small set of documents in a collection is useful for a given information extraction task. Therefore, by identifying these useful documents, and not processing the rest, we could substantially improve the efficiency and scalability of an extraction task. Existing approaches for identifying such documents often miss useful documents and also lead to the processing of useless documents unnecessarily, which in turn negatively impacts the quality and efficiency of the extraction process. To address these limitations of the state-of-the-art techniques, we propose a principled, learning-based approach for ranking documents according to their potential usefulness for an extraction task. Our low-overhead, online learning-to-rank methods exploit the information collected during extraction, as we process new documents and the fine-grained characteristics of the useful documents are revealed. Then, these methods decide when the ranking model should be updated, hence significantly improving the document ranking quality over time. Our experiments show that our approach achieves higher accuracy than the state-of-the-art alternatives. Importantly, our approach is lightweight and efficient, and hence is a substantial step towards scalable information extraction.",,https://www.semanticscholar.org/paper/0316c64a5eb9152c18433fa4b92e023881e87287,International Conference on Extending Database Technology
2657,Exploring MARS: developing indoor and outdoor user interfaces to a mobile augmented reality system,,1999-12-01,https://www.semanticscholar.org/paper/0da46b2f0c58f993af35a976033ff056c56cdf86,Computers & graphics
2913,Single-cell multi-omics defines the cell-type specific impact of splicing aberrations in human hematopoietic clonal outgrowths,"RNA splicing factors are recurrently affected by alteration-of-function mutations in clonal blood disorders, highlighting the importance of splicing regulation in hematopoiesis. However, our understanding of the impact of dysregulated RNA splicing has been hampered by the inability to distinguish mutant and wildtype cells in primary patient samples, the cell-type complexity of the hematopoietic system, and the sparse and biased coverage of splice junctions by short-read sequencing typically used in single-cell RNA sequencing. To overcome these limitations, we developed GoT-Splice by integrating Genotyping of Transcriptomes (GoT) with enhanced efficiency long-read single-cell transcriptome profiling, as well as proteogenomics (with CITE-seq). This allowed for the simultaneous single-cell profiling of gene expression, cell surface protein markers, somatic mutation status, and RNA splicing. We applied GoT-Splice to bone marrow progenitors from patients with myelodysplastic syndrome (MDS) affected by mutations in the most prevalent mutated RNA splicing factor – the core RNA splicing factor SF3B1. High-resolution mapping of SF3B1mut vs. SF3B1wt hematopoietic progenitors revealed a fitness advantage of SF3B1mut cells in the megakaryocytic-erythroid lineage, resulting in an expansion of SF3B1mut erythroid progenitor (EP) cells. SF3B1mut EP cells exhibited upregulation of genes involved in regulation of cell cycle and mRNA translation. Long-read single-cell transcriptomes revealed the previously reported increase of aberrant 3’ splicing site usage in SF3B1mut cells. However, the ability to profile splicing within individual cell populations uncovered distinct cryptic 3’ splice site usage across different progenitor populations, as well as stage-specific aberrant splicing during erythroid maturation. Lastly, as splice factor mutations occur in clonal hematopoiesis (CH) with increased risk of neoplastic transformation, we applied GoT-Splice to CH samples. These data revealed that the erythroid lineage bias, as well as cell-type specific cryptic 3’ splice site usage in SF3B1mut cells, precede overt MDS. Collectively, we present an expanded multi-omics single-cell toolkit to define the cell-type specific impact of somatic mutations on RNA splicing, from the earliest phases of clonal outgrowths to overt neoplasia, directly in human samples.",2022-06-09,https://www.semanticscholar.org/paper/d56cc7ab718dd58733360f83b0e3d2070e786510,bioRxiv
2507,Awareness of the Care Team in Electronic Health Records,"Summary Objective: To support collaboration and clinician-targeted decision support, electronic health records (EHRs) must contain accurate information about patients’ care providers. The objective of this study was to evaluate two approaches for care provider identification employed within a commercial EHR at a large academic medical center. Methods: We performed a retrospective review of EHR data for 121 patients in two cardiology wards during a four-week period. System audit logs of chart accesses were analyzed to identify the clinicians who were likely participating in the patients’ hospital care. The audit log data were compared with two functions in the EHR for documenting care team membership: 1) a vendor-supplied module called “Care Providers”, and 2) a custom “Designate Provider” order that was created primarily to improve accuracy of the attending physician of record documentation. Results: For patients with a 3–5 day hospital stay, an average of 30.8 clinicians accessed the electronic chart, including 10.2 nurses, 1.4 attending physicians, 2.3 residents, and 5.4 physician assistants. The Care Providers module identified 2.7 clinicians/patient (1.8 attending physicians and 0.9 nurses). The Designate Provider order identified 2.1 clinicians/patient (1.1 attending physicians, 0.2 resident physicians, and 0.8 physician assistants). Information about other members of patients’ care teams (social workers, dietitians, pharmacists, etc.) was absent. Conclusions: The two methods for specifying care team information failed to identify numerous individuals involved in patients’ care, suggesting that commercial EHRs may not provide adequate tools for care team designation. Improvements to EHR tools could foster greater collaboration among care teams and reduce communication-related risks to patient safety.",,https://www.semanticscholar.org/paper/5cd5ba3dbe58348497f9000f687e6789c3fe0340,Applied Clinical Informatics
1640,Context Selection for Embedding Models,"Word embeddings are an effective tool to analyze language. They have been recently extended to model other types of data beyond text, such as items in recommendation systems. Embedding models consider the probability of a target observation (a word or an item) conditioned on the elements in the context (other words or items). In this paper, we show that conditioning on all the elements in the context is not optimal. Instead, we model the probability of the target conditioned on a learned subset of the elements in the context. We use amortized variational inference to automatically choose this subset. Compared to standard embedding models, this method improves predictions and the quality of the embeddings.",,https://www.semanticscholar.org/paper/fea7064e129db2e577f8c3b84a6d11a9f835a8c3,Neural Information Processing Systems
2610,Single-handed interaction techniques for multiple pressure-sensitive strips,"We present a set of interaction techniques that make novel use of a small pressure-sensitive pad to allow one-handed direct control of a large number of parameters. The surface of the pressure-sensitive pad is logically divided into four linear strips which simulate traditional interaction metaphors and the functions of which may be modified dynamically under software control. No homing of the hand or fingers in needed once the fingers are placed above their corresponding strips. We show how the number of strips on the pad can be virtually extended from four to fourteen by detecting contact pressure differences and dual-finger motions. Due to the compact size of the device and the method of interaction, which does not rely on on-screen widgets or the 2D navigation of a cursor, the versatile input system may be used in applications, where it is advantageous to minimize the amount of visual feedback required for interaction.",2004-04-24,https://www.semanticscholar.org/paper/dead52eeb4da3a06ee471de3b9dce7f763c1eda9,CHI EA '04
968,Hemisphere opposite to vascular trunk deviation is earlier affected by glaucomatous damage in myopic high-tension glaucoma,"Purpose To investigate whether the position of the central vascular trunk, as a surrogate of lamina cribrosa (LC) shift, is associated with the initial hemisphere of visual field defect in myopic high-tension glaucoma (HTG) eyes. Methods The deviation of the central vascular trunk was measured from the center of the Bruch’s membrane opening (BMO), which was delineated by OCT imaging. The angular deviation was measured with the horizontal nasal midline as 0° and the superior location as a positive value. The initial hemisphere developing visual field defect was defined as three connected abnormal points (having a P value with less than 0.5% probability of being normal) appearing in only one hemisphere in pattern deviation plots. If those points were observed in both hemispheres initially, the eye was classified as bi-hemispheric visual field defect. Results Initially, 36 eyes (44%) had superior visual field defects, 27 (33%) inferior visual field defects, and 18 (22%) bi-hemispheric visual field defects. After a mean follow-up of 5 years, the number of bi-hemispheric visual field defects had increased to 34 (42%). A logistic regression analysis revealed that inferior deviation of vascular trunk was the only factor associated with initial inferior visual field defect (P = 0.001), while initial bi-hemispheric visual field defects were associated with worse mean deviation at initial visits (P<0.001). A conditional inference tree analysis showed that both the angular deviation (P<0.001) and initial mean deviation (P = 0.025) determined the initial hemispheres developing visual field defect. Conclusions Although both hemispheres were involved as glaucoma progression, the axons on the side counter to the vascular trunk deviation were damaged earlier in HTG. This finding implies the LC shift could add additional stress to axons exposed to high intraocular pressure.",2020-05-18,https://www.semanticscholar.org/paper/e7f71b62661f1fb128cecba7a03faa0ee60925f3,PLoS ONE
2264,Granulocyte Macrophage Colony-stimulating Factor Signaling and Proteasome Inhibition Delay Neutrophil Apoptosis by Increasing the Stability of Mcl-1*,"Human neutrophils normally have a very short half-life and die by apoptosis. Cytokines such as granulocyte-macrophage colony-stimulating factor (GM-CSF) can delay this apoptosis via increases in the cellular levels of Mcl-1, an anti-apoptotic protein of the Bcl-2 family with a rapid turnover rate. Here we have shown that inhibition of the proteasome (a) decreases the rate of Mcl-1 turnover within neutrophils and (b) significantly delays apoptosis. This led us to determine whether GM-CSF could enhance neutrophil survival by altering the rate of Mcl-1 turnover. Addition of GM-CSF to neutrophils enhanced Mcl-1 stability and delayed apoptosis by signaling pathways requiring PI3K/Akt and p44/42 Erk/Mek, because inhibitors of these pathways completely abrogated the GM-CSF-mediated effect on both Mcl-1 stability and apoptosis delay. Conversely, induction of Mcl-1 hyperphosphorylation by the phosphatase inhibitor, okadaic acid, significantly accelerated both Mcl-1 turnover and apoptosis. Neither the calpain inhibitor, carbobenzoxy-valinyl-phenylalaninal, nor the pan caspase inhibitor, benzyloxycarbonyl-VAD-fluoromethylketone, had any effect on Mcl-1 stability under these conditions. These observations indicate that profound changes in the rate of neutrophil apoptosis following cytokine signaling occur via dynamic changes in the rate of Mcl-1 turnover via the proteasome.",2004-06-25,https://www.semanticscholar.org/paper/cebcb1108cdfad92ebcbb37ecf89466006f3d2e4,Journal of Biological Chemistry
2650,Environment management for hybrid user interfaces,"As computers proliferate, becoming smaller, more mobile, more powerful and more diverse, how will the ways in which we interact with them change? In this article, we describe research in developing ""hybrid user interfaces"" that tie together the diverse displays and interaction devices that a user may encounter in a mobile, shared environment. Controlling such a dynamically changing, heterogeneous mix of computers is a problem that we refer to as ""environment management"". We sketch some ways in which publicizing semantic information about computational objects and tasks can make it possible to automate environment management operations, and we describe research testbeds we are developing within which to explore these ideas.",2000-10-01,https://www.semanticscholar.org/paper/4341c9491c9832331ba388d89a7e4f6a9f898e13,IEEE wireless communications
2015,Guest Editorial Equipment and Operations Automation in the Semiconductor Industry,"The nine The ten papers in this special issue focus on three areas: equipment automation, operations automation, and modeling.",,https://www.semanticscholar.org/paper/80021c81c9365ffb80acdfe7528a7b66f73ceae6,IEEE Transactions on Automation Science and Engineering
2736,Authoring Large Hypermedia Documents with IGD,"SUMMARY The IGD (Interactive Graphical Documents) hypermedia system was designed to make possible interactive presentations that can be explored by and customized for individual users. We describe IGD’s authoring facilities through an annotated excerpt from an editing session, emphasizing how the system’s document model and user interface help support the creation of large documents. Although we feel that IGD successfully addressed some of the issues of scale, experience with the system has convinced us that it is wrong to cast many of the problems of authoring large hypertexts as ones that can be solved by implementing editors of sufficient scope and sophistication. We believe that hypertext design systems based on direct editing of documents inherit many of the bottlenecks associated with the conventional document authoring process. These problems are compounded by the added intellectual burden of designing a connective structure of keyworded links. We contrast the reality of the author-centered, editor-based approach to document design and layout, exemplified by IGD, with the promise of a knowledge-based, automated alternative, and discuss why we feel that many of the facilities provided by IGD will still be useful even if presentations can be created entirely automatically.",1991-01-02,https://www.semanticscholar.org/paper/27e2a38eceaa51cafa84baf6199fb96c8ea9ac44,Electronic publishing
588,Inclusion dependencies and their interaction with functional dependencies,"Inclusion dependencies, or INDs (which can say, for example, that every manager is an employee) are studied, including their interaction with functional dependencies, or FDs. A simple complete axiomatization for INDs is presented, and the decision problem for INDs is shown to be PSPACE-complete. (The decision problem for INDs is the problem of determining whether or not Σ logically implies σ, given a set Σ of INDs and a single IND σ). It is shown that finite implication (implication over databases with a finite number of tuples) is the same as unrestricted implications for INDs, although finite implication and unrestricted implication are distinct for FDs and INDs taken together. It is shown that, although there are simple complete axiomatizations for FDs alone and for INDs alone, there is no complete axiomatization for FDs and INDs taken together, in which every rule is k-ary for some fixed k (and in particular, there is no finite complete axiomatization.) This is true whether we consider finite implication or unrestricted implication, and is true even if no relation scheme has more than three attributes. The nonexistence of a k-ary complete axiomatization for FDs and INDs taken together is proven by giving a condition which is necessary and sufficient in general for the existence of a k-ary complete axiomatization.",1982-03-29,https://www.semanticscholar.org/paper/1eac7a953a57dda2112cb759423c80ec723e429c,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems
2887,"The adhesive specificity of the soluble human lectin, IgE-binding protein, toward lipid-linked oligosaccharides. Presence of the blood group A, B, B-like, and H monosaccharides confers a binding activity to tetrasaccharide (lacto-N-tetraose and lacto-N-neotetraose) backbones.","The immunoglobulin E-binding protein, epsilon BP (also known as CBP35, Mac-2, L-34, and L-29), is a beta-galactoside-binding protein of approximately 30 kDa and a member of the animal lectin family termed S-type or S-Lac. Multiple biological activities have been attributed to this lectin such as mediation of IgE binding to the surface of Langerhans cells and activation of mast cells through binding to the high affinity IgE receptor. In order to better understand the cell-binding activity and the proposed role for epsilon BP as a biological response modifier, we have studied the specificity of binding of the radioiodinated epsilon BP to a series of lipid-linked, structurally defined oligosaccharide sequences of the lacto/neolacto family. The results show that the minimum lipid-linked oligosaccharides that can support epsilon BP binding are pentasaccharides of the lacto/neolacto series and that the lectin binds more strongly to oligosaccharides of this family that bear the blood group A, B, or B-like determinants than to those bearing blood group H. This preferential binding of epsilon BP is also manifest with whole cells, as erythrocytes of blood groups A and B are more strongly bound by epsilon BP than those of blood group O. Blood group Le(a) and Le(x) sequences are not bound by the lectin.(ABSTRACT TRUNCATED AT 250 WORDS)",1994-05-24,https://www.semanticscholar.org/paper/8c3c64c610ad1c22efd231d294c792d6a9caa467,Biochemistry
3231,How ecology shapes exploitation: a framework to predict the behavioural response of human and animal foragers along exploration-exploitation trade-offs.,"Understanding how humans and other animals behave in response to changes in their environments is vital for predicting population dynamics and the trajectory of coupled social-ecological systems. Here, we present a novel framework for identifying emergent social behaviours in foragers (including humans engaged in fishing or hunting) in predator-prey contexts based on the exploration difficulty and exploitation potential of a renewable natural resource. A qualitative framework is introduced that predicts when foragers should behave territorially, search collectively, act independently or switch among these states. To validate it, we derived quantitative predictions from two models of different structure: a generic mathematical model, and a lattice-based evolutionary model emphasising exploitation and exclusion costs. These models independently identified that the exploration difficulty and exploitation potential of the natural resource controls the social behaviour of resource exploiters. Our theoretical predictions were finally compared to a diverse set of empirical cases focusing on fisheries and aquatic organisms across a range of taxa, substantiating the framework's predictions. Understanding social behaviour for given social-ecological characteristics has important implications, particularly for the design of governance structures and regulations to move exploited systems, such as fisheries, towards sustainability. Our framework provides concrete steps in this direction.",2018-06-01,https://www.semanticscholar.org/paper/c29bc4ac519374f8271f956fd48e6fa053919374,Ecology Letters
964,Short foveo-disc distance in situs inversus of optic disc,,2020-10-20,https://www.semanticscholar.org/paper/365f1430ee08c48ef7dd65b634e6c8b8de72bf56,Scientific Reports
770,Algorithmic Verification of Recursive Probabilistic State Machines,,2005-04-04,https://www.semanticscholar.org/paper/8ca0e8be0f48826f8eb3ce31a9681d3cf52d4952,International Conference on Tools and Algorithms for Construction and Analysis of Systems
154,Optimal Scheduling of the Leaves of a Tree and the SVO Frequencies of Languages,,,https://www.semanticscholar.org/paper/d0ad972391c11d83b181d5dd313697b9dc590985,Learning and Intelligent Optimization
1779,"Context, learning, and extinction.","A. Redish et al. (2007) proposed a reinforcement learning model of context-dependent learning and extinction in conditioning experiments, using the idea of ""state classification"" to categorize new observations into states. In the current article, the authors propose an interpretation of this idea in terms of normative statistical inference. They focus on renewal and latent inhibition, 2 conditioning paradigms in which contextual manipulations have been studied extensively, and show that online Bayesian inference within a model that assumes an unbounded number of latent causes can characterize a diverse set of behavioral results from such manipulations, some of which pose problems for the model of Redish et al. Moreover, in both paradigms, context dependence is absent in younger animals, or if hippocampal lesions are made prior to training. The authors suggest an explanation in terms of a restricted capacity to infer new causes.",,https://www.semanticscholar.org/paper/097ade72f43a979e652ed5a6aa801f5e8dfb9066,Psychology Review
807,On the Complexity of Protein Folding,We show that the protein folding problem in the two-dimensional H-P model is NP-complete.,,https://www.semanticscholar.org/paper/492cbadcc1bb2875209004e6ad93f391d2e6cf9b,J. Comput. Biol.
2512,Session details: Gestures,,2011-05-07,https://www.semanticscholar.org/paper/b6e676f786a2e28760c23097f70354797e4badb0,CHI '11 Extended Abstracts on Human Factors in Computing Systems
2379,"Purification, morphometric analysis, and characterization of the glycosomes (microbodies) of the protozoan hemoflagellate Trypanosoma brucei","Trypanosoma brucei glycosomes (microbodies containing nine enzymes involved in glycolysis) have been purified to near homogeneity from bloodstream-form trypomastigotes for the purpose of morphologic and biochemical analysis. Differential centrifugation followed by two isopycnic centrifugations in an isotonic Percoll and in a sucrose gradient, respectively, resulted in 12- to 13-fold purified glycosomes with an overall yield of 31%. These glycosomes appeared to be highly pure and contained less than 1% mitochondrial contamination as judged by morphometric and biochemical analyses. In intact cells, glycosomes displayed a remarkably homogeneous size distribution centered on an average diameter of 0.27 micron with a standard deviation of 0.03 micron. The size distribution of isolated glycosomes differed only slightly from that measured in intact cells. One T. brucei cell contained on average 230 glycosomes, representing 4.3% of the total cell volume. The glycosomes were surrounded by a single membrane and contained as phospholipids only phosphatidyl choline and phosphatidyl ethanolamine in a ratio of 2:1. The purified glycosomal fraction had a very low DNA content of 0.18 microgram/mg protein. No DNA molecules were observed that could not have been derived from contaminating mitochondrial or nuclear debris.",1984-04-01,https://www.semanticscholar.org/paper/3f7b8754289b74492f92d7f025311b4f20382d3c,Journal of Cell Biology
2582,Editorial,,,https://www.semanticscholar.org/paper/3f4917c8eb81acd225e6ee80052ae7807407d490,Virtual Reality
1069,Projected WIMP sensitivity of the LUX-ZEPLIN dark matter experiment,"Author(s): Akerib, DS; Akerlof, CW; Alsum, SK; Araujo, HM; Arthurs, M; Bai, X; Bailey, AJ; Balajthy, J; Balashov, S; Bauer, D; Belle, J; Beltrame, P; Benson, T; Bernard, EP; Biesiadzinski, TP; Boast, KE; Boxer, B; Bras, P; Buckley, JH; Bugaev, VV; Burdin, S; Busenitz, JK; Carels, C; Carlsmith, DL; Carlson, B; Carmona-Benitez, MC; Chan, C; Cherwinka, JJ; Cole, A; Cottle, A; Craddock, WW; Currie, A; Cutter, JE; Dahl, CE; De Viveiros, L; Dobi, A; Dobson, JEY; Druszkiewicz, E; Edberg, TK; Edwards, WR; Fan, A; Fayer, S; Fiorucci, S; Fruth, T; Gaitskell, RJ; Genovesi, J; Ghag, C; Gilchriese, MGD; Van Der Grinten, MGD; Hall, CR; Hans, S; Hanzel, K; Haselschwardt, SJ; Hertel, SA; Hillbrand, S; Hjemfelt, C; Hoff, MD; Hor, JYK; Huang, DQ; Ignarra, CM; Ji, W; Kaboth, AC; Kamdin, K; Keefner, J; Khaitan, D; Khazov, A; Kim, YD; Kocher, CD; Korolkova, EV; Kraus, H; Krebs, HJ; Kreczko, L; Krikler, B; Kudryavtsev, VA; Kyre, S; Lee, J; Lenardo, BG; Leonard, DS; Lesko, KT; Levy, C; Li, J; Liao, J; Liao, FT; Lin, J; Lindote, A | Abstract: © 2020 American Physical Society. LUX-ZEPLIN (LZ) is a next-generation dark matter direct detection experiment that will operate 4850 feet underground at the Sanford Underground Research Facility (SURF) in Lead, South Dakota, USA. Using a two-phase xenon detector with an active mass of 7 tonnes, LZ will search primarily for low-energy interactions with weakly interacting massive particles (WIMPs), which are hypothesized to make up the dark matter in our galactic halo. In this paper, the projected WIMP sensitivity of LZ is presented based on the latest background estimates and simulations of the detector. For a 1000 live day run using a 5.6-tonne fiducial mass, LZ is projected to exclude at 90% confidence level spin-independent WIMP-nucleon cross sections above 1.4×10-48 cm2 for a 40 GeV/c2 mass WIMP. Additionally, a 5σ discovery potential is projected, reaching cross sections below the exclusion limits of recent experiments. For spin-dependent WIMP-neutron(-proton) scattering, a sensitivity of 2.3×10-43 cm2 (7.1×10-42 cm2) for a 40 GeV/c2 mass WIMP is expected. With underground installation well underway, LZ is on track for commissioning at SURF in 2020.",2018-02-16,https://www.semanticscholar.org/paper/1ea18173154ea7876c2166f4143d0ff7e16a181f,Physical Review D
2308,Preservation of the activity of NADPH oxidase in human monocyte/macrophages.,,1996-08-01,https://www.semanticscholar.org/paper/b91598bca25df98e9131d50c373321ac6f5fa1b2,Biochemical Society Transactions
1915,A two-phase decoding genetic algorithm for TFT-LCD array photolithography stage scheduling problem with constrained waiting time,,2018-11-01,https://www.semanticscholar.org/paper/ae6a070fdc0ffec311d2b045b2f1f3abf7edbd72,Computers & industrial engineering
3463,LP Decoding Corrects a Constant Fraction of Errors,"We show that for low-density parity-check (LDPC) codes whose Tanner graphs have sufficient expansion, the linear programming (LP) decoder of Feldman, Karger, and Wainwright can correct a constant fraction of errors. A random graph will have sufficient expansion with high probability, and recent work shows that such graphs can be constructed efficiently. A key element of our method is the use of a dual witness: a zero-valued dual solution to the decoding linear program whose existence proves decoding success. We show that as long as no more than a certain constant fraction of the bits are flipped by the channel, we can find a dual witness. This new method can be used for proving bounds on the performance of any LP decoder, even in a probabilistic setting. Our result implies that the word error rate of the LP decoder decreases exponentially in the code length under the binary-symmetric channel (BSC). This is the first such error bound for LDPC codes using an analysis based on ""pseudocodewords."" Recent work by Koetter and Vontobel shows that LP decoding and min-sum decoding of LDPC codes are closely related by the ""graph cover"" structure of their pseudocodewords; in their terminology, our result implies that that there exist families of LDPC codes where the minimum BSC pseudoweight grows linearly in the block length",2004-06-27,https://www.semanticscholar.org/paper/1e45a001f9d3c044efc680611a0394c1b89157fb,IEEE Transactions on Information Theory
1388,Search for large extra dimensions in the monojet+E(T) channel with the DØ detector.,"We present a search for large extra dimensions (ED) in pp collisions at a center-of-mass energy of 1.8 TeV using data collected by the DØ detector at the Fermilab Tevatron in 1994-1996. Data corresponding to 78.8+/-3.9 pb(-1) are examined for events with large missing transverse energy, one high-p(T) jet, and no isolated muons. There is no excess observed beyond expectation from the standard model, and we place lower limits on the fundamental Planck scale of 1.0 and 0.6 TeV for 2 and 7 ED, respectively.",2003-06-24,https://www.semanticscholar.org/paper/97532dd6ccbd122d367d6e2ef09a02c3530843bf,Physical Review Letters
2852,Inhibition of Advanced Glycation and Absence of Galectin-3 Prevent Blood-Retinal Barrier Dysfunction during Short-Term Diabetes,"Breakdown of the inner blood-retinal barrier (iBRB) occurs early in diabetes and is central to the development of sight-threatening diabetic macular edema (DME) as retinopathy progresses. In the current study, we examined how advanced glycation end products (AGEs) forming early in diabetes could modulate vasopermeability factor expression in the diabetic retina and alter inter-endothelial cell tight junction (TJ) integrity leading to iBRB dysfunction. We also investigated the potential for an AGE inhibitor to prevent this acute pathology and examined a role of the AGE-binding protein galectin-3 (Gal-3) in AGE-mediated cell retinal pathophysiology. Diabetes was induced in C57/BL6 wild-type (WT) mice and in Gal-3−/− transgenic mice. Blood glucose was monitored and AGE levels were quantified by ELISA and immunohistochemistry. The diabetic groups were subdivided, and one group was treated with the AGE-inhibitor pyridoxamine (PM) while separate groups of WT and Gal-3−/− mice were maintained as nondiabetic controls. iBRB integrity was assessed by Evans blue assay alongside visualisation of TJ protein complexes via occludin-1 immunolocalization in retinal flat mounts. Retinal expression levels of the vasopermeability factor VEGF were quantified using real-time RT-PCR and ELISA. WT diabetic mice showed significant AGE -immunoreactivity in the retinal microvasculature and also showed significant iBRB breakdown (P < .005). These diabetics had higher VEGF mRNA and protein expression in comparison to controls (P < .01). PM-treated diabetics had normal iBRB function and significantly reduced diabetes-mediated VEGF expression. Diabetic retinal vessels showed disrupted TJ integrity when compared to controls, while PM-treated diabetics demonstrated near-normal configuration. Gal-3−/− mice showed significantly less diabetes-mediated iBRB dysfunction, junctional disruption, and VEGF expression changes than their WT counterparts. The data suggests an AGE-mediated disruption of iBRB via upregulation of VEGF in the diabetic retina, possibly modulating disruption of TJ integrity, even after acute diabetes. Prevention of AGE formation or genetic deletion of Gal-3 can effectively prevent these acute diabetic retinopathy changes.",2007-04-05,https://www.semanticscholar.org/paper/8541f8204f735cc64b4e5e921439dae202f7fa24,International Journal of Experimental Diabetes Research
1771,Probabilistic topic models,"Probabilistic topic modeling provides a suite of tools for the unsupervised analysis of large collections of documents. Topic modeling algorithms can uncover the underlying themes of a collection and decompose its documents according to those themes. This analysis can be used for corpus exploration, document search, and a variety of prediction problems.
 In this tutorial, I will review the state-of-the-art in probabilistic topic models. I will describe the three components of topic modeling:
 (1) Topic modeling assumptions
 (2) Algorithms for computing with topic models
 (3) Applications of topic models
 In (1), I will describe latent Dirichlet allocation (LDA), which is one of the simplest topic models, and then describe a variety of ways that we can build on it. These include dynamic topic models, correlated topic models, supervised topic models, author-topic models, bursty topic models, Bayesian nonparametric topic models, and others. I will also discuss some of the fundamental statistical ideas that are used in building topic models, such as distributions on the simplex, hierarchical Bayesian modeling, and models of mixed-membership.
 In (2), I will review how we compute with topic models. I will describe approximate posterior inference for directed graphical models using both sampling and variational inference, and I will discuss the practical issues and pitfalls in developing these algorithms for topic models. Finally, I will describe some of our most recent work on building algorithms that can scale to millions of documents and documents arriving in a stream.
 In (3), I will discuss applications of topic models. These include applications to images, music, social networks, and other data in which we hope to uncover hidden patterns. I will describe some of our recent work on adapting topic modeling algorithms to collaborative filtering, legislative modeling, and bibliometrics without citations.
 Finally, I will discuss some future directions and open research problems in topic models.",2011-08-21,https://www.semanticscholar.org/paper/87f553e5b5cd1f1cc833cb28235889ee8c08be36,KDD '11 Tutorials
576,The Complexity of Cubical Graphs (Extended Abstract),,1984-07-16,https://www.semanticscholar.org/paper/a63cf60c2d0a7e157afe6f70bc31a845d8fda89c,"International Colloquium on Automata, Languages and Programming"
3488,Finding Real-Valued Single-Source Shortest Paths in o(n3) Expected Time,"Given ann-vertex,m-edge directed networkGwith real costs on the edges and a designated source vertexs, we give a new algorithm to compute shortest paths froms. Our algorithm is a simple deterministic one withO(n2logn) expected running time over a large class of input distributions. This is the first strongly polynomial algorithm in over 35 years to improve upon some aspect of theO(nm) running time of the Bellman?Ford algorithm. The result extends to anO(n2logn) expected running time algorithm for finding the minimum mean cycle, an improvement over Karp'sO(nm) worst-case time bound when the underlying graph is dense. Both of our time bounds are shown to be achieved with high probability.",1998-07-01,https://www.semanticscholar.org/paper/2e2db4c0a2884739fdf54bea78236348a94cf5fa,J. Algorithms
2415,A Testbed for Exploring Multi-Level Precueing in Augmented Reality,"Precueing information about upcoming subtasks prior to performing them has the potential to make an entire task faster and easier to accomplish than cueing only the current subtask. Most AR and VR research on precueing has addressed path-following tasks requiring simple actions at a series of locations, such as pushing a button or just visiting that location. We present a testbed for exploring multi-level precueing in a richer task that requires the user to move their hand between specified locations, transporting an object between some of them, and rotating it to a designated orientation.",2022-03-01,https://www.semanticscholar.org/paper/b86aa99029b3d7ac9833d464ddf9994d12e1928f,2022 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)
637,The NP-Completeness of the bandwidth minimization problem,,1976-09-01,https://www.semanticscholar.org/paper/913fea849518f26443477fbedb2a00146f78a159,Computing
1688,Automatic Variational Inference in Stan,"Variational inference is a scalable technique for approximate Bayesian inference. Deriving variational inference algorithms requires tedious model-specific calculations; this makes it difficult for non-experts to use. We propose an automatic variational inference algorithm, automatic differentiation variational inference (ADVI); we implement it in Stan (code available), a probabilistic programming system. In ADVI the user provides a Bayesian model and a dataset, nothing else. We make no conjugacy assumptions and support a broad class of models. The algorithm automatically determines an appropriate variational family and optimizes the variational objective. We compare ADVI to MCMC sampling across hierarchical generalized linear models, nonconjugate matrix factorization, and a mixture model. We train the mixture model on a quarter million images. With ADVI we can use variational inference on any model we write in Stan.",2015-06-10,https://www.semanticscholar.org/paper/c02717b88beba6eb84e6a407f58f8705de649c5e,Neural Information Processing Systems
630,The Concurrency Control Mechanism of SDD-1: A System for Distributed Databases (The Fully Redundant Case),"SDD-1, A System for Distributed Databases, is a distributed database system being developed by Computer Corporation of America (CCA), Cambridge, MA. SDD-1 permits data to be stored redundantly at several database sites in order to enhance the reliability and responsiveness of the system and to facilitate upward scaling of system capacity. This paper describes the method used by SDD-1 for updating data that are stored redundantly.",1978-05-01,https://www.semanticscholar.org/paper/12317714a66256b993558c72dc4ab685a9432638,IEEE Transactions on Software Engineering
2884,"Evidence for IgG autoantibodies to galectin-3, a β-galactoside-binding lectin (Mac-2, ε binding protein, or carbohydrate binding protein 35) in human serum",,1995-11-01,https://www.semanticscholar.org/paper/a0521d22c49890c210b6224f67837ee9b7310ba1,Journal of Clinical Immunology
844,On the hardness of approximating minimization problems,"We prove results indicating that it is hard to compute efficiently good approximate solutions to the Graph Coloring, Set Covering and other related minimization problems. Specifically, there is an c >0 such that Graph Coloring cannot be approximated with ratio n’ unless P=NP. Set Covering cannot be approximated with ratio clog n for any c < 1/4 unless NP is contained in DTIME[nPOIY log ~ ]. Similar results follow for related problems such as Clique Cover, Fractional Chromatic Number, Dominating Set and others.",1993-06-01,https://www.semanticscholar.org/paper/f2be999f55fae0c8d189565b29a975d71df7bf67,Symposium on the Theory of Computing
498,On Selecting a Satisfying Truth Assignment (Extended Abstract),,,https://www.semanticscholar.org/paper/4c89a88a678ce4a2c9b2ea66739da41c8c9a09c8,IEEE Annual Symposium on Foundations of Computer Science
2916,Influence of Microstructure on Synchrotron X-ray Diffraction Lattice Strain Measurement Uncertainty,"Accurate residual lattice strain measurements are highly dependent upon the precision of the diffraction peak location and the underlying microstructure suitability. The suitability of the microstructure is related to the requirement for valid powder diffraction sampling statistics and the associated number of appropriately orientated illuminated. In this work, these two sources of uncertainty are separated, and a method given for both the quantification of errors associated with insufficient grain sampling statistics and minimization of the total lattice strain measurement uncertainty. It is possible to reduce the total lattice strain measurement uncertainty by leveraging diffraction peak measurements made at multiple azimuthal angles. Lattice strain measurement data acquired during eight synchrotron X-ray diffraction experiments, monochromatic and energy dispersive, has been assessed as per this approach, with microstructural suitability being seen to dominate total measurement uncertainty when the number of illuminated grains was <106. More than half of the studied experimental data fell into this category, with a severe underestimation of total strain measurement uncertainty being possible when microstructural suitability is not considered. To achieve a strain measurement uncertainty under 10−4, approximately 3×105 grains must be within the sampled gauge volume, with this value varying with the multiplicity of the family of lattice planes under study. Where additional azimuthally arrayed data are available an in-plane lattice strain tensor can be extracted. This improves overall strain measurement accuracy and an uncertainty under 10−4 can then be achieved with just 4×104 grains.",2021-05-09,https://www.semanticscholar.org/paper/50c2af35e2abc3070b203faf01fda465beebc9bc,Metals
3107,AutoPod: Unscheduled System Updates with Zero Data Loss,"Patching, upgrading, and maintaining operating system software is a growing management complexity problem that can result in unacceptable system downtime. We introduce AutoPod, a system that enables unscheduled operating system updates while preserving application service availability. AutoPod provides a group of processes and associated users with an isolated machine-independent virtualized environment that is decoupled from the underlying operating system instance. This virtualized environment is integrated with a novel checkpoint-restart mechanism which allows processes to be suspended, resumed, and migrated across operating system kernel versions with different security and maintenance patches. AutoPod incorporates a system status service to determine when operating system patches need to be applied to the current host, then automatically migrates application services to another host to preserve their availability while the current host is updated and rebooted",2005-06-13,https://www.semanticscholar.org/paper/465c79de132a7179d8a0098908d0cc66b3be9494,International Conference on Automation and Computing
1309,Direct limits on the oscillation frequency.,"We report results of a study of the B(s)(0) oscillation frequency using a large sample of B(s)(0) semileptonic decays corresponding to approximately 1 fb(-1) of integrated luminosity collected by the D0 experiment at the Fermilab Tevatron Collider in 2002-2006. The amplitude method gives a lower limit on the B(s)(0) oscillation frequency at 14.8 ps(-1) at the 95% C.L. At delta m(s) = 19 ps(-1), the amplitude deviates from the hypothesis A= 0(1) by 2.5 (1.6) standard deviations, corresponding to a two-sided C.L. of 1% (10%). A likelihood scan over the oscillation frequency, delta m(s), gives a most probable value of 19 ps(-1) and a range of 17 < delta m(s) < 21 ps(-1)at the 90% C.L., assuming Gaussian uncertainties. This is the first direct two-sided bound measured by a single experiment. If delta m(s) lies above 22 ps(-1), then the probability that it would produce a likelihood minimum similar to the one observed in the interval 16-22 ps(-1) is (5.0 +/- 0.3)%.",,https://www.semanticscholar.org/paper/397552f21719e92a0152bf540bc19e45c6a1ec8d,Physical Review Letters
220,Cortical Learning via Prediction,"What is the mechanism of learning in the brain? Despite breathtaking advances in neuroscience, and in machine learning, we do not seem close to an answer. Using Valiant’s neuronal model as a foundation, we introduce PJOIN (for “predictive join”), a primitive that combines association and prediction. We show that PJOIN can be implemented naturally in Valiant’s conservative, formal model of cortical computation. Using PJOIN — and almost nothing else — we give a simple algorithm for unsupervised learning of arbitrary ensembles of binary patterns (solving an open problem in Valiant’s work). This algorithm relies crucially on prediction, and entails significant downward traffic (“feedback”) while parsing stimuli. Prediction and feedback are well-known features of neural cognition and, as far as we know, this is the first theoretical prediction of their essential role in learning.",2015-06-26,https://www.semanticscholar.org/paper/da88528713f9be28c923931f38eca36617397b6e,Annual Conference Computational Learning Theory
17,When Speed Has a Price: Fast Information Extraction Using Approximate Algorithms,"A wealth of information produced by individuals and organizations is expressed in natural language text. This is a problem since text lacks the explicit structure that is necessary to support rich querying and analysis. Information extraction systems are sophisticated software tools to discover structured information in natural language text. Unfortunately, information extraction is a challenging and time-consuming task. In this paper, we address the limitations of state-of-the-art systems for the optimization of information extraction programs, with the objective of producing efficient extraction executions. Our solution relies on exploiting a wide range of optimization opportunities. For efficiency, we consider a wide spectrum of execution plans, including approximate plans whose results differ in their precision and recall. Our optimizer accounts for these characteristics of the competing execution plans, and uses accurate predictors of their extraction time, recall, and precision. We demonstrate the efficiency and effectiveness of our optimizer through a large-scale experimental evaluation over real-world datasets and multiple extraction tasks and approaches.",2013-08-29,https://www.semanticscholar.org/paper/2f770beab965bbc9df0575b9883e83f2e9678583,Proceedings of the VLDB Endowment
842,"Primal-Dual Approximation Algorithms for Integral Flow and Multicut in Trees, with Applications to Matching and Set Cover",,1993-07-05,https://www.semanticscholar.org/paper/b78bac7d428451e77641f3f727b7df04aa4faea2,"International Colloquium on Automata, Languages and Programming"
2935,Genetic dysregulation of gene expression and splicing during a ten-year period of human aging,"Molecular and cellular changes are intrinsic to aging and age-related diseases. Prior cross-sectional studies have investigated the combined effects of age and genetics on gene expression and alternative splicing; however, there has been no long-term, longitudinal characterization of these molecular changes, especially in older age. We performed RNA sequencing in whole-blood from the same individuals from the PIVUS study at ages 70 and 80 to quantify how gene expression, alternative splicing, and their genetic regulation are altered during this 10-year period of advanced aging. We observe that individuals are more similar to their own expression profiles later in life than profiles of other individuals their own age; 93% of samples cluster with their own measurement at another age, and there is a strong correlation of genetic effects on expression between the two ages (median ρG = 0.96). Despite this, we identify 1,291 and 294 genes differentially expressed and alternatively spliced with age, as well as 529 genes with outlying individual trajectories of aging. Further, 7.8% and 9.6% of tested genes show a reduction in genetic associations with expression and alternative splicing in older age, with impacted genes enriched in DNA repair pathways. Together these findings indicate that, although gene expression and alternative splicing and their genetic regulation are mostly stable late in life, a small subset of genes is dynamic and is characterized by changes in expression and splicing and a reduction in genetic regulation.",2019-01-13,https://www.semanticscholar.org/paper/ad1be6f3c2150c099f0a82ac9c4c7f851f9d3c98,bioRxiv
3532,Thriving in a crowded and changing world: C++ 2006–2020,"By 2006, C++ had been in widespread industrial use for 20 years. It contained parts that had survived unchanged since introduced into C in the early 1970s as well as features that were novel in the early 2000s. From 2006 to 2020, the C++ developer community grew from about 3 million to about 4.5 million. It was a period where new programming models emerged, hardware architectures evolved, new application domains gained massive importance, and quite a few well-financed and professionally marketed languages fought for dominance. How did C++ -- an older language without serious commercial backing -- manage to thrive in the face of all that? This paper focuses on the major changes to the ISO C++ standard for the 2011, 2014, 2017, and 2020 revisions. The standard library is about 3/4 of the C++20 standard, but this paper's primary focus is on language features and the programming techniques they support. The paper contains long lists of features documenting the growth of C++. Significant technical points are discussed and illustrated with short code fragments. In addition, it presents some failed proposals and the discussions that led to their failure. It offers a perspective on the bewildering flow of facts and features across the years. The emphasis is on the ideas, people, and processes that shaped the language. Themes include efforts to preserve the essence of C++ through evolutionary changes, to simplify its use, to improve support for generic programming, to better support compile-time programming, to extend support for concurrency and parallel programming, and to maintain stable support for decades' old code. The ISO C++ standard evolves through a consensus process. Inevitably, there is competition among proposals and clashes (usually polite ones) over direction, design philosophies, and principles. The committee is now larger and more active than ever, with as many as 250 people turning up to week-long meetings three times a year and many more taking part electronically. We try (not always successfully) to mitigate the effects of design by committee, bureaucratic paralysis, and excessive enthusiasm for a variety of language fashions. Specific language-technical topics include the memory model, concurrency and parallelism, compile-time computation, move-semantics, exceptions, lambda expressions, and modules. Designing a mechanism for specifying a template's requirements on its arguments that is sufficiently flexible and precise yet doesn't impose run-time costs turned out to be hard. The repeated attempts to design ``concepts'' to do that have their roots back in the 1980s and touch upon many key design issues for C++ and for generic programming. The description is based on personal participation in the key events and design decisions, backed by the thousands of papers and hundreds of meeting minutes in the ISO C++ standards committee's archives.",2020-06-12,https://www.semanticscholar.org/paper/1c2688189548fecf0a030cad80b4cd3dbf98e0bc,Proc. ACM Program. Lang.
98,Automatic Classification of Text Databases Through Query Probing,,2000-03-08,https://www.semanticscholar.org/paper/485107f8f5d2581aa935ef1148a5bbd23f39f9fc,International Workshop on the Web and Databases
1934,Modeling and Analysis of Semiconductor Supply Chains (Dagstuhl Seminar 16062),In February 2016 the Dagstuhl Seminar 16062 explored the needs of the semiconductor industry for better planning and scheduling approaches at the supply chain level and the requirements for information systems to support the approaches. The seminar participants also spent time identifying the core elements of a conceptual reference model for planning and control of semiconductor manufacturing supply chains. This Executive Summary describes the process of the seminar and discusses key findings and areas for future research regarding these topics. Abstracts of presentations given during the seminar and the output of breakout sessions are collected in appendices.,,https://www.semanticscholar.org/paper/67ac6e4c0f8ffe05ed027aaa350d0864a0ae3bd7,Dagstuhl Reports
598,On the Complexity of Designing Distributed Protocols,,1982-06-01,https://www.semanticscholar.org/paper/f3d16c209190ff23ed79dd763c761b6f4094074a,Information and Control
2673,Generating Multimedia Briefings: Coordinating Language and Illustration,,1998-08-01,https://www.semanticscholar.org/paper/5919978adf560fff98ffc3d1be0e0dfa8b8c016b,Artificial Intelligence
3775,Do We Need More Training Data or Better Models for Object Detection?,"Datasets for training object recognition systems are steadily growing in size. This paper investigates the question of whether existing detectors will continue to improve as data grows, or if models are close to saturating due to limited model complexity and the Bayes risk associated with the feature spaces in which they operate. We focus on the popular paradigm of scanning-window templates defined on oriented gradient features, trained with discriminative classifiers. We investigate the performance of mixtures of templates as a function of the number of templates (complexity) and the amount of training data. We find that additional data does help, but only with correct regularization and treatment of noisy examples or “outliers” in the training data. Surprisingly, the performance of problem domain-agnostic mixture models appears to saturate quickly (∼10 templates and ∼100 positive training examples per template). However, compositional mixtures (implemented via composed parts) give much better performance because they share parameters among templates, and can synthesize new templates not encountered during training. This suggests there is still room to improve performance with linear classifiers and the existing feature space by improved representations and learning algorithms.",,https://www.semanticscholar.org/paper/4cb1494e547f1eaf51bab8038c8fab904dda9026,British Machine Vision Conference
2046,An indirect workforce (re)allocation model for semiconductor manufacturing,"Semiconductor industry is a capital intensive and knowledge intensive industry, in which human resource management and human capital enhancement is increasingly important. To maintain competitive human resource, it is critical to develop a decision framework for headcount planning and workforce allocation for indirect labors. Motivated by the needs in real setting, this study aims to develop a model for allocating indirect workforce among semiconductor fabrication facilities to meet expected outputs and labor productivity improvement. Workforce allocation and reallocation based on the overall corporate workforce level is essential so that the shortage or exceed workforce will be balanced among different production sites. The key to achieve this purpose is the proper understanding of real requirements of each production site according to its corresponding tasks assigned. Non-parametric activity analysis approach is used for the workforce requirement estimation given delegated tasks. The estimation is based on the best performance from the past with adjustments reflecting the expected productivity growth.",2008-12-07,https://www.semanticscholar.org/paper/571005a8da0f402fdce7a6dcb48fdf18b61f4dc4,Online World Conference on Soft Computing in Industrial Applications
3423,Online Stochastic Packing Applied to Display Ad Allocation,,2010-01-27,https://www.semanticscholar.org/paper/7859ed93b7d0c2a2a83d382866dbc22170faf25d,Embedded Systems and Applications
884,"Pfaffian Orientations, 0/1 Permanents, and Even Cycles in Directed Graphs",,1988-07-11,https://www.semanticscholar.org/paper/7429f8b8bae646762acfa64257162035998ffcf5,"International Colloquium on Automata, Languages and Programming"
2714,Management of broadband networks using 3D virtual world,"Just as broadband networks will enable user-to-user communications to extend from textural services to those employing multimedia, they will also enable a management environment that can take advantage of increased bandwidth and multimedia technology. The fundamental advances incorporated in such an environment can provide efficient solutions to the problem of information management. To establish this environment, the authors tackle the fundamental problems of observability and controllability of broadband networks. A virtual world provides a next-generation network management interface through which a user can observe and interact with the network directly in real time. The system that the authors are developing uses a 3D virtual world as the user interface for managing a large gigabit ATM network. It provides the capability for experimentation in all aspects of network transport, control and management.<<ETX>>",1993-07-20,https://www.semanticscholar.org/paper/437c2f3f2a496ae388468abbd2afe5b3add4fb6e,[1993] Proceedings The 2nd International Symposium on High Performance Distributed Computing
2605,Directions and frameworks for effective telepresence,"1. PANEL INTRODUCTION Following the great standards set by the panels in the workshops held in the last two years [1,2], we have this year again a distinguished panel to discuss and debate the directions and frameworks for telepresence. Some of the questions brought up in the last two years continue to be on the top of our minds: Why is telepresence not as common as we think it can be today? How much technology is too much in a telepresence system? Is there too much emphasis on creating an immersive telepresence system? Which are the suitable paradigms for telepresence? How much are they dependent on the task and application? Which are the most effective telepresence systems we have today? How do we make telepresence more effective? How do we shift the focus from transmitting multi-modal data to creating effective means of experiencing remote environments and events?",2004-10-15,https://www.semanticscholar.org/paper/aa236d618197d8bc15ccdf47379bb44875a094c0,ACM SIGMM workshop on Experiential Telepresence
68,Text joins in an RDBMS for web data integration,"The integration of data produced and collected across autonomous, heterogeneous web services is an increasingly important and challenging problem. Due to the lack of global identifiers, the same entity (e.g., a product) might have different textual representations across databases. Textual data is also often noisy because of transcription errors, incomplete information, and lack of standard formats. A fundamental task during data integration is matching of strings that refer to the same entity. In this paper, we adopt the widely used and established cosine similarity metric from the information retrieval field in order to identify potential string matches across web sources. We then use this similarity metric to characterize this key aspect of data integration as a join between relations on textual attributes, where the similarity of matches exceeds a specified threshold. Computing an exact answer to the text join can be expensive. For query processing efficiency, we propose a sampling-based join approximation strategy for execution in a standard, unmodified relational database management system (RDBMS), since more and more web sites are powered by RDBMSs with a web-based front end. We implement the join inside an RDBMS, using SQL queries, for scalability and robustness reasons. Finally, we present a detailed performance evaluation of an implementation of our algorithm within a commercial RDBMS, using real-life data sets. Our experimental results demonstrate the efficiency and accuracy of our techniques.",2003-05-20,https://www.semanticscholar.org/paper/78b729049a0135dc75a021ce5bbc127902253fde,The Web Conference
551,The performance of a precedence-based queuing discipline,"A queuing system with infinitely many servers, and with the following queuing discipline is considered: For any two jobs <italic>i</italic> and <italic>j</italic> in the system, such that <italic>i</italic> arrived later than <italic>j</italic>, there is a fixed probability <italic>p</italic> that <italic>i</italic> will have to wait for <italic>j</italic>'s execution to terminate before <italic>i</italic> starts executing. This queuing system is a very simple model for database concurrency control via “static” locking, as well as of parallel execution of programs consisting of several interdependent processes. The problem of determining the maximum arrival rate (as a function of <italic>p</italic>) that can be sustained before this system becomes unstable is studied. It is shown that this rate is inversely proportional to <italic>p</italic>, and close upper and lower bounds on the constant for the case of deterministic departures are found. The result suggests that the degree of multiprogramming of multiuser databases, or the level of parallelism of concurrent programs, is inversely proportional to the probability of conflict, and that the constant is small and known within a factor of 2. The technique used involves the computation of certain asymptotic parameters of a random infinite directed acyclic graph (dag) that seem of interest by themselves.",1986-05-01,https://www.semanticscholar.org/paper/9cf7fa4c11f0c5c2606c464c04b9c05e785162e9,JACM
2302,Expression of Fc gamma RIII in neutrophils in rheumatoid arthritis.,,1996-08-01,https://www.semanticscholar.org/paper/22521f6aea997d45b97e6240bddd570021479f24,Biochemical Society Transactions
3428,Divide-and-Conquer Approximation Algorithm for Vertex Cover,"The vertex cover problem is a classical NP-complete problem for which the best worst-case approximation ratio is $2-o(1)$. In this paper, we use a collection of simple graph transformations, each of which guarantees an approximation ratio of $\frac{3}{2}$, to find approximate vertex covers for a large collection of randomly generated graphs and test graphs from various sources. The graph reductions are extremely fast, and even though they by themselves are not guaranteed to find a vertex cover, we manage to find a $\frac{3}{2}$-approximate vertex cover for almost every single graph in our collection. The few graphs that we cannot solve have specific structure: they are triangle-free, with a minimum degree of at least 3, a lower bound of $\frac{n}{2}$ on the optimal vertex cover, and are unlikely to have a large bipartite subgraph.",2009-07-01,https://www.semanticscholar.org/paper/451f6671853c02d40b78e839471a930930ea9c5a,SIAM Journal on Discrete Mathematics
3033,Synapse: a microservices architecture for heterogeneous-database web applications,"The growing demand for data-driven features in today's Web applications -- such as targeting, recommendations, or predictions -- has transformed those applications into complex conglomerates of services operating on each others' data without a coherent, manageable architecture. We present Synapse, an easy-to-use, strong-semantic system for large-scale, data-driven Web service integration. Synapse lets independent services cleanly share data with each other in an isolated and scalable way. The services run on top of their own databases, whose layouts and engines can be completely different, and incorporate read-only views of each others' shared data. Synapse synchronizes these views in real-time using a new scalable, consistent replication mechanism that leverages the high-level data models in popular MVC-based Web applications to replicate data across heterogeneous databases. We have developed Synapse on top of the popular Web framework Ruby-on-Rails. It supports data replication among a wide variety of SQL and NoSQL databases, including MySQL, Oracle, PostgreSQL, MongoDB, Cassandra, Neo4j, and Elasticsearch. We and others have built over a dozen microservices using Synapse with great ease, some of which are running in production with over 450,000 users.",2015-04-17,https://www.semanticscholar.org/paper/3438469a51e02a22ed7f92335ce5a04e7b2fdb66,European Conference on Computer Systems
2019,Optimizing information value for supporting production decisions for semiconductor manufacturing,"• Through the proposed method, the streamlined high-priority lots can be obtained with maximum information coverage and best KPI, and the model has good ability of abnormal discrimination. • Cost reduction by catching more abnormal events • Decreasing business impact to improve customer's satisfaction",2011-09-01,https://www.semanticscholar.org/paper/e2b478d803dc1ff49066a65afeec58a96cd9ba59,2011 e-Manufacturing & Design Collaboration Symposium & International Symposium on Semiconductor Manufacturing (eMDC & ISSM)
2782,Social media (SoMe) enhances exposure of dermatology articles.,"Social media (SoMe) refers to a variety of virtual platforms used to enhance sharing of information. To evaluate the influence of SoMe with regards to views and downloads of published dermatology articles, we conducted a retrospective study from July 2020-March 2021 examining articles published on Instagram and Twitter under Dermatology Online Journal (DOJ) accounts and compared these with type-matched and issue-matched articles that were not posted on social media. During this time period, 163 total articles of the three types used for social media (Case Report, Case Presentation, and Photo Vignette) were published in DOJ and 15 were promoted via SoMe. Utilization of SoMe demonstrated a significant (P<0.0001) positive effect with regards to both views (175.5±16.4) and downloads (31.5±4.0) over matched articles not published on SoMe. Similar trends illustrating the positive effect of SoMe on readership have been previously observed in the field of dermatology as well as other medical specialties. Most direct accessions to articles arrived via Instagram rather than Twitter, diverging from previous studies on SoMe use in medical journals. Social media, in particular Instagram, can be a successful platform to enhance the exposure of peer-reviewed medical information.",2021-07-15,https://www.semanticscholar.org/paper/84778857e14b024639869887d201a97d80b71bb6,Dermatology Online Journal
3241,"Wildbook: Crowdsourcing, computer vision, and data science for conservation","Photographs, taken by field scientists, tourists, automated cameras, and incidental photographers, are the most abundant source of data on wildlife today. Wildbook is an autonomous computational system that starts from massive collections of images and, by detecting various species of animals and identifying individuals, combined with sophisticated data management, turns them into high resolution information database, enabling scientific inquiry, conservation, and citizen science. 
We have built Wildbooks for whales (flukebook.org), sharks (whaleshark.org), two species of zebras (Grevy's and plains), and several others. In January 2016, Wildbook enabled the first ever full species (the endangered Grevy's zebra) census using photographs taken by ordinary citizens in Kenya. The resulting numbers are now the official species census used by IUCN Red List: this http URL In 2016, Wildbook partnered up with WWF to build Wildbook for Sea Turtles, Internet of Turtles (IoT), as well as systems for seals and lynx. Most recently, we have demonstrated that we can now use publicly available social media images to count and track wild animals. 
In this paper we present and discuss both the impact and challenges that the use of crowdsourced images can have on wildlife conservation.",2017-10-24,https://www.semanticscholar.org/paper/8e0ca63bfb804a2f865caf4c0cfdb2ec6fc73216,arXiv.org
369,Heuristically Optimized Trade-Offs: A New Paradigm for Power Laws in the Internet,,2002-07-08,https://www.semanticscholar.org/paper/46917e113dc095247ac667a9b860a7d8afd84d9c,"International Colloquium on Automata, Languages and Programming"
446,The future of computational complexity theory: part I,"As you probably already know, there is an active discussion going on---in forums ranging from lunch-table conversations to workshops on ""strategic directions"" to formal reports---regarding the future of theoretical computer science. Since your complexity columnist does not know The Answer, I've asked a number of people to contribute their comments on the narrower issue of the future of complexity theory. The only ground rule was a loose 1-page limit; each contributor could choose what aspect(s) of the future to address, and the way in which to address them. The first installment of contributions appears in this issue, and one or two more installments will appear among the next few issues.Also coming during the next few issues: the search for the perfect theory journal, and (for the sharp-eyed) Lance Fortnow dons a clown suit. Finally, let me mention that work of Russell Impagliazzo resolves one of the open questions from Complexity Theory Column 11.",1996-09-01,https://www.semanticscholar.org/paper/1f0284fc0958d3316d4d9c55ae7a4fb7c23df248,SIGA
1812,Easy As CBA: A Simple Probabilistic Model for Tagging Music,"Many songs in large music databases are not labeled with semantic tags that could help users sort out the songs they want to listen to from those they do not. If the words that apply to a song can be predicted from audio, then those predictions can be used both to automatically annotate a song with tags, allowing users to get a sense of what qualities characterize a song at a glance. Automatic tag prediction can also drive retrieval by allowing users to search for the songs most strongly characterized by a particular word. We present a probabilistic model that learns to predict the probability that a word applies to a song from audio. Our model is simple to implement, fast to train, predicts tags for new songs quickly, and achieves state-of-the-art performance on annotation and retrieval tasks.",,https://www.semanticscholar.org/paper/c34898a44917a0f6f1018c5db6e534deec58aea5,International Society for Music Information Retrieval Conference
80,Extending SDARTS: extracting metadata from web databases and interfacing with the open archives initiative,"SDARTS is a protocol and toolkit designed to facilitate metasearching. SDARTS combines two complementary existing protocols, SDLIP and STARTS, to define a uniform interface that collections should support for searching and exporting metasearch-related metadata. SDARTS also includes a toolkit with wrappers that are easily customized to make both local and remote document collections SDARTS-compliant. This paper describes two significant ways in which we have extended the SDARTS toolkit. First, we have added a tool that automatically builds rich content summaries for remote web collections bym probing the collections with appropriate queries. These content summaries can then be used by a metasearcher to select over which collections to evaluate a given query. Second, we have enhanced the SDARTS toolkit so that all SDARTS-compliant collections export their metadata under the emerging Open Archives Initiative (OAI) protocol. Conversely, the SDARTS toolkit now also allows all OAI-compliant collections to be made SDARTS-compliant with minimal effort. As a result, we implemented a bridge between SDARTS and OAI, which will facilitate easy interoperability among a potentially large number of collections. The SDARTS toolkit, with all related documentation and source code, is publicly available at http://sdarts.cs.columbia.edu.",2002-07-14,https://www.semanticscholar.org/paper/a8a4502dc05e79de23a587c86071594ac39505de,ACM/IEEE Joint Conference on Digital Libraries
1302,"Search for neutral, long-lived particles decaying into two muons in pp[over] collisions at sqrt[s]=1.96 TeV.","We present a search for a neutral particle, pair produced in pp[over] collisions at sqrt[s]=1.96 TeV, which decays into two muons and lives long enough to travel at least 5 cm before decaying. The analysis uses approximately 380 pb(-1) of data recorded with the D0 detector. The background is estimated to be about one event. No candidates are observed, and limits are set on the pair-production cross section times branching fraction into dimuons + X for such particles. For a mass of 10 GeV and lifetime of 4x10(-11) s, we exclude values greater than 0.14 pb (95% C.L.). These results are used to limit the interpretation of NuTeV's excess of dimuon events.",2006-07-17,https://www.semanticscholar.org/paper/10d0edeef8eb4c4bb0331eca5d6164d022df86a8,Physical Review Letters
2859,Galectin-3 and Galectin-1 Bind Distinct Cell Surface Glycoprotein Receptors to Induce T Cell Death1,"Galectins are a family of mammalian β-galactoside-binding proteins that positively and negatively regulate T cell death. Extracellular galectin-1 directly induces death of T cells and thymocytes, while intracellular galectin-3 blocks T cell death. In contrast to the antiapoptotic function of intracellular galectin-3, we demonstrate that extracellular galectin-3 directly induces death of human thymocytes and T cells. However, events in galectin-3- and galectin-1-induced cell death differ in a number of ways. Thymocyte subsets demonstrate different susceptibility to the two galectins: whereas galectin-1 kills double-negative and double-positive human thymocytes with equal efficiency, galectin-3 preferentially kills double-negative thymocytes. Galectin-3 binds to a complement of T cell surface glycoprotein receptors distinct from that recognized by galectin-1. Of these glycoprotein receptors, CD45 and CD71, but not CD29 and CD43, appear to be involved in galectin-3-induced T cell death. In addition, CD7 that is required for galectin-1-induced death is not required for death triggered by galectin-3. Following galectin-3 binding, CD45 remains uniformly distributed on the cell surface, in contrast to the CD45 clustering induced by galectin-1. Thus, extracellular galectin-3 and galectin-1 induce death of T cells through distinct cell surface events. However, as galectin-3 and galectin-1 cell death are neither additive nor synergistic, the two death pathways may converge inside the cell.",2006-01-15,https://www.semanticscholar.org/paper/e9783386ba96aadfb2b43bda8d6bd70c6714815a,Journal of Immunology
2013,A novel bi-vector encoding genetic algorithm for the simultaneous multiple resources scheduling problem,,2011-07-27,https://www.semanticscholar.org/paper/7869da224d87a65f8649ffbf1f698b138038df00,Journal of Intelligent Manufacturing
3042,Session details: Invited talk,,2012-03-03,https://www.semanticscholar.org/paper/a588a9ff9d7c43049dcfb9dd90a42ac3ab8a510d,Proceedings of the 8th ACM SIGPLAN/SIGOPS conference on Virtual Execution Environments
648,Stenting of the right internal mammary artery graft and right coronary artery via a femoral approach.,"We describe a case of balloon angioplasty and stenting of the right internal mammary artery (RIMA) graft anastomosis and the native right coronary artery through an in situ RIMA graft using two Bard XT stents (USCI Division of C.R. Bard, Inc., Billerica, Massachusetts). This case illustrates the feasibility of transluminal angioplasty and stenting of RIMA grafts and the native coronary artery using a femoral artery approach.",1999-06-01,https://www.semanticscholar.org/paper/197c957c26b22dea2d4da77b485cb6cae3fdf43d,The Journal of invasive cardiology
1151,"Search for next-to-minimal supersymmetric Higgs bosons in the h --> aa --> micromicromicromicro, micromicrotautau channels using pp[over] collisions at sqrt[s] = 1.96 TeV.","We report on a first search for production of the lightest neutral CP-even Higgs boson (h) in the next-to-minimal supersymmetric standard model, where h decays to a pair of neutral pseudoscalar Higgs bosons (a), using 4.2 fb;{-1} of data recorded with the D0 detector at Fermilab. The a bosons are required to either both decay to micro;{+}micro;{-} or one to micro;{+}micro;{-} and the other to tau;{+}tau;{-}. No significant signal is observed, and we set limits on its production as functions of M_{a} and M_{h}.",2009-08-07,https://www.semanticscholar.org/paper/46cdba1e9fc10738a06f58b2f0d2ba569b574917,Physical Review Letters
2413,Synesthesia AR: Creating Sound-to-Color Synesthesia in Augmented Reality,"Sound-to-color synesthesia is a neurological condition in which people experience different colors and shapes when listening to music. We present an augmented reality application that aims to create an interactive synesthesia experience for non-synesthetes. In this application, users can visualize colors corresponding to each unique note in the 12-tone equal-temperament tuning system, and the auditory input can be selected from audio files or real-time microphone. A gestural hand-tracking interface allows users to paint the world space in visualized synesthetic colors.",2022-03-01,https://www.semanticscholar.org/paper/64f976c39f2dff85a0f68f25e89fe0a4758f8d55,2022 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)
813,An Efficient Algorithm for Minimizing Real-Time Transition Systems,,1997-08-01,https://www.semanticscholar.org/paper/049ba7d771474cfd3b903230872b5d1c7e130405,Formal Methods Syst. Des.
1527,Estimating Social Influence from Observational Data,"We consider the problem of estimating social influence, the effect that a person's behavior has on the future behavior of their peers. The key challenge is that shared behavior between friends could be equally explained by influence or by two other confounding factors: 1) latent traits that caused people to both become friends and engage in the behavior, and 2) latent preferences for the behavior. This paper addresses the challenges of estimating social influence with three contributions. First, we formalize social influence as a causal effect, one which requires inferences about hypothetical interventions. Second, we develop Poisson Influence Factorization (PIF), a method for estimating social influence from observational data. PIF fits probabilistic factor models to networks and behavior data to infer variables that serve as substitutes for the confounding latent traits. Third, we develop assumptions under which PIF recovers estimates of social influence. We empirically study PIF with semi-synthetic and real data from Last.fm, and conduct a sensitivity analysis. We find that PIF estimates social influence most accurately compared to related methods and remains robust under some violations of its assumptions.",2022-03-24,https://www.semanticscholar.org/paper/aef22855ca3475e324d92768cbc0d1d84110b786,CLEaR
1773,Online Variational Inference for the Hierarchical Dirichlet Process,"The hierarchical Dirichlet process (HDP) is a Bayesian nonparametric model that can be used to model mixed-membership data with a potentially infinite number of components. It has been applied widely in probabilistic topic modeling, where the data are documents and the components are distributions of terms that reflect recurring patterns (or “topics”) in the collection. Given a document collection, posterior inference is used to determine the number of topics needed and to characterize their distributions. One limitation of HDP analysis is that existing posterior inference algorithms require multiple passes through all the data—these algorithms are intractable for very large scale applications. We propose an online variational inference algorithm for the HDP, an algorithm that is easily applicable to massive and streaming data. Our algorithm is significantly faster than traditional inference algorithms for the HDP, and lets us analyze much larger data sets. We illustrate the approach on two large collections of text, showing improved performance over online LDA, the finite counterpart to the HDP topic model.",2011-06-14,https://www.semanticscholar.org/paper/93792105974f4d42c83172c4fc9f24be77fe781b,International Conference on Artificial Intelligence and Statistics
203,Computation as a Scientific Weltanschauung (Invited Talk),"Computation as a mechanical reality is young - almost exactly seventy years of age - and yet the spirit of computation can be traced several millennia back. Any moderately advanced civilization depends on calculation (for inventory, taxation, navigation, land partition, among many others) - our civilization is the first one that is conscious of this reliance. 
 
Computation has also been central to science for centuries. This is most immediately apparent in the case of mathematics: the idea of the algorithm as a mathematical object of some significance was pioneered by Euclid in the 4th century BC, and advanced by Archimedes a century later. But computation plays an important role in virtually all sciences: natural, life, or social. Implicit algorithmic processes are present in the great objects of scientific inquiry - the cell, the universe, the market, the brain - as well as in the models developed by scientists over the centuries for studying them. This brings about a very recent - merely a few decades old - mode of scientific inquiry, which is sometime referred to as the lens of computation: When students of computation revisit central problems in science from the computational viewpoint, often unexpected progress results. This has happened in statistical physics through the study of phase transitions in terms of the convergence of Markov chain-Monte Carlo algorithms, and in quantum mechanics through quantum computing. 
 
This talk will focus on three other manifestations of this phenomenon. Almost a decade ago, ideas and methodologies from computational complexity revealed a subtle conceptual flaw in the solution concept of Nash equilibrium, which lies at the foundations of modern economic thought. In the study of evolution, a new understanding of century-old questions has been achieved through surprisingly algorithmic ideas. Finally, current work in theoretical neuroscience suggests that the algorithmic point of view may be invaluable in the central scientific question of our era, namely understanding how behavior and cognition emerge from the structure and activity of neurons and synapses.",,https://www.semanticscholar.org/paper/86a4401869d9f929fb59ed4ef51d04a89ef53d4b,Scandinavian Workshop on Algorithm Theory
2005,Manufacturing intelligence to forecast and reduce semiconductor cycle time,,2012-12-01,https://www.semanticscholar.org/paper/d3977c6e31ccb8a4775772731ace05d09872cabb,Journal of Intelligent Manufacturing
1777,Bayesian Checking for Topic Models,"Real document collections do not fit the independence assumptions asserted by most statistical topic models, but how badly do they violate them? We present a Bayesian method for measuring how well a topic model fits a corpus. Our approach is based on posterior predictive checking, a method for diagnosing Bayesian models in user-defined ways. Our method can identify where a topic model fits the data, where it falls short, and in which directions it might be improved.",2011-07-27,https://www.semanticscholar.org/paper/d9e650bebc559b34a4b5ae379d20933a8d0c9335,Conference on Empirical Methods in Natural Language Processing
3253,Juvenile social relationships reflect adult patterns of behavior in wild geladas,"Unlike many mammals, primates spend much of their lives as reproductively‐immature juveniles. During the juvenile period, they develop social relationships and physical skills that both facilitate survival to adulthood and impact adult fitness. In this study, we use 2 years of observational data to examine the development of these skills across the juvenile period in a wild cercopithecine primate, the gelada (Theropithecus gelada). As adults, male and female geladas require different skills to be successful; we therefore expected sex differences in social behavior and partner choice during the juvenile period to already reflect these sex‐specific trajectories. For example, males, who disperse at puberty and ultimately must challenge other adult males for access to mates, should invest in high‐energy play‐fighting with other males to develop fighting and rival assessment skills. In contrast, philopatric females, who remain with their close kin throughout their lives, should invest more in forming less‐physical and more‐social bonds with other females within their group. As predicted, sex differences that foreshadowed sex‐specific adult roles were apparent in play rates, the average number of play partners per individual, grooming partner types and social partner preferences. Males played more and had more play partners than same‐age females. Males also groomed more often with individuals from outside their natal group than females, although no sex difference was observed in either grooming rates or number of grooming partners per individual. Females stopped playing earlier than males, and instead invested in grooming relationships with close relatives. Additionally, we found that individual play and grooming rates were temporally consistent for both males and females (i.e., from one year to the next year), suggesting that individuals exhibit stable behavioral phenotypes. We conclude by discussing how early life in geladas may shape adult behavior and reproductive strategies. Am. J. Primatol. 77:1086–1096, 2015. © 2015 Wiley Periodicals, Inc.",2015-10-01,https://www.semanticscholar.org/paper/2c33b007df5430fe0a38a3ec4108ffeea14fcff5,American Journal of Primatology
363,A simple algorithm for finding frequent elements in streams and bags,"We present a simple, exact algorithm for identifying in a multiset the items with frequency more than a threshold θ. The algorithm requires two passes, linear time, and space 1/θ. The first pass is an on-line algorithm, generalizing a well-known algorithm for finding a majority element, for identifying a set of at most 1/θ items that includes, possibly among others, all items with frequency greater than θ.",2003-03-01,https://www.semanticscholar.org/paper/e82c4930ec5f01508313d8ad9b41565d25ea96d5,TODS
2606,An interaction system for watch computers using tactile guidance and bidirectional segmented strokes,"We introduce an input system that is based on bidirectional strokes that are segmented by tactile landmarks. By giving the user tactile feedback about the length of a stroke during input, we decrease the dependence of the GUI on the visual display. By concatenating separate strokes into multistrokes, complex commands may be entered, which may encode commands, data content, or both simultaneously. To demonstrate their power, we show how multistrokes can be used to traverse a menu hierarchy quickly. In addition, we show how inter-landmark segments of the sensor may be used for continuous and discrete parameter entry, resulting in a multifunctional interaction paradigm. We also introduce multiwidgets, which allow the direct control of multiple virtual widgets without the need to change the state of the device or use modifier buttons. This approach to input does not depend on material displayed visually to the user, and, thanks to tactile guidance, may be used by expert users as an eyes-free user interface. We believe that these benefits make this interaction system especially suitable for wearable computer systems that use a head-worn display and wrist-worn watch-style devices.",2004-10-31,https://www.semanticscholar.org/paper/ae56689440b28be8c442ec088a111c64424ff052,Eighth International Symposium on Wearable Computers
699,Computational Hardness of the Hylland-Zeckhauser Scheme,"We study the complexity of the classic Hylland-Zeckhauser scheme [HZ'79] for one-sided matching markets. We show that the problem of finding an $\epsilon$-approximate equilibrium in the HZ scheme is PPAD-hard, and this holds even when $\epsilon$ is polynomially small and when each agent has no more than four distinct utility values. Our hardness result, when combined with the PPAD membership result of [VY'21], resolves the approximation complexity of the HZ scheme. We also show that the problem of approximating the optimal social welfare (the weight of the matching) achievable by HZ equilibria within a certain constant factor is NP-hard.",2021-07-12,https://www.semanticscholar.org/paper/a0e27f6e982852e853398a64d494ac4cad00670b,ACM-SIAM Symposium on Discrete Algorithms
55,Session details: Search engineering 1,,2004-05-17,https://www.semanticscholar.org/paper/2889d7f93a9cdb0f2cc00464b0da6c2708523913,Proceedings of the 13th international conference on World Wide Web
1663,Exponential Family Embeddings,"Word embeddings are a powerful approach for capturing semantic similarity among terms in a vocabulary. In this paper, we develop exponential family embeddings, a class of methods that extends the idea of word embeddings to other types of high-dimensional data. As examples, we studied neural data with real-valued observations, count data from a market basket analysis, and ratings data from a movie recommendation system. The main idea is to model each observation conditioned on a set of other observations. This set is called the context, and the way the context is defined is a modeling choice that depends on the problem. In language the context is the surrounding words; in neuroscience the context is close-by neurons; in market basket data the context is other items in the shopping cart. Each type of embedding model defines the context, the exponential family of conditional distributions, and how the latent embedding vectors are shared across data. We infer the embeddings with a scalable algorithm based on stochastic gradient descent. On all three applications—neural activity of zebrafish, users' shopping behavior, and movie ratings—we found exponential family embedding models to be more effective than other types of dimension reduction. They better reconstruct held-out data and find interesting qualitative structure.",2016-08-02,https://www.semanticscholar.org/paper/cec8412a03aa42778ef0f4daf61c5c8c81ac9cb1,Neural Information Processing Systems
697,Technical Perspective,"The paper Structure and Complexity of Bag Consistency by Albert Atserias and Phokion Kolaitis [1] studies fundamental structural and algorithmic questions on the global consistency of databases in the context of bag semantics. A collection D of relations is called globally consistent if there is a (so-called ""universal"") relation over all the attributes that appear in all the relations of D whose projections yield the relations in D. The basic algorithmic problem for consistency is: given a database D, determine whether D is globally consistent. An obvious necessary condition for global consistency is local (or pairwise) consistency: every pair of relations in D must be consistent. This condition is not sufficient however: It is possible that every pair is consistent, but there is no single global relation over all the attributes whose projections yield the relations in D. A natural structural question is: Which database schemas have the property that every locally consistent database over the schema is also globally consistent?",2022-05-31,https://www.semanticscholar.org/paper/c666ae62e0f726989ffd48a521493f9a4ead8952,SIGMOD record
857,On the value of information in distributed decision-making (extended abstract),The treatment of rice polish by drying into a specific moisture content so that the rice polish is stabilized and resists deterioration for long periods of time even when stored without refrigeration. The moisture content ranges from 5.8 to 8.0 grams of moisture/100 grams of fat free solids which corresponds to a single molecular layer of water (a mono layer film) on each rice polish particle. This minimizes both fatty acid development and oxidative rancidity.,1991-07-01,https://www.semanticscholar.org/paper/40b860d3ce4357660192ee43e27094e350bce50f,ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing
2945,Inferring relevant cell types for complex traits using single-cell gene expression,"Previous studies have prioritized trait-relevant cell types by looking for an enrichment of GWAS signal within functional regions. However, these studies are limited in cell resolution by the lack of functional annotations from difficult-to-characterize or rare cell populations. Measurement of single-cell gene expression has become a popular method for characterizing novel cell types, and yet, hardly any work exists linking single-cell RNA-seq to phenotypes of interest. To address this deficiency, we present RolyPoly, a regression-based polygenic model that can prioritize trait-relevant cell types and genes from GWAS summary statistics and single-cell RNA-seq. We demonstrate RolyPoly’s accuracy through simulation and validate previously known tissue-trait associations. We discover a significant association between microglia and late-onset Alzheimer’s disease, and an association between oligodendrocytes and replicating fetal cortical cells with schizophrenia. Additionally, RolyPoly computes a trait-relevance score for each gene which reflects the importance of expression specific to a cell type. We found that differentially expressed genes in the prefrontal cortex of Alzheimer’s patients were significantly enriched for highly ranked genes by RolyPoly gene scores. Overall, our method represents a powerful framework for understanding the effect of common variants on cell types contributing to complex traits.",2017-05-10,https://www.semanticscholar.org/paper/a7e975383c3a3e996e46b8fa8479016b08c1951e,bioRxiv
1701,Hierarchical topographic factor analysis,"Recent work has revealed that cognitive processes are often reflected in patterns of functional connectivity throughout the brain (for review see [16]). However, examining functional connectivity patterns using traditional methods carries a substantial computational burden (of computing time and memory). Here we present a technique, termed Hierarchical topographic factor analysis, for efficiently discovering brain networks in large multi-subject neuroimaging datasets.",2014-06-04,https://www.semanticscholar.org/paper/78db7ecd2c2b56db9f2e8a27a36cc7178bdbe34e,International Workshop on Pattern Recognition in NeuroImaging
1055,"Erratum: First Dark Matter Constraints from a SuperCDMS Single-Charge Sensitive Detector [Phys. Rev. Lett. 121, 051301 (2018)].",This corrects the article DOI: 10.1103/PhysRevLett.121.051301.,2019-02-11,https://www.semanticscholar.org/paper/02895170aa33fe8a6f5bd764fec2c1e140bc3834,Physical Review Letters
2791,Cytosolic galectin-3 and -8 regulate antibacterial autophagy through differential recognition of host glycans on damaged phagosomes,"While glycans are generally displayed on the cell surface or confined within the lumen of organelles, they can become exposed to the cytosolic milieu upon disruption of organelle membrane by various stresses or pathogens. Galectins are a family of β-galactoside-binding animal lectins synthesized and predominantly localized in the cytosol. Recent research indicates that some galectins may act as ""danger signal sensors"" by detecting unusual exposure of glycans to the cytosol. Galectin-8 was shown to promote antibacterial autophagy by recognizing host glycans on ruptured vacuolar membranes and interacting with the autophagy adaptor protein NDP52. Galectin-3 also accumulates at damaged phagosomes containing bacteria; however, its functional consequence remains obscure. By studying mouse macrophages infected with Listeria monocytogenes (LM), we showed that endogenous galectin-3 protects intracellular LM by suppressing the autophagic response through a host N-glycan-dependent mechanism. Knock out of the galectin-3 gene resulted in enhanced LC3 recruitment to LM and decreased bacterial replication, a phenotype recapitulated when Galectin-8-deficient macrophages were depleted of N-glycans. Moreover, we explored the concept that alterations in cell surface glycosylation by extracellular factors can be deciphered by cytosolic galectins during the process of phagocytosis/endocytosis, followed by rupture of phagosomal/endosomal membrane. Notably, treatment of cells with sialidase, which removes sialic acid from glycans, resulted in increased galectin-3 accumulation and decreased galectin-8 recruitment at damaged phagosomes, and led to a stronger anti-autophagic response. Our findings demonstrate that cytosolic galectins may sense changes in glycosylation at the cell surface and modulate cellular response through differential recognition of glycans on ruptured phagosomal membranes.",2018-06-01,https://www.semanticscholar.org/paper/7d1520c9447ac7331209524172be6d6d3128c66e,Glycobiology
3168,Author Correction: Zebras of all stripes repel biting flies at close range,,2023-01-31,https://www.semanticscholar.org/paper/94ca125734bced5cfbad55715748b622fd0ffd8b,Scientific Reports
2885,"Expression and function of galectin-3, a beta-galactoside-binding lectin, in human monocytes and macrophages.","A family of beta-galactoside-binding animal lectins has recently been designated as galectins. One member of this family, galectin-3, has been known as epsilon BP for its IgE-binding activity and as Mac-2, a macrophage surface antigen, CBP35, CBP30, L-29, and L-34. Although much information has accumulated on the expression of this lectin in murine macrophages and human monocytic cell lines, little is known about the expression and function of this protein in normal human monocytes/macrophages. We now report that galectin-3 is expressed in normal human peripheral blood monocytes and its level increases dramatically as human monocytes differentiate into macrophages upon culturing in vitro. Immunoblot analysis showed that there was a 5-fold increase in the level of galectin-3 after 1 day of culture and greater than a 12-fold increase after 5 days. Immunocytochemical analysis confirmed this progressive increase of galectin-3 expression in cultured monocytes. Immunogold cytochemistry/electron microscopy analysis revealed that galectin-3 was expressed on the surface of human monocytes and that the level of cell surface galectin-3 increased progressively as these cells differentiated into macrophages. The level of galectin-3 in human monocytes/macrophages was modulated by stimuli such as lipopolysaccharide and interferon-gamma, and galectin-3 was secreted when monocytes were stimulated by calcium ionophore A23187 Soluble galectin-3 caused superoxide release from human monocytes; this activity was dependent on the lectin property of galectin-3, as it was inhibitable by lactose. Thus, galectin-3 may modulate the function of this cell type in an autocrine or paracrine fashion through binding to cell surface glycoconjugates.",1995-10-01,https://www.semanticscholar.org/paper/e0aff9505a9147b9cdbf025e9b05868712edbf77,American Journal of Pathology
348,Free-riding and whitewashing in peer-to-peer systems,"We devise a model to study the phenomenon of free-riding and free-identities in peer-to-peer systems. At the heart of our model is a user of a certain type, an intrinsic and private parameter that reflects the user's willingness to contribute resources to the system. A user decides whether to contribute or free-ride based on how the current contribution cost in the system compares to her type. We study the impact of mechanisms that exclude low type users or, more realistically, penalize free-riders with degraded service. We also consider dynamic scenarios with arrivals and departures of users, and with whitewashers -users who leave the system and rejoin with new identities to avoid reputational penalties. We find that imposing penalty on all users that join the system is effective under many scenarios. In particular, system performance degrades significantly only when the turnover rate among users is high. Finally, we show that the optimal exclusion or penalty level differs significantly from the level that optimizes the performance of contributors only for a limited range of societal generosity levels.",2004-09-03,https://www.semanticscholar.org/paper/e2108dba8487ebcd8689f1ed7b261feed654e65d,IEEE Journal on Selected Areas in Communications
1049,The LUX-ZEPLIN (LZ) radioactivity and cleanliness control programs,,2020-06-03,https://www.semanticscholar.org/paper/3e9e175902eab398b8a574a23d4311bc66318b74,The European Physical Journal C
1957,Determining the operator-machine assignment for machine interference problem and an empirical study in semiconductor test facility,,2013-05-03,https://www.semanticscholar.org/paper/479b8cb285064c8aabf9e2efd6fb1142c7307b13,Journal of Intelligent Manufacturing
1516,Nonparametric Identifiability of Causal Representations from Unknown Interventions,"We study causal representation learning, the task of inferring latent causal variables and their causal relations from high-dimensional functions (""mixtures"") of the variables. Prior work relies on weak supervision, in the form of counterfactual pre- and post-intervention views or temporal structure; places restrictive assumptions, such as linearity, on the mixing function or latent causal model; or requires partial knowledge of the generative process, such as the causal graph or the intervention targets. We instead consider the general setting in which both the causal model and the mixing function are nonparametric. The learning signal takes the form of multiple datasets, or environments, arising from unknown interventions in the underlying causal model. Our goal is to identify both the ground truth latents and their causal graph up to a set of ambiguities which we show to be irresolvable from interventional data. We study the fundamental setting of two causal variables and prove that the observational distribution and one perfect intervention per node suffice for identifiability, subject to a genericity condition. This condition rules out spurious solutions that involve fine-tuning of the intervened and observational distributions, mirroring similar conditions for nonlinear cause-effect inference. For an arbitrary number of variables, we show that two distinct paired perfect interventions per node guarantee identifiability. Further, we demonstrate that the strengths of causal influences among the latent variables are preserved by all equivalent solutions, rendering the inferred representation appropriate for drawing causal conclusions from new data. Our study provides the first identifiability results for the general nonparametric setting with unknown interventions, and elucidates what is possible and impossible for causal representation learning without more direct supervision.",2023-06-01,https://www.semanticscholar.org/paper/58c4a25181c962df905bbf32f23a047d57549163,arXiv.org
1780,Bayesian Nonparametric Matrix Factorization for Recorded Music,"Recent research in machine learning has focused on breaking audio spectrograms into separate sources of sound using latent variable decompositions. These methods require that the number of sources be specified in advance, which is not always possible. To address this problem, we develop Gamma Process Nonnegative Matrix Factorization (GaP-NMF), a Bayesian nonparametric approach to decomposing spectrograms. The assumptions behind GaP-NMF are based on research in signal processing regarding the expected distributions of spectrogram data, and GaP-NMF automatically discovers the number of latent sources. We derive a mean-field variational inference algorithm and evaluate GaP-NMF on both synthetic data and recorded music.",2010-06-21,https://www.semanticscholar.org/paper/1dda1a4414675729f46594a5e609938ef3a48382,International Conference on Machine Learning
851,"Suboptimal Cuts: Their Enumeration, Weight and Number (Extended Abstract)",,1992-07-13,https://www.semanticscholar.org/paper/be762c0fcbf2f3924b266d07551eea893e0630c0,"International Colloquium on Automata, Languages and Programming"
3004,Argus: Debugging Performance Issues in Modern Desktop Applications with Annotated Causal Tracing,"Modern desktop applications involve many asynchronous, concurrent interactions that make performance issues difficult to diagnose. Although prior work has used causal tracing for debugging performance issues in distributed systems, we find that these techniques suffer from high inaccuracies for desktop applications. We present Argus, a fast, effective causal tracing tool for debugging performance anomalies in desktop applications. Argus introduces a novel notion of strong and weak edges to explicitly model and annotate trace graph ambiguities, a new beam-search-based diagnosis algorithm to select the most likely causal paths in the presence of ambiguities, and a new way to compare causal paths across normal and abnormal executions. We have implemented Argus across multiple versions of macOS and evaluated it on 12 infamous spinning pinwheel issues in popular macOS applications. Argus diagnosed the root causes for all issues, 10 of which were previously unknown, some of which have been open for several years. Argus incurs less than 5% CPU overhead when its system-wide tracing is enabled, making always-on tracing feasible.",,https://www.semanticscholar.org/paper/0192b460429fc155035ee895c2a9d3383123fc29,USENIX Annual Technical Conference
143,Two Stage View Planning for Large-Scale Site Modeling,"We present a systematic view planning method to assist construction of 3-D models of large outdoor sites using a mobile robot platform with mounted scanner. In the first stage, we begin with a 2-D site footprint and the planner generates a minimal set of sufficient covering views. These views which incorporate constraints on the scanner, including field of view, minimum and maximum scanning distance, and grazing angle, serve as the initial set of scans which yields an approximate 3-D model of the site. In the second stage, we update this model by using a voxel-based occupancy procedure to plan and acquire the next best view. The algorithm continues to update the model sequentially until an accurate and complete 3-D model is obtained. Results are shown for a segment of the Columbia University campus. The system can also be used as a planning tool for manual construction of 3-D site models.",2006-06-14,https://www.semanticscholar.org/paper/036124d58d5152f0e557f9d526473ecfb8ff93bc,3D Data Processing Visualization and Transmission
1230,First study of the radiation-amplitude zero in Wgamma production and limits on anomalous WWgamma couplings at sqrt[s]=1.96 TeV.,We present results from a study of pp-->Wgamma+X events utilizing data corresponding to 0.7 fb{-1} of integrated luminosity at sqrt[s]=1.96 TeV collected by the D0 detector at the Fermilab Tevatron Collider. We set limits on anomalous WWgamma couplings at the 95% C.L. The one-dimensional 95% C.L. limits are 0.49<kappa{gamma}<1.51 and -0.12<lambda{gamma}<0.13. We make the first study of the charge-signed rapidity difference between the lepton and the photon and find it to be indicative of the standard model radiation-amplitude zero in the Wgamma system.,2008-02-29,https://www.semanticscholar.org/paper/7d086fc55dd04dc905f68997cca6dd1b441968c1,Physical Review Letters
811,Testing for Finite State Systems,,1998-08-24,https://www.semanticscholar.org/paper/ec82e1676b6516e7a84eac905afbb08a302875b6,Annual Conference for Computer Science Logic
2785,Galectin-3 as a Therapeutic Target for NSAID-Induced Intestinal Ulcers,"Non-steroidal anti-inflammatory drugs (NSAIDs) induce ulcers in the gastrointestinal tract, including the stomach and small intestine. NSAID-induced gastric ulcers can be prevented by taking acid-neutralizing/inhibitory drugs and cytoprotective agents. In contrast, there are no medicines to control NSAID-induced small intestinal ulcers, which are accompanied by a mucosal invasion of bacteria and subsequent activation of immune cells. Galectin-3 (Gal3), an endogenous lectin, has anti-microbial and pro-inflammatory functions. In the small intestine, since Gal3 is highly expressed in epithelial cells constitutively and macrophages inducibly, the Gal3 level can affect microbiota composition and macrophage activation. We hypothesized that the modulation of Gal3 expression could be beneficial in NSAID-induced intestinal ulcers. Using Gal3 knockout (Gal3KO) mice, we determined whether Gal3 could be a therapeutic target in NSAID-induced intestinal ulcers. Following the administration of indomethacin, an NSAID, we found that small intestinal ulcers were less severe in Gal3KO mice than in wild-type (WT) mice. We also found that the composition of intestinal microbiota was different between WT and Gal3KO mice and that bactericidal antibiotic polymyxin B treatment significantly suppressed NSAID-induced ulcers. Furthermore, clodronate, a macrophage modulator, attenuated NSAID-induced ulcers. Therefore, Gal3 could be an exacerbating factor in NSAID-induced intestinal ulcers by affecting the intestinal microbiota population and macrophage activity. Inhibition of Gal3 may be a therapeutic strategy in NSAID-induced intestinal ulcers. Clinical Trial Registration www.ClinicalTrials.gov, identifier NCT03832946.",2020-09-23,https://www.semanticscholar.org/paper/36631769f28b10c853bac05de55f14812455240e,Frontiers in Immunology
2906,New Correlative Microscopy Approaches to Understand the Microstructural Origins of Creep Cavitation in Austenitic Steels,"Creep is an important degradation mechanism in high temperature, high stress environments in industries such as aerospace and nuclear reactors. Metallic components gradually accumulate creep damage over extended periods of time that can eventually build to cracking. Understanding the microstructural mechanisms behind such processes in failed components or using laboratory-based uniaxial creep tests is complicated by the large ductile deformation that occurs in the final period prior to fracture, which can obscure many of the subtler features that caused it. Typically an interrupted test gives better representative data of the early stages of cavitation, but this presents challenges to understand how nanometer-scale cavities are distributed at larger length scales [1]. Often only by combining multiple characterization techniques across different length scales can the full picture of creep damage be fully characterized [2]. In this study we will explore new ways to understand how creep cavitation forms in AISI 316H austenitic stainless steel used for advanced gas-cooled nuclear reactors (AGRs), through both ex-service material from a boiler header and laboratory uniaxial creep test specimens. We present new correlative microscopy approaches to understand the early stage nucleation of creep cavities prior to failure using a combination of scanning electron microscopy (SEM), electron backscatter diffraction (EBSD), focused ion beam (FIB) imaging assisted by xenon difluoride (XeF 2 ) gas and transmission electron microscopy (TEM), together",2022-07-22,https://www.semanticscholar.org/paper/6aa24e3a87cc493d8d6f1e17143fca415c9b667a,Microscopy and Microanalysis
3046,Teaching operating systems using android,"The computing landscape is shifting towards mobile devices. To learn about operating systems, it is increasingly important for students to gain hands-on kernel programming experience in these environments, which are quite different from traditional desktops and servers. We present our work at Columbia University to teach operating systems using Android, an open, commercially supported software platform increasingly used on mobile and embedded devices. We introduce a series of five Android kernel programming projects suitable for a one semester introductory operating systems course. Each project teaches a core operating system concept infused with Android or mobile device specific context, such as Android specific process relationships, use of sensors, and design considerations for resource constrained mobile devices. We also introduce an Android virtual laboratory based on virtual appliances, distributed version control, and live demonstrations which gives students hands-on Android experience, with minimal computing infrastructure. We have used these Android kernel programming projects and the Android virtual lab to teach an introductory operating systems course. Although this was our first time teaching the course using Android, over 80% of students surveyed enjoyed using Android and the majority of students preferred Android to traditional desktop development.",2012-02-29,https://www.semanticscholar.org/paper/f9a2301ba2cb93e2abb012ee9ac511cbe82169dd,Technical Symposium on Computer Science Education
943,Scheduling Interval-Ordered Tasks,We show that unit execution time jobs subject to a precedence constraint whose complement is chordal can be scheduled in linear time on m processors. Generalizations to arbitrary execution times are NP-complete.,1979-08-01,https://www.semanticscholar.org/paper/db659cb96064a84f7eae627cf99d4705d61a7649,SIAM journal on computing (Print)
3019,Why Joanie Can Encrypt: Easy Email Encryption with Easy Key Management,"Email privacy is of crucial importance. Existing email encryption approaches are comprehensive but seldom used due to their complexity and inconvenience. We take a new approach to simplify email encryption and improve its usability by implementing receiver-controlled encryption: newly received messages are transparently downloaded and encrypted to a locally-generated key; the original message is then replaced. To avoid the problem of moving a single private key between devices, we implement per-device key pairs: only public keys need be synchronized via a simple verification step. Compromising an email account or server only provides access to encrypted emails. We implemented this scheme on several platforms, showing it works with PGP and S/MIME, is compatible with widely used mail clients and email services including Gmail, has acceptable overhead, and that users consider it intuitive and easy to use.",2019-03-25,https://www.semanticscholar.org/paper/7111f34fa4210b117a379eb49ed9d539a6837923,European Conference on Computer Systems
1414,Ratio of isolated photon cross sections in pp macro collisions at square root of s = 630 and 1800 GeV.,The inclusive cross section for production of isolated photons has been measured in pp macro collisions at square root of s = 630 GeV with the D0 detector at the Fermilab Tevatron Collider. The photons span a transverse energy (E(T)) range from 7-49 GeV and have pseudorapidity absolute value of eta < 2.5. This measurement is combined with the previous D0 result at square root of s = 1800 GeV to form a ratio of the cross sections. Comparison of next-to-leading-order QCD with the measured cross section at 630 GeV and the ratio of cross sections show satisfactory agreement in most of the E(T) range.,2001-11-01,https://www.semanticscholar.org/paper/c0614ca5dda80264184297b9e460f64496081220,Physical Review Letters
743,An impossibility theorem for price-adjustment mechanisms,"We show that there is no discrete-time price-adjustment mechanism (any process that at each period looks at the history of prices and excess demands and updates the prices) such that for any market (a set of goods and consumers with endowments and strictly concave utilities) the price-adjustment mechanism will achieve excess demands that are at most an ϵ fraction of the total supply within a number of periods that is polynomial in the number of goods and . This holds even if one restricts markets so that excess demand functions are differentiable with derivatives bounded by a small constant. For the convergence time to the actual price equilibrium, we show by a different method a stronger result: Even in the case of three goods with a unique price equilibrium, there is no function of ϵ that bounds the number of periods needed by a price-adjustment mechanism to arrive at a set of prices that is ϵ-close to the equilibrium.",2010-01-19,https://www.semanticscholar.org/paper/65404b8164eb33d082bedc59b7dcc18c52db7839,Proceedings of the National Academy of Sciences of the United States of America
2797,Galectin-7 regulates keratinocyte proliferation and differentiation through JNK-miR-203-p63 signaling,,2015-09-29,https://www.semanticscholar.org/paper/c4651afa0403ef0ce6aa353b8bc69ae668b3fe86,Journal of Investigative Dermatology
1979,Master production schedule and system for excelling enterprise resources (SEER) in the LED industry,"Every enterprise is pursuing energy conservation and carbon reduction development. Under this trend, light-emitting diodes (LEDs) are becoming the focus of the lighting industry. Because of the different levels or types of chips and substrates, the characteristics of products will be different. LED products are categorized according to characteristics such as luminescence efficiency, wavelength (color), and the reference voltage level. Traditionally, lack of good communication channels for production and sales department led to excessive inventories, even though the demand was satisfied. These products face the risk of being unsalable due to the short product life cycle. Therefore, this study developed a system for excelling enterprise resources (SEER) for LED manufacturing that optimizes chip procurement and production planning by linear programming. The system considered the deviation of the output distribution during the production to allocate the amount of chip and process to maximize profit. This study chooses the LED packaging company in Taiwan Hsinchu Science Park as an empirical case. We analyzed the different planning criteria to find some management rules and let this system become a communication tool between the production and sales department.",2013-08-01,https://www.semanticscholar.org/paper/8310b911c9d303d460b071e2521fb34146aec52b,2013 IEEE International Conference on Automation Science and Engineering (CASE)
3740,Multi-Level Multimodal Common Semantic Space for Image-Phrase Grounding,"We address the problem of phrase grounding by learning a multi-level common semantic space shared by the textual and visual modalities. We exploit multiple levels of feature maps of a Deep Convolutional Neural Network, as well as contextualized word and sentence embeddings extracted from a character-based language model. Following dedicated non-linear mappings for visual features at each level, word, and sentence embeddings, we obtain multiple instantiations of our common semantic space in which comparisons between any target text and the visual content is performed with cosine similarity. We guide the model by a multi-level multimodal attention mechanism which outputs attended visual features at each level. The best level is chosen to be compared with text content for maximizing the pertinence scores of image-sentence pairs of the ground truth. Experiments conducted on three publicly available datasets show significant performance gains (20%-60% relative) over the state-of-the-art in phrase localization and set a new performance record on those datasets. We provide a detailed ablation study to show the contribution of each element of our approach and release our code on GitHub.",2018-11-28,https://www.semanticscholar.org/paper/2718cd594d2aa09315da52594877cd71d377dfcf,Computer Vision and Pattern Recognition
3568,Source Code Rejuvenation Is Not Refactoring,,2009-12-08,https://www.semanticscholar.org/paper/18968c4b1879ff1a84fcb64b0619264c00fb2f75,Conference on Current Trends in Theory and Practice of Informatics
268,A New Look at Selfish Routing,"We revisit price of anarchy in network routing, in a new model in which routing decisions are made by self-interested components of the network, as opposed to by the flows as in (12). This significant departure from previous work on the problem seeks to model Internet routing more accurately. We propose two models: the latency model in which the network edges seek to minimize the average latency of the flow through them on the basis of knowledge of latency conditions in the whole network, and the pricing model in which network edges advertise pricing schemes to their neighbors and seek to maximize their profit. We show two rather surprising results: the price of stability in the latency model is unbounded — Ω(n 1 60 ) — even with linear latencies (as compared with 4 in (12) for the case in which routing decisions are made by the flows themselves). However, in the pricing model in which edges advertise pricing schemes — how the price varies as a function of the total amount of flow — we show that, under a condition ruling out monopolistic situations, all Nash equilibria have optimal flows; that is, the price of anarchy in this model is one ,i n the case of linear latencies with no constant term.",,https://www.semanticscholar.org/paper/6a21e10ba5221820c801826f964a5fe0d7234b93,International Conference on Supercomputing
3195,Bothersome Flies: How Free-Ranging Horses Reduce Harm While Maintaining Nutrition,"The horses of Shackleford Banks, NC, United States are harassed by many species of biting flies. Apart from being a nuisance, their bites can lead to blood loss and transmit disease. As a result, these horses tend to avoid areas where fly abundances are high. Like other free-ranging horse populations, environmental factors such as low wind speeds and high temperatures increase fly loads per horse. Similarly, coat color matters since darker horses attract more flies than lighter ones, especially on hot sunny days. Many horse populations reduce per capita fly loads by living in large groups or by bunching tightly together. Shackleford horses do so, too, but also use wind speed differences among habitats to modulate fly numbers. By adopting a systematic pattern of moving between habitats such that they only visit a habitat when wind speed is high enough to keep fly harassment to a tolerable level, they can avoid being bitten while continuing to forage. Typically, they begin the day foraging on the salt marshes where fly abundance is inherently low and are lowered further by faint early morning breezes. Later in the morning, horses move to grassy patches (swales) when increasing wind speed reduces fly landings there to levels found on the marshes. Later still, when wind speeds peak, horses begin foraging among the sand dunes. At this point wind speeds are high enough so that horses using any habitat will be minimally harassed by flies, thus enabling them to freely choose where to feed based on which habitat meets particular dietary needs for protein, energy and nutrients on any particular day. Hence, Shackleford horses follow the breeze to solve a challenging dilemma of maintaining a high nutritional plane without succumbing to fly harassment. Other free-ranging horses populations appear to have a more limited “either-or” choice of “bite or be bitten,” thus limiting their decision-making options.",2021-09-24,https://www.semanticscholar.org/paper/968085f2b47ccd8a065a5d6f8e0f211792489863,Frontiers in Ecology and Evolution
3262,"Similar but Different: Dynamic Social Network Analysis Highlights Fundamental Differences between the Fission-Fusion Societies of Two Equid Species, the Onager and Grevy’s Zebra","Understanding why animal societies take on the form that they do has benefited from insights gained by applying social network analysis to patterns of individual associations. Such analyses typically aggregate data over long time periods even though most selective forces that shape sociality have strong temporal elements. By explicitly incorporating the temporal signal in social interaction data we re-examine the network dynamics of the social systems of the evolutionarily closely-related Grevy’s zebras and wild asses that show broadly similar social organizations. By identifying dynamic communities, previously hidden differences emerge: Grevy’s zebras show more modularity than wild asses and in wild asses most communities consist of solitary individuals; and in Grevy’s zebras, lactating females show a greater propensity to switch communities than non-lactating females and males. Both patterns were missed by static network analyses and in general, adding a temporal dimension provides insights into differences associated with the size and persistence of communities as well as the frequency and synchrony of their formation. Dynamic network analysis provides insights into the functional significance of these social differences and highlights the way dynamic community analysis can be applied to other species.",2015-10-21,https://www.semanticscholar.org/paper/e848221956677ab924417b9de721bda93d6aa826,PLoS ONE
159,Public Goods Games in Directed Networks,"Public goods games in undirected networks are generally known to have pure Nash equilibria, which are easy to find. In contrast, we prove that, in directed networks, a broad range of public goods games have intractable equilibrium problems: The existence of pure Nash equilibria is NP-hard to decide, and mixed Nash equilibria are PPAD-hard to find. We define general utility public goods games, and prove a complexity dichotomy result for finding pure equilibria, and a PPAD-completeness proof for mixed Nash equilibria. Even in the divisible goods variant of the problem, where existence is easy to prove, finding the equilibrium is PPAD-complete. Finally, when the treewidth of the directed network is appropriately bounded, we prove that polynomial-time algorithms are possible.",2021-06-01,https://www.semanticscholar.org/paper/a0393765e5559d482c7098e242fa750c56003ea0,ACM Conference on Economics and Computation
2443,Remote collaboration in AR and VR using virtual replicas,"In many complex tasks, a remote subject-matter expert may need to assist a local user, to guide their actions on objects in the local user's environment. However, effective spatial referencing and action demonstration in a remote physical environment can be challenging. We demonstrate an approach that uses Virtual Reality (VR) or Augmented Reality (AR) for the remote expert, and AR for the local user, each wearing a stereo head-worn display (HWD). Our approach allows the remote expert to create and manipulate virtual replicas of physical objects in the local environment to refer to parts of those physical objects and to indicate actions on them. This can be especially useful for parts that are occluded or difficult to access. The remote expert can demonstrate actions in 3D by manipulating virtual replicas, supported by constraints and annotations, and point in 3D to portions of virtual replicas to annotate them.",2017-07-30,https://www.semanticscholar.org/paper/10f0b32a12d04d48c4eb4b09f135e9465ea145f0,SIGGRAPH VR Village
2434,Hatching for 3D prints: line-based halftoning for dual extrusion fused deposition modeling,,2018-05-03,https://www.semanticscholar.org/paper/5c5bc850bb55d80e3748126cf6bc88fb3fb75cfb,Computers & graphics
3134,A SMART scheduler for multimedia applications,"Real-time applications such as multimedia audio and video are increasingly populating the workstation desktop. To support the execution of these applications in conjunction with traditional non-real-time applications, we have created SMART, a Scheduler for Multimedia And Real-Time applications. SMART supports applications with time constraints, and provides dynamic feedback to applications to allow them to adapt to the current load. In addition, the support for real-time applications is integrated with the support for conventional computations. This allows the user to prioritize across real-time and conventional computations, and dictate how the processor is to be shared among applications of the same priority. As the system load changes, SMART adjusts the allocation of resources dynamically and seamlessly. It can dynamically shed real-time computations and regulate the execution rates of real-time tasks when the system is overloaded, while providing better value in underloaded conditions than previously proposed schemes.We have implemented SMART in the Solaris UNIX operating system and measured its performance against other schedulers commonly used in research and practice in executing real-time, interactive, and batch applications. Our experimental results demonstrate SMART's superior performance over fair queueing and UNIX SVR4 schedulers in supporting multimedia applications.",2003-05-01,https://www.semanticscholar.org/paper/f06a958a1e03b1166f9877c746504941ec7b6758,TOCS
1007,Optic disc change with incipient myopia of childhood.,,,https://www.semanticscholar.org/paper/f1c0c6464024f76abfb76e9dada081aa9588bb7f,"Ophthalmology (Rochester, Minn.)"
3462,Scheduling an Industrial Production Facility,,2004-06-07,https://www.semanticscholar.org/paper/1af12f53bb43e8f8c357249e6d4b586c88b79b95,Conference on Integer Programming and Combinatorial Optimization
3393,"MapReduce Meets Fine-Grained Complexity: MapReduce Algorithms for APSP, Matrix Multiplication, 3-SUM, and Beyond","Distributed processing frameworks, such as MapReduce, Hadoop, and Spark are popular systems for processing large amounts of data. The design of efficient algorithms in these frameworks is a challenging problem, as the systems both require parallelism---since datasets are so large that multiple machines are necessary---and limit the degree of parallelism---since the number of machines grows sublinearly in the size of the data. Although MapReduce is over a dozen years old~\cite{dean2008mapreduce}, many fundamental problems, such as Matrix Multiplication, 3-SUM, and All Pairs Shortest Paths, 
lack efficient MapReduce algorithms. We study these problems in the MapReduce setting. Our main contribution is to exhibit smooth trade-offs between the memory available on each machine, and the total number of machines necessary for each problem. Overall, we take the memory available to each machine as a parameter, and aim to minimize the number of rounds and number of machines. 
In this paper, we build on the well-known MapReduce theoretical framework initiated by Karloff, Suri, and Vassilvitskii ~\cite{karloff2010model} and give algorithms for many of these problems. The key to efficient algorithms in this setting lies in defining a sublinear number of large (polynomially sized) subproblems, that can then be solved in parallel. We give strategies for MapReduce-friendly partitioning, that result in new algorithms for all of the above problems. Specifically, we give constant round algorithms for the Orthogonal Vector (OV) and 3-SUM problems, and $O(\log n)$-round algorithms for Matrix Multiplication, All Pairs Shortest Paths (APSP), and Fast Fourier Transform (FFT), among others. In all of these we exhibit trade-offs between the number of machines and memory per machine.",2019-05-05,https://www.semanticscholar.org/paper/eae59abe363694d4b1797d07463cfa7c03cc8cc1,arXiv.org
467,On the Complexity of the Parity Argument and Other Inefficient Proofs of Existence,,1994-06-01,https://www.semanticscholar.org/paper/72acce082bdf547d4a2687a30a62e17ae76e475e,Journal of computer and system sciences (Print)
2445,Session details: VR/AR,,2017-10-20,https://www.semanticscholar.org/paper/56ae50e3d9a86ead8317e8a3c1cebff8a1775fee,ACM Symposium on User Interface Software and Technology
3559,Understanding and Effectively Preventing the ABA Problem in Descriptor-Based Lock-Free Designs,"An increasing number of modern real-time systems and the nowadays ubiquitous multicore architectures demand the application of programming techniques for reliable and efficient concurrent synchronization. Some recently developed Compare-And-Swap (CAS) based nonblocking techniques hold the promise of delivering practical and safer concurrency. The ABA problem is a fundamental problem to many CAS-based designs. Its significance has increased with the suggested use of CAS as a core atomic primitive for the implementation of portable lock-free algorithms. The ABA problem's occurrence is due to the intricate and complex interactions of the application's concurrent operations and, if not remedied, ABA can significantly corrupt the semantics of a nonblocking algorithm. The current state of the art leaves the elimination of the ABA hazards to the ingenuity of the software designer. In this work we provide the first systematic and detailed analysis of the ABA problem in lock-free Descriptor-based designs. We study the semantics of Descriptor-based lock-free data structures and propose a classification of their operations that helps us better understand the ABA problem and subsequently derive an effective ABA prevention scheme. Our ABA prevention approach outperforms by a large factor the use of the alternative CAS-based ABA prevention schemes. It offers speeds comparable to the use of the architecture-specific CAS2 instruction used for version counting. We demonstrate our ABA prevention scheme by integrating it into an advanced nonblocking data structure, a lock-free dynamically resizable array.",2010-05-05,https://www.semanticscholar.org/paper/2fe159422eab95c284b6a9b3e9fef52a310c4527,IEEE International Symposium on Object/Component/Service-Oriented Real-Time Distributed Computing
3486,Experimental Evaluation of Approximation Algorithms for Single-Source Unsplittable Flow,,1999-06-09,https://www.semanticscholar.org/paper/842591f79f6c7613f3dcc6b527cab169308152d2,Conference on Integer Programming and Combinatorial Optimization
281,Some Recent Results in Algorithmic Game Theory,,2008-12-17,https://www.semanticscholar.org/paper/0a42d279385ee619ed7033707ed13d62f52d156e,Workshop on Internet and Network Economics
2469,Spatial computing,"Knowing where you are in space and time promises a deeper understanding of neighbors, ecosystems, and the environment.",2015-12-21,https://www.semanticscholar.org/paper/8f2a05700ec83896bd0efd4755a26e24e3e57d3b,Communications of the ACM
2603,My own private kiosk: privacy-preserving public displays,"Ubiquitous, high-resolution, large public displays offer an attractive complement to wearable displays. Unfortunately, the inherently public nature of these public displays makes them unsuitable for displaying sensitive information. We present EyeGuide, a wearable system that allows the user to obtain information quickly from a public display without sacrificing privacy. To this end, EyeGuide employs a lightweight head-worn eye-tracker for hands-free object selection and an earphone for private communication. Our system supports public displays that are dynamic (e.g., a large plasma screen) and static (e.g., a large printed map). In our printed map scenario, EyeGuide whispers verbal directions via earphone to a user, based on where they are looking on the map. Using a technique we call ""gaze steering,"" the system guides the user's eye position to specific locations. In our dynamic public display scenarios, EyeGuide presents documents (e.g., maps) that contain sensitive data in a way that preserves privacy.",2004-10-31,https://www.semanticscholar.org/paper/9b65c3f2aa4542e2d87c50f6e0d33e103697d33c,Eighth International Symposium on Wearable Computers
2281,Insoluble and soluble immune complexes activate neutrophils by distinct activation mechanisms: changes in functional responses induced by priming with cytokines,"Background: Rheumatoid synovial fluid contains both soluble and insoluble immune complexes that can activate infiltrating immune cells such as neutrophils. Objectives: To determine if these different complexes activate neutrophils through similar or different receptor signalling pathways. In particular, to determine the circumstances which result in the secretion of tissue damaging reactive oxygen metabolites and granule enzymes. Methods: Blood neutrophils were incubated with synthetic soluble and insoluble immune complexes and the ability to generate reactive oxidants tested by luminescence or spectrophotometric assays that distinguished between intracellular and extracellular production. Degranulation of myeloperoxidase and lactoferrin was determined by western blotting. The roles of FcγRII (CD32) and FcγRIIIb (CD16) were determined by incubation with Fab/F(ab`)2 fragments before activation. The effect of cytokine priming was determined by incubation with GM-CSF. Results: Insoluble immune complexes activated unprimed neutrophils, but most of the oxidants produced were intracellular. This activation required FcγRIIIb, but not FcγRII function. Soluble complexes failed to activate unprimed neutrophils but generated a rapid and extensive secretion of reactive oxygen metabolites when the cells were primed with granulocyte-macrophage colony stimulating factor (GM-CSF). This activity required both FcγRII and FcγRIIIb function. Insoluble immune complexes activated the release of granule enzymes from primed or unprimed neutrophils, but the kinetics of release did not parallel those of secretion of reactive oxygen metabolites. Only primed neutrophils released enzymes in response to soluble complexes. Conclusions: Soluble and insoluble immune complexes activate neutrophils by separate receptor signalling pathways. Profound changes in neutrophil responsiveness to these complexes occur after cytokine priming.",2002-01-01,https://www.semanticscholar.org/paper/cb0e2af4ca3e48dc9d1be6c49ec99b30b0c033fa,Annals of the Rheumatic Diseases
1859,Modeling annotated data,"We consider the problem of modeling annotated data---data with multiple types where the instance of one type (such as a caption) serves as a description of the other type (such as an image). We describe three hierarchical probabilistic mixture models which aim to describe such data, culminating in correspondence latent Dirichlet allocation, a latent variable model that is effective at modeling the joint distribution of both types and the conditional distribution of the annotation given the primary type. We conduct experiments on the Corel database of images and captions, assessing performance in terms of held-out likelihood, automatic annotation, and text-based image retrieval.",2003-07-28,https://www.semanticscholar.org/paper/473f4b7f8ae2b03dda2593f54b316ff7d55db26b,Annual International ACM SIGIR Conference on Research and Development in Information Retrieval
3016,Making It Easier to Encrypt Your Emails,"Steven M. Bellovin is Professor of Computer Science at Columbia University and affiliate faculty at its law school. His research specializes in security, privacy, and related legal and policy issues. He co-authored Firewalls and Internet Security, the first book on the subject. He is a member of the National Academy of Engineering and received the USENIX “Flame” award for co-inventing Netnews. smb@cs.columbia.edu",,https://www.semanticscholar.org/paper/4a8d7cf7c9fa8bde3e9ab79bb0b2ab62afef074f,Login: The Usenix Magazine
2163,Persistent advanced periductal fibrosis is associated with cagA‐positive Helicobacter pylori infection in post‐praziquantel treatment of opisthorchiasis,"Liver fluke infection caused by Opisthorchis viverrini is associated with several hepatobiliary diseases including advanced periductal fibrosis (APF) and cholangiocarcinoma. Recently, we demonstrated a persistent APF in over one‐third of opisthorchiasis patients after worm removal by praziquantel (PZQ) treatment. However, the underlying mechanism(s) of this phenomena is unclear. Given a co‐infection with Helicobacter pylori (H. pylori) especially cagA‐positive strain enhances APF, we hypothesized that H. pylori with CagA virulent factor contributes to persistent APF.",2022-05-09,https://www.semanticscholar.org/paper/91d88975e18941564cc8a529c9ffa6d21c57030f,Helicobacter
1681,Risk prediction for chronic kidney disease progression using heterogeneous electronic health record data and time series analysis,"Abstract Background As adoption of electronic health records continues to increase, there is an opportunity to incorporate clinical documentation as well as laboratory values and demographics into risk prediction modeling. Objective The authors develop a risk prediction model for chronic kidney disease (CKD) progression from stage III to stage IV that includes longitudinal data and features drawn from clinical documentation. Methods The study cohort consisted of 2908 primary-care clinic patients who had at least three visits prior to January 1, 2013 and developed CKD stage III during their documented history. Development and validation cohorts were randomly selected from this cohort and the study datasets included longitudinal inpatient and outpatient data from these populations. Time series analysis (Kalman filter) and survival analysis (Cox proportional hazards) were combined to produce a range of risk models. These models were evaluated using concordance, a discriminatory statistic. Results A risk model incorporating longitudinal data on clinical documentation and laboratory test results (concordance 0.849) predicts progression from state III CKD to stage IV CKD more accurately when compared to a similar model without laboratory test results (concordance 0.733, P<.001), a model that only considers the most recent laboratory test results (concordance 0.819, P < .031) and a model based on estimated glomerular filtration rate (concordance 0.779, P < .001). Conclusions A risk prediction model that takes longitudinal laboratory test results and clinical documentation into consideration can predict CKD progression from stage III to stage IV more accurately than three models that do not take all of these variables into consideration.",2015-04-20,https://www.semanticscholar.org/paper/6fc46455b6294aa2dd5e25d206a259aecdf7065b,J. Am. Medical Informatics Assoc.
3213,Multilevel Organisation of Animal Sociality.,,2020-05-27,https://www.semanticscholar.org/paper/ff3ee824863692861609ae51976fd550e45829a2,Trends in Ecology & Evolution
3353,Combat and communication in the Everglades pygmy sunfish,,1981-02-01,https://www.semanticscholar.org/paper/0bfc5320c0a8b58a58bf4fa173d48c8145408dfb,Animal Behaviour
205,Power-Law Distributions in a Two-Sided Market and Net Neutrality,,2016-10-16,https://www.semanticscholar.org/paper/8d7c3a53e485617451b5cf5c7bf5dc13fd8b93c5,Workshop on Internet and Network Economics
3548,Open and efficient type switch for C++,"Selecting operations based on the run-time type of an object is key to many object-oriented and functional programming techniques. We present a technique for implementing open and efficient type switching on hierarchical extensible data types. The technique is general and copes well with C++ multiple inheritance. To simplify experimentation and gain realistic performance using production-quality compilers and tool chains, we implement a type switch construct as an ISO C++11 library, called Mach7. This library-only implementation provides concise notation and outperforms the visitor design pattern, commonly used for case analysis on types in object-oriented programming. For closed sets of types, its performance roughly equals equivalent code in functional languages, such as OCaml and Haskell. The type-switching code is easier to use and is more expressive than hand-coded visitors are. The library is non-intrusive and circumvents most of the extensibility restrictions typical of the visitor design pattern. It was motivated by applications involving large, typed, abstract syntax trees.",2012-10-19,https://www.semanticscholar.org/paper/3ee45f31a74d34ec99da29c1d6ef5d987075c917,"Conference on Object-Oriented Programming Systems, Languages, and Applications"
2930,Phenome-scale causal network discovery with bidirectional mediated Mendelian randomization,"Inference of directed biological networks from observational genomics datasets is a crucial but notoriously difficult challenge. Modern population-scale biobanks, containing simultaneous measurements of traits, biomarkers, and genetic variation, offer an unprecedented opportunity to study biological networks. Mendelian randomization (MR) has received attention as a class of methods for inferring causal effects in observational data that uses genetic variants as instrumental variables, but MR methods rely on assumptions that limit their application to complex traits at the biobank-scale. Moreover, MR estimates the total effect of one trait on another, which may be mediated by other factors. Biobanks include measurements of many potential mediators, in principle enabling the conversion of MR estimates into direct effects representing a causal network. Here, we show that this can be accomplished by a flexible two stage procedure we call bidirectional mediated Mendelian randomization (bimmer). First, bimmer estimates the effect of every trait on every other. Next, bimmer finds a parsimonious network that explains these effects using direct and mediated causal paths. We introduce novel methods for both steps and show via extensive simulations that bimmer is able to learn causal network structures even in the presence of non-causal genetic correlation. We apply bimmer to 405 phenotypes from the UK biobank and demonstrate that learning the network structure is invaluable for interpreting the results of phenome-wide MR, while lending causal support to several recent observational studies.",2020-06-20,https://www.semanticscholar.org/paper/a323e4a821b8704240bd077bf690c9bff5b4a098,bioRxiv
379,The Joy of Theory,"This talk is meant to be a celebration of theoreticians, thier achievements, and thier unique style, drawing to a large extent on examples from this volume.Theoretical Computer Science has largely succeeded in its core mission, that is, improving a rigorous and productive foundational understanding of the power and limitations of the von Neumann computer and its software. And in the past few years it has strived to extend its reach to the Internet and the worldwide web, the central computational artifact of our times.But theoreticians have achieved much more than this. Our community has identified P vs. NP, arguably the deepest and most important mathematical question of our time -- and it is leading the assault on it. In addition we are developing ""algorithmic mirrors"" through which other sciences (notably Physics and Biology, with Economics and other Social Sciences soon to join) rediscover, fruitfully, themselves. And we have furthered and influenced crucially Combinatorics and Logic, the important mathematical fields from which we have drawn methodologically. The newfound respectability and prestige of our parent field, Computer Science, owes much to these achievements.Theoreticians comprise a microcosm with wonderful characteristics: Great scientific, social, and intellectual openness; responsibility and mutual respect, but also healthy doses of irreverence, mistrust of the establishment, willingness to experiment, and self-critical spirit; and a strong sense of an international community that is remarkably cohesive and tightly knit while celebrating the diversity of its people, of their backgrounds, and of their scientific interests and approaches.We have also developed a fascinating, complex esthetic of our work based on mathematical elegance and depth, relevance and fashion, timeliness and competition -- but also on playfulness and humor. Our esthetic has served us well: Some of our most important results were derived by long chains of contributions, each seemingly guided to a large extent by such esthetic considerations. In fact, this esthetic extends delightfully to the exposition of our work, as evidenced by the unique genre of scientific prose known as ""FOCS/STOC abstract"".",2002-05-19,https://www.semanticscholar.org/paper/f0508d72d767666aae1b925c848a13752ae996a5,Symposium on the Theory of Computing
430,"On kernels, defaults and even graphs",,1997-01-09,https://www.semanticscholar.org/paper/1bdacb0d3267afb98a6dce652ee81dee58e56756,Annals of Mathematics and Artificial Intelligence
3346,Tail size and female choice in the guppy (Poecilia reticulata),,1985-08-01,https://www.semanticscholar.org/paper/e25f72e9c1ab59de8c6a063449e3030ad913d3e7,Behavioral Ecology and Sociobiology
2532,Spatially aware handhelds for high-precision tangible interaction with large displays,"While touch-screen displays are becoming increasingly popular, many factors affect user experience and performance. Surface quality, parallax, input resolution, and robustness, for instance, can vary with sensing technology, hardware configurations, and environmental conditions.
 We have developed a framework for exploring how we could overcome some of these dependencies, by leveraging the higher visual and input resolution of small, coarsely tracked mobile devices for direct, precise, and rapid interaction on large digital displays.
 The results from a formal user study show no significant differences in performance when comparing four techniques we developed for a tracked mobile device, where two existing touch-screen techniques served as baselines. The mobile techniques, however, had more consistent performance and smaller variations among participants, and an overall higher user preference in our setup. Our results show the potential of spatially aware handhelds as an interesting complement or substitute for direct touch-interaction on large displays.",2009-02-16,https://www.semanticscholar.org/paper/4512a28cdebe66c2cc2e19e0a086409d71008be4,"International Conference on Tangible, Embedded, and Embodied Interaction"
3661,What is object-oriented programming?,"The meaning of the term 'object oriented' is examined in the context of the general-purpose programming language C++. This choice is made partly to introduce C++ and partly because C++ is one of the few languages that supports data abstraction, object-oriented programming, and traditional programming techniques. The support of programming paradigms by languages is discussed and four paradigms are examined: procedural, data hiding, data abstraction, and object-oriented programming. The support of the latter two by C++ is discussed in some detail.<<ETX>>",1987-06-15,https://www.semanticscholar.org/paper/c5a38f242075a938dc760b2ef59e8f59f2a60fe0,IEEE Software
3761,Who is Mistaken?,"Recognizing when people have false beliefs is crucial for understanding their actions. We introduce the novel problem of identifying when people in abstract scenes have incorrect beliefs. We present a dataset of scenes, each visually depicting an 8-frame story in which a character has a mistaken belief. We then create a representation of characters' beliefs for two tasks in human action understanding: predicting who is mistaken, and when they are mistaken. Experiments suggest that our method for identifying mistaken characters performs better on these tasks than simple baselines. Diagnostics on our model suggest it learns important cues for recognizing mistaken beliefs, such as gaze. We believe models of people's beliefs will have many applications in action understanding, robotics, and healthcare.",2016-12-04,https://www.semanticscholar.org/paper/fac17d4bb7268bbf33290de2915c33e6e57647da,arXiv.org
416,Reflective Relational Machines,"Abstract We propose a model of database programming with reflection (dynamic generation of queries within the host programming language), called the reflective relational machine , and characterize the power of this machine in terms of known complexity classes. In particular, the polynomial time restriction of the reflective relational machine is shown to express PSPACE, and to correspond precisely to uniform circuits of polynomial depth and exponential size. This provides an alternative, logic based formulation of the uniform circuit model, which may be more convenient for problems naturally formulated in logic terms, and establishes that reflection allows for more “intense” parallelism, which is not attainable otherwise (unless P=PSPACE). We also explore the power of the reflective relational machine subject to restrictions on the number of variables used, emphasizing the case of sublinear bounds.",1998-06-15,https://www.semanticscholar.org/paper/073ebc60d29fbdcf16223593d47f545ffea4be40,Information and Computation
3390,Parallel approximate undirected shortest paths via low hop emulators,"We present a (1+ε)-approximate parallel algorithm for computing shortest paths in undirected graphs, achieving poly(logn) depth and m poly(logn) work for n-nodes m-edges graphs. Although sequential algorithms with (nearly) optimal running time have been known for several decades, near-optimal parallel algorithms have turned out to be a much tougher challenge. For (1+ε)-approximation, all prior algorithms with poly(logn) depth perform at least Ω(mn c ) work for some constant c>0. Improving this long-standing upper bound obtained by Cohen (STOC’94) has been open for 25 years. We develop several new tools of independent interest. One of them is a new notion beyond hopsets — low hop emulator — a poly(logn)-approximate emulator graph in which every shortest path has at most O(loglogn) hops (edges). Direct applications of the low hop emulators are parallel algorithms for poly(logn)-approximate single source shortest path (SSSP), Bourgain’s embedding, metric tree embedding, and low diameter decomposition, all with poly(logn) depth and m poly(logn) work. To boost the approximation ratio to (1+ε), we introduce compressible preconditioners and apply it inside Sherman’s framework (SODA’17) to solve the more general problem of uncapacitated minimum cost flow (a.k.a., transshipment problem). Our algorithm computes a (1+ε)-approximate uncapacitated minimum cost flow in poly(logn) depth using m poly(logn) work. As a consequence, it also improves the state-of-the-art sequential running time from m· 2 O(√logn) to m poly(logn).",2019-11-05,https://www.semanticscholar.org/paper/50a9be89c2206c68f031b88b5c2cc7f27bc9b08b,Symposium on the Theory of Computing
225,A New Age of Computing and the Brain,"Throughout its history, humankind has been fascinated by a question that is simple to pose, yet remarkably resistant to resolution: ""How does the brain work?"" Philosophers have debated the workings of the mind for centuries. Da Vinci made detailed sketches of the brain. By the turn of the century, scientists began to understand some of the brain's basic structure and function. Today, we can image and record brain activity from the neural to whole-brain level. Yet, divining how the structure and function of the several billion neurons and their trillions of interconnections leads to the complexity, diversity, and adaptability of human behavior continues to elude us. It is indeed ironic that almost every advance in brain science has given us a deeper appreciation of the challenges of understanding the brain.",2014-12-03,https://www.semanticscholar.org/paper/1fbea1d40348f8e767d4d1feb6dfa45f1a55cbeb,arXiv.org
2016,Semiconductor manufacturing intelligence and key factor control mechanism for managing production cycle time,"▓ Semiconductor wafer fabrication plays a central role in electronics supply chain. Two characteristics of electronic products contribute to making the demand volatile: high variety and short life cycle. Semiconductor wafer manufacturers confront the challenges of reducing the lengthy cycle times as a means to deal with the highly variable demand in the supply chain. ▓ Cycle time reduction thus becomes a critical issue for semiconductor wafer fabrication companies to attain competitive advantages. ▓ Semiconductor manufacturing has long time been recognized as one of the most complicated manufacturing process owing to unrelated parallel machine environment, dynamic job arrival, general precedence constraint, and job re-circulation. These characteristics lead to longer mean and variance of cycle times. ▓ In order to manage cycle time, we developed a framework considering the factors that can be used to control production line status such as WIP, availability, utilization, etc, and analyzed the impacts of these factors on the production cycle time.",2011-11-28,https://www.semanticscholar.org/paper/be6c6e7a2f0ed7e0fb48c0ea0b1b2de89d34ec43,2011 e-Manufacturing & Design Collaboration Symposium & International Symposium on Semiconductor Manufacturing (eMDC & ISSM)
1365,Measurement of the ratio of B+ and B0 meson lifetimes.,"The ratio of the B+ and B0 meson lifetimes was measured using data collected in 2002-2004 by the D0 experiment in Run II of the Fermilab Tevatron Collider. These mesons were reconstructed in B-->mu(+)nuD(*-)X decays, which are dominated by B0 and B-->mu(+)nuD 0X decays, which are dominated by B+. The ratio of lifetimes is measured to be tau(+)/tau(0)=1.080+/-0.016(stat)+/-0.014(syst).",2004-10-19,https://www.semanticscholar.org/paper/33b236bdf7e68a49bb39ee46b473a93e823343cb,Physical Review Letters
3528,Improved approximation algorithms for shop scheduling problems,"In the job shop scheduling problem, there are $m$ machines and $n$ jobs. A job consists of a sequence of operations, each of which must be processed on a specified machine, and the aim is to complete all jobs as quickly as possible. This problem is strongly ${\cal NP}$-hard even for very restrictive special cases. The authors give the first randomized and deterministic polynomial-time algorithms that yield polylogarithmic approximations to the optimal length schedule. These algorithms also extend to the more general case where a job is given not by a linear ordering of the machines on which it must be processed but by an arbitrary partial order. Comparable bounds can also be obtained when there are $m'$ types of machines, a specified number of machines of each type, and each operation must be processed on one of the machines of a specified type, as well as for the problem of scheduling unrelated parallel machines subject to chain precedence constraints.",1991-03-01,https://www.semanticscholar.org/paper/5e8f017cf9d6fdf1c3031d4af0b35e7266cf55a9,ACM-SIAM Symposium on Discrete Algorithms
469,Information caching for delivery of personalized video programs on home entertainment channels,"The synergy between computing and information systems promises to herald a new epoch in which users not only have access to mundane services, such as television, radio, and telephones, but they also have access to an entirely new variety of entertainment services that are automatically customized to suit their individual needs. The authors explore the architectural considerations that underlie the realization of personalized multimedia entertainment services over metropolitan-area broadband networks, and devise near-optimal information caching strategies to determine when, where, and for how long video programs must be cached, so as to minimize the cumulative storage and network costs borne by users.<<ETX>>",1994-05-15,https://www.semanticscholar.org/paper/752b50425a2b634e8d332e400c2e895342435262,1994 Proceedings of IEEE International Conference on Multimedia Computing and Systems
760,Efficient Qualitative Analysis of Classes of Recursive Markov Decision Processes and Simple Stochastic Games,,2006-02-23,https://www.semanticscholar.org/paper/0395bc208169356de77e58684246c34f90fd3119,Symposium on Theoretical Aspects of Computer Science
2167,Isolation of Microvesicles from Human Circulating Neutrophils.,"Neutrophil-derived microvesicles (NDMVs) are liberated by neutrophils upon cell activation by molecules. Once activated, neutrophils are primarily involved in acute inflammation; however, the microvesicles they produce are largely anti-inflammatory. NDMVs have been shown to protect cartilage during inflammatory arthritis. They exert these effects by inhibiting or affecting the function of target cells, including macrophages. NDMVs have the potential to act as disease-modifying agents, especially for inflammatory diseases. This protocol describes a method using differential centrifugation to separate neutrophils from whole human blood. Subsequently, neutrophils are identified by cytospin and Wright's staining, and then the NDMVs are isolated using differential centrifugation.",2021-10-05,https://www.semanticscholar.org/paper/a9919a713fabfb2b2bce09d76cb30d10eacbb258,Bio-protocol
1538,Rationales for Sequential Predictions,"Sequence models are a critical component of modern NLP systems, but their predictions are difficult to explain. We consider model explanations though rationales, subsets of context that can explain individual model predictions. We find sequential rationales by solving a combinatorial optimization: the best rationale is the smallest subset of input tokens that would predict the same output as the full sequence. Enumerating all subsets is intractable, so we propose an efficient greedy algorithm to approximate this objective. The algorithm, which is called greedy rationalization, applies to any model. For this approach to be effective, the model should form compatible conditional distributions when making predictions on incomplete subsets of the context. This condition can be enforced with a short fine-tuning step. We study greedy rationalization on language modeling and machine translation. Compared to existing baselines, greedy rationalization is best at optimizing the sequential objective and provides the most faithful rationales. On a new dataset of annotated sequential rationales, greedy rationales are most similar to human rationales.",2021-09-14,https://www.semanticscholar.org/paper/1c709eef701d933af1383c790c13209f06806b60,Conference on Empirical Methods in Natural Language Processing
665,"A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level","Significance We demonstrate that a neural network automatically solves, explains, and generates university-level problems from the largest Massachusetts Institute of Technology (MIT) mathematics courses at a human level. Our methods combine three innovations: 1) using recent neural networks pretrained on text and fine-tuned on code rather than pretrained on text; 2) few-shot learning synthesizing programs that correctly solve course problems automatically; and 3) a pipeline to solve questions, explain solutions, and generate new questions indistinguishable by students from course questions. Our work solves university-level mathematics courses and improves upon state-of-the-art, increasing automatic accuracy on randomly sampled questions on a benchmark by order of magnitude. Implications for higher education include roles of artificial intelligence (AI) in automated course evaluation and content generation.",2021-12-31,https://www.semanticscholar.org/paper/166b64f2ae8e52f5779682fab756cbd617a6e74b,Proceedings of the National Academy of Sciences of the United States of America
799,Inference of message sequence charts,"Software designers draw Message Sequence Charts for early modeling of the individual behaviors they expect from the concurrent system under design. Can they be sure that precisely the behaviors they have described are realizable by some implementation of the components of the concurrent system? If so, can one automatically synthesize concurrent state machines realizing the given MSCs? If, on the other hand, other unspecified and possibly unwanted scenarios are ""implied"" by their MSCs, can the software designer be automatically warned and provided the implied MSCs? In this paper we provide a framework in which all these questions are answered positively. We first describe the formal framework within which one can derive implied MSCs, and we then provide polynomial-time algorithms for implication, realizability, and synthesis. In particular, we describe a novel algorithm for checking deadlock-free (safe) realizability.",2000-06-01,https://www.semanticscholar.org/paper/b7797eaedd724c8f4ad5e26f1ab01c391785759c,Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium
2066,Optimize die size design to enhance owe for design for manufacturing,"To enhance competitive advantages, it is crucial for wafer fabs to reduce average die cost through productivity improvement via increasing the number of gross dies per wafer and throughput. However, gross die number is influenced by die size in design phase, while the existing size of integrated circuit die was designed without considering the effect on wafer throughput in fabrication phase. This research aims to develop a die size optimization algorithm based on decision tree to construct the rules between the number of gross dies per wafer, mask utilization, and the die feature including length, width, and area. Without losing generality, an empirical study has been done for validation by using transformed data from a fab in Taiwan.",2007-10-01,https://www.semanticscholar.org/paper/6d561e3b6aa64dad1eb239492d39b69bc700a11a,International Symposium on Semiconductor Manufacturing
59,Selectivity estimation for string predicates: overcoming the underestimation problem,"Queries with (equality or LIKE) selection predicates over string attributes are widely used in relational databases. However, state-of-the-art techniques for estimating selectivities of string predicates are often biased towards severely underestimating selectivities. We develop accurate selectivity estimators for string predicates that adapt to data and query characteristics, and which can exploit and build on a variety of existing estimators. A thorough experimental evaluation over real data sets demonstrates the resilience of our estimators to variations in both data and query characteristics.",2004-03-30,https://www.semanticscholar.org/paper/5b777468a9e4dceef957872dfc033e790e352dab,Proceedings / International Conference on Data Engineering
488,The Complexity of the Lin-Kernighan Heuristic for the Traveling Salesman Problem,"It is shown that finding a local optimum solution with respect to the Lin–Kernighan heuristic for the traveling salesman problem is PLS-complete, and thus as hard as any local search problem.",1992-06-01,https://www.semanticscholar.org/paper/4a75769dd195e97de23ae05f6c60a5eb4c8a51a0,SIAM journal on computing (Print)
1286,Measurement of the Lambda b lifetime in the exclusive decay Lambda b --> J/psi Lambda.,"We have measured the Lambda b lifetime using the exclusive decay Lambda b --> J/psi Lambda, based on 1.2 fb(-1) of data collected with the D0 detector during 2002-2006. From 171 reconstructed Lambda b decays, where the J/psi and Lambda are identified via the decays J/psi --> mu+ mu- and Lambda --> ppi, we measured the Lambda b lifetime to be tau(Lambda b)=1.218 (+0.130)/(-0.115) (stat) +/- 0.042(syst) ps. We also measured the B0 lifetime in the decay B0 --> J/psi(mu+ mu-)K(0)/(S)(pi+ pi-) to be tau(B0)=1.501 (+0.078)/(-0.074) (stat) +/- 0.050(syst) ps, yielding a lifetime ratio of tau(Lambda b)/tau(B0)=0.811 (+0.096)/(-0.087) (stat) +/- 0.034(syst).",2007-04-01,https://www.semanticscholar.org/paper/a88bb01caff9738506076c1f40a886394ea4a01a,Physical Review Letters
2238,Neutrophil TLR4 expression is reduced in the airways of infants with severe bronchiolitis,"Background: In respiratory syncytial virus (RSV) bronchiolitis, neutrophils account for >80% of cells recovered from the airways in bronchoalveolar lavage (BAL) fluid. This study investigated neutrophil activation and Toll-like receptor (TLR) expression in the blood and lungs of infants with severe RSV bronchiolitis. Methods: BAL fluid and (blood) samples were collected from 24 (16) preterm and 23 (15) term infants ventilated with RSV bronchiolitis, and 12 (8) control infants. Protein levels and mRNA expression of CD11b, myeloperoxidase (MPO) and TLRs 2, 4, 7, 8 and 9 were measured in neutrophils. Results: Blood neutrophils had more CD11b in preterm and term infants with RSV bronchiolitis than control infants (p<0.025) but similar amounts of MPO. BAL fluid neutrophils from infants with RSV bronchiolitis had greater amounts of CD11b and MPO than blood neutrophils and BAL fluid neutrophils from controls (p<0.01). Blood neutrophils from term infants with RSV bronchiolitis had less total TLR4 protein than preterm infants with RSV bronchiolitis (p = 0.005), and both had less than controls (p<0.04). Total TLR4 for each group was greater in BAL fluid neutrophils than in blood neutrophils. Blood neutrophils from preterm infants with RSV bronchiolitis had greater TLR4 mRNA expression than term infants with RSV bronchiolitis (p = 0.005) who had similar expression to controls (p = 0.625). Conclusions: In infants with severe RSV bronchiolitis, neutrophil activation starts in the blood and progresses as they are recruited into the airways. Total neutrophil TLR4 remains low in both compartments. TLR4 mRNA expression is unimpaired. This suggests that neutrophil TLR4 expression is deficient in these infants, which may explain why they develop severe RSV bronchiolitis.",2009-06-03,https://www.semanticscholar.org/paper/eadc04f5e385bf86f84e175b7662bddad9eef0e7,Thorax
3636,Why C++ is not just an object-oriented programming language,"C++ directly supports a variety of programming styles. In this, C ++ deliberately differs from languages designed to support a single way of writing programs. This paper briefly presents key programming styles directly supported by C++ and argues that the support for multiple styles is one of its major strengths. The styles presented include: traditional C-style, concrete classes, abstract classes, traditional class hierarchies, abstract classes and class hierarchies, and generic programming. To provide a context for this overview, I discuss criteria for a reasonable and useful definition of ‘‘object-oriented programming.’’",1995-10-01,https://www.semanticscholar.org/paper/fd8464521c5b6471ec9db25068a6513a6760c46a,OOPSLA Addendum
1531,Mapping interstellar dust with Gaussian processes,"Interstellar dust corrupts nearly every stellar observation, and accounting for it is crucial to measuring physical properties of stars. We model the dust distribution as a spatially varying latent field with a Gaussian process (GP) and develop a likelihood model and inference method that scales to millions of astronomical observations. Modeling interstellar dust is complicated by two factors. The first is integrated observations. The data come from a vantage point on Earth and each observation is an integral of the unobserved function along our line of sight, resulting in a complex likelihood and a more difficult inference problem than in classical GP inference. The second complication is scale; stellar catalogs have millions of observations. To address these challenges we develop ziggy, a scalable approach to GP inference with integrated observations based on stochastic variational inference. We study ziggy on synthetic data and the Ananke dataset, a high-fidelity mechanistic model of the Milky Way with millions of stars. ziggy reliably infers the spatial dust map with well-calibrated posterior uncertainties.",2022-02-14,https://www.semanticscholar.org/paper/cde5bd28b06c8d89af4c372bc74311ae44e4de78,Annals of Applied Statistics
2343,Sequential phospholipase activation in the stimulation of the neutrophil NADPH oxidase.,"Stimulation of human neutrophils with the chemotactic peptide fMet-Leu-Phe results in activation of a rapid, transient burst of oxidant secretion, which reaches a maximal rate by about 1 min after stimulation. This phase of oxidant secretion is then followed by intracellular oxidant production, which is detected by luminol chemiluminescence but not by assays such as cytochrome c reduction or scopoletin oxidation. The rapid phase of oxidant secretion requires increases in intracellular free Ca2+ and phospholipase A2 activity, but not the activities of phospholipase D or protein kinase C. In contrast, intracellular oxidant production requires the activities of phospholipase D and protein kinase C. A model is thus proposed suggesting the sequential activation of different phospholipases which activate oxidase molecules on the plasma membrane or else from the membranes of specific granules.",1992-12-01,https://www.semanticscholar.org/paper/e6ac3dae7a1750361d7845e8db4935c0dd39bcba,FEMS Microbiology Immunology
2410,Exploring the Educational Value and Impact of Vision-Impairment Simulations on Sympathy and Empathy with XREye,"To create a truly accessible and inclusive society, we need to take the more than 2.2 billion people with vision impairments worldwide into account when we design our cities, buildings, and everyday objects. This requires sympathy and empathy, as well as a certain level of understanding of the impact of vision impairments on perception. In this study, we explore the potential of an extended version of our vision-impairment simulation system XREye to increase sympathy and empathy and evaluate its educational value in an expert study with 56 educators and education students. We include data from a previous study in related work on sympathy and empathy as a baseline for comparison with our data. Our results show increased sympathy and empathy after experiencing XREye and positive feedback regarding its educational value. Hence, we believe that vision-impairment simulations, such as XREye, have merit to be used for educational purposes in order to increase awareness for the challenges people with vision impairments face in their everyday lives.",2023-07-06,https://www.semanticscholar.org/paper/a52407068e37bbac2703f8ceb1eb70ea84141505,Multimodal Technologies and Interaction
385,Game theory and mathematical economics: a theoretical computer scientist's introduction,"There has been recently increasing interaction between game theory and, more generally, economic theory, with theoretical computer science, mainly in the context of the Internet. The paper is an invitation to this important frontier.",2001-10-14,https://www.semanticscholar.org/paper/6016924e8dfff12fec172480a629e893f3091026,Proceedings IEEE International Conference on Cluster Computing
1183,Measurement of the angular and lifetime parameters of the decays Bd0-->J/psiK*0 and Bs0-->J/psiphi.,We present measurements of the linear polarization amplitudes and the strong relative phases that describe the flavor-untagged decays Bd0-->J/psiK*0 and Bs0-->J/psiphi in the transversity basis. We also measure the mean lifetime taus of the Bs0 mass eigenstates and the lifetime ratio taus/taud. The analyses are based on approximately 2.8 fb(-1) of data recorded with the D0 detector. From our measurements of the angular parameters we conclude that there is no evidence for a deviation from flavor SU(3) symmetry for these decays and that the factorization assumption is not valid for the Bd0-->J/psiK*0 decay.,,https://www.semanticscholar.org/paper/c77dde68f72af9d8f20a073aef6d69977ae4d62e,Physical Review Letters
2422,Scene Editing as Teleoperation: A Case Study in 6DoF Kit Assembly,"Studies in robot teleoperation have been centered around action specifications-from continuous joint control to discrete end-effector pose control. However, these “robot-centric” interfaces often require skilled operators with extensive robotics expertise. To make teleoperation accessible to nonexpert users, we propose the framework “Scene Editing as Teleoperation” (SEaT), where the key idea is to transform the traditional “robot-centric” interface into a “scene-centric” interface-instead of controlling the robot, users focus on specifying the task's goal by manipulating digital twins of the real-world objects. As a result, a user can perform teleoperation without any expert knowledge of the robot hardware. To achieve this goal, we utilize a category-agnostic scene-completion algorithm that translates the real-world workspace (with unknown objects) into a manipulable virtual scene representation and an action-snapping algorithm that refines the user input before generating the robot's action plan. To train the algorithms, we procedurely generated a large-scale, diverse kit-assembly dataset that contains object-kit pairs that mimic real-world object-kitting tasks. Our experiments in simulation and on a real-world system demonstrate that our framework improves both the efficiency and success rate for 6DoF kit-assembly tasks. A user study demonstrates that SEaT framework participants achieve a higher task success rate and report a lower subjective workload compared to an alternative robot-centric interface.",2021-10-09,https://www.semanticscholar.org/paper/ac93430356cf7656a0cd9134904c63e20cca3b77,IEEE/RJS International Conference on Intelligent RObots and Systems
3321,The Effect of Space‐Use Patterns of Reintroduced Asiatic Wild Ass on Effective Population Size,"Abstract: Empirical data on behavior, such as space‐use patterns, are important to the success of animal reintroductions. We studied space‐use patterns in a growing population of Asiatic wild ass (   Equus hemionus) reintroduced into the Ramon erosion cirque in the Negev desert, Israel. Between 1988 and 1995 we used direct observation to determine the location and association of males and females. All adult females and dominant males were individually recognized. Home ranges of dominant males overlapped little, suggesting that in this population males are territorial. After the first release of males and females into the wild, only one territory was established, and it covered most of the 20,000 ha of the cirque. After 6 years the number of male territories increased as the number of males in the population increased, and average territory size decreased. Male territories were near permanent and ephemeral water sources, but the water sources were at the peripheries of the territories and were not centers of activity. When there was only one territorial male, female home ranges were almost entirely within the territory. As male territory size decreased, so did the spatial association of females with a single male. During the breeding season, males spent more time in close association with female groups, adopting what may temporarily appear to be a harem breeding strategy. Although demographic and environmental factors pose a greater threat to small populations, our data support the hypothesis that in small, reintroduced populations of territorial, polygynous species, effective population size (  Ne  ) may be dangerously small. Our data suggest that this situation may last for several years until new males are recruited into the population. Thereafter, rapid male turnover and female use of several male territories may ameliorate this problem. We found no relationship between male turnover rate and female reproductive success. The establishment of more male territories is key to increasing Ne and should be the basis for planning reserves for territorial, polygynous species.",2000-12-01,https://www.semanticscholar.org/paper/09f45b6c3b170b4c959c157c6e0b6078f5468db7,Conservation Biology
2546,Rubbing and tapping for precise and rapid selection on touch-screen displays,"We introduce two families of techniques, rubbing and tapping, that use zooming to make possible precise interaction on passive touch screens, and describe examples of each. Rub-Pointing uses a diagonal rubbing gesture to integrate pointing and zooming in a single-handed technique. In contrast, Zoom-Tapping is a two-handed technique in which the dominant hand points, while the non-dominant hand taps to zoom, simulating multi-touch functionality on a single-touch display. Rub-Tapping is a hybrid technique that integrates rubbing with the dominant hand to point and zoom, and tapping with the non-dominant hand to confirm selection. We describe the results of a formal user study comparing these techniques with each other and with the well-known Take-Off and Zoom-Pointing selection techniques. Rub-Pointing and Zoom-Tapping had significantly fewer errors than Take-Off for small targets, and were significantly faster than Take-Off and Zoom-Pointing. We show how the techniques can be used for fluid interaction in an image viewer and in Google Maps.",2008-04-06,https://www.semanticscholar.org/paper/94c0f6e243534886331b4c2f3cbeafa9cc224cc2,International Conference on Human Factors in Computing Systems
2762,An Architecture for Knowledge-Based Graphical Interfaces,"The construction of intelligent interfaces can be greatly facilitated by classifying the information that users and programs communicate, and by separating the user interface from the functionality of a program. This paper presents a scheme for classifying this information, and show how, by structuring the interface around this scheme, the user interface and functionality of a program can be separated, and tools can be built that provide assistance to both users and developers of user interfaces.",1988-07-01,https://www.semanticscholar.org/paper/16e5b48f3b0d612643dbebaf1ab3181479a44824,SGCH
606,Covering Graphs by Simple Circuits,"We show that any biconnected graph with n nodes and m edges can be covered by simple circuits whose total length is at most $\min (3m,m + 6n)$. Our proof suggests an efficient algorithm for finding such a cover.",1981-11-01,https://www.semanticscholar.org/paper/8c79014f35a8f9d9ca14b8a65aab846902a6a0e4,SIAM journal on computing (Print)
226,On Simplex Pivoting Rules and Complexity Theory,,2014-04-12,https://www.semanticscholar.org/paper/34d71cd53c7c615454b6cf46ac3d5eb58b330453,Conference on Integer Programming and Combinatorial Optimization
948,Hemisphere opposite to central retinal vascular trunk deviation is earlier affected by glaucomatous damage in primary angle‐closure glaucoma,"The purpose of the study was to investigate whether the position of the central retinal vascular trunk (CRVT), as a surrogate for lamina cribrosa (LC) offset, is associated with the dominant hemisphere of visual field defect in primary angle‐closure glaucoma (PACG) eyes.",2022-09-22,https://www.semanticscholar.org/paper/33d6f6085b72312db8b5753bfd0831b7ade302d6,Acta ophthalmologica
2420,Augmented Reality Guidance for Configuring an Anesthesia Machine to Serve as a Ventilator for COVID-19 Patients,"When there is a shortage of ventilators in a hospital, an anesthesia machine can be used as a ventilator. However, using an anesthesia machine as a ventilator requires that it be set up in a way that would not be familiar to medical personnel who normally work with ventilators. To teach medical staff how to do this, we developed a smartphone augmented reality app that allows a user to interact with a life-size virtual anesthesia machine, and leads them through the necessary steps. This makes it possible for the user to practice the setup procedures in a way that preserves the 3D spatial layout of the tasks without requiring access to the physical machine.",2021-03-01,https://www.semanticscholar.org/paper/824adff7b0c0a2f1aa204ef5f5fb7a4c9c0abcf3,2021 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)
2552,Visual Hints for Tangible Gestures in Augmented Reality,"Tangible augmented reality (AR) systems imbue physical objects with the ability to act and respond in new ways. In particular, physical objects and gestures made with them gain meaning that does not exist outside the tangible AR environment. The existence of this new set of possible actions and outcomes is not always apparent, making it necessary to learn new movements or gestures. Addressing this opportunity, we present visual hints, which are graphical representations in AR of potential actions and their consequences in the augmented physical world. Visual hints enable discovery, learning, and completion of gestures and manipulation in tangible AR. Here, we discuss our investigation of a variety of representations of visual hints and methods for activating them. We then describe a specific implementation that supports gestures developed for a tangible AR user interface to an electronic field guide for botanists, and present results from a pilot study.",2007-11-13,https://www.semanticscholar.org/paper/0ff3f23b5ce9630b25e5066c159c7154919b6464,2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality
901,Querying weak instances,"1. INTRODUCTION The universal relation model gives the user a view of the data as though it was stored in a single relation, in which every attribute plays a unique role. Thus, in posing queries, the user does not have to navigate among the different base relations; the system performs this navigation automatically by transforming the query on the universal relation into one involving the stored base relations. For example, in a films database with relations FP (Film-Producer) and FD (Film-Diiwtor), ln response to the query rcrrieve P where D-FelJmi (which producers has Fellmi worked for), the system would "" know "" that diiectors are related to produc-ers' through films. Thus, the relation between producers and directors would be constructed by taking the join of relations FP and FD and projecting on the attributes P, D. The fust approach towards defmlng the relationship between the actual database (the base relations) and the universal relation is known as the pure universal instance assumption. This assumption postulates that the database is formed by projecting some universal relation (satisfying the dependencies), and the universal relation is formed by taking the join of all the base relations. This assumption places very severe restrictions on the database, since incomplete information is not allowed-each tuple of a base relation must match with tuples in all the other relations. One could allow dao-glmg tuples (tuples that don't match), but then in taking the joii of all the relations these tuples would get lost. For example, suppose that in the films database we have also a relation FA (Film-Actor). Then, if we take the join of all three relations FP, FD, FA , we loose the relationship between directors of documentaries and their",1984-04-02,https://www.semanticscholar.org/paper/6faa1e2ba9b70392faff6e3baa7dba2603bedd3b,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems
1558,Double Empirical Bayes Testing,"Analysing data from large‐scale, multiexperiment studies requires scientists to both analyse each experiment and to assess the results as a whole. In this article, we develop double empirical Bayes testing (DEBT), an empirical Bayes method for analysing multiexperiment studies when many covariates are gathered per experiment. DEBT is a two‐stage method: in the first stage, it reports which experiments yielded significant outcomes and in the second stage, it hypothesises which covariates drive the experimental significance. In both of its stages, DEBT builds on the work of Efron, who laid out an elegant empirical Bayes approach to testing. DEBT enhances this framework by learning a series of black box predictive models to boost power and control the false discovery rate. In Stage 1, it uses a deep neural network prior to report which experiments yielded significant outcomes. In Stage 2, it uses an empirical Bayes version of the knockoff filter to select covariates that have significant predictive power of Stage 1 significance. In both simulated and real data, DEBT increases the proportion of discovered significant outcomes and selects more features when signals are weak. In a real study of cancer cell lines, DEBT selects a robust set of biologically plausible genomic drivers of drug sensitivity and resistance in cancer.",2020-11-25,https://www.semanticscholar.org/paper/552facf726bd0ee7c4a54632d2ce36c2cf67efe1,International statistical review = Revue internationale de statistique
2312,Neutrophils from preterm neonates and adults show similar cell surface receptor expression: analysis using a whole blood assay.,"Previous work has shown that Fc gamma RIII expression in isolated neonate neutrophils is defective. We have re-examined this phenomenon in view of the facts that (1) the receptor is present on mobilisable subcellular stores and (2) commonly used isolation procedures can affect receptor expression in suspensions of isolated neutrophils. Receptor expression was measured by fluorescence-activated cell sorter analysis of neutrophils in unfractionated whole blood. Examination of receptor expression in preterm, term and adult neutrophils indicated small but significantly decreased expression of CR1 and CR3 in preterm neutrophils compared with term neutrophils (p < 0.01). A small decrease in expression was found for Fc gamma RI and Fc gamma RIII (p < 0.05). No significant difference in expression of Fc gamma RII was observed in all groups analysed. These data suggest that isolated preterm neonate neutrophils have greatly decreased expression of Fc gamma RIII because of impaired composition or mobilisation of the subcellular stores of this receptor and/or increased lability of the surface receptor which leads to its shedding during purification.",,https://www.semanticscholar.org/paper/641d6dabbecfbcffe0cbc2a48fa6b22d8390cfa3,Biology of the Neonate
2740,Automating the generation of coordinated multimedia explanations,"An explanation system developed to overcome the disadvantages of conventional authoring in multimedia applications is presented. This experimental test bed for the automated generation of multimedia explanations is called COMET (coordinated multimedia explanation testbed), and has as its goal the coordinated, interactive generation of explanations that combine text and three-dimensional graphics, all of which is generated on the fly. In response to a user request for an explanation, COMET dynamically determines the explanation's content using constraints based on the type of request, the information available in a set of underlying knowledge bases, and information about the user's background and goals. Having determined what to say, COMET also determines how to express it at the time of generation. A brief overview of COMET's domain and architecture is followed by a description of the specific ways in which COMET can coordinate its text and graphics. The current status and limitations of COMET are discussed.<<ETX>>",1991-10-01,https://www.semanticscholar.org/paper/6ad11caa1f61fb19563f55cf84d20625523bc828,Computer
1524,Variational Inference for Infinitely Deep Neural Networks,"We introduce the unbounded depth neural network (UDN), an infinitely deep probabilistic model that adapts its complexity to the training data. The UDN contains an infinite sequence of hidden layers and places an unbounded prior on a truncation L, the layer from which it produces its data. Given a dataset of observations, the posterior UDN provides a conditional distribution of both the parameters of the infinite neural network and its truncation. We develop a novel variational inference algorithm to approximate this posterior, optimizing a distribution of the neural network weights and of the truncation depth L, and without any upper limit on L. To this end, the variational family has a special structure: it models neural network weights of arbitrary depth, and it dynamically creates or removes free variational parameters as its distribution of the truncation is optimized. (Unlike heuristic approaches to model search, it is solely through gradient-based optimization that this algorithm explores the space of truncations.) We study the UDN on real and synthetic data. We find that the UDN adapts its posterior depth to the dataset complexity; it outperforms standard neural networks of similar computational complexity; and it outperforms other approaches to infinite-depth neural networks.",2022-09-21,https://www.semanticscholar.org/paper/6eac963198c68e8e54d3f95c7dc87dedd4c34ea0,International Conference on Machine Learning
1909,Modelling and analysis of semiconductor supply chains,"Supply chain management (SCM) problems have existed in the semiconductor industry since the early days of the industry, but have become increasingly important in the last decade (Chien et al. 2011)...",2018-04-25,https://www.semanticscholar.org/paper/76a0d6d93823d2bf003e056c11e7c34c004aeab5,International Journal of Production Research
2081,Data mining for improving the solder bumping process in the semiconductor packaging industry,"Modern semiconductor manufacturing is very complex and expensive. Maintaining high quality and yield enhancement have been recognized as important factors to build core competences for semiconductor manufacturing companies. Data mining can find potentially useful information from huge databases. This paper proposes a data-mining framework based on decision-tree induction for improving the yield of the solder bumping process in which the various (physical and chemical) input variables that affect the bumping process exhibit highly complex interactions. We conducted an empirical study in a semiconductor fabrication facility in Taiwan to validate this approach. The results show that the proposed approach can effectively derive the causal relationships among controllable input process factors and the target class to enhance the yield. Copyright © 2007 John Wiley & Sons, Ltd.",,https://www.semanticscholar.org/paper/e720dc803b3c37f973bb6d06f167c89742a09e01,Intell. Syst. Account. Finance Manag.
2370,Oxygen-dependent killing of Staphylococcus aureus by human neutrophils.,Luminol-dependent chemiluminescence was used as a monitor of reactive oxidant generation during phagocytosis of Staphylococcus aureus by human neutrophils. Reactive oxidants play a crucial role in the killing of this organism because: (a) S. aureus was killed most rapidly when the rate of increase of chemiluminescence was greatest; (b) neutrophils which had been activated to generate reactive oxidants by re-aeration of anaerobic suspensions killed this bacterium more efficiently than control suspensions; and (c) neutrophils from a patient with chronic granulomatous disease could neither generate reactive oxidants nor kill S. aureus.,1987-12-01,https://www.semanticscholar.org/paper/2e288fc22119228b81c8d08173c748a19229b483,Journal of General Microbiology
640,On the complexity of edge traversing,"It is shown that the Chinese Postman Problem, although tractable in the totally directed and the totally undirected cases, is NP-complete in the mixed case. A simpler version of the same problem is shown algorithmically equivalent to the max-flow problem with unit edge capacities.",1976-07-01,https://www.semanticscholar.org/paper/ca0442ff09eab30ceb3b883da3cd45fbe902f719,Journal of the ACM
2587,Immersive mixed-reality configuration of hybrid user interfaces,"Information in hybrid user interfaces can be spread over a variety of different, but complementary, displays, with which users interact through a potentially equally varied range of interaction devices. Since the exact configuration of these displays and devices may not be known in advance, it is desirable for users to be able to reconfigure at runtime the dataflow between interaction devices and objects on the displays. To make this possible, we present the design and implementation of a prototype mixed reality system that allows users to immersively reconfigure a running hybrid user interface.",2005-10-05,https://www.semanticscholar.org/paper/b32c07456c748aa7e234e48b04cff58b1ce8e2b0,International Symposium on Mixed and Augmented Reality
2806,The roles of Galectin-3 in autoimmunity and tumor progression,,2012-03-15,https://www.semanticscholar.org/paper/6b5941ff590d02abd6ca8d90e1bfd6c67f9852a1,Immunologic research
3256,Lemurs groom-at-a-distance through vocal networks,,2015-12-01,https://www.semanticscholar.org/paper/4e0292248163d0cad3273208f0d1730b75941ea2,Animal Behaviour
3313,The Impact of Increased Environmental Stochasticity Due to Climate Change on the Dynamics of Asiatic Wild Ass,"Abstract:  Theory proposes that increased environmental stochasticity negatively impacts population viability. Thus, in addition to the directional changes predicted for weather parameters under global climate change (GCC), the increase in variance of these parameters may also have a negative effect on biodiversity. As a case study, we assessed the impact of interannual variance in precipitation on the viability of an Asiatic wild ass (Equus hemionus) population reintroduced in Makhtesh Ramon Nature Reserve, Israel. We monitored the population from 1985 to 1999 to determine what environmental factors affect reproductive success. Annual precipitation during the year before conception, drought conditions during gestation, and population size determined reproductive success. We used the parameters derived from this model to assess population performance under various scenarios in a Leslie matrix type model with demographic and environmental stochasticity. Specifically, we used a change in the precipitation regime in our study area to formulate a GCC scenario and compared the simulated dynamics of the population with a no‐change scenario. The coefficient of variation in population size under the global change scenario was 30% higher than under the no‐change scenario. Minor die‐offs (≥15%) following droughts increased extinction probability nearly 10‐fold. Our results support the idea that an increase in environmental stochasticity due to GCC may, in itself, pose a significant threat to biodiversity.",2006-10-01,https://www.semanticscholar.org/paper/e2f19cc8a134a717e16af97a458b988650b8b2cd,Conservation Biology
3456,Approximation Algorithms for Semidefinite Packing Problems with Applications to Maxcut and Graph Coloring,,2005-06-08,https://www.semanticscholar.org/paper/86e52ce64ec6e929573c5a2c252cc54bcfde51da,Conference on Integer Programming and Combinatorial Optimization
3069,Guest Editors' Introduction: Virtual Machines,A system virtual machine is a software implementation of a real computer that can execute unmodified applications and an operating system. This issue contains articles and interviews that explore virtual machine use in pervasive computing.,2009-10-01,https://www.semanticscholar.org/paper/1aa77b746e2b7dbb128bffe564ebb02e49b9ce5b,IEEE pervasive computing
2692,Adding insight through animation in augmented reality,"Most of the virtual world systems that have been so well publicized over the past ten years use opaque head mounted displays that block off the wearer from the surrounding real world, effectively immersing her within a synthesized environment. These systems hold tremendous promise for certain applications ranging from fantasy games to scientific research. In contrast, we believe that the most powerful and commonplace virtual worlds of the future will not replace the real world, but will rather augment it with additional information. This approach is called augmented reality and was pioneered by Ivan Sutherland, who, over a quarter century ago, developed the first see-through head mounted display (I. Sutherland, 1968). When completed, his system presented graphics to the user on a pair of stereo displays, worn on the user's head. The image produced by the displays was combined with the user's view of the world using mirror beam splitters. A 3D tracking system determined the position and orientation of the user's head. This enabled the system to change the view, based on the direction in which the wearer was looking.",1996-06-03,https://www.semanticscholar.org/paper/35f8ffb33d48224df524ad289527fcb2b320f206,Proceedings Computer Animation '96
2573,Evaluation of an Eyes-Free Cursorless Numeric Entry System for Wearable Computers,"We report on the results of a user study to investigate the utility of passive haptics for eyes-free numeric entry. This work targets cursorless user interfaces designed for use with a watch-sized wrist-worn computer. Our study compared three approaches for selecting one of a set of eight numeric parameters and entering its value, both with and without visual feedback. The three selection methods utilized physical buttons alone, buttons with a touch-sensor utilizing passive haptics, and the touch sensor with passive haptics alone. The results show that passive haptics allowed users to perform parameter selection and number entry tasks, with statistically insignificant differences in accuracy and speed when used with and without visual feedback. Furthermore, there was no statistically significant difference in accuracy and speed between the button-based methods and the purely touch-sensor-based approaches.",2006-10-01,https://www.semanticscholar.org/paper/82e39caed1672b5bd0c58395337934d5467a3d55,International Semantic Web Conference
3469,Algorithm Engineering and Experiments,,,https://www.semanticscholar.org/paper/ce8a91f2506b87ecfb40abe9fc78c4d65302cf7d,Lecture Notes in Computer Science
2166,An improved culture protocol for the differentiation and survival of human promyelocytic leukaemia PLB-985 cell-line into mature neutrophil-like granulocytes,"Circulating blood neutrophils are short-lived, lack proliferation capacity and cannot be transfected in vitro to express exogenous genes or proteins. These properties have made the ex vivo genetic manipulation of neutrophils challenging and hindered biochemical and molecular studies investigating the function of specific genes and proteins. Improved methodology for differentiating cell lines into mature neutrophil-like phenotypes, with similar morphological and functional properties to blood neutrophils would, therefore, be an important tool to probe the molecular properties of mature cells. The PLB-985 cell line was cultured in RPMI-1640 medium supplemented foetal calf serum (FCS) and penicillin/streptomycin. For induction of differentiation into neutrophil-like cells, the medium was supplemented with sodium pyruvate, N, N-dimethylformamide (DMF) and all-trans retinoic acid (ATRA), FCS and penicillin/streptomycin. The cytokines G-CSF and GM-CSF were used to enhance differentiation, prolong viability and delay the progression of the differentiated cells into apoptosis. The modified culture protocol and conditions induced PLB-985 cells to differentiate into mature, neutrophil-like granulocytes that resembled the morphology of mature blood neutrophils as evident by acquisition of a multi-lobed nucleus and granulated cytoplasm. These modified culture conditions resulted in enhanced differentiation into neutrophil-like cells and the apoptosis of these differentiated cells was delayed by supplementation with cytokines. This experimental system should be useful for studies probing the function of specific genes and proteins in human neutrophils.",2021-09-15,https://www.semanticscholar.org/paper/a2f93ffd8e961611d07328d25bc14dfc3cb66d65,bioRxiv
167,Brain Computation: A Computer Science Perspective,,2019-10-05,https://www.semanticscholar.org/paper/2086dc4dca34965c944ca4833230d2e0b5b88ef0,Computing and Software Science
2979,Dichotomous cellular properties of mouse orexin/hypocretin neurons,"Non‐technical summary  Orexin/hypocretin neurons are widely projecting, ‘multi‐tasking’ brain cells that promote alertness, reward seeking and feeding. They are vital for stable consciousness in higher mammals. Loss of orexin/hypocretin cells produces narcolepsy. It was originally assumed that orexin/hypocretin neurons are one uniform population of cells, but recent studies hinted that they may be split into subsystems. To explore this, we performed unbiased statistical analysis of electrical properties of orexin/hypocretin cells in combination with 3‐D analysis of their shape. Our results pointed to an existence of two subgroups of orexin/hypocretin neurons, that have unique ‘electrical fingerprints’ and distinct ways of receiving information from other neurons.",2011-06-01,https://www.semanticscholar.org/paper/3298bf1af03f90ab95266e6f516c43e9cb2ccbc8,Journal of Physiology
1864,Topic segmentation with an aspect hidden Markov model,"We present a novel probabilistic method for topic segmentation on unstructured text. One previous approach to this problem utilizes the hidden Markov model (HMM) method for probabilistically modeling sequence data [7]. The HMM treats a document as mutually independent sets of words generated by a latent topic variable in a time series. We extend this idea by embedding Hofmann's aspect model for text [5] into the segmenting HMM to form an aspect HMM (AHMM). In doing so, we provide an intuitive topical dependency between words and a cohesive segmentation model. We apply this method to segment unbroken streams of New York Times articles as well as noisy transcripts of radio programs on SpeechBot, an online audio archive indexed by an automatic speech recognition engine. We provide experimental comparisons which show that the AHMM outperforms the HMM for this task.",2001-09-01,https://www.semanticscholar.org/paper/845c20b482765922c67d3ee0ae650c069d7013b0,Annual International ACM SIGIR Conference on Research and Development in Information Retrieval
2490,Session details: Games: community + communication,,2012-05-05,https://www.semanticscholar.org/paper/0431a1e58e44b063772389a51d85f301abed54a6,International Conference on Human Factors in Computing Systems
1547,The Posterior Predictive Null,". Bayesian model criticism is an important part of the practice of Bayesian statistics. Traditionally, model criticism methods have been based on the predictive check, an adaptation of goodness-of-ﬁt testing to Bayesian modeling and an eﬀective method to understand how well a model captures the distribution of the data. In modern practice, however, researchers iteratively build and develop many models, exploring a space of models to help solve the problem at hand. While classical predictive checks can help assess each one, they cannot help the researcher understand how the models relate to each other. This paper introduces the posterior predictive null check (PPN), a method for Bayesian model criticism that helps characterize the relationships between models. The idea behind the PPN is to check whether data from one model’s predictive distribution can pass a predictive check designed for another model. This form of criticism complements the classical predictive check by providing a comparative tool. A collection of PPNs, which we call a PPN study, can help us understand which models are equivalent and which models provide diﬀerent perspectives on the data. With mixture models, we demonstrate how a PPN study, along with traditional predictive checks, can help select the number of components by the principle of parsimony. With probabilistic factor models, we demonstrate how a PPN study can help understand relationships between diﬀerent classes of models, such as linear models and models based on neural networks. Finally, we analyze data from the literature on predictive checks to show how a PPN study can improve the practice of Bayesian model criticism. Code to replicate the results in this paper is available at https://github.com/gemoran/ppn-code .",2021-12-06,https://www.semanticscholar.org/paper/ab830a6e9fd3319e75e3b871a6e2f0f0a9168751,Bayesian Analysis
2213,DcR3 Mutations in Patients with Juvenile-onset Systemic Lupus Erythematosus Lead to Enhanced Lymphocyte Proliferation,"Objective. Previous studies suggested a role for the death decoy receptor 3 (DcR3) in the pathogenesis of adult systemic lupus erythematosus (SLE). We investigated the role of DcR3 in juvenile-onset SLE, to identify polymorphisms that might alter the function of this protein. Methods. DcR3 was measured in the serum of 61 patients with juvenile SLE. The coding region of the DcR3 gene was sequenced in 100 juvenile and 103 adult patients with SLE, together with 500 healthy controls. Results. DcR3 was elevated in the serum of juvenile patients with active SLE disease (440.8 ± 169.1 pg/ml), compared to patients with inactive disease (122.6 ± 28.05 pg/ml; p = 0.0014) and controls (69.27 ± 20.23 pg/ml; p = 0.0009). DNA sequencing identified 2 novel missense mutations: c.C167T (p.T56I) in an adult SLE patient and c.C364T (p.H122Y) in a juvenile patient. Recombinant proteins containing these mutations exhibited altered binding kinetics to FasL and they significantly increased lymphocyte proliferation, compared to the wild-type protein (p < 0.05). The adult patient with SLE carrying the p.T56I mutation had significantly increased lymphocyte proliferation compared to 3 SLE controls matched for age, sex, and disease severity. Conclusion. DcR3 may play an etiologic role in SLE through either elevated serum levels of wild-type DcR3 or normal levels of gain-of-function DcR3 proteins that increase lymphocyte proliferation.",2013-08-01,https://www.semanticscholar.org/paper/1c20f7846314b2e131ec26dc62364f9d6f177c86,Journal of Rheumatology
3206,The non-invasive measurement of faecal immunoglobulin in African equids,,2020-05-18,https://www.semanticscholar.org/paper/3989fbbb1d30150cbc55ed211bd0f16bb956a2e9,International Journal for Parasitology: Parasites and Wildlife
2857,Functional Capacity of Macrophages Determines the Induction of Type 1 Diabetes,"Abstract:  Macrophages are potent immune regulators and are critical in the development and pathogenesis of autoimmune diabetes. They are said to be the first cell type to infiltrate the pancreatic islet, serve as antigen‐presenting cells, and are important as effector cells during diabetogenesis. The article examines the role of macrophages in autoimmune diabetes with particular emphasis on the role of galectin‐3, a β‐galactoside‐binding lectin, and T1/ST2, an IL‐1 receptor‐like protein, both of which play significant roles in the immunomodulatory functions of macrophages. Multiple low‐dose streptozotocin (MLD‐STZ) induces infiltration of mononuclear cells in the islets of susceptible strains leading to insulitis. Deletion of the galectin‐3 gene from C57BL/6 mice significantly attenuates this effect as evaluated by quantitative histology of mononuclear cells and loss of insulin‐producing β cells. In contrast, deletion of the ST2 gene enhanced insulitis after MLD‐STZ treatment when compared with relatively resistant wild‐type BALB/c mice. Thus, it appears that functional capacity of macrophages influences their participation in T helper (Th) 1‐mediated autoimmunity and the development of autoimmune diabetogenesis.",2006-11-01,https://www.semanticscholar.org/paper/8ab494ff928151b629377413534eb3cf8973553c,Annals of the New York Academy of Sciences
2150,On Lower Bounds for the Capacity of Deletion Channels,"This correspondence considers binary deletion channels, where bits are deleted independently with probability d; it improves upon the framework used to analyze the capacity of binary deletion channels established by Diggavi and Grossglauser, improving on their lower bounds. Diggavi and Grossglauser considered codebooks with codewords generated by a first-order Markov chain. They only consider typical outputs, where an output is typical if an N bit input gives an N(1-d)(1-epsi) bit output. The improvements in this correspondence arise from two considerations. First, a stronger notion of a typical output from the channel is used, which yields better bounds even for the codebooks studied by Diggavi and Grossglauser. Second, codewords generated by more general processes than first-order Markov chains are considered",2006-10-01,https://www.semanticscholar.org/paper/9845e84f4f9785ee898343b96204d0a255eb1762,IEEE Transactions on Information Theory
2357,Role of myeloperoxidase in the killing of Staphylococcus aureus by human neutrophils: studies with the myeloperoxidase inhibitor salicylhydroxamic acid.,"We have used salicylhydroxamic acid (SHAM) to inhibit intraphagosomal myeloperoxidase activity in order to evaluate the role of this enzyme in the killing of Staphylococcus aureus by human neutrophils. 50 microM-SHAM reduced the luminol-dependent chemiluminescence response stimulated during phagocytosis of unopsonized latex beads and opsonized S. aureus by over 80% and 60%, respectively. When opsonized S. aureus were incubated with neutrophils, 45% were killed within 15 min incubation and 60% by 1 h. However, in neutrophil suspensions incubated with 50 microM-SHAM, only 13% were killed by 15 min whilst 71% still remained viable after 1 h. This inhibitor had no effect upon the number of bacteria phagocytosed or upon degranulation. In a cell-free system, 2.5 microM-H2O2 alone killed 55% of the bacteria, whereas in the presence of myeloperoxidase (i.e. 10 mU myeloperoxidase and 2.5 microM-H2O2) virtually all of the bacteria were killed: the addition of 50 microM-SHAM abolished this myeloperoxidase-enhanced killing but did not affect the H2O2-dependent killing. We therefore conclude that in normal neutrophils whilst H2O2 is required for killing of this pathogen, both myeloperoxidase-dependent and -independent pathways exist.",1989-05-01,https://www.semanticscholar.org/paper/63ec0a0fa9d7c7b85b53004e8043be7d8f4e3df9,Journal of General Microbiology
3557,Design of Concept Libraries for C++,,2011-07-03,https://www.semanticscholar.org/paper/650f103dc99e3907028af458051a5dd241b58aab,Software Language Engineering
1813,Data-Driven Recomposition using the Hierarchical Dirichlet Process Hidden Markov Model,"Hidden Markov Models (HMMs) have been widely used in various audio analysis tasks such as speech recognition and genre classification. In this paper we show how HMMs can be used to synthesize new audio clips of unlimited length inspired by the temporal structure and perceptual content of a training recording or set of such recordings. We use Markov chain techniques similar to those that have long been used to generate symbolic data such as text and musical scores to instead generate sequences of continuous audio feature data that can then be transformed into audio using feature-based and concatenative synthesis techniques. Additionally, we explore the use of the Hierarchical Dirichlet Process HMM (HDP-HMM) for music, which sidesteps some difficulties with traditional HMMs, and extend the HDP-HMM to allow multiple song models to be trained simultaneously in a way that allows the blending of different models to produce output that is a hybrid of multiple input recordings.",2009-10-01,https://www.semanticscholar.org/paper/e651bf5fbf5aa2ca7fc3c7ce05d7c02f6fed1509,International Conference on Mathematics and Computing
2436,Hands-Free Interaction for Augmented Reality in Vascular Interventions,"Vascular interventions are minimally invasive surgical procedures in which a physician navigates a catheter through a patient's vasculature to a desired destination in the patient's body. Since perception of relevant patient anatomy is limited in procedures of this sort, virtual reality and augmented reality systems have been developed to assist in 3D navigation. These systems often require user interaction, yet both of the physician's hands may already be busy performing the procedure. To address this need, we demonstrate hands-free interaction techniques that use voice and head tracking to allow the physician to interact with 3D virtual content on a head-worn display while making both hands available intraoperatively. Our approach supports rotation and scaling of 3D anatomical models that appear to reside in the surrounding environment through small head rotations using first-order control, and rigid body transformation of those models using zero-order control. This allows the physician to easily manipulate a model while it stays close to the center of their field of view.",2018-03-01,https://www.semanticscholar.org/paper/78f728321e5547a472356deb49f94fda0bab6ad6,IEEE Conference on Virtual Reality and 3D User Interfaces
3197,Stewardship of global collective behavior,"Collective behavior provides a framework for understanding how the actions and properties of groups emerge from the way individuals generate and share information. In humans, information flows were initially shaped by natural selection yet are increasingly structured by emerging communication technologies. Our larger, more complex social networks now transfer high-fidelity information over vast distances at low cost. The digital age and the rise of social media have accelerated changes to our social systems, with poorly understood functional consequences. This gap in our knowledge represents a principal challenge to scientific progress, democracy, and actions to address global crises. We argue that the study of collective behavior must rise to a “crisis discipline” just as medicine, conservation, and climate science have, with a focus on providing actionable insight to policymakers and regulators for the stewardship of social systems.",2021-06-21,https://www.semanticscholar.org/paper/ce5f44e3e62c3223629792fdd5c1e6c80deaac48,Proceedings of the National Academy of Sciences of the United States of America
277,Algorithmic Game Theory: A Snapshot,,2009-07-06,https://www.semanticscholar.org/paper/9339a426c2a437d441a96744e5a9879d0099e22d,"International Colloquium on Automata, Languages and Programming"
2228,Respiratory Syncytial Virus Binds and Undergoes Transcription in Neutrophils From the Blood and Airways of Infants With Severe Bronchiolitis,"Background. Neutrophils are the predominant cell in the lung inflammatory infiltrate of infants with respiratory syncytial virus (RSV) bronchiolitis. Although it has previously been shown that neutrophils from both blood and bronchoalveolar lavage (BAL) are activated, little is understood about their role in response to RSV infection. This study investigated whether RSV proteins and mRNA are present in neutrophils from blood and BAL of infected infants. Methods. We obtained blood and BAL samples from 20 infants with severe RSV bronchiolitis and 8 healthy control infants. Neutrophil RSV F, G, and N proteins, RSV N genomic RNA, and messenger RNA (mRNA) were quantified. Results. RSV proteins were found in BAL and blood neutrophils in infants with RSV disease but not in neutrophils from healthy infants. BAL and blood neutrophils from infants with RSV disease, but not those from healthy infants, expressed RSV N genomic RNA, indicating uptake of whole virus; 17 of 20 BAL and 8 of 9 blood neutrophils from patients expressed RSV N mRNA. Conclusions. This work shows, for the first time, the presence of RSV proteins and mRNA transcripts within BAL and blood neutrophils from infants with severe RSV bronchiolitis.",2011-08-01,https://www.semanticscholar.org/paper/46af2674c6b5f4e0a9c7d4374e544085f31a16ff,Journal of Infectious Diseases
1005,Anterior Chamber Configuration Changes after Cataract Surgery in Eyes with Glaucoma,"Purpose To evaluate changes in anterior chamber depth (ACD) and angle width induced by phacoemulsification and intraocular lens (IOL) implantation in eyes with glaucoma, using anterior segment optical coherence tomography (AS-OCT). Methods Eleven eyes of 11 patients with angle-closure glaucoma (ACG) and 12 eyes of 12 patients with open-angle glaucoma (OAG) underwent phacoemulsification and IOL implantation. Using AS-OCT, ACD and angle parameters were measured before and 2 days after surgery. Change in intraocular pressure (IOP) and number of ocular hypotensive drugs were evaluated. Results After surgery, central ACD and angle parameters increased significantly in eyes with glaucoma (p < 0.05). Prior to surgery, mean central ACD in the ACG group was approximately 1.0 mm smaller than that in the OAG group (p < 0.001). Post surgery, mean ACD of the ACG group was still significantly smaller than that of the OAG group. No significant differences were found in angle parameters between the ACG and OAG groups. In the ACG group, postoperative IOP at the final visit was significantly lower than preoperative IOP (p = 0.018) and there was no significant change in the number of ocular hypotensive medications used, although clinically, patients required fewer medications. In the OAG group, the IOP and number of ocular hypotensive drugs were almost unchanged after surgery. Conclusions The ACD and angle width in eyes with glaucoma increased significantly after phacoemulsification and IOL implantation. Postoperative ACD significantly differed between the ACG and OAG groups, whereas angle parameters did not differ.",2012-03-22,https://www.semanticscholar.org/paper/9ff843f4c389ee532e777be16d5a1762c3cc5944,Korean Journal of Ophthalmology
2963,Allelic Expression of Deleterious Protein-Coding Variants across Human Tissues,"Personal exome and genome sequencing provides access to loss-of-function and rare deleterious alleles whose interpretation is expected to provide insight into individual disease burden. However, for each allele, accurate interpretation of its effect will depend on both its penetrance and the trait's expressivity. In this regard, an important factor that can modify the effect of a pathogenic coding allele is its level of expression; a factor which itself characteristically changes across tissues. To better inform the degree to which pathogenic alleles can be modified by expression level across multiple tissues, we have conducted exome, RNA and deep, targeted allele-specific expression (ASE) sequencing in ten tissues obtained from a single individual. By combining such data, we report the impact of rare and common loss-of-function variants on allelic expression exposing stronger allelic bias for rare stop-gain variants and informing the extent to which rare deleterious coding alleles are consistently expressed across tissues. This study demonstrates the potential importance of transcriptome data to the interpretation of pathogenic protein-coding variants.",2014-05-01,https://www.semanticscholar.org/paper/339ae721e765c79f1f4b169f57a82ff21d1058f4,PLoS Genetics
558,Interval graphs and seatching,,1985-07-01,https://www.semanticscholar.org/paper/7b89479af8243dbadb4bc514fdf9692a1b0391c8,Discrete Mathematics
999,Intraocular Pressure–lowering Efficacy of Dorzolamide/Timolol Fixed Combination in Normal-tension Glaucoma,"Purpose:To investigate the intraocular pressure (IOP)-lowering efficacy and safety of dorzolamide/timolol fixed combination (DTFC) in patients with normal-tension glaucoma (NTG). Methods:An open-label, 12-week, 2-center study was conducted. Thirty-seven patients with treatment-naïve NTG received DTFC for 12 weeks to reduce IOP. Primary outcome measures were changes in IOP from baseline to 12 weeks of treatment at a peak drug effect. Secondary outcome measures were changes in IOP from baseline to 12 weeks of treatment a trough drug effect and 8 hours after drug administration. At each visit, IOP was measured at 9 AM and then DTFC was administered by a hospital personnel. IOP was also measured at 11 AM and 5 PM At week 12, the IOP was measured at 1 and 3 PM as well. Results:The IOP at peak drug effect (11 AM) at 12 weeks was significantly reduced to 11.9±2.6 mm Hg from the baseline of 15.6±2.5 mm Hg (23.7%, P<0.0001). Significant reduction in the IOP was also achieved at trough drug effect (9 AM) and at 8 hours after drug administration (5 PM) at 12 weeks (20.5% and 24.4%, respectively, all P<0.0001). Eye irritation (59.5%) was the most frequently reported adverse event followed by ocular hyperemia (16.2%). The majority of eye irritations were mild in intensity. No patients discontinued the DTFC due to an adverse event and no systemic adverse event was considered related to study medication. Conclusions:DTFC is a safe and effective IOP-lowering agent in patients with NTG.",2014-06-01,https://www.semanticscholar.org/paper/8abfa2be7d692b2cfeaed755ca68488d73c1fb90,Journal of glaucoma
2483,Webizing mobile AR contents,"This paper presents a content structure to build mobile AR applications in HTML5 to achieve a clean separation of mobile AR contents from their application logic to scale like the web. By extending POIs (Point of Interest) to objects and places with Uniform Resource Identifier (URI), we could build objects of interest for mobile AR application as DOM (Document Object Model) elements and control their behavior and user interactions through DOM events. Using our content structure, a mobile AR applications can be developed as normal HTML documents seamlessly under current web eco-system.",2013-03-18,https://www.semanticscholar.org/paper/2957395fc48b1c2338ae3dbf696b940379d1eb89,IEEE Conference on Virtual Reality and 3D User Interfaces
1744,The Issue-Adjusted Ideal Point Model,"We develop a model of issue-specific voting behavior. This model can be used to explore lawmakers' personal voting patterns of voting by issue area, providing an exploratory window into how the language of the law is correlated with political support. We derive approximate posterior inference algorithms based on variational methods. Across 12 years of legislative data, we demonstrate both improvement in heldout prediction performance and the model's utility in interpreting an inherently multi-dimensional space.",2012-09-26,https://www.semanticscholar.org/paper/1658c6306d019819abe8f70eb2cb4df1465745dd,arXiv.org
678,Sample Complexity of Learning Mahalanobis Distance Metrics,"Metric learning seeks a transformation of the feature space that enhances prediction quality for the given task at hand. In this work we provide PAC-style sample complexity rates for supervised metric learning. We give matching lower- and upper-bounds showing that the sample complexity scales with the representation dimension when no assumptions are made about the underlying data distribution. However, by leveraging the structure of the data distribution, we show that one can achieve rates that are fine-tuned to a specific notion of intrinsic complexity for a given dataset. Our analysis reveals that augmenting the metric learning optimization criterion with a simple norm-based regularization can help adapt to a dataset's intrinsic complexity, yielding better generalization. Experiments on benchmark datasets validate our analysis and show that regularizing the metric can help discern the signal even when the data contains high amounts of noise.",2015-05-11,https://www.semanticscholar.org/paper/5872d29e682b5ffa9b09de44b4fa3d19279f15f1,Neural Information Processing Systems
3132,A holistic approach to service survivability,"We present SABER (Survivability Architecture: Block, Evade, React), a proposed survivability architecture that blocks, evades and reacts to a variety of attacks by using several security and survivability mechanisms in an automated and coordinated fashion. Contrary to the ad hoc manner in which contemporary survivable systems are built-using isolated, independent security mechanisms such as firewalls, intrusion detection systems and software sandboxes-SABER integrates several different technologies in an attempt to provide a unified framework for responding to the wide range of attacks malicious insiders and outsiders can launch.
 This coordinated multi-layer approach will be capable of defending against attacks targeted at various levels of the network stack, such as congestion-based DoS attacks, software-based DoS or code-injection attacks, and others. Our fundamental insight is that while multiple lines of defense are useful, most conventional, uncoordinated approaches fail to exploit the full range of available responses to incidents. By coordinating the response, the ability to survive successful security breaches increases substantially.
 We discuss the key components of SABER, how they will be integrated together, and how we can leverage on the promising results of the individual components to improve survivability in a variety of coordinated attack scenarios. SABER is currently in the prototyping stages, with several interesting open research topics.",2003-10-31,https://www.semanticscholar.org/paper/669675ece7a64db4305d9c68aac860db7ebd5e4b,SSRS '03
3396,Approximate Matchings in Massive Graphs via Local Structure (Invited Talk),"Finding a maximum matching is a fundamental algorithmic problem and is fairly well understood in traditional sequential computing models. Some modern applications require that we handle massive graphs and hence we need to consider algorithms in models that do not allow the entire input graph to be held in the memory of one computer, or models in which the graph is evolving over time. We introduce a new concept called an “Edge Degree Constrained Subgraph (EDCS)”, which is a subgraph that is guaranteed to contain a large matching, and which can be identified via local conditions. We then show how to use an EDCS to find 1.5-approximate matchings in several different models including Map Reduce, streaming and distributed computing. We can also use an EDCS to maintain a 1.5-optimal matching in a dynamic graph. This work is joint with Sepehr Asadi, Aaron Bernstein, Mohammad Hossein Bateni and Vahab Marrokni. 2012 ACM Subject Classification Theory of computation → Parallel algorithms, Theory of computation → Online algorithms",,https://www.semanticscholar.org/paper/400e667e909e8d4ffbd67773d5d786104a636482,International Symposium on Algorithms and Computation
2764,Designing effective pictures: is photographic realism the only answer?,"ion you don't want to do that. Here's the theory of relativity. This is a space-time diagram. Again, no realistic object. We did a sort of two-dimensional cartoon of it and we made it look flat and two-dimensional because when we extrude it in the time dimension we wanted to have only three dimensions to deal with. Again, mathematical abstractions, things for which there is no real thing. The extrusions are made out of transparent polygons, but the lighting and so forth doesn't make them look like they're really made of shiny plastic. It's just something that you can see, that there's a trail left behind on these things. Here's some of my realistic cartoon animation. Again, we've seen a lot of attempts at realistic human faces, which is an interesting research topic, but you can do really interesting cartoons by making them very cartoony. In fact, this is a subject which I",1988-08-01,https://www.semanticscholar.org/paper/62ec999af315f5b6d1b2c3b746a9f466658296a1,International Conference on Computer Graphics and Interactive Techniques
1217,Evidence of WW and WZ production with lepton + jets final states in pp collisions at square root s=1.96 TeV.,"We present first evidence for WW+WZ production in lepton + jets final states at a hadron collider. The data correspond to 1.07 fb-1 of integrated luminosity collected with the D0 detector at the Fermilab Tevatron in pp collisions at square root s=1.96 TeV. The observed cross section for WW+WZ production is 20.2+/-4.5 pb, consistent with the standard model and more precise than previous measurements in fully leptonic final states. The probability that background fluctuations alone produce this excess is <5.4 x 10-6, which corresponds to a significance of 4.4 standard deviations.",2008-10-21,https://www.semanticscholar.org/paper/4cd7be7bc84861e359c1a7a1651e1eae4dcf58d1,Physical Review Letters
2475,Patient engagement in the inpatient setting: a systematic review,"OBJECTIVE
To systematically review existing literature regarding patient engagement technologies used in the inpatient setting.


METHODS
PubMed, Association for Computing Machinery (ACM) Digital Library, Institute of Electrical and Electronics Engineers (IEEE) Xplore, and Cochrane databases were searched for studies that discussed patient engagement ('self-efficacy', 'patient empowerment', 'patient activation', or 'patient engagement'), (2) involved health information technology ('technology', 'games', 'electronic health record', 'electronic medical record', or 'personal health record'), and (3) took place in the inpatient setting ('inpatient' or 'hospital'). Only English language studies were reviewed.


RESULTS
17 articles were identified describing the topic of inpatient patient engagement. A few articles identified design requirements for inpatient engagement technology. The remainder described interventions, which we grouped into five categories: entertainment, generic health information delivery, patient-specific information delivery, advanced communication tools, and personalized decision support.


CONCLUSIONS
Examination of the current literature shows there are considerable gaps in knowledge regarding patient engagement in the hospital setting and inconsistent use of terminology regarding patient engagement overall. Research on inpatient engagement technologies has been limited, especially concerning the impact on health outcomes and cost-effectiveness.",2014-07-01,https://www.semanticscholar.org/paper/5dc94597a5bbcfd7773c94aa9d41136b92a75673,J. Am. Medical Informatics Assoc.
2464,Interim Results of a Randomized Controlled Trial on Inpatient Engagement,,,https://www.semanticscholar.org/paper/33eb4a2de6feaf91c5e8ecca8ef76a2449d04489,American Medical Informatics Association Annual Symposium
3529,Implementation of a Combinatorial Multicommodity Flow Algorithm,"The multicommodityow probleminvolves simultaneouslyship-ping multiplecommoditiesthrough a single network so that the total amount of ow on each edge is no more than the capacity of the edge. This problem can be expressed as a large linear program, and most known algorithms for it, both theoretical and practical, are linear programming algorithms designed to take advantage of the structure of multicommodity ow problems. The size of the linear programs, however, makes it prohibitively diicult to solve large multicommodity ow problems. In this paper, we describe and examine a multicommodity ow implementation based on the recent combinatorial approximation algorithm of Leighton et al. 13]. The theory predicts that the running time of the algorithm increases linearly with the number of commodities. Our experiments verify this behavior. The theory also predicts that the running time increases as the square of the desired precision. Our experiments show that the running time increases at most this fast, and often slower. We also compare our combinatorial implementation against two diierent linear programming-based codes. First we compare our code to that of of Kennington 10], which is a network simplex code known to perform well on multicommodity ow problems. For many problems, our combinatorial algorithm outperforms this simplex-based linear programming algorithm. More precisely, as the number of commodities increases, the running time of our algorithm grows much more slowly than that of Kennington's linear programming-based algorithm. Second, we compared our code to an interior point code of Karmarkar and Ramakrishnan. Here too, we achieved similar, but less dramatic results. Our results suggest that our algorithm may be able to solve larger multicommodity ow problems than have been solved in the past.",,https://www.semanticscholar.org/paper/cd51f99b4c8018cef18b3a1b779a94da08b4d7b7,Network Flows And Matching
3758,Cross-Modal Scene Networks,"People can recognize scenes across many different modalities beyond natural images. In this paper, we investigate how to learn cross-modal scene representations that transfer across modalities. To study this problem, we introduce a new cross-modal scene dataset. While convolutional neural networks can categorize scenes well, they also learn an intermediate representation not aligned across modalities, which is undesirable for cross-modal transfer applications. We present methods to regularize cross-modal convolutional neural networks so that they have a shared representation that is agnostic of the modality. Our experiments suggest that our scene representation can help transfer representations across modalities for retrieval. Moreover, our visualizations suggest that units emerge in the shared representation that tend to activate on consistent concepts independently of the modality.",2016-10-27,https://www.semanticscholar.org/paper/a4162e328aacba376ea95a7654378423e504ca3d,IEEE Transactions on Pattern Analysis and Machine Intelligence
144,View planning for automated site modeling,"We present a systematic method for constructing 3-D models of large outdoor sites. The method is designed for a mobile robot platform and incorporates automated acquisition of scanned data as well as automated view planning and model construction. In our modeling process, we first use a preliminary view or set of preplanned views to yield an initial, approximate, 3-D model of the target structure. Then, we update this model by using a voxel-based procedure to plan and acquire the next best view. This updating is repeated sequentially until an accurate and complete 3-D model is finally obtained. The method was successfully tested on a portion of the Columbia University campus",2006-05-15,https://www.semanticscholar.org/paper/1f2b869e8f8a9985ab1f8e0dd4fabf4c629371e7,"Proceedings 2006 IEEE International Conference on Robotics and Automation, 2006. ICRA 2006."
1861,Matching Words and Pictures,"We present a new approach for modeling multi-modal data sets, focusing on the specific case of segmented images with associated text. Learning the joint distribution of image regions and words has many applications. We consider in detail predicting words associated with whole images (auto-annotation) and corresponding to particular image regions (region naming). Auto-annotation might help organize and access large collections of images. Region naming is a model of object recognition as a process of translating image regions to words, much as one might translate from one language to another. Learning the relationships between image regions and semantic correlates (words) is an interesting example of multi-modal data mining, particularly because it is typically hard to apply data mining techniques to collections of images. We develop a number of models for the joint distribution of image regions and words, including several which explicitly learn the correspondence between regions and words. We study multi-modal and correspondence extensions to Hofmann's hierarchical clustering/aspect model, a translation model adapted from statistical machine translation (Brown et al.), and a multi-modal extension to mixture of latent Dirichlet allocation (MoM-LDA). All models are assessed using a large collection of annotated images of real scenes. We study in depth the difficult problem of measuring performance. For the annotation task, we look at prediction performance on held out data. We present three alternative measures, oriented toward different types of task. Measuring the performance of correspondence methods is harder, because one must determine whether a word has been placed on the right region of an image. We can use annotation performance as a proxy measure, but accurate measurement requires hand labeled data, and thus must occur on a smaller scale. We show results using both an annotation proxy, and manually labeled data.",2003-03-01,https://www.semanticscholar.org/paper/6a26268d2ba9d34e5b59ae6e5c11a83cdca1a85e,Journal of machine learning research
3228,Above- and below-ground allocation and functional trait response to soil water inputs and drying rates of two common savanna grasses,,2018-10-01,https://www.semanticscholar.org/paper/9a7284430c6d6c3fd965b8b7f0e2ddcd6c047ca5,Journal of Arid Environments
1589,The Deconfounded Recommender: A Causal Inference Approach to Recommendation,"The goal of a recommender system is to show its users items that they will like. In forming its prediction, the recommender system tries to answer: ""what would the rating be if we 'forced' the user to watch the movie?"" This is a question about an intervention in the world, a causal question, and so traditional recommender systems are doing causal inference from observational data. This paper develops a causal inference approach to recommendation. Traditional recommenders are likely biased by unobserved confounders, variables that affect both the ""treatment assignments"" (which movies the users watch) and the ""outcomes"" (how they rate them). We develop the deconfounded recommender, a strategy to leverage classical recommendation models for causal predictions. The deconfounded recommender uses Poisson factorization on which movies users watched to infer latent confounders in the data; it then augments common recommendation models to correct for potential confounding bias. The deconfounded recommender improves recommendation and it enjoys stable performance against interventions on test sets.",2018-08-20,https://www.semanticscholar.org/paper/0e2cce2843780cd03a4108829728833dbfa23632,arXiv.org
365,On a model of indexability and its bounds for range queries,"We develop a theoretical framework to characterize the hardness of indexing data sets on block-access memory devices like hard disks. We define an indexing workload by a data set and a set of potential queries. For a workload, we can construct an indexing scheme, which is a collection of fixed-sized subsets of the data. We identify two measures of efficiency for an indexing scheme on a workload: storage redundancy, r (how many times each item in the data set is stored), and access overhead, A (how many times more blocks than necessary does a query retrieve).For many interesting families of workloads, there exists a trade-off between storage redundancy and access overhead. Given a desired access overhead A, there is a minimum redundancy that any indexing scheme must exhibit. We prove a lower-bound theorem for deriving the minimum redundancy. By applying this theorem, we show interesting upper and lower bounds and trade-offs between A and r in the case of multidimensional range queries and set queries.",,https://www.semanticscholar.org/paper/0249cfd6464a78eeb1dac5f67f67c939624f5d02,JACM
3032,Flux: multi-surface computing in Android,"With the continued proliferation of mobile devices, apps will increasingly become multi-surface, running seamlessly across multiple user devices (e.g., phone, tablet, etc.). Yet general systems support for multi-surface app is limited to (1) screencasting, which relies on a single master device's computing power and battery life or (2) cloud backing, which is unsuitable in the face of disconnected operation or untrusted cloud providers. We present an alternative approach: Flux, an Android-based system that enables any app to become multi-surface through app migration. Flux overcomes device heterogeneity and residual dependencies through two key mechanisms. Selective Record/Adaptive Replay records just those device-agnostic app calls that lead to the generation of app-specific device-dependent state in system services and replays them on the target. Checkpoint/Restore in Android (CRIA) transitions an app into a state in which device-specific information can be safely discarded before checkpointing and restoring the app. Our implementation of Flux can migrate many popular, unmodified Android apps---including those with extensive device interactions like 3D accelerated graphics---across heterogeneous devices and is fast enough for interactive use.",2015-04-17,https://www.semanticscholar.org/paper/2dbd0bca3fb1a57f441f1867ac0fa7dfc245ae66,European Conference on Computer Systems
1350,Search for Randall-Sundrum gravitons in dilepton and diphoton final states.,"We report the first direct search for the Kaluza-Klein (KK) modes of Randall-Sundrum gravitons using dielectron, dimuon, and diphoton events observed with the D0 detector operating at the Fermilab Tevatron pp(-) Collider at sqrt[s]=1.96 TeV. No evidence for resonant production of gravitons has been found in the data corresponding to an integrated luminosity of approximately equal to 260 pb(-1). Lower limits on the mass of the first KK mode at the 95% C.L. have been set between 250 and 785 GeV, depending on its coupling to standard model particles.",2005-05-09,https://www.semanticscholar.org/paper/916132cd350319dc961af05e3fbbeb8b4c802fa6,Physical Review Letters
1965,A Multiobjective Hybrid Genetic Algorithm for TFT-LCD Module Assembly Scheduling,"The thin-film transistor-liquid crystal display (TFT-LCD) module assembly production is a flexible job-shop scheduling problem that is critical to satisfy the customer demands on time. On the module assembly shop floor, each workstation has identical and non-identical parallel machines that access the jobs at various processing velocities depending on the product families. To satisfy the various jobs, the machines need to be set up as the numerous tools to conduct consecutive products. This study aims to propose a novel approach to address the TFT-LCD module assembly scheduling problem by simultaneously considering the following multiple and often conflicting objectives such as the makespan, the weighted number of tardy jobs, and the total machine setup time, subject to the constraints of product families, non-identical parallel machines, and sequence-dependent setup times. In particular, we developed a multiobjective hybrid genetic algorithm (MO-HGA) that hybridizes with the variable neighborhood descent (VND) algorithm as a local search and TOPSIS evaluation technique to derive the best compromised solution. To estimate the validity of the proposed MO-HGA, experiments based on empirical data were conducted to compare the results with conventional approaches. The results have shown the validity of this approach. This study concludes with a discussion of future research directions.",2014-05-06,https://www.semanticscholar.org/paper/8e7a4f1648ced4338260c4c0147e56ac3444a7fa,IEEE Transactions on Automation Science and Engineering
2188,Oral Ulcers in Juvenile-Onset Systemic Lupus Erythematosus: A Review of the Literature,,2017-05-05,https://www.semanticscholar.org/paper/13e5479f7bb9736698f02c869ffcda9c136e8f37,American Journal of Clinical Dermatology
518,The Optimum Execution Order of Queries in Linear Storage,,1990-11-01,https://www.semanticscholar.org/paper/e893038c3d2435b18cc2fd60294fc561abd53aed,Information Processing Letters
116,Data structures for efficient broker implementation,"With the profusion of text databases on the Internet, it is becoming increasingly hard to find the most useful databases for a given query. To attack this problem, several existing and proposed systems employ brokers to direct user queries, using a local database of summary information about the available databases. This summary information must effectively distinguish relevant databases and must be compact while allowing efficient access. We offer evidence that one broker, GlOSS, can be effective at locating databases of interest even in a system of hundreds of databased and can examine the performance of accessing theGlOSS summeries for two promising storage methods: the grid file and partitioned hashing. We show that both methods can be tuned to provide good performance for a particular workload (within a broad range of workloads), and we discuss the tradeoffs between the two data structures. As a side effect of our work, we show that grid files are more broadly applicable than previously thought; inparticular, we show that by varying the policies used to construct the grid file we can provide good performance for a wide range of workloads even when storing highly skewed data.",1997-07-01,https://www.semanticscholar.org/paper/2b5d18de54908db492dbc9315c9eea1ed86ac521,TOIS
1653,Variational Inference: A Review for Statisticians,"ABSTRACTOne of the core problems of modern statistics is to approximate difficult-to-compute probability densities. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation involving the posterior density. In this article, we review variational inference (VI), a method from machine learning that approximates probability densities through optimization. VI has been used in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of densities and then to find a member of that family which is close to the target density. Closeness is measured by Kullback–Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data...",2016-01-04,https://www.semanticscholar.org/paper/6f24d7a6e1c88828e18d16c6db20f5329f6a6827,arXiv.org
1019,"Swim: A General-Purpose, High-Performing, and Efficient Activation Function for Locomotion Control Tasks","Activation functions play a significant role in the performance of deep learning algorithms. In particular, the Swish activation function tends to outperform ReLU on deeper models, including deep reinforcement learning models, across challenging tasks. Despite this progress, ReLU is the preferred function partly because it is more efficient than Swish. Furthermore, in contrast to the fields of computer vision and natural language processing, the deep reinforcement learning and robotics domains have seen less inclination to adopt new activation functions, such as Swish, and instead continue to use more traditional functions, like ReLU. To tackle those issues, we propose Swim, a general-purpose, efficient, and high-performing alternative to Swish, and then provide an analysis of its properties as well as an explanation for its high-performance relative to Swish, in terms of both reward-achievement and efficiency. We focus on testing Swim on MuJoCo's locomotion continuous control tasks since they exhibit more complex dynamics and would therefore benefit most from a high-performing and efficient activation function. We also use the TD3 algorithm in conjunction with Swim and explain this choice in the context of the robot locomotion domain. We then conclude that Swim is a state-of-the-art activation function for continuous control locomotion tasks and recommend using it with TD3 as a working framework.",2023-03-05,https://www.semanticscholar.org/paper/e7b503d48f4f5864fafb9e3fe6c1391a1f32328d,arXiv.org
1933,UNISON framework of data-driven innovation for extracting user experience of product design of wearable devices,,2016-09-01,https://www.semanticscholar.org/paper/3ea3672aac2ad95b0ae7c804aefff6881c5457ec,Computers & industrial engineering
3018,Protecting Cloud Virtual Machines from Hypervisor and Host Operating System Exploits,"Hypervisors are widely deployed by cloud computing providers to support virtual machines, but their growing complexity poses a security risk as large codebases contain many vulnerabilities. We have created HypSec, a new hypervisor design for retrofitting an existing commodity hypervisor using microkernel principles to reduce its trusted computing base while protecting the confidentiality and integrity of virtual machines. HypSec partitions the hypervisor into an untrusted host that performs most complex hypervisor functionality without access to virtual machine data, and a trusted core that provides access control to virtual machine data and performs basic CPU and memory virtualization. Hardware virtualization support is used to isolate and protect the trusted core and execute it at a higher privilege level so it can mediate virtual machine exceptions and protect VM data in CPU and memory. HypSec takes an end-to-end approach to securing I/O to simplify its design, with applications increasingly using secure network connections in the cloud. We have used HypSec to retrofit KVM, showing how our approach can support a widely-used full-featured hypervisor integrated with a commodity operating system. The implementation has a trusted computing base of only a few thousand lines of code, many orders of magnitude less than KVM. We show that HypSec protects the confidentiality and integrity of virtual machines running unmodified guest operating systems while only incurring modest performance overhead for real application workloads.",,https://www.semanticscholar.org/paper/6c5c5be93b58e417220e0841ed5da62d1e8c7b5b,USENIX Security Symposium
1562,Causal Inference for Recommender Systems,"The task of recommender systems is classically framed as a prediction of users’ preferences and users’ ratings. However, its spirit is to answer a counterfactual question: “What would the rating be if we ‘forced’ the user to watch the movie?” This is a question about an intervention, that is a causal inference question. The key challenge of this causal inference is unobserved confounders, variables that affect both which items the users decide to interact with and how they rate them. To this end, we develop an algorithm that leverages classical recommendation models for causal recommendation. Across simulated and real datasets, we demonstrate that the proposed algorithm is more robust to unobserved confounders and improves recommendation.",2020-09-22,https://www.semanticscholar.org/paper/ca94b305307b8df8997fc14ffaf90fa96623cc1e,ACM Conference on Recommender Systems
2970,"Gene expression changes with age in skin, adipose tissue, blood and brain",,2013-07-01,https://www.semanticscholar.org/paper/083a1627e327721de3dc8b5818a351fd6b7939a4,Genome Biology
910,A polynomial algorithm for the MIN CUT linear arrangement of trees,An algorithm is presented which finds a min-cut linear arrangement of a tree in O(nlogn) time. An extension of the algorithm determines the number of pebbles needed to play the black and white pebble game on a tree.,1983-11-07,https://www.semanticscholar.org/paper/e706a00fffa188b409c40e3e13fc4a70330efbc9,24th Annual Symposium on Foundations of Computer Science (sfcs 1983)
2999,"DuoAI: Fast, Automated Inference of Inductive Invariants for Verifying Distributed Protocols","Distributed systems are complex and difficult to build correctly. Formal verification can provably rule out bugs in such systems, but finding an inductive invariant that implies the safety property of the system is often the hardest part of the proof. We present DuoAI, an automated system that quickly finds inductive invariants for verifying distributed protocols by reducing SMT query costs in checking invariants with existential quantifiers. DuoAI enumerates the strongest candidate invariants that hold on validate states from protocol simulations, then applies two methods in parallel, returning the result from the method that succeeds first. One checks all candidate invariants and weakens them as needed until it finds an inductive invariant that implies the safety property. Another checks invariants without existential quantifiers to find an inductive invariant without the safety property, then adds candidate invariants with existential quantifiers to strengthen it until the safety property holds. Both methods are guaranteed to find an inductive invariant that proves desired safety properties, if one exists, but the first reduces SMT query costs when more candidate invariants with existential quantifiers are needed, while the second reduces SMT query costs when few candidate invariants with existential quantifiers suffice. We show that DuoAI verifies more than two dozen common distributed protocols automatically, including various versions of Paxos, and outperforms alternative methods both in the number of protocols it verifies and the speed at which it does so,including solving Paxos more than two orders of magnitude faster than previous methods. which is difficult to do without first knowing the ground-truth invariants. The 4 protocols have much simpler inductive invariants when expressed on top of these clauses, with all except the simplest, client server db ae, becoming ∃ -free. Ivy fails when checking the invariants generated by IC3PO for Paxos and flexible Paxos. The IC3PO authors [9] imply that the invariants had to be manually checked against the human-expert invariants.",,https://www.semanticscholar.org/paper/23d2ce87033bbeaddcfbeb889c6d30b00d12ad7f,USENIX Symposium on Operating Systems Design and Implementation
3355,"Population density, resource patterning, and territoriality in the Everglades pygmy sunfish",,1981-02-01,https://www.semanticscholar.org/paper/40484c6735cab8d05676c656c5e7cdacb17dc96d,Animal Behaviour
2533,ActiveNotes: computer-assisted creation of patient progress notes,"We present activeNotes, a prototype application that supports the creation of Critical Care Notes by physicians in a hospital intensive care unit. activeNotes integrates automated, context-sensitive patient data retrieval and user control of automated data updates and alerts into the note-creation process. In a user study at New York Presbyterian Hospital, we gathered qualitative feedback on the prototype from 15 physicians. The physicians found activeNotes to be valuable and said they would use it to create both formal notes for medical records and informal notes. One surprising finding is that while physicians have rejected template-based clinical documentation systems in the past, they expressed a desire to use activeNotes to create personalized, physician-specific note templates to be reused with a given patient, or for a given condition.",2009-04-04,https://www.semanticscholar.org/paper/567334b48820323b2472bc1584f2ceb909bc27e7,CHI Extended Abstracts
1875,Integrated circuit probe card troubleshooting based on rough set theory for advanced quality control and an empirical study,,2022-11-03,https://www.semanticscholar.org/paper/e13a880180ba424f00796f8c388d162f3df9cb11,Journal of Intelligent Manufacturing
3057,Apiary: Easy-to-Use Desktop Application Fault Containment on Commodity Operating Systems,"Desktop computers are often compromised by the interaction of untrusted data and buggy software. To address this problem, we present Apiary, a system that transparently contains application faults while retaining the usage metaphors of a traditional desktop environment. Apiary accomplishes this with three key mechanisms. It isolates applications in containers that integrate in a controlled manner at the display and file system. It introduces ephemeral containers that are quickly instantiated for single application execution, to prevent any exploit that occurs from persisting and to protect user privacy. It introduces the Virtual Layered File System to make instantiating containers fast and space efficient, and to make managing many containers no more complex than a single traditional desktop. We have implemented Apiary on Linux without any application or operating system kernel changes. Our results with real applications, known exploits, and a 24-person user study show that Apiary has modest performance overhead, is effective in limiting the damage from real vulnerabilities, and is as easy for users to use as a traditional desktop.",2010-06-23,https://www.semanticscholar.org/paper/5d39ee861cfdb90a61cc4d1d0890ff7043bca39f,USENIX Annual Technical Conference
2417,Adaptive Visual Cues for Guiding a Bimanual Unordered Task in Virtual Reality,"Work on cueing performance in AR and VR has focused on sequential tasks in which each step must be completed in order before the user can proceed to the next. However, for unordered tasks such as putting books back on a library shelf, the user may be able to perform multiple steps concurrently without needing to follow a specific order. In such situations, giving the user multiple cues for potentially concurrent steps may improve performance time. To investigate this, we built a bimanual VR testbed in which the user needs to move objects to designated destinations, guided by different numbers of cues. The user can decide the order to perform the cued steps and, in some conditions, can affect which cues are shown.In a formal user study, we found that in most conditions, participants perform fastest with three cues. Dynamically updating the set of displayed cues based on hand proximity improves performance, and updating the set based on eye gaze improves performance even more. Finally, for both the hand-proximity and eye-gaze mechanisms, performance can be further improved by locking the cues for objects predicted to be moved next based on hand distance.",2022-10-01,https://www.semanticscholar.org/paper/efee354668c112d0d8cabb9c58a78110ca9423fd,International Symposium on Mixed and Augmented Reality
2838,Nonalcoholic steatohepatitis and hepatocellular carcinoma in galectin‐3 knockout mice,"Aim:  Nonalcoholic fatty liver disease (NAFLD) represents a growing health concern due to its rapidly increasing prevalence worldwide. Nonalcoholic steatohepatitis (NASH) is a progressing form of NAFLD, and recently many studies have reported that it could eventually develop into hepatocellular carcinoma (HCC). We previously reported that 6‐month‐old male galectin‐3 knockout (gal3−/−) mice developed clinicopathological features similar to those of NAFLD in humans. Our aim was to investigate the changes in liver histology in gal3−/− mice by long‐term observation.",2008-12-01,https://www.semanticscholar.org/paper/0edb56a38b7b2d4587be10eeca9b018aa4779513,Hepatology Research
629,The serializability of concurrent database updates,"A sequence of interleaved user transactions in a database system may not be ser:ahzable, t e, equivalent to some sequential execution of the individual transactions Using a simple transaction model, it ~s shown that recognizing the transaction histories that are serlahzable is an NP-complete problem. Several efficiently recognizable subclasses of the class of senahzable histories are therefore introduced; most of these subclasses correspond to senahzabdity principles existing in the hterature and used in practice Two new principles that subsume all previously known ones are also proposed Necessary and sufficient conditions are given for a class of histories to be the output of an efficient history scheduler, these conditions imply that there can be no efficient scheduler that outputs all of senahzable histories, and also that all subclasses of senalizable histories studied above have an efficient scheduler Finally, it is shown how these results can be extended to far more general transaction models, to transactions with partly interpreted functions, and to distributed database systems",1979-10-01,https://www.semanticscholar.org/paper/e7ab23d011e5183db78cfea48e303210f6e57e2e,JACM
18,Effective Event Identification in Social Media,"Online social media sites are extensively used by individuals to produce and distribute content related to real-world events. Unfortunately, this social media content associated with an event is generally not provided in any structured and readily available form. Thus, identifying the event-related content on social media sites is a challenging task. Prior work has addressed the event identification task under two different scenarios, namely, when the events are known ahead of time, as is sometimes the case for planned events, and when the events are unknown, as is the case for spontaneous, unplanned events. In this article, we discuss both the unknown- and known-event identification scenarios, and attempt to characterize the key factors in the identification process, including the nature of social media content as well as the behavior and characteristics of event content over time. Furthermore, we propose enhancements to our earlier techniques that consider these factors and improve the state-of-the-art unknown-event identification strategies. Specifically, we propose novel features of the social media content that we can exploit, as well as the modeling of the typical time decay of event-related content. Large-scale experiments show that our approach exhibits improved effectiveness relative to the state-of-the-art approaches.",,https://www.semanticscholar.org/paper/957104cb5951987380e6263ddc498fca1f008798,IEEE Data Engineering Bulletin
3397,Scheduling (Dagstuhl Seminar 18101),,,https://www.semanticscholar.org/paper/4504e38b4efbf9b53ec48242ae67a3a4e04377fa,Dagstuhl Reports
1514,Density Uncertainty Layers for Reliable Uncertainty Estimation,"Assessing the predictive uncertainty of deep neural networks is crucial for safety-related applications of deep learning. Although Bayesian deep learning offers a principled framework for estimating model uncertainty, the approaches that are commonly used to approximate the posterior often fail to deliver reliable estimates of predictive uncertainty. In this paper we propose a novel criterion for predictive uncertainty, that a model's predictive variance should be grounded in the empirical density of the input. It should produce higher uncertainty for inputs that are improbable in the training data and lower uncertainty for those inputs that are more probable. To operationalize this criterion, we develop the density uncertainty layer, an architectural element for a stochastic neural network that guarantees that the density uncertain criterion is satisfied. We study neural networks with density uncertainty layers on the CIFAR-10 and CIFAR-100 uncertainty benchmarks. Compared to existing approaches, we find that density uncertainty layers provide reliable uncertainty estimates and robust out-of-distribution detection performance.",2023-06-21,https://www.semanticscholar.org/paper/3cb746ee9ab49920ead4bd832d94ca0bc1ee5d3b,arXiv.org
327,Computing equilibria in multi-player games,"We initiate the systematic study of algorithmic issues involved in finding equilibria (Nash and correlated) in games with a large number of players; such games, in order to be computationally meaningful, must be presented in some succinct, game-specific way. We develop a general framework for obtaining polynomial-time algorithms for optimizing over correlated equilibria in such settings, and show how it can be applied successfully to symmetric games (for which we actually find an exact polytopal characterization), graphical games, and congestion games, among others. We also present complexity results implying that such algorithms are not possible in certain other such games. Finally, we present a polynomial-time algorithm, based on quantifier elimination, for finding a Nash equilibrium in symmetric games when the number of strategies is relatively small.",2005-01-23,https://www.semanticscholar.org/paper/1cede7f04cbc628802990b1bcf6463259c65ac6b,ACM-SIAM Symposium on Discrete Algorithms
2272,Anaplasma phagocytophilum Reduces Neutrophil Apoptosis In Vivo,"ABSTRACT Ovine neutrophils spontaneously underwent apoptosis during culture in vitro, as assessed by morphological changes and exposure of annexin V binding sites on their cell surfaces. The addition of conditioned medium from concanavalin A-treated ovine peripheral blood mononuclear cells (PBMC) could partially protect against this progression into apoptosis, but dexamethasone and sodium butyrate could not. Actinomycin D accelerated the rate at which ovine neutrophils underwent apoptosis. Neutrophils isolated from sheep experimentally infected with Anaplasma phagocytophilum showed significantly delayed apoptosis during culture ex vivo, and the addition of conditioned medium from PBMC to these cells could not delay apoptosis above the protective effects observed after in vivo infection. The ability of neutrophils from A. phagocytophilum-infected sheep to activate a respiratory burst was increased compared to the activity measured in neutrophils from uninfected sheep, but chemotaxis was decreased in neutrophils from infected sheep. These data are the first demonstration that in vivo infection with A. phagocytophilum results in changes in rates of apoptosis of infected immune cells. This may help explain how these bacteria replicate in these normally short-lived cells.",2003-04-01,https://www.semanticscholar.org/paper/b7bbe6734e076d9f2b8b45b2b3810387adea1f12,Infection and Immunity
958,Relationship between Three-Dimensional Magnetic Resonance Imaging Eyeball Shape and Optic Nerve Head Morphology,,2021-04-01,https://www.semanticscholar.org/paper/e2c546a87550b8baa8c156d9c4cfb0389a2b33c1,"Ophthalmology (Rochester, Minn.)"
294,Papers about papers.,,,https://www.semanticscholar.org/paper/b7e6d3f405a15084cf8dfe877378897df9ac927a,Nature Nanotechnology
2458,Evaluating Positional Head-Tracking in Immersive VR for 3D Designers,"With the ongoing introduction of wide-FOV VR head-worn displays into the consumer market, the application of VR 3D UIs to professional work environments is attracting increasing attention. One of the most conspicuous concepts is immersive 3D modeling and content creation. In spite of the long research history, there have been very few analyses of the effect of 3D UIs on productivity in 3D design. In this work, we explore the effect of positional head-tracking on task performance in 3D design. Previous studies have come to different conclusions on the importance of headtracking and did not investigate professional 3D modeling tools. In contrast, we performed a user study with design students using professional software on a task that closely emulates their work. Surprisingly, we did not find a significant effect of head-tracking on task-completion time, neither when using a traditional 2D mouse nor when using a pinch glove as a 3D input device.",2016-09-01,https://www.semanticscholar.org/paper/81ccb0aca2ab0d2b5c5b3844b4dfd96fdc899586,2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)
315,The Game World Is Flat: The Complexity of Nash Equilibria in Succinct Games,,2006-07-10,https://www.semanticscholar.org/paper/1438521df22b070002759b72cb5f6c31fc028b38,"International Colloquium on Automata, Languages and Programming"
1053,Bound-free pair production in relativistic nuclear collisions from the NICA to the HE LHC colliders,,2020-06-02,https://www.semanticscholar.org/paper/99482751b565309e528c1f464037729dcc579463,European Physical Journal A
2794,Galectin-3 Plays an Important Role in Innate Immunity to Gastric Infection by Helicobacter pylori,"ABSTRACT We studied the role of galectin-3 (Gal3) in gastric infection by Helicobacter pylori. We first demonstrated that Gal3 was selectively expressed by gastric surface epithelial cells and abundantly secreted into the surface mucus layer. We next inoculated H. pylori Sydney strain 1 into wild-type (WT) and Gal3-deficient mice using a stomach tube. At 2 weeks postinoculation, the bacterial cells were mostly trapped within the surface mucus layer in WT mice. In sharp contrast, they infiltrated deep into the gastric glands in Gal3-deficient mice. Bacterial loads in the gastric tissues were also much higher in Gal3-deficient mice than in WT mice. At 6 months postinoculation, H. pylori had successfully colonized within the gastric glands of both WT and Gal3-deficient mice, although the bacterial loads were still higher in the latter. Furthermore, large lymphoid clusters mostly consisting of B cells were frequently observed in the gastric submucosa of Gal3-deficient mice. In vitro, peritoneal macrophages from Gal3-deficient mice were inefficient in killing engulfed H. pylori. Furthermore, recombinant Gal3 not only induced rapid aggregation of H. pylori but also exerted a potent bactericidal effect on H. pylori as revealed by propidium iodide uptake and a morphological shift from spiral to coccoid form. However, a minor fraction of bacterial cells, probably transient phase variants of Gal3-binding sugar moieties, escaped killing by Gal3. Collectively, our data demonstrate that Gal3 plays an important role in innate immunity to infection and colonization of H. pylori.",2016-02-08,https://www.semanticscholar.org/paper/764f8cc3498427c13e94133888ed5bd8a8209e07,Infection and Immunity
320,A note on approximate Nash equilibria,,2006-12-15,https://www.semanticscholar.org/paper/6d2a49d83d28b383eb846f385dccbbde1146bc64,Theoretical Computer Science
814,Primal-dual approximation algorithms for integral flow and multicut in trees,,1997-05-01,https://www.semanticscholar.org/paper/9ec089c12e76a55a2c00cb63b394b05fc070c1f0,Algorithmica
3077,Two-Person Control Administation: Preventing Administation Faults through Duplication,"Modern computing systems are complex and difficult to administer, making them more prone to system administration faults. Faults can occur simply due to mistakes in the process of administering a complex system. These mistakes can make the system insecure or unavailable. Faults can also occur due to a malicious act of the system administrator. Systems provide little protection against system administrators who install a backdoor or otherwise hide their actions. To prevent these types of system administration faults, we created ISE-T (I See Everything Twice), a system that applies the two-person control model to system administration. ISE-T requires two separate system administrators to perform each administration task. ISE-T then compares the results of the two administrators' actions for equivalence. ISE-T only applies the results of the actions to the real system if they are equivalent. This provides a higher level of assurance that administration tasks are completed in a manner that will not introduce faults into the system. While the two-person control model is expensive, it is a natural fit for many financial, government, and military systems that require higher levels of assurance. We implemented a prototype ISE-T system for Linux using virtual machines and a unioning file system. Using this system, we conducted a real user study to test its ability to capture changes performed by seperate system administrators and compare them for equivalence. Our results show that ISE-T is effective at determining equivalence for many common administration tasks, even when administrators perform those tasks in different ways.",2009-11-01,https://www.semanticscholar.org/paper/d7e88f2dcbdae94addf108bd67f09f02126512e1,LiSA
1046,Measurement of the W boson mass,,2021-09-02,https://www.semanticscholar.org/paper/58f51391b829be6594a4485e8cc20879972f8454,Journal of High Energy Physics
2397,Oscillations in protein and RNA content during synchronous growth of Acanthamoeba castellanii,,,https://www.semanticscholar.org/paper/4d21337680b56e8f4f2d25a6bab97c5bb8682757,FEBS Letters
622,Bounds for sorting by prefix reversal,,,https://www.semanticscholar.org/paper/2bfc23fef4a712b943098f15c0505041af996d87,Discrete Mathematics
2923,A novel methodology for estimating tensile properties in a small punch test employing in-situ DIC based deflection mapping,,2020-09-01,https://www.semanticscholar.org/paper/02c12d42ae5c110a7b3e5415d2943b810f18017e,Journal of Nuclear Materials
2433,SpaceTokens: Interactive Map Widgets for Location-centric Interactions,"Map users often need to interact repetitively with multiple important locations. For example, a traveler may frequently check her hotel or a train station on a map, use them to localize an unknown location, or investigate routes involving them. Ironically, these location-centric tasks cannot be performed using locations directly; users must instead pan and zoom the map or use a menu to access locations. We propose SpaceTokens, interactive widgets that act as clones of locations, and which users can create and place on map edges like virtual whiteboard magnets. SpaceTokens make location a first-class citizen of map interaction. They empower users to rapidly perform location-centric tasks directly using locations: users can select combinations of on-screen locations and SpaceTokens to control the map window, or connect them to create routes. Participants in a study overwhelmingly preferred a SpaceTokens prototype over Google Maps on identical smartphones for the majority of tasks.",2018-04-21,https://www.semanticscholar.org/paper/376a65003a650c8c09199a7561a16e49bdfea63e,International Conference on Human Factors in Computing Systems
548,Searching and Pebbling,,1986-11-02,https://www.semanticscholar.org/paper/49ec7fe784eac2d8593a1d1b5b8953cfb79396f0,Theoretical Computer Science
3308,"Network metrics reveal differences in social organization between two fission–fusion species, Grevy’s zebra and onager",,2007-01-05,https://www.semanticscholar.org/paper/6797f2ae05d31bc271b29ee75782a1319fa1e971,Oecologia
809,On the complexity of protein folding (extended abstract),forefront of today’s science (often referred to dramatically as “breaking the genetic code” or “the last phase of the We ahow that the protein folding problem in the two-dimensional Mendelian revolution”). This mapping cm be rou&ly &H-P model io NP-complete.,1998-05-23,https://www.semanticscholar.org/paper/a56ef182b5d7484ed4bd3602cfe7829ed53636f6,Symposium on the Theory of Computing
776,Guest Editors' foreword,,,https://www.semanticscholar.org/paper/8b95445907f1f09399a072eb495bfa8212ca18ff,Journal of computer and system sciences (Print)
727,"Upper Bounds for Newton’s Method on Monotone Polynomial Systems, and P-Time Model Checking of Probabilistic One-Counter Automata","A central computational problem for analyzing and model checking various classes of infinite-state recursive probabilistic systems (including quasi-birth-death processes, multitype branching processes, stochastic context-free grammars, probabilistic pushdown automata and recursive Markov chains) is the computation of termination probabilities, and computing these probabilities in turn boils down to computing the least fixed point (LFP) solution of a corresponding monotone polynomial system (MPS) of equations, denoted x=P(x). It was shown in Etessami and Yannakakis [2009] that a decomposed variant of Newton’s method converges monotonically to the LFP solution for any MPS that has a nonnegative solution. Subsequently, Esparza et al. [2010] obtained upper bounds on the convergence rate of Newton’s method for certain classes of MPSs. More recently, better upper bounds have been obtained for special classes of MPSs [Etessami et al. 2010, 2012]. However, prior to this article, for arbitrary (not necessarily strongly connected) MPSs, no upper bounds at all were known on the convergence rate of Newton’s method as a function of the encoding size |P| of the input MPS, x=P(x). In this article, we provide worst-case upper bounds, as a function of both the input encoding size |P|, and ε > 0, on the number of iterations required for decomposed Newton’s method (even with rounding) to converge to within additive error ε > 0 of q*, for an arbitrary MPS with LFP solution q*. Our upper bounds are essentially optimal in terms of several important parameters of the problem. Using our upper bounds, and building on prior work, we obtain the first P-time algorithm (in the standard Turing model of computation) for quantitative model checking, to within arbitrary desired precision, of discrete-time QBDs and (equivalently) probabilistic 1-counter automata, with respect to any (fixed) ω-regular or LTL property.",2013-02-15,https://www.semanticscholar.org/paper/12ee4fe3cd8ea4cd84e80828832a28bb4f93e17b,Journal of the ACM
627,Optimality of the Fast Fourier transform,"A graph-theoretic model for a class of linear algorithms computing the discrete Fourier transform of sequences of length a power of 2, the mformat~on flow network, is presented The information flow network correspondmg to the fast Fourier transform IS shown to be umquely optimal in tim class with respect to a naturally defined cost",,https://www.semanticscholar.org/paper/c11879cb3dfc01758672d149063d9133c1aff4e7,JACM
435,Planar Topological Queries,,1997-01-11,https://www.semanticscholar.org/paper/61869bed4e5749fef6ed6a363625cb74811eb55c,International Symposium on the Applications of Constraint Databases
2836,Galectin-3 negatively regulates TCR-mediated CD4+ T-cell activation at the immunological synapse,"We have investigated the function of endogenous galectin-3 in T cells. Galectin-3-deficient (gal3−/−) CD4+ T cells secreted more IFN-γ and IL-4 than gal3+/+CD4+ T cells after T-cell receptor (TCR) engagement. Galectin-3 was recruited to the cytoplasmic side of the immunological synapse (IS) in activated T cells. In T cells stimulated on supported lipid bilayers, galectin-3 was primarily located at the peripheral supramolecular activation cluster (pSMAC). Gal3+/+ T cells formed central SMAC on lipid bilayers less effectively and adhered to antigen-presenting cells less firmly than gal3−/− T cells, suggesting that galectin-3 destabilizes the IS. Galectin-3 expression was associated with lower levels of early signaling events and phosphotyrosine signals at the pSMAC. Additional data suggest that galectin-3 potentiates down-regulation of TCR in T cells. By yeast two-hybrid screening, we identified as a galectin-3-binding partner, Alix, which is known to be involved in protein transport and regulation of cell surface expression of certain receptors. Co-immunoprecipitation confirmed galectin-3-Alix association and immunofluorescence analysis demonstrated the translocation of Alix to the IS in activated T cells. We conclude that galectin-3 is an inhibitory regulator of T-cell activation and functions intracellularly by promoting TCR down-regulation, possibly through modulating Alix's function at the IS.",2009-08-25,https://www.semanticscholar.org/paper/da04fdf9d5956f3db2049f95497af94227e1f1b6,Proceedings of the National Academy of Sciences of the United States of America
3680,Objaverse-XL: A Universe of 10M+ 3D Objects,"Natural language processing and 2D vision models have attained remarkable proficiency on many tasks primarily by escalating the scale of training data. However, 3D vision tasks have not seen the same progress, in part due to the challenges of acquiring high-quality 3D data. In this work, we present Objaverse-XL, a dataset of over 10 million 3D objects. Our dataset comprises deduplicated 3D objects from a diverse set of sources, including manually designed objects, photogrammetry scans of landmarks and everyday items, and professional scans of historic and antique artifacts. Representing the largest scale and diversity in the realm of 3D datasets, Objaverse-XL enables significant new possibilities for 3D vision. Our experiments demonstrate the improvements enabled with the scale provided by Objaverse-XL. We show that by training Zero123 on novel view synthesis, utilizing over 100 million multi-view rendered images, we achieve strong zero-shot generalization abilities. We hope that releasing Objaverse-XL will enable further innovations in the field of 3D vision at scale.",2023-07-11,https://www.semanticscholar.org/paper/1b90e9e9734bed6b379ae87d688cb3b887baf597,arXiv.org
339,Games Other People Play,,2005-08-23,https://www.semanticscholar.org/paper/e0a4360d0882507a7e2b95b9131813141a9be12c,International Conference on Concurrency Theory
804,Communicating Hierarchical State Machines,,1999-07-11,https://www.semanticscholar.org/paper/8384b2063fdf5106e64013251b06d771cc7b9b44,"International Colloquium on Automata, Languages and Programming"
2822,LPS-Induced Galectin-3 Oligomerization Results in Enhancement of Neutrophil Activation,"Galectin-3 (Gal 3) is a glycan-binding protein that can be secreted by activated macrophages and mast cells at inflammation sites and plays an important role in inflammatory diseases caused by Bacteria and their products, such as lipopolysaccharides (LPS). Although it is well established that Gal 3 can interact with LPS, the pathophysiological importance of LPS/Gal 3 interactions is not fully understood. Data presented herein demonstrate for the first time that the interaction of Gal 3, either via its carbohydrate binding C-terminal domain or via its N-terminal part, with LPS from different bacterial strains, enhances the LPS-mediated neutrophil activation in vitro. Gal 3 allowed low LPS concentrations (1 µg/mL without serum, 1 ng/mL with serum) to upregulate CD11b expression and reactive oxygen species (ROS) generation on human neutrophils in vitro and drastically enhanced the binding efficiency of LPS to the neutrophil surface. These effects required LPS preincubation with Gal 3, before neutrophil stimulation and involved specific Gal 3/LPS interaction. A C-terminal Gal-3 fragment, which retains the lectin domain but lacks the N-terminal part, was still able to bind both to Escherichia coli LPS and to neutrophils, but had lost the ability to enhance neutrophil response to LPS. This result emphasizes the importance of an N-terminus-mediated Gal 3 oligomerization induced by its interaction with LPS. Finally we demonstrated that Balb/C mice were more susceptible to LPS-mediated shock when LPS was pretreated with Gal 3. Altogether, these results suggest that multimeric interactions between Gal 3 oligomers and LPS potentiate its pro-inflammatory effects on neutrophils.",2011-10-21,https://www.semanticscholar.org/paper/882a9d9af87852f7a281c14101107d5bf9f28322,PLoS ONE
1689,Copula variational inference,"We develop a general variational inference method that preserves dependency among the latent variables. Our method uses copulas to augment the families of distributions used in mean-field and structured approximations. Copulas model the dependency that is not captured by the original variational distribution, and thus the augmented variational family guarantees better approximations to the posterior. With stochastic optimization, inference on the augmented distribution is scalable. Furthermore, our strategy is generic: it can be applied to any inference procedure that currently uses the mean-field or structured approach. Copula variational inference has many advantages: it reduces bias; it is less sensitive to local optima; it is less sensitive to hyperparameters; and it helps characterize and interpret the dependency among the latent variables.",2015-06-10,https://www.semanticscholar.org/paper/c8d44c88691ec27e37038db9cbea15d96b06d3af,Neural Information Processing Systems
2816,The roles of Galectin-3 in autoimmunity and tumor progression,,2012-03-15,https://www.semanticscholar.org/paper/ff94ac157ce9be6d382c1142d0a7d025baf85266,Immunologic research
2872,Galectin-7 (PIG1) Exhibits Pro-apoptotic Function through JNK Activation and Mitochondrial Cytochrome cRelease* 210,"Galectin-7 is normally expressed in all types of stratified epithelia, but is significantly down-regulated in squamous cell carcinomas. This protein was recently found to be highly inducible by p53 in a colon carcinoma cell line, DLD-1, and designated as PIG1 (for p53-inducedgene 1). We studied transfectants of HeLa and DLD-1 cells ectopically expressing this protein and found that they were more susceptible to apoptosis than control transfectants. This was observed in apoptosis induced by mechanistically distinct stimuli, suggesting that galectin-7 acts on a common point in the apoptosis signaling pathways. Further analyses of actinomycin D-induced apoptosis demonstrated that galectin-7 expression causes enhanced caspase-3 activity and poly(ADP-ribose) polymerase cleavage, and the potentiation of apoptosis by galectin-7 was completely abrogated by a caspase inhibitor, benzyloxycarbonyl-Val-Ala-Asp-fluoromethyl ketone. In addition, galectin-7 transfectants displayed accelerated mitochondrial cytochrome c release and up-regulated JNK activity upon apoptosis induction. Several lines of evidence indicate that the effect on apoptosis is not due to the lectin functioning extracellularly through interactions with cell surface glycoconjugates. In fact, this lectin is found to localize in nuclei and cytoplasm of the transfectants and the transformed keratinocyte line HaCaT. Therefore, galectin-7 is a pro-apoptotic protein that functions intracellularly upstream of JNK activation and cytochrome c release. DNA microarray analysis revealed genes that are differentially expressed between galectin-7 and control transfectants. Some of them are potentially contributory to this lectin's proapoptotic function and these include redox-related genes monoamine oxidase B, ryanodine receptor 2, and glutathione S-transferase Mu 3.",2002-02-01,https://www.semanticscholar.org/paper/e3b64f35624eb03594cbf277125ca50e430643dc,Journal of Biological Chemistry
3502,Approximating Disjoint-Path Problems Using Greedy Algorithms and Packing Integer Programs,,1997-10-01,https://www.semanticscholar.org/paper/f5978f6136255ba7d0d91c30c69c54140ac2c776,Conference on Integer Programming and Combinatorial Optimization
1772,Collaborative topic modeling for recommending scientific articles,"Researchers have access to large online archives of scientific articles. As a consequence, finding relevant papers has become more difficult. Newly formed online communities of researchers sharing citations provides a new way to solve this problem. In this paper, we develop an algorithm to recommend scientific articles to users of an online community. Our approach combines the merits of traditional collaborative filtering and probabilistic topic modeling. It provides an interpretable latent structure for users and items, and can form recommendations about both existing and newly published articles. We study a large subset of data from CiteULike, a bibliography sharing service, and show that our algorithm provides a more effective recommender system than traditional collaborative filtering.",2011-08-21,https://www.semanticscholar.org/paper/92eb167f30ad59f6949667021760eb41078cf85c,Knowledge Discovery and Data Mining
285,The myth of the folk theorem,"A well-known result in game theory known as ""the Folk Theorem"" suggests that finding Nash equilibria in repeated games should be easier than in one-shot games. In contrast, we show that the problem of finding any (approximate) Nash equilibrium for a three-player infinitely-repeated game is computationally intractable (even when all payoffs are in {-1,0,1}), unless all of PPAD can be solved in randomized polynomial time. This is done by showing that finding Nash equilibria of (k+1)-player infinitely-repeated games is as hard as finding Nash equilibria of k-player one-shot games, for which PPAD-hardness is known (Daskalakis, Goldberg and Papadimitriou, 2006; Chen, Deng and Teng, 2006; Chen, Teng and Valiant, 2007). This also explains why no computationally-efficient learning dynamics, such as the ""no regret"" algorithms, can be ""rational"" (in general games with three or more players) in the sense that, when one's opponents use such a strategy, it is not in general a best reply to follow suit.",2008-05-17,https://www.semanticscholar.org/paper/3ef9fdcf15411cb175c966102f637c8e1a2d1b0f,Games Econ. Behav.
1829,Supervised Topic Models,"We introduce supervised latent Dirichlet allocation (sLDA), a statistical model of labelled documents. The model accommodates a variety of response types. We derive a maximum-likelihood procedure for parameter estimation, which relies on variational approximations to handle intractable posterior expectations. Prediction problems motivate this research: we use the fitted model to predict response values for new documents. We test sLDA on two real-world problems: movie ratings predicted from reviews, and web page popularity predicted from text descriptions. We illustrate the benefits of sLDA versus modern regularized regression, as well as versus an unsupervised LDA analysis followed by a separate regression.",2007-12-03,https://www.semanticscholar.org/paper/c13aa63ccd5cf972a0a8c6b236c1dfad95b19b4e,Neural Information Processing Systems
613,Flowshop scheduling with limited temporary storage,"We examine the problem of scheduling 2-machine flowshops in order to minimize makespan, using a limited amount of intermediate storage buffers. Although there are efficient algorithms for the extreme cases of zero and infinite buffer capacities, we show that all the intermediate (finite capacity) cases are NP-complete. We prove exact bounds for the relative improvement of execution times when a given buffer capacity is used. We also analyze an efficient heuristic for solving the 1-buffer problem, showing that it has a 3/2 worst-case performance. Furthermore, we show that the ""no-wait"" (i.e., zero buffer) flowshop scheduling problem with 4 machines is NP-complete. This partly settles a well-known open question, although the 3-machine case is left open here.",1980-07-01,https://www.semanticscholar.org/paper/007089460cf583bc9ae297878d36bf23721e71c9,JACM
392,On certain rigorous approaches to data mining (invited talk) (abstract only),,2000-08-01,https://www.semanticscholar.org/paper/34272981642265777a78e5b0160e15dbc9358974,Knowledge Discovery and Data Mining
615,Local Search for the Asymmetric Traveling Salesman Problem,We present an extension of the Lin-Kernighan local search algorithm for the solution of the asymmetric traveling salesman problem. Computational results suggest that our heuristic is feasible for fairly large instances. We also present some theoretical results which guided our design of the heuristic.,1980-10-01,https://www.semanticscholar.org/paper/315b95c957a5e13d33e743b5f86a6ee996635af3,Operational Research
2688,Generating efficient virtual worlds for visualization using partial evaluation and dynamic compilation,"We argue that runtime program transformation, partial evaluation, and dynamic compilation are essential tools for automated generation of flexible, highly interactive graphical interfaces. In particular, these techniques help bridge the gap between a high-level, functional description and an efficient implementation. To support our claim, we describe our application of these techniques to a functional implementation of n-Vision, a real-time visualization system that represents multivariate relations as nested 3D interactors, and to Auto Visual, a rule-based system that designs n-Vision visualizations from high-level task specifications. n-Vision visualizations are specified using a simple functional language. These programs are transformed into a cached dataflow graph. A partial evaluator is used on particular computation-intensive function applications, and the results are compiled to native code. The functional representation simplifies generation of correct code, and the program transformations ensure good performance. We demonstrate why these transformations improve performance and why they cannot be done at compile time.",1997-12-01,https://www.semanticscholar.org/paper/6cc84504b696fae7bc2a285490bd88002d1c1ead,ACM SIGPLAN Workshop on Partial Evaluation and Program Manipulation
1628,Evaluating Bayesian Models with Posterior Dispersion Indices,"Probabilistic modeling is cyclical: we specify a model, infer its posterior, and evaluate its performance. Evaluation drives the cycle, as we revise our model based on how it performs. This requires a metric. Traditionally, predictive accuracy prevails. Yet, predictive accuracy does not tell the whole story. We propose to evaluate a model through posterior dispersion. The idea is to analyze how each datapoint fares in relation to posterior uncertainty around the hidden structure. This highlights datapoints the model struggles to explain and provides complimentary insight to datapoints with low predictive accuracy. We present a family of posterior dispersion indices (PDI) that capture this idea. We show how a PDI identifies patterns of model mismatch in three real data examples: voting preferences, supermarket shopping, and population genetics.",2017-07-17,https://www.semanticscholar.org/paper/7d520c022397773ab25feef5c5c6492bf1900999,International Conference on Machine Learning
2429,Designing AR Visualizations to Facilitate Stair Navigation for People with Low Vision,"Navigating stairs is a dangerous mobility challenge for people with low vision, who have a visual impairment that falls short of blindness. Prior research contributed systems for stair navigation that provide audio or tactile feedback, but people with low vision have usable vision and don't typically use nonvisual aids. We conducted the first exploration of augmented reality (AR) visualizations to facilitate stair navigation for people with low vision. We designed visualizations for a projection-based AR platform and smartglasses, considering the different characteristics of these platforms. For projection-based AR, we designed visual highlights that are projected directly on the stairs. In contrast, for smartglasses that have a limited vertical field of view, we designed visualizations that indicate the user's position on the stairs, without directly augmenting the stairs themselves. We evaluated our visualizations on each platform with 12 people with low vision, finding that the visualizations for projection-based AR increased participants' walking speed. Our designs on both platforms largely increased participants' self-reported psychological security.",2019-10-17,https://www.semanticscholar.org/paper/c5e254fe8571ae1cc251ac30db8fcad7eab1c753,ACM Symposium on User Interface Software and Technology
3494,An Implementation of a Combinatorial Approximation Algorithm for Minimum-Cost Multicommodity Flow,,1997-12-01,https://www.semanticscholar.org/paper/1bc0da2739f47c77abbe0770daa31b925d75f2a3,Conference on Integer Programming and Combinatorial Optimization
2362,Impaired microbial killing in two patients with defective degranulation of myeloperoxidase.,,,https://www.semanticscholar.org/paper/838554c47c99673ee9f4eb69d6009de1a1bf0646,Acta paediatrica Hungarica
3506,A new approach to the minimum cut problem,"This paper present a new approach to finding minimum cuts in undirected graphs. The fundamental principle is simple: the edges in a graph's minimum cut form an extremely small fraction of the graph's edges. Using this idea, we give a randomized, strongly polynomial algorithm that finds the minimum cut in an arbitrarily weighted undirected graph with high probability. The algorithm runs in <italic>O(n<supscrpt>2</supscrpt>log<supscrpt>3</supscrpt>n)</italic> time, a significant improvement over the previous <italic>O˜(mn)</italic> time bounds based on maximum flows. It is simple and intuitive and uses no complex data structures. Our algorithm can be parallelized to run in <italic>RNC</italic> with <italic>n<supscrpt>2</supscrpt></italic> processors; this gives the first proof   that the minimum cut problem can be solved in <italic>RNC</italic>. The algorithm does more than find a single minimum cut; it finds all of them.
With minor modifications, our algorithm solves two other problems of interest. Our algorithm finds all cuts with value within a multiplicative factor of α of the minimum cut's in expected <italic>O˜(n<supscrpt>2α</supscrpt>)</italic> time, or in <italic>RNC</italic> with <italic>n<supscrpt>2α</supscrpt></italic> processors. The problem of finding a minimum multiway cut of graph into <italic>r</italic> pieces is solved in expected <italic>O˜(n<supscrpt>2(r-1)</supscrpt>)</italic> time, or in <italic>RNC</italic> with <italic>n<supscrpt>2(r-1)</supscrpt></italic> processors. The “trace” of the    algorithm's execution on these two problems forms a new compact data structure for representing all small cuts and all multiway cuts in a graph. This data structure can be efficiently transformed into the more standard cactus representing for minimum cuts.",1996-07-01,https://www.semanticscholar.org/paper/9c4f25dc2739b8fff579264374e6ded0679958b5,JACM
3208,Linking Multiscalar Fisheries Using Metacoupling Models,"Marine fisheries are social-ecological systems important for human health and livelihoods. However, research approaches that consider human–nature interactions within as well as between adjacent and distant fisheries are scarce. As such, we measured and modeled marine fisheries catches at local and regional scales over 65 years (1950–2014), assessed cross-scalar interactions among fishing types (artisanal, subsistence, industrial, recreational), and predicted future catches using the metacoupling framework, a new approach for evaluating human-nature interactions within and across adjacent and distant fisheries (metacouplings). Across taxa examined (mahi-mahi [Coryphaena hippurus], Atlantic bluefin tuna [Thunnus thynnus], cods [Gadidae]), 75% of catches (8.5 million metric tons [MMT]) were made by nations in their own exclusive economic zones (EEZs; Type 1 fishing). However, catches in adjacent EEZs (Type 2 fishing, 1.0 MMT) and distant EEZs and the high seas (Type 3 fishing, 1.9 MMT) increased substantially for all taxa at certain times, becoming consistently important for tuna and cods after 1980. Moreover, Types 1–3 fishing interacted in ways that affect humans differentially across fisheries. For instance, tuna artisanal and subsistence catches (Type 1) decreased with increasing Type 2 and Type 3 industrial fishing, respectively. Cod subsistence catches declined with increasing Type 2/3 industrial fishing and Type 1 artisanal fishing, whereas fishing-type interactions were largely positive for mahi-mahi, causing catches to increase across sectors. Overall, metacouplings affect humans in positive and negative ways that vary across scales and fisheries systems, galvanizing the need for metacoupling-informed fisheries research, policy, and management programs.",2020-07-28,https://www.semanticscholar.org/paper/afddf32a0ea3e3107206dd3d0ae9c1ac43447052,Frontiers in Marine Science
3454,Vertex Cover Approximations: Experiments and Observations,,2005-05-10,https://www.semanticscholar.org/paper/832e5c3c51cdc75891254af9eaf4141293878d43,Workshop on Engineering Applications
1525,Starfysh reveals heterogeneous spatial dynamics in the breast tumor microenvironment,"Spatially-resolved gene expression profiling provides valuable insight into tissue organization and cell-cell crosstalk; however, spatial transcriptomics (ST) lacks single-cell resolution. Current ST analysis methods require single-cell RNA sequencing data as a reference for a rigorous interpretation of cell states and do not utilize associated histology images. Significant sample variation further complicates the integration of ST datasets, which is essential for identifying commonalities across tissues or altered cellular wiring in disease. Here, we present Starfysh, the first comprehensive computational toolbox for joint modeling of ST and histology data, dissection of refined cell states, and systematic integration of multiple ST datasets from complex tissues. Starfysh uses an auxiliary deep generative model that incorporates archetypal analysis and any known cell state markers to avoid the need for a single-cell-resolution reference in characterizing known or novel tissue-specific cell states. Additionally, Starfysh improves the characterization of spatial dynamics in complex tissues by leveraging histology images and enables the comparison of niches as spatial “hubs” across tissues. Integrative analysis of primary estrogen receptor-positive (ER+) breast cancer, triple-negative breast cancer (TNBC), and metaplastic breast cancer (MBC) tumors using Starfysh led to the identification of heterogeneous patient- and disease-specific hubs as well as a shared stromal hub with varying spatial orientation. Our results show the ability to delineate the spatial co-evolution of tumor and immune cell states and their crosstalk underlying intratumoral heterogeneity in TNBC and revealed metabolic reprogramming shaping immunosuppressive hubs in aggressive MBC. Starfysh is publicly available (https://github.com/azizilab/starfysh).",2022-11-24,https://www.semanticscholar.org/paper/76057f3f3a5816e004477837b25e742b1fa7d7f1,bioRxiv
263,"Sex, mixability, and modularity","The assumption that different genetic elements can make separate contributions to the same quantitative trait was originally made in order to reconcile biometry and Mendelism and ever since has been used in population genetics, specifically for the trait of fitness. Here we show that sex is responsible for the existence of separate genetic effects on fitness and, more generally, for the existence of a hierarchy of genetic evolutionary modules. Using the tools developed in the process, we also demonstrate that in terms of their fitness effects, separation and fusion of genes are associated with the increase and decrease of the recombination rate between them, respectively. Implications for sex and evolution theory are discussed.",2010-01-08,https://www.semanticscholar.org/paper/116b678d7a155845956de2204f6ee5151f8dd98b,Proceedings of the National Academy of Sciences of the United States of America
2502,Using an Inpatient Personal Health Record to Enhance Patient-Provider Communication,,,https://www.semanticscholar.org/paper/dedb783a986ae4031896150a9d45d12514def8df,American Medical Informatics Association Annual Symposium
3493,Task Scheduling in Networks,"Scheduling a set of tasks on a set of machines so as to yield an efficient schedule is a basic problem in computer science and operations research. Most of the research on this problem incorporates the potentially unrealistic assumption that communication between the different machines is instantaneous. In this paper we remove this assumption and study the problem of network scheduling, where each job originates at some node of a network, and in order to be processed at another node must take the time to travel through the network to that node. 
Our main contribution is to give approximation algorithms and hardness proofs for fully general forms of the fundamental problems in network scheduling. We consider two basic scheduling objectives: minimizing the makespan and minimizing the average completion time. For the makespan, we prove small constant factor hardness-to-approximate and approximation results. For the average completion time, we give a log-squared approximation algorithm for the most general form of the problem. The techniques used in this approximation are fairly general and have several other applications. For example, we give the first nontrivial approximation algorithm to minimize the average weighted completion time of a set of jobs on related or unrelated machines, with or without a network.",1997-08-01,https://www.semanticscholar.org/paper/13ae8c0c690b6f4ec3441c696536f2a103f6e733,SIAM Journal on Discrete Mathematics
63,"Proceedings of the 2004 ACM CIKM International Conference on Information and Knowledge Management, Washington, DC, USA, November 8-13, 2004",,,https://www.semanticscholar.org/paper/eea35842b6621527b5ea0f2f4f02f449a42741d5,International Conference on Information and Knowledge Management
2743,Automated generation of intent-based 3D Illustrations,"This paper describes an automated intent-based approach to illustration. An illustrution is a picture that is designed to fulfill a communicative intent such as showing the location of an object or showing how an object is manipulated. An illustration is generated by implementing a set of stylistic decisions, ranging from determining the way in which an individual object is lit, to deciding the general composition of the illustration. The design of an illustration is treated as a goal-driven process within a system of constraints. The goal is to achieve communicative intent; the constraints are the illustrative techniques an illustrator can apply.We have developed IBIS (Intent-Based Illustration System), a system that puts these ideas into practice. IBIS designs illustrations using a generate-and-test approach, relying upon a rule-based system of methods and evaluators. Methods are rules that specify how to accomplish visual effects, while evaluators are rules that specify how to determine how well a visual effect is accomplished in an illustration. Examples of illustrations designed by IBIS are included.",1991-07-01,https://www.semanticscholar.org/paper/f29476094033d28e15891c7bfa0ba5e05e12d540,International Conference on Computer Graphics and Interactive Techniques
2034,Cluster analysis of genome-wide expression data for feature extraction,,2009-03-01,https://www.semanticscholar.org/paper/a6ffd8b1c5ed2a65614f0e95ed0b6473f8d7f712,Expert systems with applications
909,On notions of information transfer in VLSI circuits,"Several papers have recently dealt with techniques for proving area-time lower bounds for VLSI computation by “crossing sequence” methods. A number of natural questions are raised by these definitions. 1.Is the fooling set approach the most powerful way to get information-transfer-based lower bounds? We shall show it is not, and offer a candidate for the title “most powerful.” Of course, without a precise definition of “information transfer argument,” there could be other contenders. 2.Are the notions of the three papers cited equivalent? We shall exhibit certain inequivalences among the three notions, although open questions remain. However, we can resolve an open question of Papadimitriou and Sipser [PS] concerning the relationship between nondeterministic and deterministic communication complexity.",1983-12-01,https://www.semanticscholar.org/paper/b2027f32f45b30c810acf95bf2748eb064c3bb90,Symposium on the Theory of Computing
1220,Search for neutral Higgs bosons in multi-b-jet events in pp[over] collisions at sqrt[s]=1.96 TeV.,Data recorded by the D0 experiment at the Fermilab Tevatron Collider are analyzed to search for neutral Higgs bosons produced in association with b quarks. This production mode can be enhanced in the minimal supersymmetric standard model (MSSM). The search is performed in the three b quark channel using multijet triggered events corresponding to an integrated luminosity of 1 fb(-1). No statistically significant excess of events with respect to the predicted background is observed and limits are set in the MSSM parameter space.,,https://www.semanticscholar.org/paper/5402271e59ac2c36c89e5098da6bae6cc47025bc,Physical Review Letters
2293,Mcl-1 expression in human neutrophils: regulation by cytokines and correlation with cell survival.,"Human neutrophils possess a very short half-life because they constitutively undergo apoptosis. Cytokines, such as granulocyte-macrophage colony-stimulating factor (GM-CSF), and other agents can rescue neutrophils from apoptosis but the molecular mechanisms involved in this rescue are undefined. Here, we show by Western blotting that human neutrophils do not express Bcl-2 or Bcl-X but constitutively express Bax. However, cellular levels of these proteins are unaffected by agents which either accelerate or delay neutrophil apoptosis. In contrast, neutrophils express the antiapoptotic protein Mcl-1 and levels of this protein correlate with neutrophil survival. Thus, cellular levels of Mcl-1 decline as neutrophils undergo apoptosis and are enhanced by agents (eg, GM-CSF, interleukin-1beta, sodium butyrate, and lipopolysaccharide) that promote neutrophil survival. Neutrophils only possess few, small mitochondria, and much of the Mcl-1 protein seems to be located in nuclear fractions. These observations provide the first evidence implicating a Bcl-2 family member in the regulation of neutrophil survival. Moreover, this work also provides a potential mechanism whereby cytokine-regulated gene expression regulates the functional lifespan of neutrophils and hence their ability to function for extended time periods during acute inflammation.",,https://www.semanticscholar.org/paper/6a622a3b1d46985f8ad7880f1c35361bf3707086,Blood
432,Towards a theory of indexability,,,https://www.semanticscholar.org/paper/3bc8f7b350b506b3bfb58e89f9882d897d5e1f7a,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems
2203,Human filarial Wolbachia lipopeptide directly activates human neutrophils in vitro,"The host inflammatory response to the Onchocerca volvulus endosymbiont, Wolbachia, is a major contributing factor in the development of chronic pathology in humans (onchocerciasis/river blindness). Recently, the toll‐like pattern recognition receptor motif of the major inflammatory ligands of filarial Wolbachia, membrane‐associated diacylated lipoproteins, was functionally defined in murine models of pathology, including mediation of neutrophil recruitment to the cornea. However, the extent to which human neutrophils can be activated in response to this Wolbachia pattern recognition motif is not known. Therefore, the responses of purified peripheral blood human neutrophils to a synthetic N‐terminal diacylated lipopeptide (WoLP) of filarial Wolbachia peptidoglycan‐associated lipoprotein (PAL) were characterized. WoLP exposure led to a dose‐dependent activation of healthy, human neutrophils that included gross morphological alterations and modulation of surface expressed integrins involved in tethering, rolling and extravasation. WoLP exposure induced chemotaxis but not chemokinesis of neutrophils, and secretion of the major neutrophil chemokine, interleukin 8. WoLP also induced and primed the respiratory burst, and enhanced neutrophil survival by delay of apoptosis. These results indicate that the major inflammatory motif of filarial Wolbachia lipoproteins directly activates human neutrophils in vitro and promotes a molecular pathway by which human neutrophils are recruited to sites of Onchocerca parasitism.",2014-09-17,https://www.semanticscholar.org/paper/116ba42b528ed8c60931bb022adc76a9bad77efc,Parasite immunology (Print)
1768,Predicting Legislative Roll Calls from Text,"We develop several predictive models linking legislative sentiment to legislative text. Our models, which draw on ideas from ideal point estimation and topic models, predict voting patterns based on the contents of bills and infer the political leanings of legislators. With supervised topics, we provide an exploratory window into how the language of the law is correlated with political support. We also derive approximate posterior inference algorithms based on variational methods. Across 12 years of legislative data, we predict specific voting patterns with high accuracy.",2011-06-28,https://www.semanticscholar.org/paper/62e14dec73970514a5e3f81b059d63b34e9ad37c,International Conference on Machine Learning
236,Optimum Statistical Estimation with Strategic Data Sources,"We propose an optimum mechanism for providing monetary incentives to the data sources of a statistical estimator such as linear regression, so that high quality data is provided at low cost, in the sense that the sum of payments and estimation error is minimized. The mechanism applies to a broad range of estimators, including linear and polynomial regression, kernel regression, and, under some additional assumptions, ridge regression. It also generalizes to several objectives, including minimizing estimation error subject to budget constraints. Besides our concrete results for regression problems, we contribute a mechanism design framework through which to design and analyze statistical estimators whose examples are supplied by workers with cost for labeling said examples.",2014-08-11,https://www.semanticscholar.org/paper/c7e3a344cacf01214382ec7c16edcf18a634c9c3,Annual Conference Computational Learning Theory
1881,Dynamic Support Vector Regression Control System for Overlay Error Compensation With Stochastic Metrology Delay,"This study aims to develop a robust monitoring system for advanced control and compensation of the overlay errors based on <inline-formula> <tex-math notation=""LaTeX"">$\boldsymbol {\epsilon }$ </tex-math></inline-formula>-insensitive support vector regression (SVR), considering metrology delay. The proposed <inline-formula> <tex-math notation=""LaTeX"">$\boldsymbol {\epsilon }$ </tex-math></inline-formula>-insensitive SVR control system has the ability to solve quadratic optimization problems in real settings. To investigate the consistency and reliability of the proposed algorithm, a simulation study based on empirical data was conducted to validate the solution quality enhancement by the proposed approach. The stability of the system under metrology delay was investigated when Lyapunov stability function takes place as the kernel function of the <inline-formula> <tex-math notation=""LaTeX"">$\boldsymbol {\epsilon }$ </tex-math></inline-formula>-insensitive SVR optimization system. For sensitivity analysis, we compared and analyzed the effect of noise and time-varying metrology delay, within an online process with a simulation study based on empirical data. This approach can effectively reduce the misalignment of the overlay errors through the self-tuning process of <inline-formula> <tex-math notation=""LaTeX"">$\boldsymbol {\epsilon }$ </tex-math></inline-formula>-insensitive SVR and provide real-time decision aid for process engineers. <italic>Note to Practitioners</italic>—In practice, there are dynamic metrology delays that have not been adequately addressed. This study developed a robust monitoring system that can consider metrology delay for advanced control and effective compensation of the overlay errors. A study based on empirical data has validated the practical viability of the proposed approach. Indeed, the proposed algorithm can obtain a high degree of reliability for the measurement data in the complicated semiconductor fabrication process. Indeed, the developed solution is implemented in real practice.",2020-01-01,https://www.semanticscholar.org/paper/00318e348c2e51e901c8c404fc5ea51d03332701,IEEE Transactions on Automation Science and Engineering
3175,Zebras of all stripes repel biting flies at close range,,2022-11-03,https://www.semanticscholar.org/paper/44331c7df0d282182757b593179155c999c1e364,Scientific Reports
859,Modularity of cycles and paths in graphs,"Certain problems related to the length of cycles and paths modulo a given integer are studied. Linear-time algorithms are presented that determine whether all cycles in an undirected graph are of length <italic>P</italic> mod <italic>Q</italic> and whether all paths between two specified nodes are of length <italic>P</italic> mod <italic>Q</italic>, for fixed integers <italic>P</italic>.<italic>Q</italic>. These results are compared to those for directed graphs.",1991-04-01,https://www.semanticscholar.org/paper/5d348bdba7f33d23d539b7f9463e26f7e41eda8f,JACM
93,STHoles: a multidimensional workload-aware histogram,"Attributes of a relation are not typically independent. Multidimensional histograms can be an effective tool for accurate multiattribute query selectivity estimation. In this paper, we introduce STHoles, a “workload-aware” histogram that allows bucket nesting to capture data regions with reasonably uniform tuple density. STHoles histograms are built without examining the data sets, but rather by just analyzing query results. Buckets are allocated where needed the most as indicated by the workload, which leads to accurate query selectivity estimations. Our extensive experiments demonstrate that STHoles histograms consistently produce good selectivity estimates across synthetic and real-world data sets and across query workloads, and, in many cases, outperform the best multidimensional histogram techniques that require access to and processing of the full data sets during histogram construction.",2001-05-01,https://www.semanticscholar.org/paper/ed920f7547ce5d0b72ce69393357a323f89c70cf,ACM SIGMOD Conference
700,The Platform Design Problem,,2020-09-13,https://www.semanticscholar.org/paper/5f392561552741d9e6aba1951bc7663e3698d417,Workshop on Internet and Network Economics
1009,Retinal Oximetry Based on Nonsimultaneous Image Acquisition Using a Conventional Fundus Camera,"To measure the retinal arteriole and venule oxygen saturation (SO2) using a conventional fundus camera, retinal oximetry based on nonsimultaneous image acquisition was developed and evaluated. Two retinal images were sequentially acquired using a conventional fundus camera with two bandpass filters (568 nm: isobestic, 600 nm: nonisobestic wavelength), one after another, instead of a built-in green filter. The images were registered to compensate for the differences caused by eye movements during the image acquisition. Retinal SO2 was measured using two wavelength oximetry. To evaluate sensitivity of the proposed method, SO2 in the arterioles and venules before and after inhalation of 100% O2 were compared, respectively, in 11 healthy subjects. After inhalation of 100% O2, SO2 increased from 96.0 ± 6.0% to 98.8% ± 7.1% in the arterioles (p = 0.002) and from 54.0 ± 8.0% to 66.7% ± 7.2% in the venules (p = 0.005) (paired t-test, n = 11). Reproducibility of the method was 2.6% and 5.2% in the arterioles and venules, respectively (average standard deviation of five measurements, n = 11).",2011-04-07,https://www.semanticscholar.org/paper/5627936cb807ddb7d18949bccdf868db9f258e10,IEEE Transactions on Medical Imaging
2156,An Experimental Evaluation of a Monte-Carlo Algorithm for Singular Value Decomposition,,2001-11-08,https://www.semanticscholar.org/paper/6700cbf389ae198190e672a3e5995b32f7a6d7c9,Panhellenic Conference on Informatics
3434,Adding Trust to P2P Distribution of Paid Content,,2009-09-04,https://www.semanticscholar.org/paper/d5116f7cc5ec69502d02459ccf83ba6427067c1c,Information Security Conference
1219,Measurement of the inclusive jet cross section in pp[over] collisions at square root [s]=1.96 TeV.,"We report on a measurement of the inclusive jet cross section in pp[over ] collisions at a center-of-mass energy sqrt[s]=1.96 TeV using data collected by the D0 experiment at the Fermilab Tevatron Collider corresponding to an integrated luminosity of 0.70 fb;{-1}. The data cover jet transverse momenta from 50 to 600 GeV and jet rapidities in the range -2.4 to 2.4. Detailed studies of correlations between systematic uncertainties in transverse momentum and rapidity are presented, and the cross section measurements are found to be in good agreement with next-to-leading order QCD calculations.",2008-02-17,https://www.semanticscholar.org/paper/4e695ffc959bed9ff6b98df95d3588adb178ac49,Physical Review Letters
3780,A large-scale benchmark dataset for event recognition in surveillance video,"We introduce a new large-scale video dataset designed to assess the performance of diverse visual event recognition algorithms with a focus on continuous visual event recognition (CVER) in outdoor areas with wide coverage. Previous datasets for action recognition are unrealistic for real-world surveillance because they consist of short clips showing one action by one individual [15, 8]. Datasets have been developed for movies [11] and sports [12], but, these actions and scene conditions do not apply effectively to surveillance videos. Our dataset consists of many outdoor scenes with actions occurring naturally by non-actors in continuously captured videos of the real world. The dataset includes large numbers of instances for 23 event types distributed throughout 29 hours of video. This data is accompanied by detailed annotations which include both moving object tracks and event examples, which will provide solid basis for large-scale evaluation. Additionally, we propose different types of evaluation modes for visual recognition tasks and evaluation metrics along with our preliminary experimental results. We believe that this dataset will stimulate diverse aspects of computer vision research and help us to advance the CVER tasks in the years ahead.",2011-06-20,https://www.semanticscholar.org/paper/89d80d34bc0cf1cc9f62a76c0ffa5b01f05b9ee7,Computer Vision and Pattern Recognition
3194,Increased vigilance of plains zebras (Equus quagga) in response to more bush coverage in a Kenyan savanna,,2021-02-22,https://www.semanticscholar.org/paper/6e79bc7a884ff76b701f3c364a1f502e6731424a,Climate Change Ecology
769,Testing hierarchical systems,"We investigate the testing of hierarchical (modular) systems, in which individual modules are modeled by finite state machines. Given a hierarchical system, we are interested in finding a small set of tests that exercises all the transitions of the system. We present tight approximation algorithms and hardness results for the problem. Our techniques extend to other criteria and metrics.",2005-01-23,https://www.semanticscholar.org/paper/7a0be5eddcd5d3e131fa9b02d90d07f30abe22ea,ACM-SIAM Symposium on Discrete Algorithms
828,Distinguishing tests for nondeterministic and probabilistic machines,"We study the problem of uniquely identifying the initial state of a given finite-state machine from among a set of possible choices, based on the input-output behavior. Equivalently, given a set of machines, the problem is to design a test that distinguishes among them. We consider nondeterministic machines as well as probabilistic machines. In both cases, we show that it is PsPAcE-complete to decide whether there is a preset distinguishing strategy (i.e. a sequence of inputs fixed in advance), and it is ExPTIME-complete to decide whether there is an adaptive distinguishing strategy (i.e. when the next input can be chosen based on the outputs observed so far). The probabilistic testing is closely related to probabilistic games, or Markov Decision Processes, with incomplete information. We also provide optimal bounds for deciding whether such games have strategies winning with probability 1.",1995-05-29,https://www.semanticscholar.org/paper/b9907976c3ac7f6bd63f18db3459f59a04b852e4,Symposium on the Theory of Computing
1551,Correction to: Counterfactual inference for consumer choice across many product categories,,2021-12-01,https://www.semanticscholar.org/paper/e597faf0a561de858600ccead4e44ff782a37c3d,Quantitative Marketing and Economics
1279,Lifetime difference and CP-violating phase in the Bs0 system.,"From an analysis of the decay Bs0-->J/psi phi, we obtain the width difference between the light and heavy mass eigenstates DeltaGamma identical with (GammaL-GammaH)=0.17+/-0.09(stat)+/-0.02(syst) ps-1 and the CP-violating phase phis=-0.79+/-0.56(stat)(-0.01)(+0.14)(syst). Under the hypothesis of no CP violation (phis identical with 0), we obtain 1/Gamma=tau(Bs0)=1.52+/-0.08(stat)(-0.03)(+0.01)(syst) ps and DeltaGamma=0.12(-0.10)(+0.08)(stat)+/-0.02(syst) ps-1. The data sample corresponds to an integrated luminosity of about 1.1 fb-1 accumulated with the D0 detector at the Fermilab Tevatron collider. This is the first direct measurement of the CP-violating mixing phase in the Bs0 system.",2007-01-09,https://www.semanticscholar.org/paper/55279c2b8f4813b1f0c617b5cf7fb47978314d63,Physical Review Letters
3380,A Competitive Algorithm for Throughout Maximization on Identical Machines,This paper considers the basic problem of scheduling jobs online with preemption to maximize the number of jobs completed by their deadline on $m$ identical machines. The main result is an $O(1)$ competitive deterministic algorithm for any number of machines $m>1$.,2021-11-12,https://www.semanticscholar.org/paper/0336879a1746aa5f2032159c7f6e5d00432acb4f,arXiv.org
1879,Adaptive parametric yield enhancement via collinear multivariate analytics for semiconductor intelligent manufacturing,,2021-04-01,https://www.semanticscholar.org/paper/66245084511f29e39aba1c7e5533e80108016b11,Applied Soft Computing
1047,"Projected sensitivity of the LUX-ZEPLIN experiment to the two-neutrino and neutrinoless double 
β
 decays of 
Xe134","The projected sensitivity of the LUX-ZEPLIN (LZ) experiment to two-neutrino and neutrinoless double β decay of 134 Xe is presented. LZ is a 10-tonne xenon time-projection chamber optimized for the detection of dark matter particles and is expected to start operating in 2021 at Sanford Underground Research Facility, USA. Its large mass of natural xenon provides an exceptional opportunity to search for the double β decay of 134 Xe, for which xenon detectors enriched in 136 Xe are less effective. For the two-neutrino decay mode, LZ is predicted to exclude values of the half-life up to 1 . 7 × 10 24 years at 90% conﬁdence level (CL) and has a three-sigma observation potential of 8 . 7 × 10 23 years, approaching the predictions of nuclear models. For the neutrinoless decay mode LZ, is projected to exclude values of the half-life up to 7 . 3 × 10 24 years at 90% CL.",2021-04-26,https://www.semanticscholar.org/paper/907a9b8dcdb2d487c548cbb9d2fec854bf2ed109,Physical Review C
521,Exploring an unknown graph,"It is desired to explore all edges of an unknown directed, strongly connected graph. At each point one has a map of all nodes and edges visited, one can recognize these nodes and edges upon seeing them again, and it is known how many unexplored edges emanate from each node visited. The goal is to minimize the ratio of the total number of edges traversed to the optimum number of traversals had the graph been known. For Eulerian graphs this ratio cannot be better than 2, and 2 is achievable by a simple algorithm. In contrast, the ratio is unbounded when the deficiency of the graph (the number of edges that have to be added to make it Eulerian) is unbounded. The main result is an algorithm that achieves a bounded ratio when the deficiency is bounded; unfortunately the ratio is exponential in the deficiency. It is also shown that, when partial information about the graph is available, minimizing the worst-case ratio is PSPACE-complete.<<ETX>>",1990-10-22,https://www.semanticscholar.org/paper/ffe64fd3019d58fb86915aeb9108c3a7757eef37,Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
2879,Role of the carboxyl-terminal lectin domain in self-association of galectin-3.,"Galectin-3 is a member of a large family of beta-galactoside-binding animal lectins and is composed of a carboxyl-terminal lectin domain connected to an amino-terminal nonlectin part. Previous experimental results suggest that, when bound to multivalent glycoconjugates, galectin-3 self-associates through intermolecular interactions involving the amino-terminal domain. In this study, we obtained evidence suggesting that the protein self-associates in the absence of its saccharide ligands, in a manner that is dependent on the carboxyl-terminal domain. This mode of self-association is inhibitable by the lectin's saccharide ligands. Specifically, recombinant human galectin-3 was found to bind to galectin-3C (the carboxyl-terminal domain fragment) conjugated to Sepharose 4B and the binding was inhibitable by lactose. In addition, biotinylated galectin-3 bound to galectin-3 immobilized on plastic surfaces and the binding could also be inhibited by various saccharide ligands of the lectin. A mutant with a tryptophan to leucine replacement in the carboxyl-terminal domain, which exhibited diminished carbohydrate-binding activity, did not bind to galectin-3C-Sepharose 4B. Furthermore, galectin-3C formed covalent homodimers when it was treated with a chemical cross-linker and the dimer formation was completely inhibited by lactose. Therefore, galectin-3 can self-associate through intermolecular interactions involving both the amino- and the carboxyl-terminal domains and the relative contribution of each depends on whether the lectin is bound to its saccharide ligands.",1998-03-05,https://www.semanticscholar.org/paper/6fd7ef1aca7d472d622ff5df2cfbd8734b47bab6,Biochemistry
99,Computing Geographical Scopes of Web Resources,"Many information resources on the web are relevant primarily to limited geographical communities. For instance, web sites containing information on restaurants, theaters, and apartment rentals are relevant primarily to web users in geographical proximity to these locations. In contrast, other information resources are relevant to a broader geographical community. For instance, an on-line newspaper may be relevant to users across the United States. Unfortunately, current web search engines largely ignore the geographical scope of web resources. In this paper, we introduce techniques for automatically computing the geographical scope of web resources, based on the textual content of the resources, as well as on the geographical distribution of hyperlinks to them. We report an extensive experimental evaluation of our strategies using real web data. Finally, we describe a geographicallyaware search engine that we have built to showcase our techniques.",2000-09-10,https://www.semanticscholar.org/paper/4abe2165b0f43588134dc06ea4b4e917ba74924e,Very Large Data Bases Conference
1593,Estimating Heterogeneous Consumer Preferences for Restaurants and Travel Time Using Mobile Location Data,"This paper analyzes consumer choices over lunchtime restaurants using data from a sample of several thousand anonymous mobile phone users in the San Francisco Bay Area. The data is used to identify users' approximate typical morning location, as well as their choices of lunchtime restaurants. We build a model where restaurants have latent characteristics (whose distribution may depend on restaurant observables, such as star ratings, food category, and price range), each user has preferences for these latent characteristics, and these preferences are heterogeneous across users. Similarly, each item has latent characteristics that describe users' willingness to travel to the restaurant, and each user has individual-specific preferences for those latent characteristics. Thus, both users' willingness to travel and their base utility for each restaurant vary across user-restaurant pairs. We use a Bayesian approach to estimation. To make the estimation computationally feasible, we rely on variational inference to approximate the posterior distribution, as well as stochastic gradient descent as a computational approach. Our model performs better than more standard competing models such as multinomial logit and nested logit models, in part due to the personalization of the estimates. We analyze how consumers re-allocate their demand after a restaurant closes to nearby restaurants versus more distant restaurants with similar characteristics, and we compare our predictions to actual outcomes. Finally, we show how the model can be used to analyze counterfactual questions such as what type of restaurant would attract the most consumers in a given location.",2018-01-22,https://www.semanticscholar.org/paper/394929d51af693e42063f6e033add1db35096553,arXiv.org
3097,pTHINC: a thin-client architecture for mobile wireless web,"Although web applications are gaining popularity on mobile wireless PDAs, web browsers on these systems can be quite slow and often lack adequate functionality to access many web sites. We have developed pTHINC, a PDA thin-client solution that leverages more powerful servers to run full-function web browsers and other application logic, then sends simple screen updates to the PDA for display. pTHINC uses server-side screen scaling to provide high-fidelity display and seamless mobility across a broad range of different clients and screen sizes, including both portrait and landscape viewing modes. pTHINC also leverages existing PDA control buttons to improve system usability and maximize available screen resolution for application display. We have implemented pTHINC on Windows Mobile and evaluated its performance on mobile wireless devices. Our results compared to local PDA web browsers and other thin-client approaches demonstrate that pTHINC provides superior web browsing performance and is the only PDA thin client that effectively supports crucial browser helper applications such as video playback.",2006-05-23,https://www.semanticscholar.org/paper/69a3aa909aec09f6a33631a266ac4b696c4664b5,The Web Conference
972,Position of Central Vascular Trunk and Shape of Optic Nerve Head in Newborns.,"Purpose
To investigate the baseline position of the central vascular trunk (CVT) and the characteristics of the optic nerve head (ONH) in newborns.


Methods
CVT position was evaluated based on fundus images obtained from newborns who had undergone eye-screening examinations. It was then graded according to the optic disc area as follows: grade 1, within central 4%; grade 2, within central 9%; grade 3, within central 16%; grade 4, within central 25%; grade 5, outside central 25% of optic disc area. The direction of the CVT position was determined in cases of grade 2 or more as superior, inferior, nasal, and temporal, relative to the optic disc center. The ovality index and the vertical cup-to-disc ratio were determined as well.


Results
In 1000 fundus images from 1000 newborns, 87.1% showed grade 1 (95% confidence interval 84.7-88.8), and 10.7% showed grade 2. The most common CVT direction was central (87.1%, grade 1), followed by nasal (11.0%) and inferior (1.2%). The ovality index was 1.28 ± 0.09 (range, 1.01-1.61). The ONH shape was vertically oval and highly uniform. The average vertical cup-to-disc ratio was 0.29 ± 0.13 (range, 0.00-0.67).


Conclusions
The CVT of newborns was located in the central area of the ONH in most cases. The shape of the optic disc was vertically oval, and very similar among the newborns. Considering the high variability of ONH morphology and the diverse location of the CVT in adults, our result suggests that the shape of the ONH and the CVT position might change during eyeball growth.",2019-08-01,https://www.semanticscholar.org/paper/68009394da5c0c9bdea392e6e4225a3d9ed497fd,Investigative Ophthalmology and Visual Science
1564,General linear-time inference for Gaussian Processes on one dimension,"Gaussian Processes (GPs) provide a powerful probabilistic framework for interpolation, forecasting, and smoothing, but have been hampered by computational scaling issues. Here we prove that for data sampled on one dimension (e.g., a time series sampled at arbitrarily-spaced intervals), approximate GP inference at any desired level of accuracy requires computational effort that scales linearly with the number of observations; this new theorem enables inference on much larger datasets than was previously feasible. To achieve this improved scaling we propose a new family of stationary covariance kernels: the Latent Exponentially Generated (LEG) family, which admits a convenient stable state-space representation that allows linear-time inference. We prove that any continuous integrable stationary kernel can be approximated arbitrarily well by some member of the LEG family. The proof draws connections to Spectral Mixture Kernels, providing new insight about the flexibility of this popular family of kernels. We propose parallelized algorithms for performing inference and learning in the LEG model, test the algorithm on real and synthetic data, and demonstrate scaling to datasets with billions of samples.",2020-03-11,https://www.semanticscholar.org/paper/ee56f900dcfea827590a4fa9d50df4db03c7c551,arXiv.org
246,An Algorithmic View of the Universe,"In the years since Alan Turing, and following his lead, computer scientists advanced their understanding of computational phenomena by developing a very specialized, original and penetrating way of rigorous thinking. Now it turns out that this ""algorithmic"" way of thinking can be applied productively to the study of important phenomena outside computation proper (examples: the cell, the brain, the market, the universe, indeed mathematical truth itself). This development is an exquisite unintended consequence of the fact that there is latent computation underlying each of these phenomena, or the ways in which science studies them.",2012-06-15,https://www.semanticscholar.org/paper/99c913f58fb3aaf365fc67a7627ce6fafe7942c6,ACM-TURING '12
291,A mixability theory for the role of sex in evolution,"The question of what role sex plays in evolution is still open despite decades of research. It has often been assumed that sex should facilitate the increase in fitness. Hence, the fact that it may break down highly favorable genetic combinations has been seen as a problem. Here, we consider an alternative approach. We define a measure that represents the ability of alleles to perform well across different combinations and, using numerical iterations within a classical population-genetic framework, show that selection in the presence of sex favors this ability in a highly robust manner. We also show that the mechanism responsible for this effect has been out of the purview of previous theory, because it operates during the evolutionary transient, and that the breaking down of favorable genetic combinations is an integral part of it. Implications of these results and more to evolutionary theory are discussed.",2008-12-16,https://www.semanticscholar.org/paper/7be7d4c57d8b7ff865ff53217e30cc82655de9fd,Proceedings of the National Academy of Sciences of the United States of America
117,STARTS: Stanford proposal for Internet meta-searching,"Document sources are available everywhere, both within the internal networks of organizations and on the Internet. Even individual organizations use search engines from different vendors to index their internal document collections. These search engines are typically incompatible in that they support different query models and interfaces, they do not return enough information with the query results for adequate merging of the results, and finally, in that they do not export metadata about the collections that they index (e.g., to assist in resource discovery). This paper describes STARTS, an emerging protocol for Internet retrieval and search that facilitates the task of querying multiple document sources. STARTS has been developed in a unique way. It is not a standard, but a group effort coordinated by Stanford's Digital Library project, and involving over 11 companies and organizations. The objective of this paper is not only to give an overview of the STARTS protocol proposal, but also to discuss the process that led to its definition.",1997-06-01,https://www.semanticscholar.org/paper/5296edc2dae04f759bc06f959ba83989920f6182,ACM SIGMOD Conference
2092,A container packing support system for determining and visualizing container packing patterns,,2004-04-01,https://www.semanticscholar.org/paper/019bf14116cdf17f1a609f8818a34393d716db48,Decision Support Systems
951,Central retinal vascular trunk deviation in unilateral normal-tension glaucoma,"Purpose To investigate whether the position of the central retinal vascular trunk (CRVT), as a surrogate of lamina cribrosa (LC) offset, was associated with the presence of glaucoma in normal-tension glaucoma (NTG) patients. Methods The position of the CRVT was measured as the deviation from the center of the Bruch’s membrane opening (BMO), as delineated by spectral-domain optical coherence tomography imaging. The offset index was calculated as the distance of the CRVT from the BMO center relative to that of the BMO margin. The angular deviation of CRVT was measured with the horizontal nasal midline as 0° and the superior location as a positive value. The offset index and angular deviation were compared between glaucoma and fellow control eyes within individuals. Results NTG eyes had higher baseline intraocular pressure (P = 0.001), a larger β-zone parapapillary atrophy area (P = 0.013), and a larger offset index (P<0.001). In a generalized linear mixed-effects model, larger offset index was the only risk factor of NTG diagnosis (OR = 31.625, P<0.001). A generalized estimating equation regression model revealed that the offset index was larger in the NTG eyes than in the control eyes for all ranges of axial length, while it was the smallest for the axial length of 23.4 mm (all P<0.001). Conclusions The offset index was larger in the unilateral NTG eyes, which fact is suggestive of the potential role of LC/BMO offset as a loco-regional susceptibility factor.",2021-07-20,https://www.semanticscholar.org/paper/2623e58e75be11428f65ceb460d95fc5582609b1,PLoS ONE
1006,Risk factors for primary open-angle glaucoma in South Korea: the Namil study,,2012-06-05,https://www.semanticscholar.org/paper/c37bea7aa947f20b13459fe47f78e9afbab61487,Japanese Journal of Ophthalmology
321,Computing pure nash equilibria in graphical games via markov random fields,"We present a reduction from graphical games to Markov random fields so that pure Nash equilibria in the former can be found by statistical inference on the latter. Our result, when combined with the junction tree algorithm for statistical inference, yields a unified proof of all previously known tractable cases of the NP-complete problem of finding pure Nash equilibria in graphical games, but also implies efficient algorithms for new classes, such as the games with O(log n) treewidth. Furthermore, this important problem becomes susceptible to a wealth of sophisticated and empirically successful techniques from Machine Learning.",2006-06-11,https://www.semanticscholar.org/paper/96516223d950939f19a0536c34d962cf33b0abb6,ACM Conference on Economics and Computation
2960,Allele-specific expression reveals interactions between genetic variation and environment,,2015-09-13,https://www.semanticscholar.org/paper/de683687eec3674612e8db66723629cfe20e583e,Nature Methods
878,"Pfaffian orientations, 0-1 permanents, and even cycles in directed graphs",,1989-09-01,https://www.semanticscholar.org/paper/4efb49c67ced92e20385af11db5cfd71bf3fe256,Discrete Applied Mathematics
3059,"Transparent, lightweight application execution replay on commodity multiprocessor operating systems","We present Scribe, the first system to provide transparent, low-overhead application record-replay and the ability to go live from replayed execution. Scribe introduces new lightweight operating system mechanisms, rendezvous and sync points, to efficiently record nondeterministic interactions such as related system calls, signals, and shared memory accesses. Rendezvous points make a partial ordering of execution based on system call dependencies sufficient for replay, avoiding the recording overhead of maintaining an exact execution ordering. Sync points convert asynchronous interactions that can occur at arbitrary times into synchronous events that are much easier to record and replay.
 We have implemented Scribe without changing, relinking, or recompiling applications, libraries, or operating system kernels, and without any specialized hardware support such as hardware performance counters. It works on commodity Linux operating systems, and commodity multi-core and multiprocessor hardware. Our results show for the first time that an operating system mechanism can correctly and transparently record and replay multi-process and multi-threaded applications on commodity multiprocessors. Scribe recording overhead is less than 2.5% for server applications including Apache and MySQL, and less than 15% for desktop applications including Firefox, Acrobat, OpenOffice, parallel kernel compilation, and movie playback.",2010-06-12,https://www.semanticscholar.org/paper/72657b0428f9b8f705546eb5a9147203a534d8f6,Measurement and Modeling of Computer Systems
3744,Actor-Centric Relation Network,,2018-07-28,https://www.semanticscholar.org/paper/6b325af08a63bc8f94c39ae1c12509dce23d755c,European Conference on Computer Vision
3457,A parallel algorithm for approximating the minimum cycle cover,,,https://www.semanticscholar.org/paper/929a33c5ec2083432e6348995e0e0115cdc527a4,Algorithmica
2807,Comparative transcriptomic analyses of atopic dermatitis and psoriasis reveal shared neutrophilic inflammation.,,2012-12-01,https://www.semanticscholar.org/paper/70c39850e22935773d4012c7e8b0ff6bde14be34,Journal of Allergy and Clinical Immunology
676,Stochastic Neighbor Embedding under f-divergences,"The t-distributed Stochastic Neighbor Embedding (t-SNE) is a powerful and popular method for visualizing high-dimensional data. It minimizes the Kullback-Leibler (KL) divergence between the original and embedded data distributions. In this work, we propose extending this method to other f-divergences. We analytically and empirically evaluate the types of latent structure-manifold, cluster, and hierarchical-that are well-captured using both the original KL-divergence as well as the proposed f-divergence generalization, and find that different divergences perform better for different types of structure. 
A common concern with $t$-SNE criterion is that it is optimized using gradient descent, and can become stuck in poor local minima. We propose optimizing the f-divergence based loss criteria by minimizing a variational bound. This typically performs better than optimizing the primal form, and our experiments show that it can improve upon the embedding results obtained from the original $t$-SNE criterion as well.",2018-11-03,https://www.semanticscholar.org/paper/c15038da6acbc23df3c21ccf7f890c2c46b25322,arXiv.org
876,The input/output complexity of transitive closure,,1990-05-01,https://www.semanticscholar.org/paper/fbee9cc952cebd87aa46245435072fc5013febee,ACM SIGMOD Conference
1566,Variational Bayes under Model Misspecification,"Variational Bayes (VB) is a scalable alternative to Markov chain Monte Carlo (MCMC) for Bayesian posterior inference. Though popular, VB comes with few theoretical guarantees, most of which focus on well-specified models. However, models are rarely well-specified in practice. In this work, we study VB under model misspecification. We prove the VB posterior is asymptotically normal and centers at the value that minimizes the Kullback-Leibler (KL) divergence to the true data-generating distribution. Moreover, the VB posterior mean centers at the same value and is also asymptotically normal. These results generalize the variational Bernstein--von Mises theorem [29] to misspecified models. As a consequence of these results, we find that the model misspecification error dominates the variational approximation error in VB posterior predictive distributions. It explains the widely observed phenomenon that VB achieves comparable predictive accuracy with MCMC even though VB uses an approximating family. As illustrations, we study VB under three forms of model misspecification, ranging from model over-/under-dispersion to latent dimensionality misspecification. We conduct two simulation studies that demonstrate the theoretical results.",2019-05-26,https://www.semanticscholar.org/paper/0a538c7e1f943a30f048eec0a3b0dfff25062b1a,Neural Information Processing Systems
2206,A lack of confirmation with alternative assays questions the validity of IL-17A expression in human neutrophils using immunohistochemistry.,,2014-12-01,https://www.semanticscholar.org/paper/31a15d46e7b827789c555cf60e18741593ca33ef,Immunology Letters
984,Effect of Nitric Oxide on Human Corneal Epithelial Cell Viability and Corneal Wound Healing,,2017-08-14,https://www.semanticscholar.org/paper/25d2fcc9c45916e6a3c6eae7d56f3b33f3099307,Scientific Reports
2609,Turning VR inside out: thoughts about where we are heading,"Our field and the world have changed greatly in the ten years since the first VRST was held in Singapore in 1994. Computers have grown smaller, faster, and cheaper, while polygon counts, frame rates, and display resolutions have increased impressively, true to the promise of Moore's Law. But, what comes next?This talk will sketch some of the directions in which I feel virtual reality is (or should be) heading. I will discuss the potential for taking virtual reality outside, through wearable and mobile computing; for bring the outside in, by capturing the real world; and for accommodating large numbers of displays, users, and tasks, by embedding them in a fluid and collaborative augmented environment.",2004-11-10,https://www.semanticscholar.org/paper/da81db575f5a46de1d08cc50714a7cdb90a9f51c,Virtual Reality Software and Technology
1923,Modeling collinear WATs for parametric yield enhancement in semiconductor manufacturing,"Yield enhancement is one of the major objectives for semiconductor manufacturing. In addition to the usual functional yield that needs to achieve a much higher ratio of good dies in a wafer, parametric yield requires other important factors that can improve the quality and capability of the wafer. Owing to the much more complexity for the processes of wafers in today's semiconductor technology, it is difficult to find the root causes that may affect the parameter yield. Based on the selected important factors that may affect the parametric yield, this study aims to provide an efficient analytic framework for modeling the factors with high correlations. A modified PLS approach (mPLS) is applied in this study for model building that can simultaneously handle collinear problems and provide reasonable explanations with physical meaning.",2017-08-01,https://www.semanticscholar.org/paper/bc5a968d9bec1f69db8d1a579fdb7a242abadce4,CASE
2784,A small molecule CCR2 antagonist depletes tumor macrophages and synergizes with anti-PD1 in a murine model of cutaneous T cell lymphoma (CTCL).,,2020-01-13,https://www.semanticscholar.org/paper/3054c107cbf9d2803463b040e766e341e71817d6,Journal of Investigative Dermatology
60,When one sample is not enough: improving text database selection using shrinkage,"Database selection is an important step when searching over large numbers of distributed text databases. The database selection task relies on statistical summaries of the database contents, which are not typically exported by databases. Previous research has developed algorithms for constructing an approximate content summary of a text database from a small document sample extracted via querying. Unfortunately, Zipf's law practically guarantees that content summaries built this way for any relatively large database will fail to cover many low-frequency words. Incomplete content summaries might negatively affect the database selection process, especially for short queries with infrequent words. To improve the coverage of approximate content summaries, we build on the observation that topically similar databases tend to have related vocabularies. Therefore, the approximate content summaries of topically related databases can complement each other and increase their coverage. Specifically, we exploit a (given or derived) hierarchical categorization of the databases and adapt the notion of ""shrinkage"" -a form of smoothing that has been used successfully for document classification-to the content summary construction task. A thorough evaluation over 315 real web databases as well as over TREC data suggests that the shrinkage-based content summaries are substantially more complete than their ""unshrunk"" counterparts. We also describe how to modify existing database selection algorithms to adaptively decide -at run-time-whether to apply shrinkage for a query. Our experiments, which rely on TREC data sets, queries, and the associated ""relevance judgments,"" show that our shrinkage-based approach significantly improves state-of-the-art database selection algorithms, and also outperforms a recently proposed hierarchical strategy that exploits database classification as well.",2004-06-13,https://www.semanticscholar.org/paper/71de6625e881220fad562972f0af288cb5649e85,ACM SIGMOD Conference
2180,Human neutrophils activated via TLR8 promote Th17 polarization through IL‐23,"Human neutrophils contribute to the regulation of inflammation via the generation of a range of cytokines that affect all elements of the immune system. Here, we investigated their ability to express some of the members of the IL‐12 family after incubation with TLR8 agonists. Highly pure human neutrophils were thus incubated for up to 48 h with or without R848, or other TLR8 agonists, to then measure the expression levels of transcripts and proteins for IL‐12 family member subunits by RNA‐seq, reverse transcription quantitative PCR, and ELISA. We show a TLR8‐mediated inducible expression of IL‐12B and IL‐23A, but not IL‐12A, mRNA, which occurs via chromatin remodeling (as assessed by ChIP‐seq), and subsequent production of IL‐23 and IL‐12B, but no IL‐12, proteins. Induction of IL‐23 requires endogenous TNF‐α, as both mRNA and protein levels were blocked in TLR8‐activated neutrophils via a TNF‐α‐neutralizing Ab. We also show that supernatants from TLR8‐activated neutrophils, but not autologous monocytes, induce the differentiation of Th17 cells from naïve T cells in an IL‐23‐dependent fashion. This study unequivocally demonstrates that highly pure human neutrophils express and produce IL‐23, further supporting the key roles played by these cells in the important IL‐17/IL‐23 network and Th17 responses.",2019-06-01,https://www.semanticscholar.org/paper/7112dd22a835bf5efcfd79a514e42c7cea409764,Journal of Leukocyte Biology
2720,Supporting interactivity in automated 3D illustrations,"An interactive intent-based illustration is a picture designed to satisfy an input communicative intent, and which can be interactively redesigned as it is viewed. We describe how the architecture of IBIS (Intent-Based Illustration System) automates the design of 3D interactive intent-based illustrations. The types of interaction that IBIS supports include changes in the world, as objects move and otherwise change state; changes in the communicative intent, as modified by the user or other programs; queries, in which the user can request additional information; and selfeva.luation, in which other programs can request IBIS to analyze various properties of the illustration, such as how well an illustration’s communicative intent is satisfied or which graphical techniques it uses. We show how IBIS’s design process and rule base make possible these forms of interaction.",1993-02-01,https://www.semanticscholar.org/paper/ae76c05e80e2b5f7bdbfdce369bd6ec58150b1d8,International Conference on Intelligent User Interfaces
3521,Long Tours and Short Superstrings (Preliminary Version),,,https://www.semanticscholar.org/paper/f14b57f477b4024ab2b584617d3dc7e10a622b2c,IEEE Annual Symposium on Foundations of Computer Science
1805,Reading Tea Leaves: How Humans Interpret Topic Models,"Probabilistic topic models are a popular tool for the unsupervised analysis of text, providing both a predictive model of future text and a latent topic representation of the corpus. Practitioners typically assume that the latent space is semantically meaningful. It is used to check models, summarize the corpus, and guide exploration of its contents. However, whether the latent space is interpretable is in need of quantitative evaluation. In this paper, we present new quantitative methods for measuring semantic meaning in inferred topics. We back these measures with large-scale user studies, showing that they capture aspects of the model that are undetected by previous measures of model quality based on held-out likelihood. Surprisingly, topic models which perform better on held-out likelihood may infer less semantically meaningful topics.",2009-12-07,https://www.semanticscholar.org/paper/87156b2e9b7eadd04af1438d0c7d3e733a2ecb84,Neural Information Processing Systems
2130,A hardware-software co-simulator for embedded system design and debugging,"One of the interesting problems in hardware-software co-design is that of debugging embedded software in conjunction with hardware. Currently, most software designers wait until a working hardware prototype is available before debugging software. Bugs discovered in hardware during the software debugging phase require re-design and re-fabrication, thereby not only delaying the project but also increasing cost. It also puts software debugging on hold until a new hardware prototype is available. In this paper we describe a hardware-software co-simulator that can be used in the design, debugging and verification of embedded systems. This tool contains simulators for different parts of the system and a backplane which is used to integrate the simulators. This enables us to simulate hardware, software and their interaction efficiently. We also address the problem of simulation speed. Currently, the more accurate (in terms of timing) the models used, the longer it takes to simulate a system. Our main contribution is a set of techniques to speed up simulation of processors and peripherals without significant loss in timing accuracy. Finally, we describe applications used to test the co-simulator and our experience in using it.",1995-08-01,https://www.semanticscholar.org/paper/038ca383073178be26c238a5218a627e5dc20bd9,Proceedings of ASP-DAC'95/CHDL'95/VLSI'95 with EDA Technofair
852,Tie-breaking semantics and structural totality,"We address the question of when the structure of a Datalog program with negation guarantees the existence of a fixpoint. We propose a semantics of Datalog programs with negation, which we call the tie–breaking semantics. The tie–breaking semantics can be computed in polynomial time, and results in a fix-point whenever the rule–goal graph of the program has no cycle with an odd number of negative edges. We show that, in some well-defined sense, this is the most general fixpoint semantics of negation possible; in particular we show that if a cycle with an odd number of negative edges is present, then the logic program is not structurally total, that is, it has an alphabetic variant which has no fixpoint semantics whatsoever. Determining whether a program is (nonstructurally) total is undecidable.",1992-07-01,https://www.semanticscholar.org/paper/c5fe320a1643d77975ccfa05394d31dfa5b0a4d9,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems
240,Satisfiability and Evolution,"We show that, if truth assignments on n variables reproduce through recombination so that satisfaction of a particular Boolean function confers a small evolutionary advantage, then a polynomially large population over polynomially many generations (polynomial in n and the inverse of the initial satisfaction probability) will end up almost certainly consisting exclusively of satisfying truth assignments. We argue that this theorem sheds light on the problem of the evolution of complex adaptations.",2013-12-06,https://www.semanticscholar.org/paper/f1f66ec5dfc47bdd96fe6771b049304f235f7152,IEEE Annual Symposium on Foundations of Computer Science
1517,Practical and Asymptotically Exact Conditional Sampling in Diffusion Models,"Diffusion models have been successful on a range of conditional generation tasks including molecular design and text-to-image generation. However, these achievements have primarily depended on task-specific conditional training or error-prone heuristic approximations. Ideally, a conditional generation method should provide exact samples for a broad range of conditional distributions without requiring task-specific training. To this end, we introduce the Twisted Diffusion Sampler, or TDS. TDS is a sequential Monte Carlo (SMC) algorithm that targets the conditional distributions of diffusion models. The main idea is to use twisting, an SMC technique that enjoys good computational efficiency, to incorporate heuristic approximations without compromising asymptotic exactness. We first find in simulation and on MNIST image inpainting and class-conditional generation tasks that TDS provides a computational statistical trade-off, yielding more accurate approximations with many particles but with empirical improvements over heuristics with as few as two particles. We then turn to motif-scaffolding, a core task in protein design, using a TDS extension to Riemannian diffusion models. On benchmark test cases, TDS allows flexible conditioning criteria and often outperforms the state of the art.",2023-06-30,https://www.semanticscholar.org/paper/79531b47bb27cb18022891eb2ab1fcb41745fca6,arXiv.org
516,EXPLORING AN UNKNOWN GRAPH (Extended Abstract),"We wish to explore all edges of an unknown directed, strongly connected graph. At each point we have a map of all nodes and edges we have visited, we can recognize these nodes and edges if we see them again, and we know how many unexplored edges emanate from each node we have visited (but we cannot tell where each leads until we follow it). We wish to minimize the ratio of the total number of edges traversed divided by the optimum number of traversals, had we known the graph. For Eulerian graphs this ratio cannot be better than two, and two is achievable by a simple algorithm. In contrast, the ratio is unbounded when the deficiency of the graph (the number of edges that have to be added to make it Eulerian) is unbounded. Our main result is an algorithm that achieves a bounded ratio when the deficiency is bounded; unfortunately, the ratio is ex- ponential in the deficiency. We also show that, when partial information about the graph is available, min- imizing the worst-case ratio is PSPACE-complete.",,https://www.semanticscholar.org/paper/9945344efd5e75b0e371b56fc42e65885b2ebe64,IEEE Annual Symposium on Foundations of Computer Science
3776,Efficiently Scaling up Crowdsourced Video Annotation,,2012-09-05,https://www.semanticscholar.org/paper/981e7c22aaeb7756e2f7bb33186d44b4929bd76e,International Journal of Computer Vision
689,Learning the structure of manifolds using random projections,We present a simple variant of the k-d tree which automatically adapts to intrinsic low dimensional structure in data.,2007-12-03,https://www.semanticscholar.org/paper/ba0ef595dd32c6d770bc7bc1c155061512f9389b,Neural Information Processing Systems
24,Automatic Identification and Presentation of Twitter Content for Planned Events,"
 
 We demonstrate a system for augmenting information about planned events with Twitter messages, using a set of automatic query building strategies. We present two alternative interfaces to our system, namely, a browser plug-in and a customizable Web interface.
 
",2011-07-05,https://www.semanticscholar.org/paper/b16f594e3ed94e864cf4aa1ff183532107bffed0,International Conference on Web and Social Media
1375,Search for Wbb and WH production in pp collisions at square root [s]=1.96 TeV.,"We present a search for Wbb production in pp collisions at sqrt[s]=1.96 TeV in events containing one electron, an imbalance in transverse momentum, and two b-tagged jets. Using 174 pb(-1) of integrated luminosity accumulated by the D0 experiment at the Fermilab Tevatron collider, and the standard-model description of such events, we set a 95% C.L. upper limit on Wbb production of 6.6 pb for b quarks with transverse momenta p(b)(T)>20 GeV and bb separation in pseudorapidity-azimuth space DeltaR(bb)>0.75. Restricting the search to optimized bb mass intervals provides upper limits on WH production of 9.0-12.2 pb for Higgs-boson masses of 105-135 GeV.",2004-10-21,https://www.semanticscholar.org/paper/af9848c6cd86bef3a460e96c74ee810734605736,Physical Review Letters
872,High-probability parallel transitive closure algorithms,"There is a straightforward algorithm for computing the transitive closure of an n-node directed graph in O(log2 n) time on an EREW-PRAM, using n3/log n processors, or indeed with M(n)/ logn processors if one can do serial matrix multiplication in time M(n). Th is algorithm is within a log factor of optimal in work (processor-time product), for solving the all-pairs transitive-closure problem for dense graphs. However, this algorithm is far from optimal when either (a) the graph is sparse, or (b) we want to solve the single-source transitive closure problem. Ideally, we would like an AfC algorithm for transitive closure that took about e processors for the single-source problem on a graph with n nodes and e 2 n arcs, or about en processors for the all-pairs problem on the same graph. While we cannot offer an algorithm that good, we can offer algorithms_with the following performance. (1) For single-source, O(nc) time with O(enlm2’) processors,1 provided e > n2-3c, and (2) for all-pairs, @no time and d(enlvc) processors, provided e 2 n2-2c. Each of these claims assumes 0 < E 5 l/2. Importantly, the algorithms are (only) high-probability algorithms; that is, if they find a path, then a path exists, but they may fail to find a path that exists with probability at most 2-ac, where Q is some positive constant, and c is a multiplier for the time taken by the algorithm. However, we show that incorrect results can be detected, thus putting the algorithm in the “Las Vegas” class. Finally, t Work partially supported by ONR contract N00014-88-K0166 and a Guggenheim fellowship. 1 6 is the notation, proposed by Luks and furthered by A. Bhnn for “within some number of log factors of.” That is, we say f(n) is s(g(n)) if for some constants c and k, and all sufficiently large n, we have j(n) 5 clogk ng(n). we show how to do “breadth-first-search” with the same performance as we are able to achieve for single-source transitive closure.",1990-05-01,https://www.semanticscholar.org/paper/96b5f6dc10022ccad5e61a51b20cd0aea7862617,ACM Symposium on Parallelism in Algorithms and Architectures
198,On Satisfiability Problems with a Linear Structure,"It was recently shown \cite{STV} that satisfiability is polynomially solvable when the incidence graph is an interval bipartite graph (an interval graph turned into a bipartite graph by omitting all edges within each partite set). Here we relax this condition in several directions: First, we show that it holds for $k$-interval bigraphs, bipartite graphs which can be converted to interval bipartite graphs by adding to each node of one side at most $k$ edges; the same result holds for the counting and the weighted maximization version of satisfiability. Second, given two linear orders, one for the variables and one for the clauses, we show how to find, in polynomial time, the smallest $k$ such that there is a $k$-interval bigraph compatible with these two orders. On the negative side we prove that, barring complexity collapses, no such extensions are possible for CSPs more general than satisfiability. We also show NP-hardness of recognizing 1-interval bigraphs.",2016-02-01,https://www.semanticscholar.org/paper/5b3c339a2c9a3ce11c34d119bcab940f6c56842d,International Symposium on Parameterized and Exact Computation
2645,"PERSIVAL, a system for personalized search and summarization over multimedia healthcare information","In healthcare settings, patients need access to online information tha t can help them understand their medical situation. Physicians need information that is clinically relevant to an individual patient. In this paper, we present our progress on developing a system, PERSIVAL, that is designed to provide personalized access to a distributed patient care digital library. Using the secure, online patient records at New York Presbyterian Hospital as a user model, PERSIVAL's components tailor search, presentation and summarization of online multimedia information to both patients and healthcare providers.",,https://www.semanticscholar.org/paper/6ceda089cbc5ad2a31264b75b2c84edbaaa58950,ACM/IEEE Joint Conference on Digital Libraries
360,Games and Networks,,2003-08-12,https://www.semanticscholar.org/paper/bd82c62a19a07b42a8f515fdad1037f22f77b0e4,International Symposium on Fundamentals of Computation Theory
2907,A finite element method to calculate geometrically necessary dislocation density: Accounting for orientation discontinuities in polycrystals,,2022-12-01,https://www.semanticscholar.org/paper/8ba009d5816d07f953d3e5229946efa9237e924a,Acta Materialia
2491,Message from the Paper Chairs and Guest Editors,"The IEEE Virtual Reality 2012 full papers program, contained in this special issue, includes 15 papers that present research, applications, and systems in the field of virtual reality. They were selected from 95 submissions by an international program committee of 63 members, supported by 147 external expert reviewers, leading to an acceptance rate for IEEE Virtual Reality 2012 of 15.8%. All papers appearing in this issue have undergone a two-round review process. In the first round review, at least four expert reviewers reviewed the work. The paper chairs selected the primary and secondary reviewers from the international program committee, and the primary reviewer then recruited at least two external experts. After completion of all reviews, the primary reviewer led an online discussion phase, which resulted in an initial recommendation for acceptance or rejection and a set of modifications that were deemed necessary. Based on this recommendation, the program committee, at the two-day online web meeting, selected an initial set of papers for preliminary acceptance. The authors of these papers were given the opportunity to refine and resubmit their work. In the second round review, IPC members checked whether the changes made were sufficient to warrant final acceptance. Based on their input, paper chairs made the final decisions for papers appearing in the TVCG issue. The IEEE VR scientific program also includes 13 short papers published in a separate report. Many individuals have contributed a great deal of time and energy to making the IEEE Virtual Reality 2012 conference and this special issue a success. We would like to thank the authors of all submitted papers, the members of the Program Committee, as well as all the other reviewers for their many hours of hard work. We also wish to acknowledge James Stewart for his outstanding and timely support with the PCS review system. This year, the Program Committee meeting was run under the Elluminate web conferencing system, access to which was graciously provided by University of Florida. We are grateful to Ben Lok for his help and his expertise with Elluminate. As usual, the paper chairs are indebted to the IEEE Visualization and Graphics Technical Committee (VGTC) publication team, especially the Publications Coordinator, Meghan Haley, for coordinating schedules, collecting materials, and producing these conference proceedings. We warmly thank the Virtual Reality steering committee, especially its chair Doug Bowman for his valuable advice at every stage, and Ming C. Lin, Editor-in-Chief …",,https://www.semanticscholar.org/paper/1c4ebd55bbe510738b1484adb99cdc3f80eafa8e,IEEE Transactions on Visualization and Computer Graphics
3320,Energy-efficient computing for wildlife tracking: design tradeoffs and early experiences with ZebraNet,"Over the past decade, mobile computing and wireless communication have become increasingly important drivers of many new computing applications. The field of wireless sensor networks particularly focuses on applications involving autonomous use of compute, sensing, and wireless communication devices for both scientific and commercial purposes. This paper examines the research decisions and design tradeoffs that arise when applying wireless peer-to-peer networking techniques in a mobile sensor network designed to support wildlife tracking for biology research.The ZebraNet system includes custom tracking collars (nodes) carried by animals under study across a large, wild area; the collars operate as a peer-to-peer network to deliver logged data back to researchers. The collars include global positioning system (GPS), Flash memory, wireless transceivers, and a small CPU; essentially each node is a small, wireless computing device. Since there is no cellular service or broadcast communication covering the region where animals are studied, ad hoc, peer-to-peer routing is needed. Although numerous ad hoc protocols exist, additional challenges arise because the researchers themselves are mobile and thus there is no fixed base station towards which to aim data. Overall, our goal is to use the least energy, storage, and other resources necessary to maintain a reliable system with a very high `data homing' success rate. We plan to deploy a 30-node ZebraNet system at the Mpala Research Centre in central Kenya. More broadly, we believe that the domain-centric protocols and energy tradeoffs presented here for ZebraNet will have general applicability in other wireless and sensor applications.",2002-10-01,https://www.semanticscholar.org/paper/d342c54cbba28cf4fdc1f97df7ab72f96d965d23,ASPLOS X
782,Compression of Partially Ordered Strings,,2003-09-03,https://www.semanticscholar.org/paper/f430e4abf2ef6447572e23f892e9d78b08247f30,International Conference on Concurrency Theory
2655,Information filtering for mobile augmented reality,"Augmented reality is a potentially powerful paradigm for annotating the (real) environment with computer-generated material. These benefits will be even greater when augmented reality systems become mobile and wearable. However, to minimize the problem of clutter and to maximize the effectiveness of the display, algorithms must be developed to select only the most important information for the user. In this paper, we describe a region-based information filtering algorithm. The algorithm takes account of the state of the user (location and intent) and the state of individual objects about which information can be presented. It can dynamically respond to changes in the environment and the user's state. We also describe how simple temporal, distance and angle cues can be used to refine the transitions between different information sets.",2000-10-05,https://www.semanticscholar.org/paper/b3c4c934ecd427e9fff563fba229dbc8b527a34e,Proceedings IEEE and ACM International Symposium on Augmented Reality (ISAR 2000)
1819,PU-BCD: Exponential Family Models for the Coarse- and Fine-Grained All-Words Tasks,This paper describes an exponential family model of word sense which captures both occurrences and co-occurrences of words and senses in a joint probability distribution. This statistical framework lends itself to the task of word sense disambiguation. We evaluate the performance of the model in its participation on the SemEval-2007 coarse- and fine-grained all-words tasks under a variety of parameters.,2007-06-23,https://www.semanticscholar.org/paper/1df5e35f08cfb8105d34bd97848b877ebfa6ccf6,International Workshop on Semantic Evaluation
2455,Wireless wearables for virtual and augmented reality,"Virtual Reality (VR) and Augmented Reality (AR) have been explored by researchers for nearly a half century. Over the past few years, however, VR has experienced a renaissance, as head-worn display developer kits have metamorphosed into early consumer products, all far superior to what most VR researchers previously had available. Meanwhile, AR head-worn display developer kits are being released, even as Pokémon Go has caused public awareness of hand-held AR to skyrocket. What role will wireless computing play in the future of wearable VR and AR systems? I will suggest some of the ways in which future wearable hardware, software, and user interfaces could benefit from wireless technology, and describe some of the performance constraints that wireless systems must meet for this to be possible. For example, in the quest to develop lightweight eyewear, it will be advantageous to move as much technology as possible off the head, to other parts of the body or to the nearby environment. This will demand reliably low latency and high throughput for the asymmetric round trip from head-worn sensors, to off-head processors, back to head-worn displays. Furthermore, when AR eyewear becomes commonplace, so will multi-user AR collaboration, and that means handling intricate interactions between users, both co-located and remote.",2016-10-03,https://www.semanticscholar.org/paper/498a8b4399463bc82a514f313040eec751074216,HotWireless@MobiCom
2628,Session details: Session 2: environments,,2003-04-27,https://www.semanticscholar.org/paper/fb1973fc5e585a48bbf84804a206a68431198093,Proceedings of the 2003 symposium on Interactive 3D graphics
310,Progress in approximate nash equilibria,"It is known [5] that an additively ε-approximate Nash equilibrium (with supports of size at most two) can be computed in polynomial time in any 2-player game with ε=.5. It is also known that no approximation better than .5 is possible unless equilibria with support larger than logn are considered, where n is the number of strategies per player. We give a polynomial algorithm for computing an ε-approximate Nash equilibrium in 2-player games with ε ≈ .38; our algorithm computes equilibria with arbitrarily large supports.",2007-06-11,https://www.semanticscholar.org/paper/a7699857f899a3d4e1c1df679a8ec669cbe63618,ACM Conference on Economics and Computation
71,QProber,"The contents of many valuable Web-accessible databases are only available through search interfaces and are hence invisible to traditional Web ""crawlers."" Recently, commercial Web sites have started to manually organize Web-accessible databases into Yahoo!-like hierarchical classification schemes. Here we introduce QProber, a modular system that automates this classification process by using a small number of query probes, generated by document classifiers. QProber can use a variety of types of classifiers to generate the probes. To classify a database, QProber does not retrieve or inspect any documents or pages from the database, but rather just exploits the number of matches that each query probe generates at the database in question. We have conducted an extensive experimental evaluation of QProber over collections of real documents, experimenting with different types of document classifiers and retrieval models. We have also tested our system with over one hundred Web-accessible databases. Our experiments show that our system has low overhead and achieves high classification accuracy across a variety of databases.",2003-01-01,https://www.semanticscholar.org/paper/9fb27fb5d86ece45b7e0529de70c446d4c37041d,ACM Transactions on Information Systems
69,Categorizing web queries according to geographical locality,"Web pages (and resources, in general) can be characterized according to their geographical locality. For example, a web page with general information about wildflowers could be considered a global page, likely to be of interest to a geographically broad audience. In contrast, a web page with listings on houses for sale in a specific city could be regarded as a local page, likely to be of interest only to an audience in a relatively narrow region. Similarly, some search engine queries (implicitly) target global pages, while other queries are after local pages. For example, the best results for query [wildflowers] are probably global pages about wildflowers such as the one discussed above. However, local pages that are relevant to, say, San Francisco are likely to be good matches for a query [houses for sale] that was issued by a San Francisco resident or by somebody moving to that city. Unfortunately, search engines do not analyze the geographical locality of queries and users, and hence often produce sub-optimal results. Thus query [wildflowers] might return pages that discuss wildflowers in specific U.S. states (and not general information about wildflowers), while query [houses for sale] might return pages with real estate listings for locations other than that of interest to the person who issued the query. Deciding whether an unseen query should produce mostly local or global pages---without placing this burden on the search engine users---is an important and challenging problem, because queries are often ambiguous or underspecify the information they are after. In this paper, we address this problem by first defining how to categorize queries according to their (often implicit) geographical locality. We then introduce several alternatives for automatically and efficiently categorizing queries in our scheme, using a variety of state-of-the-art machine learning tools. We report a thorough evaluation of our classifiers using a large sample of queries from a real web search engine, and conclude by discussing how our query categorization approach can help improve query result quality.",2003-11-03,https://www.semanticscholar.org/paper/8e5f1448c34cfd78b85ef2601ddc2fda84fde101,International Conference on Information and Knowledge Management
1623,Hierarchical Implicit Models and Likelihood-Free Variational Inference,"Implicit probabilistic models are a flexible class of models defined by a simulation process for data. They form the basis for theories which encompass our understanding of the physical world. Despite this fundamental nature, the use of implicit models remains limited due to challenges in specifying complex latent structure in them, and in performing inferences in such models with large data sets. In this paper, we first introduce hierarchical implicit models (HIMs). HIMs combine the idea of implicit densities with hierarchical Bayesian modeling, thereby defining models via simulators of data with rich hidden structure. Next, we develop likelihood-free variational inference (LFVI), a scalable variational inference algorithm for HIMs. Key to LFVI is specifying a variational family that is also implicit. This matches the model's flexibility and allows for accurate approximation of the posterior. We demonstrate diverse applications: a large-scale physical simulator for predator-prey populations in ecology; a Bayesian generative adversarial network for discrete data; and a deep implicit model for text generation.",2017-02-28,https://www.semanticscholar.org/paper/375aa8e5ed444c6f83e2abeff8475f76833a0a2e,Neural Information Processing Systems
2003,A new morphology-based approach for similarity searching on wafer bin maps in semiconductor manufacturing,"Due to increases in the complexity of processes involved in semiconductor manufacturing, increasingly high inspection costs associated with defective wafers has become a critical concern of modern manufacturers. More importantly, because of high-dimensional wafer bin maps (WBMs), it is difficult to capture the variations of each dimension via traditional pattern recognition or classification methods. By contrast, this work proposes a novel two-phase morphology-based similarity search consisting of: (1) training sample generation based on the morphological method and (2) SVM categorization for test datasets according to the variant degrees of similarities. The morphology-based samples contain five kinds of features, including original morphology definitions in addition to our proposed features. The second phase, using SVM for similarity searches, extends the usage of pattern recognition to real applications in large sample dimensions. The preliminary results demonstrate the usefulness of our approach in the context of yield improvements in semiconductor manufacturing.",2012-05-23,https://www.semanticscholar.org/paper/9d72a85f69ec4811db36141b069ed9dfdc0df732,International Conference on Computer Supported Cooperative Work in Design
2100,Machine grouping algorithm for stepper back-up and an empirical study,"Considering the limitations of the operation cost and the flexibility need of the manufacturing, the wafer was unable to expose in the same stepper from layer to layer in the real setting. In addition, the lithographic systems also require appreciate back-ups to avoid the yield loss as the equipment fault or shut-down for maintenance. This study aims to develop a machine group algorithm for the stepper and thus propose appropriate back-up based on the similarity of systematic overlay errors and residuals. The results are confirmed with judgments of domain experts and thus validated this approach",2004-09-09,https://www.semanticscholar.org/paper/f3152d7920f1fe938648f5fd1961c5467dedcfc8,2004 Semiconductor Manufacturing Technology Workshop Proceedings (IEEE Cat. No.04EX846)
3178,Individual identification and photographic techniques in mammalian ecological and behavioural research—Part 1: Methods and concepts,,2022-06-01,https://www.semanticscholar.org/paper/68ddca179b35d3107fc1ec113ad1eaae876a1b8c,Mammalian Biology
1874,An integrated framework of Industry 3.5 and an empirical study of simulation-based automated material handling system for semiconductor manufacturing,,2022-06-17,https://www.semanticscholar.org/paper/6f9f0d85b98d526e125b506dc8be29078a32bd42,International Journal of Logistics Research and Applications
3183,Evaluating expert‐based habitat suitability information of terrestrial mammals with GPS‐tracking data,"Abstract Aim Macroecological studies that require habitat suitability data for many species often derive this information from expert opinion. However, expert‐based information is inherently subjective and thus prone to errors. The increasing availability of GPS tracking data offers opportunities to evaluate and supplement expert‐based information with detailed empirical evidence. Here, we compared expert‐based habitat suitability information from the International Union for Conservation of Nature (IUCN) with habitat suitability information derived from GPS‐tracking data of 1,498 individuals from 49 mammal species. Location Worldwide. Time period 1998–2021. Major taxa studied Forty‐nine terrestrial mammal species. Methods Using GPS data, we estimated two measures of habitat suitability for each individual animal: proportional habitat use (proportion of GPS locations within a habitat type), and selection ratio (habitat use relative to its availability). For each individual we then evaluated whether the GPS‐based habitat suitability measures were in agreement with the IUCN data. To that end, we calculated the probability that the ranking of empirical habitat suitability measures was in agreement with IUCN's classification into suitable, marginal and unsuitable habitat types. Results IUCN habitat suitability data were in accordance with the GPS data (> 95% probability of agreement) for 33 out of 49 species based on proportional habitat use estimates and for 25 out of 49 species based on selection ratios. In addition, 37 and 34 species had a > 50% probability of agreement based on proportional habitat use and selection ratios, respectively. Main conclusions We show how GPS‐tracking data can be used to evaluate IUCN habitat suitability data. Our findings indicate that for the majority of species included in this study, it is appropriate to use IUCN habitat suitability data in macroecological studies. Furthermore, we show that GPS‐tracking data can be used to identify and prioritize species and habitat types for re‐evaluation of IUCN habitat suitability data.",2022-05-09,https://www.semanticscholar.org/paper/b22f1f9b93a5550a80c5120401918b65e504821a,Global Ecology and Biogeography
1621,Structured Embedding Models for Grouped Data,"Word embeddings are a powerful approach for analyzing language, and exponential family embeddings (EFE) extend them to other types of data. Here we develop structured exponential family embeddings (S-EFE), a method for discovering embeddings that vary across related groups of data. We study how the word usage of U.S. Congressional speeches varies across states and party affiliation, how words are used differently across sections of the ArXiv, and how the co-purchase patterns of groceries can vary across seasons. Key to the success of our method is that the groups share statistical information. We develop two sharing strategies: hierarchical modeling and amortization. We demonstrate the benefits of this approach in empirical studies of speeches, abstracts, and shopping baskets. We show how S-EFE enables group-specific interpretation of word usage, and outperforms EFE in predicting held-out data.",2017-09-28,https://www.semanticscholar.org/paper/22f3c0a727bc7f07e7d5073b6d6bb4f73102ba74,Neural Information Processing Systems
3679,Tracking Through Containers and Occluders in the Wild,"Tracking objects with persistence in cluttered and dynamic environments remains a difficult challenge for computer vision systems. In this paper, we introduce TCOW, a new benchmark and model for visual tracking through heavy occlusion and containment. We set up a task where the goal is to, given a video sequence, segment both the projected extent of the target object, as well as the surrounding container or occluder whenever one exists. To study this task, we create a mixture of synthetic and annotated real datasets to support both supervised learning and structured evaluation of model performance under various forms of task variation, such as moving or nested containment. We evaluate two recent transformer-based video models and find that while they can be surprisingly capable of tracking targets under certain settings of task variation, there remains a considerable performance gap before we can claim a tracking model to have acquired a true notion of object permanence.",2023-05-04,https://www.semanticscholar.org/paper/1a96f901dad704fafa88fa7525174b15c720bcf1,Computer Vision and Pattern Recognition
2985,Gaussian Process Regression Networks,"We introduce a new regression framework, Gaussian process regression networks (GPRN), which combines the structural properties of Bayesian neural networks with the nonparametric exibility of Gaussian processes. GPRN accommodates input (predictor) dependent signal and noise correlations between multiple output (response) variables, input dependent length-scales and amplitudes, and heavy-tailed predictive distributions. We derive both elliptical slice sampling and variational Bayes inference procedures for GPRN. We apply GPRN as a multiple output regression and multivariate volatility model, demonstrating substantially improved performance over eight popular multiple output (multi-task) Gaussian process models and three multivariate volatility models on real datasets, including a 1000 dimensional gene expression dataset.",2011-10-19,https://www.semanticscholar.org/paper/ebdd2ac920461d6ed9b5ba0cc3fd74d844494690,International Conference on Machine Learning
1325,Evidence for production of single top quarks and first direct measurement of |Vtb|.,"The D0 Collaboration presents first evidence for the production of single top quarks at the Fermilab Tevatron pp[over ] collider. Using a 0.9 fb(-1) dataset, we apply a multivariate analysis to separate signal from background and measure sigma(pp[over ]-->tb+X,tqb+X)=4.9+/-1.4 pb. The probability to measure a cross section at this value or higher in the absence of a signal is 0.035%, corresponding to a 3.4 standard deviation significance. We use the cross section measurement to directly determine the Cabibbo-Kobayashi-Maskawa matrix element that describes the Wtb coupling and find 0.68<|V(tb)|</=1 at 95% C.L. within the standard model.",2006-12-21,https://www.semanticscholar.org/paper/b9f70a7645c276372882b05e580a780ba7637e3c,Physical Review Letters
836,Recent Developments on the Approximability of Combinatorial Problems,,1993-12-15,https://www.semanticscholar.org/paper/1fa69a8d690f3603b27a09d7206fe641602ea823,International Symposium on Algorithms and Computation
2398,Carbon monoxide-reacting haemoproteins in the mitochondrial fraction of Acanthamoeba castellanii.,"1. Room-temperature (18 degrees C) CO difference spectra of mitochondrial fractions from the amoeba Acanthamoeba castellanii reveal the presence of at least four CO-reacting haemoproteins. As well as cytochrome a3, other components reacting with CO are: (i) a c-type cytochrome; (ii) a b-type cytochrome; and (iii) another a-type cytochrome. 2. The same components can be identified in low-temperature photodissociation experiments with intact cells or mitochondria. 3. The time of exposure to CO and the nature of the reductant are both important in identifying all the components present, in that the b-type cytochrome is more readily distinguished after longer exposure to CO and more of the c-type cytochrome is detectable when NADH is the reductant 4. Treatment of mitochondria with ultrasound releases two components, identifiable in low-temperature difference spectra as a c-type and a b-type cytochrome; only the latter appears to have any reaction with CO, and the CO-reacting c-type cytochrome is retained in submitochondrial particles. 5. The complexity of the CO-reacting haemoproteins in this organism is compared with the simpler systems found in other eukaryotic organisms.",1980-03-15,https://www.semanticscholar.org/paper/843244fef0dcfb9d93ec66303dd3d234ae462ddd,Biochemical Journal
3439,Models of malicious behavior in sponsored search,"Search engines such as Google, Yahoo, and MSN now auction off search terms to potential advertisers. The potential advertisers place their bids on each search term of interest, as well as specifying a daily budget. Each search on this term displays an advertisement that is linked to the advertiser's website, and the advertiser pays the search engine every time the link is activated. When an advertiser's budget is reached, the search engine stops displaying their ad. This kind of advertising is extremely popular -- the combined revenue of Yahoo and Google in 2005 was estimated at over 4.5 billion dollars. We develop small models which still have the property that malicious behavior such as bid-jamming still occurs as a rational best-response strategy. Such malicious behavior occurs frequently in practice. We are able to derive bidding strategies which are the best-responses when the budget of the bidder is low relative to her competitors, as well as strategies which protect against bid-jamming.",2007-03-25,https://www.semanticscholar.org/paper/05507d3c0b78768b0936674565a155c0668b4c96,Spring Simulation Multiconference
2779,Distinct features of the host-parasite interactions between nonadherent and adherent Trichomonas vaginalis isolates,"Cytoadherence of Trichomonas vaginalis to human vaginal epithelial cells (hVECs) was previously shown to involve surface lipoglycans and several reputed adhesins on the parasite. Herein, we report some new observations on the host-parasite interactions of adherent versus nonadherent T. vaginalis isolates to hVECs. The binding of the TH17 adherent isolate to hVECs exhibited an initial discrete phase followed by an aggregation phase inhibited by lactose. T. vaginalis infection immediately induced surface expression of galectin-1 and -3, with extracellular amounts in the spent medium initially decreasing and then increasing thereafter over the next 60 min. Extracellular galectin-1 and -3 were detected on the parasite surface but only the TH17 adherent isolate could uptake galectin-3 via the lysosomes. Only the adherent isolate could morphologically transform from the round-up flagellate with numerous transient protrusions into a flat amoeboid form on contact with the solid surface. Cytochalasin D challenge revealed that actin organization was essential to parasite morphogenesis and cytoadherence. Real-time microscopy showed that parasite exploring and anchoring on hVECs via the axostyle may be required for initial cytoadherence. Together, the parasite cytoskeleton behaviors may collaborate with cell surface adhesion molecules for cytoadherence. The nonadherent isolate migrated faster than the adherent isolate, with motility transiently increasing in the presence of hVECs. Meanwhile, differential histone acetylation was detected between the two isolates. Also, TH17 without Mycoplasma symbiosis suggests that symbiont might not determine TH17 innate cytoadherence. Our findings regarding distinctive host-parasite interactions of the isolates may provide novel insights into T. vaginalis infection.",2023-01-01,https://www.semanticscholar.org/paper/aa036de029bdfc112d473e0228e087c40f95ccb6,PLoS Neglected Tropical Diseases
3064,Mediapod: a pocket-Sized and Personalized Multimedia Desktop,"We present MediaPod, a portable system that allows mobile users to maintain the same persistent, personalized multimedia desktop environment on any available computer. Regardless of which computer is being used, MediaPod provides a consistent multimedia desktop session, maintaining all of a user's applications, documents and configuration settings. This is achieved by leveraging rapid improvements in capacity, cost, and size of portable storage devices. MediaPod provides a virtualization and checkpoint-restart mechanism that decouples a desktop environment and its applications from the host, enabling multimedia desktop sessions to be suspended to portable storage, carried around, and resumed from the storage device on another computer. MediaPod virtualization also isolates desktop sessions from the host, protecting the privacy of the user and preventing malicious applications from damaging the host. We have implemented a Linux MediaPod prototype and demonstrate its ability to quickly suspend and resume multimedia desktop sessions, enabling a seamless computing experience for mobile users as they move among computers.",2010-06-01,https://www.semanticscholar.org/paper/9b4df8b1d6894adf590e5a7e43c3588247ca7a7e,International Journal of Semantic Computing
2155,Balls and bins models with feedback,"We examine generalizations of the classical balls and bins models, where the probability a ball lands in a bin is proportional to the number of balls already in the bin raised to some exponent p. Such systems exhibit positive or negative feedback, depending on the exponent p, with a phase transition occurring at p = 1. Similar models have proven useful in economics and chemistry; for example, systems with positive feedback (p > 1) tend naturally toward monopoly. We provide several results and useful heuristics for these models, including showing a bound on the time to achieve monopoly with high probability.",2002-01-06,https://www.semanticscholar.org/paper/e305e18a2f42bbe01aabc44392e93b86191723df,ACM-SIAM Symposium on Discrete Algorithms
3401,Submodular Secretary Problem with Shortlists,"In submodular $k$-secretary problem, the goal is to select $k$ items in a randomly ordered input so as to maximize the expected value of a given monotone submodular function on the set of selected items. In this paper, we introduce a relaxation of this problem, which we refer to as submodular $k$-secretary problem with shortlists. In the proposed problem setting, the algorithm is allowed to choose more than $k$ items as part of a shortlist. Then, after seeing the entire input, the algorithm can choose a subset of size $k$ from the bigger set of items in the shortlist. We are interested in understanding to what extent this relaxation can improve the achievable competitive ratio for the submodular $k$-secretary problem. In particular, using an $O(k)$ shortlist, can an online algorithm achieve a competitive ratio close to the best achievable online approximation factor for this problem? We answer this question affirmatively by giving a polynomial time algorithm that achieves a $1-1/e-\epsilon -O(k^{-1})$ competitive ratio for any constant $\epsilon > 0$, using a shortlist of size $\eta_\epsilon(k) = O(k)$. Also, for the special case of m-submodular functions, we demonstrate an algorithm that achieves a $1-\epsilon$ competitive ratio for any constant $\epsilon > 0$, using an $O(1)$ shortlist. Finally, we show that our algorithm can be implemented in the streaming setting using a memory buffer of size $\eta_\epsilon(k) = O(k)$ to achieve a $1 - 1/e - \epsilon-O(k^{-1})$ approximation for submodular function maximization in the random order streaming model. This substantially improves upon the previously best known approximation factor of $1/2 + 8 \times 10^{-14}$ [Norouzi-Fard et al. 2018] that used a memory buffer of size $O(k \log k)$.",2018-09-13,https://www.semanticscholar.org/paper/e795fbb0b7d89eca94d6710c77da10417ca62f6a,Information Technology Convergence and Services
2233,Endothelial activation and apoptosis mediated by neutrophil-dependent interleukin 6 trans-signalling: a novel target for systemic sclerosis?,"Objectives Systemic sclerosis (SSc) is a connective tissue disease associated with significant morbidity and mortality and generally inadequate treatment. Endothelial cell activation and apoptosis are thought to be pivotal in the pathogenesis of this disease, but the mechanisms that mediate this remain unknown. Methods Human dermal microvascular endothelial cells were cultured with healthy control neutrophils in the presence of 25% healthy control or SSc serum for 24 h. Apoptosis was measured by annexin V-FITC binding and endothelial cell activation was measured using an allophycocyanin-conjugated E-selectin antibody. Fluorescence was quantified and localised using confocal microscopy. Results SSc serum resulted in significantly increased apoptosis (p=0.006) and E-selectin expression (p=0.00004) in endothelial cells compared with control serum, effects that were critically dependent on the presence of neutrophils. Recombinant interleukin 6 (IL-6) reproduced these findings. Immunodepletion of IL-6 and the use of an IL-6 neutralising antibody decreased the effect of SSc serum on E-selectin expression. Soluble gp130, which specifically blocks IL-6 trans-signalling, negated the effect of SSc serum on both E-selectin expression and apoptosis. Conclusions SSc serum induces endothelial cell activation and apoptosis in endothelial cell-neutrophil co-cultures, mediated largely by IL-6 and dependent on the presence of neutrophils. Together with other pathologically relevant effects of IL-6, these data justify further exploration of IL-6 as a therapeutic target in SSc.",2010-11-10,https://www.semanticscholar.org/paper/cd75421afe179f795b6653e4d2726af8be79b410,Annals of the Rheumatic Diseases
3331,Horse signals: The sounds and scents of fury,,1992-05-01,https://www.semanticscholar.org/paper/83b24035107da74706f2251239c5ae7aee9a7532,Evolutionary Ecology
2153,On lower bounds for the capacity of deletion channels,"We consider binary deletion channels, where bits are deleted independently with probability d. We extend the framework used to analyze the capacity of binary deletion channels established by Diggavi and Grossglauser [2001], improving on their lower bounds. In Diggavi and Grossglauser, the codewords are generated by a first order Markov chain. Our improvements arise from two considerations. First, Diggavi and Grossglauser consider typical outputs, where an output is typical if an N bit input produces an output of at least N(1-d)(1-/spl epsi/) bits. We provide a stronger notion of a typical output that yields better bounds even for the cases studied in Diggavi and Grossglauser. Second, we consider codewords generated by more general processes than first order Markov chains.",2004-06-27,https://www.semanticscholar.org/paper/9f540c371bfd92c622a6321a15001b941b0b7e63,"International Symposium onInformation Theory, 2004. ISIT 2004. Proceedings."
1192,Measurement of gamma+b+X and gamma+c+X Production Cross Sections in pp[over ] Collisions at sqrt[s]=1.96 TeV.,"First measurements of the differential cross sections d;{3}sigma/(dp_{T};{gamma}dy;{gamma}dy;{jet}) for the inclusive production of a photon in association with a heavy quark (b, c) jet are presented, covering photon transverse momenta 30<p_{T};{gamma}<150 GeV, photon rapidities |y;{gamma}|<1.0, jet rapidities |y;{jet}|<0.8, and jet transverse momenta p_{T};{jet}>15 GeV. The results are based on an integrated luminosity of 1 fb;{-1} in pp[over ] collisions at sqrt[s]=1.96 TeV recorded with the D0 detector at the Fermilab Tevatron Collider. The results are compared with next-to-leading order perturbative QCD predictions.",,https://www.semanticscholar.org/paper/f9fc95db5c46fbe1329f4687d50d116ef8cfdbc7,Physical Review Letters
2689,A touring machine: Prototyping 3D mobile augmented reality systems for exploring the urban environment,,1997-10-13,https://www.semanticscholar.org/paper/dc9404cb021d7f859271de89aec145dd8b0fee87,Digest of Papers. First International Symposium on Wearable Computers
3523,An Õ(n2) algorithm for minimum cuts,"A minimum cut is a set of edges of minimum weight whose removal disconnects a given graph. Minimum cut algorithms historically applied duality with maximum flows and thus had the same 0 (inn) running time as maximum flow algorithms. More recent algorithms which are not based on maximum flows also require fl (inn) time. In this paper, we present the first algorithm that breaks the tl(mn) “max-flow barrier” for finding minimum cuts in weighted undirected graphs. We give a strongly polynomial randomized algorithm which finds a minimum cut with high probability in 0(n2 log3 n) time. This suggests that the rein-cut problem might be fundamentally easier to solve than the maximum flow problem. Our algorithm can be implemented in 72JUC using only nz processors—this is the first efficient 7UfC algorithm for the rein-cut problem. Our algorithm is simple and uses no complicated data structures.",1993-06-01,https://www.semanticscholar.org/paper/b4a4877c1bd4e514440dbbb877384b71fdc3c52b,Symposium on the Theory of Computing
2269,"The Mitochondrial Network of Human Neutrophils: Role in Chemotaxis, Phagocytosis, Respiratory Burst Activation, and Commitment to Apoptosis 1","It is commonly assumed that human neutrophils possess few, if any, functional mitochondria and that they do not depend on these organelles for cell function. We have used the fluorescent mitochondrial indicators, JC-1, MitoTracker Red, and dihydrorhodamine 123 to show that live neutrophils possess a complex mitochondrial network that extends through the cytoplasm. The membrane potential of these mitochondria was rapidly (within 2 min) disrupted by the addition of FCCP (IC50 = 20 nM), but not by the Fo-ATPase inhibitor, oligomycin (at up to 7 μg/ml). However, inhibition of mitochondrial function with both agents resulted in cell shape changes. Neither activation of the respiratory burst nor phagocytosis of either latex particles or serum-opsonized Staphylococcus aureus was affected by the addition of FCCP or oligomycin. However, FCCP inhibited chemotaxis at concentrations that paralleled disruption of mitochondrial membrane potential. Furthermore, prolonged (2-h) incubation with oligomycin resulted in an impaired ability to activate a respiratory burst and also inhibited chemotaxis. These observations indicate that intact mitochondrial function is required to sustain some neutrophil functions, but not for the rapid initiation of the respiratory burst or phagocytosis. Loss of mitochondrial membrane potential was a very early marker for commitment of neutrophils into apoptosis and preceded the appearance of phosphatidylserine on the cell surface. However, inhibition of mitochondrial function did not accelerate the rate of neutrophil apoptosis. These data shed important insights into the hitherto unrecognized importance of mitochondria in the function of neutrophils during infection and inflammation.",2003-02-15,https://www.semanticscholar.org/paper/5941a8add4c97d55a0e63af4feac93619f6e3d8b,Journal of Immunology
86,Learning search engine specific query transformations for question answering,"We introduce a method for learning query transformations that improves the ability to retrieve answers to questions from an information retrieval system. During the training stage the method involves automatically learning phrase features for classifying questions into different types, automatically generating candidate query transformations from a training set of question/answer pairs, and automatically evaluating the candidate transforms on target information retrieval systems such as real-world general purpose search engines. At run time, questions are transformed into a set of queries, and re-ranking is performed on the documents retrieved. We present a prototype search engine, Tritus, that applies the method to web search engines. Blind evaluation on a set of real queries from a web search engine log shows that the method significantly outperforms the underlying web search engines as well as a commercial search engine specializing in question answering.",2001-05-01,https://www.semanticscholar.org/paper/54cbc1e24f5a480c668ef8b1f55ba50e3f350434,The Web Conference
184,About Place Cells and Grid Cells - About Place Cells and Grid Cells,,,https://www.semanticscholar.org/paper/c96650186b5f28b975d67d6e72ffa32140193cd5,Adventures Between Lower Bounds and Higher Altitudes
1904,A hybrid genetic algorithm with 2D encoding for the scheduling of rehabilitation patients,,2018-11-01,https://www.semanticscholar.org/paper/627c9e0beefc1c5e21b686cccecd3862a206ddde,Computers & industrial engineering
174,Sex: The power of randomization.,,2019-10-01,https://www.semanticscholar.org/paper/9ffb06970cd9b71fe590e26fea287129b0477c3d,Theoretical Population Biology
595,On Concurrency Control by Multiple Versions,We examine the problem of concurrency control when the database management system supports multiple versions of the data. We characterize the limit of the parallelism achievable by the multiversion approach and demonstrate the resulting space-parallelism trade-off.,1982-03-29,https://www.semanticscholar.org/paper/9166b33bb042c0623b037a74158be0f0e797d1be,TODS
856,Simple Local Search Problems That are Hard to Solve,"Many algorithms for NP-hard optimization problems find solutions that are locally optimal, in the sense that the solutions cannot be improved by a polynomially computable perturbation. Very little is known about the complexity of finding locally optimal solutions, either by local search algorithms or using other indirect methods. Johnson, Papadimitriou, and Yannakakis [J. Comput. System Sci., 37 (1988), pp. 79–100] studied this question by defining a complexity class PLS that captures local search problems. It was proved that finding a partition of a graph that is locally optimal into equal parts with respect to the acclaimed Kernighan-Lin algorithm is PLS-complete. It is shown here that several natural, simple local search problems are PLS-complete, and thus just as hard. Two examples are: finding a partition that cannot be improved by a single swap of two vertices, and finding a stable configuration for an undirected connectionist network.When edges or other objects are unweighted, then a local optimum ...",1991-02-01,https://www.semanticscholar.org/paper/3be5c11282e645343e780322ec2c2b626ee5a4fe,SIAM journal on computing (Print)
3,Cross-Lingual Text Classification with Minimal Resources by Transferring a Sparse Teacher,"Cross-lingual text classification alleviates the need for manually labeled documents in a target language by leveraging labeled documents from other languages. Existing approaches for transferring supervision across languages require expensive cross-lingual resources, such as parallel corpora, while less expensive cross-lingual representation learning approaches train classifiers without target labeled documents. In this work, we propose a cross-lingual teacher-student method, CLTS, that generates “weak” supervision in the target language using minimal cross-lingual resources, in the form of a small number of word translations. Given a limited translation budget, CLTS extracts and transfers only the most important task-specific seed words across languages and initializes a teacher classifier based on the translated seed words. Then, CLTS iteratively trains a more powerful student that also exploits the context of the seed words in unlabeled target documents and outperforms the teacher. CLTS is simple and surprisingly effective in 18 diverse languages: by transferring just 20 seed words, even a bag-of-words logistic regression student outperforms state-of-the-art cross-lingual methods (e.g., based on multilingual BERT). Moreover, CLTS can accommodate any type of student classifier: leveraging a monolingual BERT student leads to further improvements and outperforms even more expensive approaches by up to 12% in accuracy. Finally, CLTS addresses emerging tasks in low-resource languages using just a small number of word translations.",2020-10-06,https://www.semanticscholar.org/paper/2c6161e57952eca8dabf25c1a48de1f40e2c9b5e,Findings
1603,Black Box FDR,"Analyzing large-scale, multi-experiment studies requires scientists to test each experimental outcome for statistical significance and then assess the results as a whole. We present Black Box FDR (BB-FDR), an empirical-Bayes method for analyzing multi-experiment studies when many covariates are gathered per experiment. BB-FDR learns a series of black box predictive models to boost power and control the false discovery rate (FDR) at two stages of study analysis. In Stage 1, it uses a deep neural network prior to report which experiments yielded significant outcomes. In Stage 2, a separate black box model of each covariate is used to select features that have significant predictive power across all experiments. In benchmarks, BB-FDR outperforms competing state-of-the-art methods in both stages of analysis. We apply BB-FDR to two real studies on cancer drug efficacy. For both studies, BB-FDR increases the proportion of significant outcomes discovered and selects variables that reveal key genomic drivers of drug sensitivity and resistance in cancer.",2018-06-08,https://www.semanticscholar.org/paper/852bcc3cc41df93e079582a2ef3c40689ff8411a,International Conference on Machine Learning
3476,Scheduling multi-task multi-agent systems,We present a centralized and a distributed algorithms for scheduling multi-task agents in heterogeneous networks. Our centralized algorithm has an upper bound on the overall completion time and is used as a module in the distributed algorithm. Extensive simulations show promising results.,2001-05-28,https://www.semanticscholar.org/paper/323128767bbf6894e2d57f7ebc65fd998c316bb2,International Conference on Autonomous Agents
2737,Generating customized text and graphics in the COMET explanation testbed,"It is shown how COMET, a system that uses natural language and graphics generation components to produce the text and pictures of its explanations dynamically, can create a variety of different explanations to explain the same concepts, and thus better meets the needs of different users. The authors focus on four ways in which COMET produces different explanations for the same concepts, describing how it is influenced by the plan containing the concept, by user background knowledge, by previous dialogue, and by interactive exploration of an explanation.<<ETX>>",1991-12-01,https://www.semanticscholar.org/paper/2d40e59c3c8bbc3194200d2a1d2c513cd4a0695e,Winter simulation conference : proceedings
3598,Lock-Free Dynamically Resizable Arrays,,2006-12-12,https://www.semanticscholar.org/paper/a2975daae9ad05d9b1e084021ccb153d35cdf8dc,International Conference on Principles of Distributed Systems
231,"Algorithms, Games, and Evolution (Invited Talk)","Even the most seasoned students of evolution, starting with Darwin himself, have occasionally expressed amazement at the fact that the mechanism of natural selection has produced the whole of Life as we see it around us. From a computational perspective, it is natural to marvel at evolution's solution to the problems of robotics, vision and theorem proving! What, then, is the complexity of evolution, viewed as an algorithm? One answer to this question is 10^{12}, roughly the number of sequential steps or generations from the earliest single celled creatures to today's Homo Sapiens. To put this into perspective, the processor of a modern cell phone can perform 10^{12} steps in less than an hour. Another answer is 10^30, the degree of parallelism, roughly the maximum number of organisms living on the Earth at any time. Perhaps the answer should be the product of the two numbers, roughly 10^42, to reflect the total work done by evolution, viewed as a parallel algorithm. 
 
Here we argue, interpreting our recently published paper, that none of the above answers is really correct. Viewing evolution as an algorithm poses an additional challenge: recombination. Even if evolution succeeds in producing a particularly good solution (a highly fit individual), its offspring would only inherit half its genes, and therefore appear unlikely to be a good solution. This is the core of the problem of explaining the role of sex in evolution, known as the ""queen of problems in evolutionary biology"". 
 
The starting point is the diffusion-equation-based approach of theoretical population geneticists, who analyze the changing allele frequencies (over the generations) in the gene pool, consisting of the aggregate of the genetic variants (or ""alleles"") over all genes (or ""loci"") and over all individuals in a species. Taking this viewpoint to its logical conclusion, rather than acting on individuals or species or genes, evolution acts on this gene pool, or genetic soup, by making it more ""potent"", in the sense that it increases the expected fitness of genotype drawn randomly from this soup. Moreover, for much genetic variation, this soup may be assumed to be in the regime of weak selection, a regime where the probability of occurrence of a certain genotype involving various alleles at different loci is simply the product of the probabilities of each of its alleles. In this regime, we show that evolution in the regime of weak selection can be formulated as a game, where the recombining loci are the players, the alleles in those loci are possible moves or actions of each player, and the expected payoff of each player-locus is precisely the organism's expected fitness across the genotypes that are present in the population. Moreover, the dynamics specified by the diffusion equations of theoretical population geneticists is closely approximated by the dynamics of multiplicative weight updates (MWUA). 
 
The algorithmic connection to MWUA brings with it new insights for evolutionary biology, specifically, into the question of how genetic diversity is maintained in the presence of natural selection. For this it is useful to consider a dual view of MWUA, which expresses ""what each gene is optimizing"" as it plays the game. Remarkably this turns out to be a particular convex combination of the entropy of its distribution over alleles and cumulative expected fitness. This sheds new light on the maintenance of diversity in evolution. 
 
All of this suggests that the complexity of evolution should indeed be viewed as 10^12, but for a subtle reason. It is the number of steps of multiplicative weight updates carried out on allele frequencies in the genetic soup. A closer examination of this reveals further that the accurate tracking of allele frequencies over the generations requires the simulation of a quadratic dynamical system (two parents for each offspring). Moreover the simulation of even simple quadratic dynamical systems is known to be PSPACE-hard. This suggests that the tracking of allele frequencies might require large population sizes for each species, putting into perspective the number 10^30. Finally, it is worth noting that in this view there is a primacy to recombination or sex, which serve to provide robustness to the mechanism of evolution, as well as the framework within which MWUA operates.",,https://www.semanticscholar.org/paper/9918cbee1ee84ed2129a8a0091f48207ad41ce7d,Foundations of Software Technology and Theoretical Computer Science
687,Which Spatial Partition Trees are Adaptive to Intrinsic Dimension?,"Recent theory work has found that a special type of spatial partition tree -- called a random projection tree -- is adaptive to the intrinsic dimension of the data from which it is built. Here we examine this same question, with a combination of theory and experiments, for a broader class of trees that includes k-d trees, dyadic trees, and PCA trees. Our motivation is to get a feel for (i) the kind of intrinsic low dimensional structure that can be empirically verified, (ii) the extent to which a spatial partition can exploit such structure, and (iii) the implications for standard statistical tasks such as regression, vector quantization, and nearest neighbor search.",2009-06-18,https://www.semanticscholar.org/paper/6782ca4b600d823ef5734f11779f7b8a41dc4812,Conference on Uncertainty in Artificial Intelligence
3422,Energy Aware Scheduling for Weighted Completion Time and Weighted Tardiness,,2011-10-04,https://www.semanticscholar.org/paper/c9df0bccf3132346c2bf97bfc19b3e4fe3f75da5,arXiv.org
1898,A Data Mining Approach for Optimizing Manufacturing Parameters of Wire Bonding Process in IC Packaging Industry and Empirical Study,"In the wire bonding process, different setting value of manufacturing parameters affect the quality of the solder joints and the speed of wire bonding significantly. And when a new product is introduced, it is difficult to quickly obtain effective parameters simply by relying on the experience of the engineers. The study aims to propose a data mining framework to help the IC packaging factories to increase production capacity by reducing the number of downtimes and speeding up the wire bonding time. The proposed data mining framework employed random forest and XGBoost method to classify the occurrence of defect, MARS algorithm to predict the wire bonding time, and genetic algorithm to search for the manufacturing parameters. By exploring the production data, the proposed framework finally provides a set of recommended manufacturing parameters of the second bonding process. This study cooperates with a leading assembly company in Taiwan to validate the proposed framework. The empirical result reveals that it improves the packaging yield by 0.03%. The domain engineers can also find parameters more systematically and quickly by the proposed framework.",2019-04-01,https://www.semanticscholar.org/paper/c5f806f700c84b052e28228bad9ff8de5ba97412,3D Structure from Multiple Images of Large-Scale Environments
3289,Immunocontraception in Wild Horses (Equus caballus) Extends Reproductive Cycling Beyond the Normal Breeding Season,"Background Although the physiological effects of immunocontraceptive treatment with porcine zona pellucida (PZP) have been well studied, little is known about PZP's effects on the scheduling of reproductive cycling. Recent behavioral research has suggested that recipients of PZP extend the receptive breeding period into what is normally the non-breeding season. Methodology/Principal Findings To determine if this is the case, we compiled foaling data from wild horses (Equus caballus) living on Shackleford Banks, North Carolina for 4 years pre- and 8 years post-contraception management with PZP (pre-contraception, n = 65 births from 45 mares; post-contraception, n = 97 births from 46 mares). Gestation lasts approximately 11–12 months in wild horses, placing conception at approximately 11.5 months prior to birth. Since the contraception program began in January 2000, foaling has occurred over a significantly broader range than it had before the contraception program. Foaling in PZP recipients (n = 45 births from 27 mares) has consistently occurred over a broader range than has foaling in non-recipients (n = 52 births from 19 mares). In addition, current recipients of PZP foaled later in the year than did prior recipient and non-recipient mares. Females receiving more consecutive PZP applications gave birth later in the season than did females receiving fewer applications. Finally, the efficacy of PZP declined with increasing consecutive applications before reaching 100% after five consecutive applications. Conclusions/Significance For a gregarious species such as the horse, the extension of reproductive cycling into the fall months has important social consequences, including decreased group stability and the extension of male reproductive behavior. In addition, reproductive cycling into the fall months could have long-term effects on foal survivorship. Managers should consider these factors before enacting immunocontraceptive programs in new populations. We suggest minor alterations to management strategies to help alleviate such unintended effects in new populations.",2010-10-26,https://www.semanticscholar.org/paper/2b70ad4e141ba6ef7a8a57a207c25e78bf97e074,PLoS ONE
1930,Exploit the Value of Production Data to Discover Opportunities for Saving Power Consumption of Production Tools,"Semiconductor industry is both technology and energy intensive. There is a critical need to develop effective ways for energy saving to support smart and green production. This paper aims to develop data mining approach based on neural networks to exploit the value of production data and derive improvement directions for energy saving. In particular, the power consumption per wafer processed step (kilowatt hour per move, kwh/move) of individual production tool sets can be estimated, in which the relationships between kwh/move and 19 individual input factors, including “lot size,” “process time,” “uptime,” “usable machine,” “Q-time constrain,” and “sampling rate” are derived. An empirical study was conducted in a leading wafer fab and the results have shown practical viability of the proposed approach to discover effective opportunities for saving 17.21% power consumption by production tool sets.",2016-12-01,https://www.semanticscholar.org/paper/0b9a274631707af50afa0088e6a97eb28db8939b,IEEE transactions on semiconductor manufacturing
885,"Optimization, Approximation, and Complexity Classes (Extended Abstract)",,,https://www.semanticscholar.org/paper/7c7484bb16789764ac8ccab671d3027f3bfc6376,Symposium on the Theory of Computing
2169,"Rheumatoid Arthritis Synovial Fluid Neutrophils Drive Inflammation Through Production of Chemokines, Reactive Oxygen Species, and Neutrophil Extracellular Traps","Rheumatoid arthritis (RA) is a chronic inflammatory disorder affecting synovial joints. Neutrophils are believed to play an important role in both the initiation and progression of RA, and large numbers of activated neutrophils are found within both synovial fluid (SF) and synovial tissue from RA joints. In this study we analyzed paired blood and SF neutrophils from patients with severe, active RA (DAS28>5.1, n=3) using RNA-seq. 772 genes were significantly different between blood and SF neutrophils. IPA analysis predicted that SF neutrophils had increased expression of chemokines and ROS production, delayed apoptosis, and activation of signaling cascades regulating the production of NETs. This activated phenotype was confirmed experimentally by incubating healthy control neutrophils in cell-free RA SF, which was able to delay apoptosis and induce ROS production in both unprimed and TNFα primed neutrophils (p<0.05). RA SF significantly increased neutrophil migration through 3μM transwell chambers (p<0.05) and also increased production of NETs by healthy control neutrophils (p<0.001), including exposure of myeloperoxidase (MPO) and citrullinated histone-H3-positive DNA NETs. IPA analysis predicted NET production was mediated by signaling networks including AKT, RAF1, SRC, and NF-κB. Our results expand the understanding of the molecular changes that take place in the neutrophil transcriptome during migration into inflamed joints in RA, and the altered phenotype in RA SF neutrophils. Specifically, RA SF neutrophils lose their migratory properties, residing within the joint to generate signals that promote joint damage, as well as inflammation via recruitment and activation of both innate and adaptive immune cells. We propose that this activated SF neutrophil phenotype contributes to the chronic inflammation and progressive damage to cartilage and bone observed in patients with RA.",2020-07-19,https://www.semanticscholar.org/paper/3e9e7d8192ae3f5b9d88a2008447be689bab73b8,Frontiers in Immunology
1201,Observation of ZZ production in p-p collisions at sqrt s=1.96 TeV.,"We present an observation for ZZ-->l+l-l'+l'- (l, l'=e or mu) production in p[over]p collisions at a center-of-mass energy of sqrt[s]=1.96 TeV. Using 1.7 fb(-1) of data collected by the D0 experiment at the Fermilab Tevatron Collider, we observe three candidate events with an expected background of 0.14(+0.03)_(-0.02) events. The significance of this observation is 5.3 standard deviations. The combination of D0 results in this channel, as well as in ZZ-->l+l- nu[over]nu, yields a significance of 5.7 standard deviations and a combined cross section of sigma(ZZ)=1.60+/-0.63(stat)+0.16_-0.17(syst) pb.",2008-08-05,https://www.semanticscholar.org/paper/151d7ddd904150ec3c18c0c262a3343cb3ace085,Physical Review Letters
2252,Neutrophil apoptosis in rheumatoid arthritis is regulated by local oxygen tensions within joints,"Neutrophils are normally short‐lived cells and die by apoptosis, but when recruited into tissues, their apoptosis is delayed, and they survive for much longer time periods. In inflammatory diseases, such as rheumatoid arthritis (RA), this delayed apoptosis may lead to increased tissue damage and a failure of the inflammation to resolve. However, there are conflicting reports in the literature as to whether neutrophil apoptosis is delayed or accelerated in rheumatoid joints. In this report, we show that neutrophils isolated from the ynovial fluid (SF) of patients with RA show accelerated rates of apoptosis when incubated ex vivo and that SF, despite containing a variety of antiapoptotic cytokines, is proapoptotic. Paradoxically, levels of the key neutrophil survival protein Mcl‐1 are elevated in freshly isolated SF neutrophils compared with matched peripheral blood samples from the same patients, indicating that delayed neutrophil apoptosis has been signaled in vivo as the cells enter the joints. However, when SF was added to neutrophils and incubated under hypoxia (1% O2), conditions known to exist in vivo within joints, the SF was antiapoptotic. These data reveal that the rheumatoid synovial joint contains a complex mixture of pro‐ and antiapoptotic factors and that the low, local oxygen tensions that exist within these joints can exert profound effects on neutrophil survival. These experiments also highlight the importance of performing in vitro experiments under laboratory conditions that closely mimic those that occur in vivo; otherwise, misleading conclusions may be drawn.",2006-09-01,https://www.semanticscholar.org/paper/280b7723e1dbb36943ad3d1da3105306e6748c9e,Journal of Leukocyte Biology
1637,Stochastic Gradient Descent as Approximate Bayesian Inference,"Stochastic Gradient Descent with a constant learning rate (constant SGD) simulates a Markov chain with a stationary distribution. With this perspective, we derive several new results. (1) We show that constant SGD can be used as an approximate Bayesian posterior inference algorithm. Specifically, we show how to adjust the tuning parameters of constant SGD to best match the stationary distribution to a posterior, minimizing the Kullback-Leibler divergence between these two distributions. (2) We demonstrate that constant SGD gives rise to a new variational EM algorithm that optimizes hyperparameters in complex probabilistic models. (3) We also propose SGD with momentum for sampling and show how to adjust the damping coefficient accordingly. (4) We analyze MCMC algorithms. For Langevin Dynamics and Stochastic Gradient Fisher Scoring, we quantify the approximation errors due to finite learning rates. Finally (5), we use the stochastic process perspective to give a short proof of why Polyak averaging is optimal. Based on this idea, we propose a scalable approximate MCMC algorithm, the Averaged Stochastic Gradient Sampler.",2017-04-13,https://www.semanticscholar.org/paper/ea68a5c75e0e228e54efd91db972f71c1a917e51,Journal of machine learning research
1938,An Overview of the Development of a GPU with Integrated HBM on Silicon Interposer,"In recent years, the 2.5D IC (Integrated Circuit) package with TSV (Through Silicon Vias) has become important for high-bandwidth and high-performance applications. It is well known that 2.5D technology requires significant innovation in the areas of process technology, packaging, design, thermals, and test solutions leading to several hundred new technologies in a single product. With these complex material sets and process steps combined with new design requirements, methods, and components, a significant amount of research and engineering development is required to bring a successful product to market. After an 8 year development, in July of 2015 AMD reported it began shipment of a new generation of AMD Radeon Fury graphics cards, based on the development of the ""Fiji"" GPU and the first generation of High Bandwidth Memory (HBM1). This announcement is the next major step in an era for 2.5D IC package technology which will be used in a wide variety of applications. Given the scope of this undertaking many collaborations were required. This paper provides an overview of the collaboration between AMD and ASE. It chronicles the work from initial proof of concept to technology feasibility to product development to production ramp and ultimately high volume production. Details of several functional prototypes are outlined including warpage characterization, stress reduction, materials selection methodologies, and the use of finite element analysis (FEA) and advanced Warpage Metrology Analyzer (WMA) for optimization. The result is a reliable 2.5D process flow with >99% assembly yield for a 1011mm2 interposer with 5 dies attached in a 300+W 55x55mm2 package.",2016-05-01,https://www.semanticscholar.org/paper/aa96944f002e97690a05006490b25c5887cfcb56,Electronic Components and Technology Conference
2432,Collaborative exploration of urban data in virtual and augmented reality,"From emergency planning to real estate, many domains can benefit from collaborative exploration of urban environments in VR and AR. We have created an interactive experience that allows multiple users to explore live datasets in context of an immersive scale model of the urban environment with which they are related.",2018-08-12,https://www.semanticscholar.org/paper/24ade600e1a8edd247639d64f0ba4e81628af28a,"ACM SIGGRAPH 2018 Virtual, Augmented, and Mixed Reality"
3686,What You Can Reconstruct from a Shadow,"3D reconstruction is a fundamental problem in computer vision, and the task is especially challenging when the object to reconstruct is partially or fully occluded. We introduce a method that uses the shadows cast by an unobserved object in order to infer the possible 3D volumes under occlusion. We create a differentiable image formation model that allows us to jointly infer the 3D shape of an object, its pose, and the position of a light source. Since the approach is end-to-end differentiable, we are able to integrate learned priors of object geometry in order to generate realistic 3D shapes of different object categories. Experiments and visualizations show that the method is able to generate multiple possible solutions that are consistent with the observation of the shadow. Our approach works even when the position of the light source and object pose are both unknown. Our approach is also robust to real-world images where ground-truth shadow mask is unknown.",2023-06-01,https://www.semanticscholar.org/paper/7f368c2255a3dbd1356c21af574f493b8e90174a,Computer Vision and Pattern Recognition
1212,Observation of the Bc meson in the exclusive decay Bc-->J/psipi.,"A fully reconstructed Bc-->J/psipi signal is observed with the D0 detector at the Fermilab Tevatron pp[over] collider using 1.3 fb(-1) of integrated luminosity. The signal consists of 54+/-12 candidates with a significance that exceeds 5 standard deviations, and confirms earlier observations of this decay. The measured mass of the Bc meson is 6300+/-14(stat)+/-5(syst) MeV/c2.",2008-02-01,https://www.semanticscholar.org/paper/495f40ba5f865e68758250b0d6586bb9e15bacac,Physical Review Letters
28,Learning similarity metrics for event identification in social media,"Social media sites (e.g., Flickr, YouTube, and Facebook) are a popular distribution outlet for users looking to share their experiences and interests on the Web. These sites host substantial amounts of user-contributed materials (e.g., photographs, videos, and textual content) for a wide variety of real-world events of different type and scale. By automatically identifying these events and their associated user-contributed social media documents, which is the focus of this paper, we can enable event browsing and search in state-of-the-art search engines. To address this problem, we exploit the rich ""context"" associated with social media content, including user-provided annotations (e.g., title, tags) and automatically generated information (e.g., content creation time). Using this rich context, which includes both textual and non-textual features, we can define appropriate document similarity metrics to enable online clustering of media to events. As a key contribution of this paper, we explore a variety of techniques for learning multi-feature similarity metrics for social media documents in a principled manner. We evaluate our techniques on large-scale, real-world datasets of event images from Flickr. Our evaluation results suggest that our approach identifies events, and their associated social media documents, more effectively than the state-of-the-art strategies on which we build.",2010-02-04,https://www.semanticscholar.org/paper/d7f72ac79527b9e3fd612fc5202b5c83721d9d52,Web Search and Data Mining
3411,Experimental Analysis of Algorithms for Coflow Scheduling,,2016-03-25,https://www.semanticscholar.org/paper/a1c78027e41b73b0c38f3578a9dfae323c25cbeb,The Sea
3657,The Evolution of C++ 1985 to 1987,,,https://www.semanticscholar.org/paper/197ad3a0ac052dea3423a8186c9af6cf60a5f130,C++ Workshop
261,"The Complexity of the Homotopy Method, Equilibrium Selection, and Lemke-Howson Solutions","We show that the widely used homotopy method for solving fix point problems, as well as the Harsanyi-Selten equilibrium selection process for games, are PSPACE-complete to implement. Extending our result for the Harsanyi-Selten process, we show that several other homotopy-based algorithms for finding equilibria of games are also PSPACE-complete to implement. A further application of our techniques yields the result that it is PSPACE-complete to compute any of the equilibria that could be found via the classical Lemke-How son algorithm, a complexity-theoretic strengthening of the result in [24]. These results show that our techniques can be widely applied and suggest that the PSPACE-completeness of implementing homotopy methods is a general principle.",2010-06-28,https://www.semanticscholar.org/paper/057664d4849ad4483d3c1c18d72a7e783fe437a3,IEEE Annual Symposium on Foundations of Computer Science
767,Probability and Recursion,,2005-12-19,https://www.semanticscholar.org/paper/2c66f7fb9311cab3fe117a533ab2880ba5c0fd3f,International Symposium on Algorithms and Computation
855,"Optimization, approximation, and complexity classes","We define a natural variant of NP, MAX NP, and also a subclass called MAX SNP. These are classes of optimization problems, and in fact contain several natural, well-studied ones. We show that problems in these classes can be approximated with some bounded error. Furthermore, we show that a number of common optimization problems are complete under a kind of careful transformation (called L-reduction) that preserves approximability. It follows that such a complete problem has a polynomial-time approximation scheme iff the whole class does. These results may help explain the lack of progress on the approximability of a host of optimization problems.",1991-12-01,https://www.semanticscholar.org/paper/2a078f34447498e5eff8e3922f076c24fdb26f4d,Symposium on the Theory of Computing
3569,Minimizing dependencies within generic classes for faster and smaller programs,"Generic classes can be used to improve performance by allowing compile-time polymorphism. But the applicability of compile-time polymorphism is narrower than that of runtime polymorphism, and it might bloat the object code. We advocate a programming principle whereby a generic class should be implemented in a way that minimizes the dependencies between its members (nested types, methods) and its generic type parameters. Conforming to this principle (1) reduces the bloat and (2) gives rise to a previously unconceived manner of using the language that expands the applicability of compile-time polymorphism to a wider range of problems. Our contribution is thus a programming technique that generates faster and smaller programs. We apply our ideas to GCC's STL containers and iterators, and we demonstrate notable speedups and reduction in object code size (real application runs 1.2x to 2.1x faster and STL code is 1x to 25x smaller). We conclude that standard generic APIs (like STL) should be amended to reflect the proposed principle in the interest of efficiency and compactness. Such modifications will not break old code, simply increase flexibility. Our findings apply to languages like C++, C#, and D, which realize generic programming through multiple instantiations.",2009-10-25,https://www.semanticscholar.org/paper/1ae79c162636fb99316896916af62954df761ae8,"Conference on Object-Oriented Programming Systems, Languages, and Applications"
2780,Galectin-3 facilitates cell-to-cell HIV-1 transmission by altering the composition of membrane lipid rafts in CD4 T cells.,"Galectin-3 (GAL3) is a β-galactoside-binding lectin expressed in CD4 T cells infected with human immunodeficiency virus-1 (HIV-1). GAL3 promotes HIV-1 budding by associating with ALIX and Gag p6. GAL3 has been shown to localize in membrane lipid rafts in dendritic cells and positively regulate cell migration. HIV-1 spreads between T cells by forming supramolecular structures (virological synapses [VSs]), whose integrity depends on lipid rafts. Here, we addressed the potential role of GAL3 in cell-to-cell transmission of HIV-1 in CD4 T cells. GAL3 expressed in donor cells was more important for facilitating HIV-1 cell-to-cell transfer than GAL3 expressed in target cells. GAL3 was found to be co-transferred with Gag from HIV-1-positive donor to HIV-1-negative target T cells. HIV-1 infection induced translocation of GAL3 together with Gag to the cell-cell interfaces and colocalize with GM1, where GAL3 facilitated VS formation. GAL3 regulated the coordinated transfer of Gag and flotillin-1 into plasma membrane fractions. Finally, depletion of GAL3 reduced the cholesterol levels in membrane lipid rafts in CD4 T cells. These findings provide evidence that endogenous GAL3 stimulates lipid raft components and facilitates intercellular HIV-1 transfer among CD4 T cells, offering another pathway by which GAL3 regulates HIV-1 infection. These findings may inform the treatment of HIV-1 infection based on targeting GAL3 to modulate lipid rafts.",2022-07-05,https://www.semanticscholar.org/paper/61d06c295c147b03cd6d0ee258d9b87ffa28ab0a,Glycobiology
2541,Searching the World's Herbaria: A System for Visual Identification of Plant Species,,2008-10-12,https://www.semanticscholar.org/paper/090d99b041023c1f4af445122906baa41e41372a,European Conference on Computer Vision
3003,Design and Verification of the Arm Confidential Compute Architecture,"The increasing use of sensitive private data in computing is matched by a growing concern regarding data privacy. System software such as hypervisors and operating systems are supposed to protect and isolate applications and their private data, but their large codebases contain many vulnerabilities that can risk data conﬁdentiality and integrity. We introduce Realms, a new abstraction for conﬁdential computing to protect the data conﬁdentiality and integrity of virtual machines. Hardware creates and enforces Realm world, a new physical address space for Realms. Firmware controls the hardware to secure Realms and handles requests from untrusted system software to manage Realms, including creating and running them. Untrusted system software retains control of the dynamic allocation of memory to Realms, but cannot access Realm memory contents, even if run at a higher privileged level. To guarantee the security of Realms, we veriﬁed the ﬁrmware, introducing novel veriﬁcation techniques that enable us to prove, for the ﬁrst time, the security and correctness of concurrent software with hand-over-hand locking and dynamically allocated shared page tables, data races in kernel code running on relaxed memory hardware, integrated C and Arm assembly code calling one another, and untrusted software being in full control of allocating system resources. Realms are included in the Arm Conﬁdential Compute Architecture.",,https://www.semanticscholar.org/paper/882cc7dfb01a96ae08388e4bac3bdcc5caffdf01,USENIX Symposium on Operating Systems Design and Implementation
906,On the Desirability of Acyclic Database Schemes,"A class of database schemes, called acychc, was recently introduced. It is shown that this class has a number of desirable properties. In particular, several desirable properties that have been studied by other researchers m very different terms are all shown to be eqmvalent to acydicity. In addition, several equivalent charactenzauons of the class m terms of graphs and hypergraphs are given, and a smaple algorithm for determining acychclty is presented. Also given are several eqmvalent characterizations of those sets M of multivalued dependencies such that M is the set of muRlvalued dependencies that are the consequences of a given join dependency. Several characterizations for a conflict-free (in the sense of Lien) set of muluvalued dependencies are provided.",1983-07-01,https://www.semanticscholar.org/paper/666dfa8258914bf17970b20d2f7247c7c1468307,JACM
2921,An integrated approach to identify environmental modulators of genetic risk factors for complex traits,"Complex traits and diseases can be influenced by both genetics and environment. However, given the large number of environmental stimuli and power challenges for gene-by-environment testing, it remains a critical challenge to identify and prioritize specific disease-relevant environmental exposures. We propose a novel framework for leveraging signals from transcriptional responses to environmental perturbations to identify disease-relevant perturbations that can modulate genetic risk for complex traits and inform the functions of genetic variants associated with complex traits. We perturbed human skeletal muscle, fat, and liver relevant cell lines with 21 perturbations affecting insulin resistance, glucose homeostasis, and metabolic regulation in humans and identified thousands of environmentally responsive genes. By combining these data with GWAS from 31 distinct polygenic traits, we show that heritability of multiple traits is enriched in regions surrounding genes responsive to specific perturbations and, further, that environmentally responsive genes are enriched for associations with specific diseases and phenotypes from the GWAS catalogue. Overall, we demonstrate the advantages of large-scale characterization of transcriptional changes in diversely stimulated and pathologically relevant cells to identify disease-relevant perturbations.",2021-02-25,https://www.semanticscholar.org/paper/f1152db12ffdfc80d33070e8fa86dce089a0f864,bioRxiv
1728,Scalable Recommendation with Poisson Factorization,"We develop a Bayesian Poisson matrix factorization model for forming recommendations from sparse user behavior data. These data are large user/item matrices where each user has provided feedback on only a small subset of items, either explicitly (e.g., through star ratings) or implicitly (e.g., through views or purchases). In contrast to traditional matrix factorization approaches, Poisson factorization implicitly models each user's limited attention to consume items. Moreover, because of the mathematical form of the Poisson likelihood, the model needs only to explicitly consider the observed entries in the matrix, leading to both scalable computation and good predictive performance. We develop a variational inference algorithm for approximate posterior inference that scales up to massive data sets. This is an efficient algorithm that iterates over the observed entries and adjusts an approximate posterior over the user/item representations. We apply our method to large real-world user data containing users rating movies, users listening to songs, and users reading scientific papers. In all these settings, Bayesian Poisson factorization outperforms state-of-the-art matrix factorization methods.",2013-11-07,https://www.semanticscholar.org/paper/1b145525d29300f47330d2486f2cf3509fe2308a,arXiv.org
2924,A human lung tumor microenvironment interactome identifies clinically relevant cell-type cross-talk,,2020-05-07,https://www.semanticscholar.org/paper/08f398d38914d7a261f764db287957f3923380c7,Genome Biology
1615,The Holdout Randomization Test for Feature Selection in Black Box Models,"Abstract We propose the holdout randomization test (HRT), an approach to feature selection using black box predictive models. The HRT is a specialized version of the conditional randomization test (CRT) that uses data splitting for feasible computation. The HRT works with any predictive model and produces a valid p-value for each feature. To make the HRT more practical, we propose a set of extensions to maximize power and speed up computation. In simulations, these extensions lead to greater power than a competing knockoffs-based approach, without sacrificing control of the error rate. We apply the HRT to two case studies from the scientific literature where heuristics were originally used to select important features for predictive models. The results illustrate how such heuristics can be misleading relative to principled methods like the HRT. Code is available at https://github.com/tansey/hrt. Supplementary materials for this article are available online.",2018-11-01,https://www.semanticscholar.org/paper/f7725abd17be6bf40f286de54be6aeeee61e0bcf,Journal of Computational And Graphical Statistics
1822,Hierarchical maximum entropy density estimation,"We study the problem of simultaneously estimating several densities where the datasets are organized into overlapping groups, such as a hierarchy. For this problem, we propose a maximum entropy formulation, which systematically incorporates the groups and allows us to share the strength of prediction across similar datasets. We derive general performance guarantees, and show how some previous approaches, such as hierarchical shrinkage and hierarchical priors, can be derived as special cases. We demonstrate the proposed technique on synthetic data and in a real-world application to modeling the geographic distributions of species hierarchically grouped in a taxonomy. Specifically, we model the geographic distributions of species in the Australian wet tropics and Northeast New South Wales. In these regions, small numbers of samples per species significantly hinder effective prediction. Substantial benefits are obtained by combining information across taxonomic groups.",2007-06-20,https://www.semanticscholar.org/paper/4854e506a1b19e6158bce5e5b43697d4aaa1c12a,International Conference on Machine Learning
3167,New estimates indicate that males are not larger than females in most mammals,"Sexual size dimorphism (SSD) has motivated a large body of research on mammalian mating strategies and sexual selection. Despite some contrary evidence, the narrative that larger males are the norm in mammals – upheld since Darwin’s Descent of Man – still dominates today, supported by meta-analyses that use crude measures of dimorphism and taxonomically-biased data. With newly-available datasets and primary sources reporting sex-segregated means and variances in adult body mass, we estimated statistically-determined rates of SSD in mammals, sampling taxa by their species richness at the family level. Our analyses of >400 species indicate that although males tend to be larger than females when dimorphism occurs, males are not larger in most mammals, and suggest a need to revisit other assumptions in sexual selection research. One-Sentence Summary Taxonomically-balanced estimates of rates of sexual size dimorphism in mammals refute the ‘larger males’ narrative.",2023-02-23,https://www.semanticscholar.org/paper/4c41e5c180212eb295e2335ba64725701fc459fb,bioRxiv
3204,"Landscape sustainability science in the drylands: mobility, rangelands and livelihoods",,2020-07-17,https://www.semanticscholar.org/paper/27573bf322281e181cd19f8acfd4602f55f90d83,Landscape Ecology
3510,Scheduling Jobs that Arrive Over Time (Extended Abstract),,1995-08-16,https://www.semanticscholar.org/paper/36bbfc2de85b3dd268b78b8774dd19148dd6e58c,Workshop on Algorithms and Data Structures
2760,Seeing the forest for the trees: hierarchical displays of hypertext structures,"Most recent hypertext systems support hierarchy only as a restricted subset of directed graph structure. Consequently they do not provide many of the capabilities for graphical information hiding and structure manipulation that a tree makes possible. This paper describes display techniques developed for IGD, a hypertext system that supports the creation of large graphical documents whose arbitrary directed graph structure is embedded in a strict hierarchy. IGD offers the full generality of arbitrary keyworded links, while simultaneously allowing hierarchies to be easily manipulated and displayed with much of their structural detail selectively abstracted.",1988-04-01,https://www.semanticscholar.org/paper/0759e40a01b100f9d0b67fc6abe126e640a852f9,Conference on Organizational Computing Systems
742,Computation of Equilibria and Stable Solutions,,2010-09-20,https://www.semanticscholar.org/paper/5642a3a2c86063b99b4599039b00404f146599fe,Safety-critical Systems Symposium
2600,Seeing into the past: creating a 3D modeling pipeline for archaeological visualization,"Archaeology is a destructive process in which accurate and detailed recording of a site is imperative. As a site is exposed, documentation is required in order to recreate and understand the site in context. We have developed a 3D modeling pipeline that can assist archaeologists in the documentation effort by building rich, geometrically and photometrically accurate 3D models of the site. The modeling effort begins with data acquisition (images, range scans, GIS data, and video) and ends with the use of a sophisticated visualization tool that can be used by researchers to explore and understand the site. The pipeline includes new methods for shadow-based registration of 2D images and temporal change detection. Our multimodal augmented reality system allows users wearing head-tracked, see-through, head-worn displays to visualize the site model and associated archaeological artifacts, and to interact with them using speech and gesture.",2004-09-06,https://www.semanticscholar.org/paper/52a7435d23345c156be50bc15a450f14c3b5605a,"Proceedings. 2nd International Symposium on 3D Data Processing, Visualization and Transmission, 2004. 3DPVT 2004."
1379,A precision measurement of the mass of the top quark.,"The standard model of particle physics contains parameters--such as particle masses--whose origins are still unknown and which cannot be predicted, but whose values are constrained through their interactions. In particular, the masses of the top quark (M(t)) and W boson (M(W)) constrain the mass of the long-hypothesized, but thus far not observed, Higgs boson. A precise measurement of M(t) can therefore indicate where to look for the Higgs, and indeed whether the hypothesis of a standard model Higgs is consistent with experimental data. As top quarks are produced in pairs and decay in only about 10(-24) s into various final states, reconstructing their masses from their decay products is very challenging. Here we report a technique that extracts more information from each top-quark event and yields a greatly improved precision (of +/- 5.3 GeV/c2) when compared to previous measurements. When our new result is combined with our published measurement in a complementary decay mode and with the only other measurements available, the new world average for M(t) becomes 178.0 +/- 4.3 GeV/c2. As a result, the most likely Higgs mass increases from the experimentally excluded value of 96 to 117 GeV/c2, which is beyond current experimental sensitivity. The upper limit on the Higgs mass at the 95% confidence level is raised from 219 to 251 GeV/c2.",,https://www.semanticscholar.org/paper/e3ba7046752113f610952b9bc36f3b54b627bd56,Nature
936,Algebraic dependencies,"The relational model for databases [Codd 1970, Ullman 1979] has gained recognition as a valuable formal framework for understanding the semantics, design, and even implementation, of databases. At the heart of the research on relational databases lies the notion of data dependency. Data dependencies are domain-independent (Le., invariant under consistent renamings of domain elements) predicates on databases. Starting ,vith functional [Armstrong 1974] and muttivalued [Fagin 1977] dependencies, a dozen of different kinds of data dependencies have been proposed in the literature [Nicolas 1978, Paradaens 1979, Sagiv and Walecka 1979, and others]. New, more and more general, kinds of data dependencies have been put forward in a rather arbitrary and heuristic fashion. This reflected two major frustrations of the research in this area: First, no natural, stable closure of this process was in sight. Secondly, the elegant complete axiomatizations of functional [Armstrong 1974] and multivalued dependencies [Beeri et ale 1977] did not appear to carry over to the more general kinds; thus the further generalizations were futile attempts at ""enriching the language"" enough so as to obtain a complete axiomatization.",1980-10-13,https://www.semanticscholar.org/paper/fbd716e7164f5d5b12ba4608421d9d3aa88ccb2b,21st Annual Symposium on Foundations of Computer Science (sfcs 1980)
2383,"Decrease in apparent K
m for oxygen after stimulation of respiration of rat polymorphonuclear leukocytes",,1983-09-05,https://www.semanticscholar.org/paper/8769c2bbce94889c9a1c2b85fa235a89420a1b87,FEBS Letters
3507,Improved Length Bounds for the Shortest Superstring Problem (Extended Abstract),,1995-08-16,https://www.semanticscholar.org/paper/08471e3f0bb3ac33269388440ccde34af6171c69,Workshop on Algorithms and Data Structures
902,On Monotone Formulae with Restricted Depth (Preliminary Version),,,https://www.semanticscholar.org/paper/bbf97217be5b73ca76f1f71e3af51279facef311,Symposium on the Theory of Computing
2345,Protein kinase C-dependent and -independent activation of the NADPH oxidase of human neutrophils.,"The protein kinase C inhibitor, staurosporine, inhibited NADPH oxidase activity of human neutrophils activated by phorbol myristate acetate. However, this inhibitor had no effect on either the initiation or the maximal rate of O2- secretion activated by the chemotactic peptide, fMet-Leu-Phe, but resulted in a more rapid termination of oxidant production. Similarly, staurosporine had no effect on the rapid (1 min) increase in luminol-dependent chemiluminescence activated by fMet-Leu-Phe, but the second (intracellular) phase of oxidant production was inhibited. The initial burst of oxidant production during phagocytosis was similarly protein kinase C-independent, but again the later phases of oxidase activity were staurosporine-sensitive. Neutrophils loaded with Quin-2 at concentrations sufficient to act as a Ca2+ buffer could not secrete O2- in response to fMet-Leu-Phe; although the initial (protein kinase C-independent) burst of luminol chemiluminescence was not observed in fMet-Leu-Phe-stimulated Ca2(+)-buffered cells, the second phase of (protein kinase C-dependent) oxidant production was largely unaffected. Hence, the initial burst of oxidant production activated by fMet-Leu-Phe, opsonized zymosan, and latex beads is independent of the activity of protein kinase C-dependent intracellular activation processes, but the activity of this kinase is required to extend or sustain the duration of oxidant production.",1991-04-25,https://www.semanticscholar.org/paper/8ded598e0b3951647253fd6e183c7c4a28f5fb1a,Journal of Biological Chemistry
3426,Online Stochastic Ad Allocation: Efficiency and Fairness,,2010-01-29,https://www.semanticscholar.org/paper/ec3c72bf4bd5d690cf3015b92b27faeaa55bbfd9,arXiv.org
2462,Combating VR sickness through subtle dynamic field-of-view modification,"Virtual Reality (VR) sickness can cause intense discomfort, shorten the duration of a VR experience, and create an aversion to further use of VR. High-quality tracking systems can minimize the mismatch between a user's visual perception of the virtual environment (VE) and the response of their vestibular system, diminishing VR sickness for moving users. However, this does not help users who do not or cannot move physically the way they move virtually, because of preference or physical limitations such as a disability. It has been noted that decreasing field of view (FOV) tends to decrease VR sickness, though at the expense of sense of presence. To address this tradeoff, we explore the effect of dynamically, yet subtly, changing a physically stationary person's FOV in response to visually perceived motion as they virtually traverse a VE. We report the results of a two-session, multi-day study with 30 participants. Each participant was seated in a stationary chair, wearing a stereoscopic head-worn display, and used control and FOV-modifying conditions in the same VE. Our data suggests that by strategically and automatically manipulating FOV during a VR session, we can reduce the degree of VR sickness perceived by participants and help them adapt to VR, without decreasing their subjective level of presence, and minimizing their awareness of the intervention.",2016-03-19,https://www.semanticscholar.org/paper/c2378b9809763e862533c1edd2771b1b68fde5ad,IEEE Symposium on 3D User Interfaces
1552,Identifiable Variational Autoencoders via Sparse Decoding,"We develop the Sparse VAE, a deep generative model for unsupervised representation learning on high-dimensional data. Given a dataset of observations, the Sparse VAE learns a set of latent factors that captures its distribution. The model is sparse in the sense that each feature of the dataset (i.e., each dimension) depends on a small subset of the latent factors. As examples, in ratings data each movie is only described by a few genres; in text data each word is only applicable to a few topics; in genomics, each gene is active in only a few biological processes. We first show that the Sparse VAE is identifiable: given data drawn from the model, there exists a uniquely optimal set of factors. (In contrast, most VAE-based models are not identifiable.) The key assumption behind Sparse-VAE identifiability is the existence of “anchor features”, where for each factor there exists a feature that depends only on that factor. Importantly, the anchor features do not need to be known in advance. We then show how to fit the Sparse VAE with variational EM. Finally, we empirically study the Sparse VAE with both simulated and real data. We find that it recovers meaningful latent factors and has smaller heldout reconstruction error than related methods.",,https://www.semanticscholar.org/paper/fdb74161249e7d787c9b641afcacd248af71c48d,arXiv.org
105,GlOSS: text-source discovery over the Internet,"The dramatic growth of the Internet has created a new problem for users: location of the relevant sources of documents. This article presents a framework for (and experimentally analyzes a solution to) this problem, which we call the text-source discovery problem. Our approach consists of two phases. First, each text source exports its contents to a centralized service. Second, users present queries to the service, which returns an ordered list of promising text sources. This article describes GlOSS, Glossary of Servers Server, with two versions: bGlOSS, which provides a Boolean query retrieval model, and vGlOSS, which provides a vector-space retrieval model. We also present hGlOSS, which provides a decentralized version of the system. We extensively describe the methodology for measuring the retrieval effectiveness of these systems and provide experimental evidence, based on actual data, that all three systems are highly effective in determining promising text sources for a given query.",1999-06-01,https://www.semanticscholar.org/paper/8cd0e93f0c4a79a6a4cd6af1e89d3ffa6123d30c,TODS
242,Alan and I,A personal account of Alan Turing's life and impact.,2012-09-01,https://www.semanticscholar.org/paper/1c0230cc790ab89e3cf197014d960e45aeecc1ed,CACM
2223,"Mavrilimumab, a human monoclonal GM-CSF receptor-α antibody for the management of rheumatoid arthritis: a novel approach to therapy","Introduction: Mavrilimumab, formerly known as CAM-3001, a GM-CSF receptor-α antibody, is the first human monoclonal antibody to be used in Phase II studies (2011) to modulate the innate immunity pathway targeting GM-CSF signaling in moderate rheumatoid arthritis (RA). Areas covered: Analysis of available clinical trial data on GM-CSF receptor-α antibody and medical literature search using MEDLINE for molecular mechanisms of pathogenesis of RA and its treatment forms the basis of this expert opinion review. The mavrilimumab Phase II double blind, randomized, placebo-controlled ascending dose trial demonstrated statistically significant achievement of primary and secondary end points in patients with moderate RA. The trial demonstrated significant clinical benefit in the 100 mg mavrilimumab cohort compared to the placebo group. Expert opinion: The novel molecular targeting mechanism of mavrilimumab together with its demonstrated clinical efficacy, tolerability and safety profile in Phase II clinical trials in moderate RA, suggests significant potential utility for this drug to induce clinical remission, reduce flares and improve morbidity and mortality in patients with RA.",2012-11-10,https://www.semanticscholar.org/paper/53059edc8379f7ba6573b4da84f726fba07f6760,Expert Opinion on Biological Therapy
596,Is distributed locking harder?,"We examine the problem of determining whether a set of locked transactions, accessing a distributed database, is guaranteed to produce only serializable schedules. For a pair of transactions we prove that this concurrency control problem (which is polynomially solvable for centralized databases) is in general coNP-complete. We employ a new graph-theoretic technique and provide an efficient test for the special case of databases distributed between two sites only.",1982-03-29,https://www.semanticscholar.org/paper/a6e21280448a5adf4a6d3423837f5053f0ce59ed,Journal of computer and system sciences (Print)
2630,Data Integration and Access - The Digital Government Research Center's Energy Data Collection (EDC) Project,,,https://www.semanticscholar.org/paper/3d6c0991420130c94efecace92e99233ad0e8683,Advances in Digital Government
199,Variable Binding through Assemblies in Spiking Neural Networks,"We propose a model for the binding of variables to concrete fillers in the human brain. The model is based on recent experimental data about corresponding neural processes in humans. First, electrode recordings from the human brain suggest that concepts are represented in the medial temporal lobe (MTL) through sparse sets of neurons (assemblies). Second, fMRI recordings from the human brain suggest that specific subregions of the temporal cortex are dedicated to the representation of specific roles (e.g., subject or object) of concepts in a sentence or visually presented episode. We propose that quickly recruited assemblies of neurons in these subregions act as pointers to previously created assemblies that represent concepts. As a proof of principle, we performed computer simulations of a spiking neural network model that implemented the proposed paradigm for binding through assembly pointers. We show that the model supports basic operations of brain computations, such as structured recall and copying of information.",,https://www.semanticscholar.org/paper/64455bc7b86c1f4dff304f61c6aff3133a9c84ad,CoCo@NIPS
862,Testing Finite State Machines (Extended Abstract),,,https://www.semanticscholar.org/paper/6f964cb3a1e03ebcd79b9805a11ba542f8f8f2cb,Symposium on the Theory of Computing
1636,Deep and Hierarchical Implicit Models,"Implicit probabilistic models are a flexible class for modeling data. They define a process to simulate observations, and unlike traditional models, they do not require a tractable likelihood function. In this paper, we develop two families of models: hierarchical implicit models and deep implicit models. They combine the idea of implicit densities with hierarchical Bayesian modeling and deep neural networks. The use of implicit models with Bayesian analysis has been limited by our ability to perform accurate and scalable inference. We develop likelihood-free variational inference (LFVI). Key to LFVI is specifying a variational family that is also implicit. This matches the model's flexibility and allows for accurate approximation of the posterior. Our work scales up implicit models to sizes previously not possible and advances their modeling design. We demonstrate diverse applications: a large-scale physical simulator for predator-prey populations in ecology; a Bayesian generative adversarial network for discrete data; and a deep implicit model for text generation.",2017-02-28,https://www.semanticscholar.org/paper/e57c09d50d67060b18806f9597674fdc28320dd2,arXiv.org
2173,Enhanced neutrophil functions during Opisthorchis viverrini infections and correlation with advanced periductal fibrosis.,,2020-01-29,https://www.semanticscholar.org/paper/76a0604294d5983dc48e4767591cd252570ce2ef,International Journal of Parasitology
319,Recognizing Hole-Free 4-Map Graphs in Cubic Time,,2006-06-01,https://www.semanticscholar.org/paper/6a6d0c9e2bf9a1b894ee98b91accffc673b98802,Algorithmica
543,The Discrete Geodesic Problem,"We present an algorithm for determining the shortest path between a source and a destination on an arbitrary (possibly nonconvex) polyhedral surface. The path is constrained to lie on the surface, and distances are measured according to the Euclidean metric. Our algorithm runs in time O(n log n) and requires O(n2) space, where n is the number ofedges ofthe surface. Afterwe run our algorithm, the distance from the source to any other destination may be determined using standard techniques in time O(log n) by locating the destination in the subdivision created by the algorithm. The actual shortest path from the source to a destination can be reported in time O(k+ log n), where k is the number of faces crossed by the path. The algorithm generalizes to the case of multiple source points to build the Voronoi diagram on the surface, where n is now the maximum of the number of vertices and the number of sources.",1987-08-01,https://www.semanticscholar.org/paper/f8902dc723ac2a49ee52efbc58947c21f8d0970b,SIAM journal on computing (Print)
2547,Opportunistic controls: leveraging natural affordances as tangible user interfaces for augmented reality,"We present Opportunistic Controls, a class of user interaction techniques for augmented reality (AR) applications that support gesturing on, and receiving feedback from, otherwise unused affordances already present in the domain environment. Opportunistic Controls leverage characteristics of these affordances to provide passive haptics that ease gesture input, simplify gesture recognition, and provide tangible feedback to the user. 3D widgets are tightly coupled with affordances to provide visual feedback and hints about the functionality of the control. For example, a set of buttons is mapped to existing tactile features on domain objects. We describe examples of Opportunistic Controls that we have designed and implemented using optical marker tracking, combined with appearance-based gesture recognition. We present the results of a user study in which participants performed a simulated maintenance inspection of an aircraft engine using a set of virtual buttons implemented both as Opportunistic Controls and using simpler passive haptics. Opportunistic Controls allowed participants to complete their tasks significantly faster and were preferred over the baseline technique.",2008-10-27,https://www.semanticscholar.org/paper/9fbfa193b1f2bf3ce6e2612a1ba715d7e7538e68,Virtual Reality Software and Technology
2592,Interacting with hidden content using content-aware free-space transparency,"We present <i>content-aware free-space transparency</i>, an approach to viewing and manipulating the otherwise hidden content of obscured windows through unimportant regions of overlapping windows. Traditional approaches to interacting with otherwise obscured content in a window system render an entire window uniformly transparent. In contrast, content-aware free-space transparency uses opaque-to-transparent gradients and image-processing filters to minimize the interference from overlapping material, based on properties of that material. By increasing the amount of simultaneously visible content and allowing basic interaction with otherwise obscured content, without modifying window geometry, we believe that free-space transparency has the potential to improve user productivity.",2004-10-24,https://www.semanticscholar.org/paper/18868cc02732042da589d0206322aab463f92dfa,ACM Symposium on User Interface Software and Technology
2799,Galectin-3 translocates to virological synapse and promotes HIV-1 transfer (VIR1P.1000),"
 Galectin-3 contains β-galactoside-binding domain and is mainly expressed in immune cells and epithelial cells. Galectin-3 is known to play regulatory roles in the immune system to defend against pathogens. Previous studies showed that Alix is a host factor employed by HIV-1 for its replication. Here, we report that galectin-3 is another such factor. We have reported that galectin-3 is translocated to the immunological synapse and affects T cell cytokine production. We have now studied the role of galectin-3 in the virological synapse (VS). Our data showed that HIV-1 infection induces galectin-3 expression in primary CD4 T cells. Immunofluorescence imaging showed that galectin-3 is translocted to the virological synapse and co-localized with Gag and Env at the cell-to-cell junction of HIV-1-infected cells. We have studied the effects of galectin-3 in the VS by using fluorescent-tagged HIV-1 and incubation of fluorescent-labeled T cells with galectin-3 knockeddown (KD). Our analysis indicates that HIV-1 transmission efficacy is significantly attenuated in galectin-3 KD cells comparing to galectin-3-expressing wild type cells. Our analysis also shows that Alix plays a role in promoting cell-to-cell transfer of HIV-1. Further analysis indicate that effects of galectin-3 in effector cells is likely more important than expression of galectin-3 in target cells. We conclude that endogenous galectin-3 translocates to VS and promotes cell-to-cell viral transfer upon HIV-1 infection.",2014-05-01,https://www.semanticscholar.org/paper/59c814ab1436c93e5287ea8e82dbd76a0b6f15c3,Journal of Immunology
1000,Additive diagnostic role of imaging in glaucoma: optical coherence tomography and retinal nerve fiber layer photography.,"PURPOSE
To investigate the additive diagnostic role of spectral-domain optical coherence tomography (SD-OCT) and red-free retinal nerve fiber layer photography (RNFLP) in making clinical glaucoma diagnosis.


METHODS
Four diagnostic combination sets, including the most recent image from each measurement of 196 glaucoma eyes (including the 44 preperimetric glaucoma eyes) and 101 healthy eyes, were prepared: (1) stereo disc photography and Humphrey visual field (SH), (2) SH and SD-OCT (SHO), (3) SH and RNFLP (SHR), and (4) SHR and SD-OCT (SHRO). Each randomly sorted set was serially presented at 1-month intervals to five glaucoma specialists who were asked to evaluate them in a subjective and independent manner. The specialists' glaucoma-diagnostic performances based on the sets were then compared.


RESULTS
For each specialist, adding SD-OCT to SH or SHR increased the glaucoma-diagnostic sensitivity but not to a level of statistical significance. For one specialist, adding RNFLP to SH significantly increased the sensitivity. Each specialist showed a high level of specificity regardless of the diagnostic set. The overall sensitivity of all specialists' assessments was significantly increased by adding RNFLP or the combination of SD-OCT and RNFLP to SH (P < 0.001); however, adding SD-OCT to SH or SHR did not significantly increase the sensitivity. A similar relationship was noted also for the preperimetric glaucoma subgroup.


CONCLUSIONS
In contrast to RNFLP, SD-OCT did not significantly enhance the diagnostic accuracy of detecting glaucoma or even of preperimetric glaucoma. Our results suggest that, at least for glaucoma specialists, the additive diagnostic role of OCT is limited.",2014-12-01,https://www.semanticscholar.org/paper/cae54ce995df4b392c36aceee13aa06842d42392,Investigative Ophthalmology and Visual Science
3192,Body size and digestive system shape resource selection by ungulates: A cross-taxa test of the forage maturation hypothesis.,"The forage maturation hypothesis (FMH) states that energy intake for ungulates is maximised when forage biomass is at intermediate levels. Nevertheless, metabolic allometry and different digestive systems suggest that resource selection should vary across ungulate species. By combining GPS relocations with remotely sensed data on forage characteristics and surface water, we quantified the effect of body size and digestive system in determining movements of 30 populations of hindgut fermenters (equids) and ruminants across biomes. Selection for intermediate forage biomass was negatively related to body size, regardless of digestive system. Selection for proximity to surface water was stronger for equids relative to ruminants, regardless of body size. To be more generalisable, we suggest that the FMH explicitly incorporate contingencies in body size and digestive system, with small-bodied ruminants selecting more strongly for potential energy intake, and hindgut fermenters selecting more strongly for surface water.",2021-07-26,https://www.semanticscholar.org/paper/5386dde0054ad71a7d01a2b30166269a8f35cff2,Ecology Letters
3498,Distributed Job Scheduling in Rings,"We give a distributed approximation algorithm for job scheduling in a ring architecture. In contrast to many other parallel scheduling models, the model we consider captures the influence of the underlying communications network by specifying that task migration from one processor to another takes time proportional to the distance between those two processors in the network. As a result, our algorithm must balance computational load and communication time. The algorithm is simple, requires no global control, and yields schedules of length at most 4.22 times optimal. We also give a lower bound on the performance of any distributed algorithm and the results of simulation experiments which suggest better performance than does our worst-case analysis.",1997-09-15,https://www.semanticscholar.org/paper/93305a605a37c444041bb3c5350790cd6f004cea,J. Parallel Distributed Comput.
2148,Improved Lower Bounds for the Capacity of i.i.d. Deletion and Duplication Channels,"This paper considers the capacity of binary deletion channels, where bits are deleted independently with probability d. It improves significantly upon the best previous framework used to obtain provable lower bounds on this capacity by utilizing a stronger definition of a typical output from the channel. The new results give the best known provable bounds on the capacity for all values of d. Moreover, the techniques presented here extend to yield lower bounds for channels with certain types of random insertions, namely, duplications, or combinations of duplications and deletions. To demonstrate these techniques in this context, two binary channels are analyzed: a channel where each transmitted bit is copied with probability nu and a channel where each transmitted bit is copied a geometrically distributed number of times.",2007-08-01,https://www.semanticscholar.org/paper/06f9b73c9afc6171f467e9467eb1cfc8dd0f4898,IEEE Transactions on Information Theory
3060,VMtorrent: virtual appliances on-demand,"Virtual Appliances (VAs) are Virtual Machines (VMs) geared towards a specific set of tasks. They require little or no configuration, working out-of-the-box. VAs fit neatly into the Cloud Computing paradigm - many copies of an identical machine can be launched in a data center, or home/business users can grab the appliance they need from the cloud to run locally just for so long as required. Companies and projects whose sole offerings are VAs ready for either desktop or data center use [3, 11] attest to the growing popularity of VAs. VMware's Appliance directory alone currently lists over 1400 VAs available for the VMware family of Virtual Machine Monitors (VMMs) [13].
 Current VA distribution generally requires download of the complete virtual disk image, only after which the VA can be run. Given that compressed VA sizes run anywhere from several hundred MB to a few GB, there can be significant delays from the time a user decides he/she wants to run a particular VA until the time that VA can be used. These problems are only exacerbated when demand for particular VAs spikes and server bandwidth resources become the distribution bottleneck.",2010-08-16,https://www.semanticscholar.org/paper/738018d8ebb9fa875bd3b5575e19a080d71a24fd,"Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication"
2158,"Neutrophils form extracellular traps in response to Opisthorchis viverrini crude antigens, which are elevated in neutrophils from opisthorchiasis patients with hepatobiliary abnormalities","ABSTRACT Opisthorchis viverrini (Ov) infection can cause several disease conditions of the bile duct including hepatobiliary abnormalities (HBAs) and the most severe, cholangiocarcinoma (CCA). Fibrosis occurs when tissues are damaged and normal wound-healing responses are dysregulated. Neutrophils are the first cells to migrate to an infection site to protect the host from intruding extracellular pathogens through a wide range of effector mechanisms such as phagocytosis, production of reactive oxygen species, proteases, or release of neutrophil extracellular traps (NETs). In this work, we used confocal microscopy to assess whether Ov crude antigens can cause release of NETs from neutrophils from Ov-free individuals. We demonstrated for the first time that these antigens could induce release of NETs ex vivo in a dose-dependent manner from neutrophils isolated from Ov-free individuals. Intriguingly, when we measured NETs from neutrophils isolated from Ov-infected patients, we found increased spontaneous production of NETs in patients with HBAs. Interestingly, exposure to Ov crude antigens lowered the level of NETs released by neutrophils from patients with active Ov infection regardless of HBA status. We propose that in the case of acute Ov infection, even when concentration of Ov antigens is relatively low, neutrophils can form NETs. However, when this infection becomes chronic, manifesting as a definite HBA, the levels of NET production are reduced when treated with Ov crude antigens. Excessive production of proinflammatory mediators from these NETs might have effects on the parasites, but may also lead to excessive injury of surrounding tissues resulting in HBAs and may lead eventually to the most severe complications such as CCA.",2023-07-26,https://www.semanticscholar.org/paper/14e5adb5f91f22d64b86845a001bc48fed8a98ce,Biology Open
1598,The Blessings of Multiple Causes,"Abstract Causal inference from observational data is a vital problem, but it comes with strong assumptions. Most methods assume that we observe all confounders, variables that affect both the causal variables and the outcome variables. This assumption is standard but it is also untestable. In this article, we develop the deconfounder, a way to do causal inference with weaker assumptions than the traditional methods require. The deconfounder is designed for problems of multiple causal inference: scientific studies that involve multiple causes whose effects are simultaneously of interest. Specifically, the deconfounder combines unsupervised machine learning and predictive model checking to use the dependencies among multiple causes as indirect evidence for some of the unobserved confounders. We develop the deconfounder algorithm, prove that it is unbiased, and show that it requires weaker assumptions than traditional causal inference. We analyze its performance in three types of studies: semi-simulated data around smoking and lung cancer, semi-simulated data around genome-wide association studies, and a real dataset about actors and movie revenue. The deconfounder is an effective approach to estimating causal effects in problems of multiple causal inference. Supplementary materials for this article are available online.",2018-05-17,https://www.semanticscholar.org/paper/4e6b198578895bbfe968c92d64dd1e3ef2e977be,Journal of the American Statistical Association
3215,"Updated geographic range maps for giraffe,
 Giraffa
 spp., throughout sub‐Saharan Africa, and implications of changing distributions for conservation","1 . Giraffe populations have declined in abundance by almost 40% over the last three decades, and the geographic ranges of the species (previously believed to be one, now defined as four species) have been significantly reduced or altered. With substantial changes in land uses, loss of habitat, declining abundance, translocations, and data gaps, the existing geographic range maps for giraffe need to be updated. 2 . We performed a review of existing giraffe range data, including aerial and ground observations of giraffe, existing geographic range maps, and available literature. The information we collected was discussed with and validated by subject-matter experts. Our updates may serve to correct inaccuracies or omissions in the baseline map, or may reflect actual changes in the distribution of giraffe. 3 . Relative to the 2016 List Assessment range map, the updated geographic range maps show a 5.6% decline in the range area of all giraffe taxa combined. The ranges of Giraffa camelopardalis (northern giraffe) and Giraffa tippelskirchi (Masai giraffe) de-creased in area by 37% (122432 km 2 ) and 4.7% (20816 km 2 ) respectively, whereas 14% (41696 km 2 ) of the range of Giraffa reticulata (reticulated giraffe) had not been included in the original geographic range map and has now been added. The range of Giraffa giraffa (southern giraffe) showed little overall change; it increased by 0.1% (419 km 2 ).",2019-08-06,https://www.semanticscholar.org/paper/1da4ca74e3921f527e67352d5df5b41a233e51bd,Mammal Review
193,STDP Forms Associations between Memory Traces in Networks of Spiking Neurons,"Memory traces and associations between them are fundamental for cognitive brain function. Neuron recordings suggest that distributed assemblies of neurons in the brain serve as memory traces for spatial information, real-world items, and concepts. However, there is conflicting evidence regarding neural codes for associated memory traces. Some studies suggest the emergence of overlaps between assemblies during an association, while others suggest that the assemblies themselves remain largely unchanged and new assemblies emerge as neural codes for associated memory items. Here we study the emergence of neural codes for associated memory items in a generic computational model of recurrent networks of spiking neurons with a data-based rule for spike-timing-dependent plasticity (STDP). The model depends critically on two parameters, which control the excitability of neurons and the scale of initial synaptic weights. By modifying these two parameters, the model can reproduce both experimental data from the human brain on the fast formation of associations through emergent overlaps between assemblies, and rodent data where new neurons are recruited to encode the associated memories. Hence our findings suggest that the brain can use both of these two neural codes for associations, and dynamically switch between them during consolidation.",2017-09-14,https://www.semanticscholar.org/paper/ea98f42ac8adb6cefccbc3b33099915a48c44eae,bioRxiv
1072,First Dark Matter Constraints from a SuperCDMS Single-Charge Sensitive Detector.,"We present the first limits on inelastic electron-scattering dark matter and dark photon absorption using a prototype SuperCDMS detector having a charge resolution of 0.1 electron-hole pairs (CDMS HVeV, a 0.93 g CDMS high-voltage device). These electron-recoil limits significantly improve experimental constraints on dark matter particles with masses as low as 1  MeV/c^{2}. We demonstrate a sensitivity to dark photons competitive with other leading approaches but using substantially less exposure (0.49 g d). These results demonstrate the scientific potential of phonon-mediated semiconductor detectors that are sensitive to single electronic excitations.",2018-04-27,https://www.semanticscholar.org/paper/7cf7ab28d8c0c2256f2cf7b617e99d69e5443549,Physical Review Letters
1782,Probabilistic Topic Models,"In this article, we review probabilistic topic models: graphical models that can be used to summarize a large collection of documents with a smaller number of distributions over words. Those distributions are called ""topics"" because, when fit to data, they capture the salient themes that run through the collection. We describe both finite-dimensional parametric topic models and their Bayesian nonparametric counterparts, which are based on the hierarchical Dirichlet process (HDP). We discuss two extensions of topic models to time-series data-one that lets the topics slowly change over time and one that lets the assumed prevalence of the topics change. Finally, we illustrate the application of topic models to nontext data, summarizing some recent research results in image analysis.",2010-10-18,https://www.semanticscholar.org/paper/44ab78d0605f2b4905e19ba56d6e51529da4bed9,IEEE Signal Processing Magazine
2059,Data mining to improve personnel selection and enhance human capital: A case study in high-technology industry,,,https://www.semanticscholar.org/paper/e427b25e719e2d165e4d53f078c1d2b791e09729,Expert systems with applications
2051,OR in the electronics industry,,2008-01-11,https://www.semanticscholar.org/paper/73877d51d9a8cdf0ae6eecdb4604368ed9353835,OR Spectr.
941,Modeling communications protocols by automata,"Using a pair of finite-state automata to model the transmitter-receiver protocol in a data communications system, we derive lower bounds on the size of automata needed to achieve reliable communication across an error-phone channel. We also show that, at the cost of increasing the size of the automata, a transmission rate close to the theoretical maximum can be achieved.",1979-10-29,https://www.semanticscholar.org/paper/c6df09c5e61c006237ab487584d148ab6fbadde7,20th Annual Symposium on Foundations of Computer Science (sfcs 1979)
900,Serializability by Locking,"The power of locking as a primitive for controlling concurrency in database systems is examined. It is accepted that the concurrent execution (or schedule) of different transactions must be serializable; that is, it must behave like a serial schedule, one in which the transactions run one at a time. It is shown that locking cannot achieve the full power of serializability. An exact characterization of the schedules that can be produced if locking is used to control concurrency is given for two versions of serializability. In the first one, state serializability, only the effect of the schedule on the database is taken into account. In the second one, view serializability, the view of the data received by the transactions is also taken into account. The author shows that it is possible to determine efficiently whether the transactions in a given set can be permitted to run safely by themselves without the need of any control while ensuring view serializability, although the problem is np-complete in the case of state serializability. 20 references.",1984-03-30,https://www.semanticscholar.org/paper/27b37c7073a241a4071c29bff5a43f73d6388086,JACM
1634,Proximity Variational Inference,"Variational inference is a powerful approach for approximate posterior inference. However, it is sensitive to initialization and can be subject to poor local optima. In this paper, we develop proximity variational inference (PVI). PVI is a new method for optimizing the variational objective that constrains subsequent iterates of the variational parameters to robustify the optimization path. Consequently, PVI is less sensitive to initialization and optimization quirks and finds better local optima. We demonstrate our method on three proximity statistics. We study PVI on a Bernoulli factor model and sigmoid belief network with both real and synthetic data and compare to deterministic annealing (Katahira et al., 2008). We highlight the flexibility of PVI by designing a proximity statistic for Bayesian deep learning models such as the variational autoencoder (Kingma and Welling, 2014; Rezende et al., 2014). Empirically, we show that PVI consistently finds better local optima and gives better predictive performance.",2017-05-24,https://www.semanticscholar.org/paper/b5c504b1383170939a1b122e400a906332fe0423,International Conference on Artificial Intelligence and Statistics
1656,The Generalized Reparameterization Gradient,"The reparameterization gradient has become a widely used method to obtain Monte Carlo gradients to optimize the variational objective. However, this technique does not easily apply to commonly used distributions such as beta or gamma without further approximations, and most practical applications of the reparameterization gradient fit Gaussian distributions. In this paper, we introduce the generalized reparameterization gradient, a method that extends the reparameterization gradient to a wider class of variational distributions. Generalized reparameterizations use invertible transformations of the latent variables which lead to transformed distributions that weakly depend on the variational parameters. This results in new Monte Carlo gradients that combine reparameterization gradients and score function gradients. We demonstrate our approach on variational inference for two complex probabilistic models. The generalized reparameterization is effective: even a single sample from the variational distribution is enough to obtain a low-variance gradient.",2016-10-07,https://www.semanticscholar.org/paper/787ffb182e0555691d1c90047e16b8f3ff49bf0b,Neural Information Processing Systems
30,Session details: Research session 9: data on the web,,2009-06-29,https://www.semanticscholar.org/paper/6e629e0276c4fa3396c351657d918f91ec873a42,ACM SIGMOD Conference
500,How to learn an unknown environment,"The authors consider the problem faced by a newborn that must explore and learn an unknown room with obstacles in it. They seek algorithms that achieve a bounded ratio of the worst-case distance traversed in order to see all visible points of the environment (thus creating a map), divided by the optimum distance needed to verify the map. The situation is complicated by the fact that the latter offline problem (optimally verifying a map) is NP-hard and thus must be solved approximately. Although the authors show that there is no such competitive algorithm for general obstacle courses, they give a competitive algorithm for the case of a polygonal room with a bounded number of obstacles in it.<<ETX>>",1991-09-01,https://www.semanticscholar.org/paper/76796667145fda08526b455d23e66952ba17eaca,[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
2428,The Effect of Narrow Field of View and Information Density on Visual Search Performance in Augmented Reality,"Many optical-see-through displays have a relatively narrow field of view. However, a limited field of view can constrain how information can be presented and searched through. To understand these constraints, we present a series of experiments that address the interrelationships between field of view, information density, and search performance. We do so by simulating various fields of view using two approaches: limiting the field of view presented on a Microsoft HoloLens optical-see-through head-worn display and dynamically changing the portion of a large tiled-display wall on which information is presented, for head-tracked users in both cases. Our results indicate a significant effect of information density and field of view on search performance, with potential search performance benefits of using a larger FOV between ca. 7-28%. Furthermore, while grids guided visual search, they did not significantly affect performance.",2019-03-01,https://www.semanticscholar.org/paper/98f4ffb6b0c5d79e0e735f1f061db025a50c9bdd,IEEE Conference on Virtual Reality and 3D User Interfaces
390,Theoretical Problems Related to the Internet,,2000-07-26,https://www.semanticscholar.org/paper/2112dd2d9f125e3d987b91f0dfde76f6f4bc5f7c,International Computing and Combinatorics Conference
2792,IL-10 is overexpressed in human cutaneous T-cell lymphoma and is required for maximal tumor growth in a mouse model,"Abstract A crucial question pertains to a role of IL-10 as a tumorigenic factor, or just a marker of advanced disease in cutaneous T-cell lymphoma (CTCL). Herein, we measured significantly elevated IL-10 mRNA in a cohort of skin samples of patients with CTCL. Increased IL-10 was also detected in the tumor microenvironment of an established inflammation-dependent murine model of using MBL2 T lymphoma cells. Conditioned media from MBL2 cells was able to stimulate IL-10 production in bone marrow-derived macrophages in an IL-4-dependent manner. Implanted MBL2 T-cell lymphomas in IL-10KO mice were 50% smaller, accompanied by decreased numbers of infiltrating macrophages and reduced efficiency of M2-polarization compared with wild-type mice. With anti-IL-10R mAb treatment, both wild-type tumor-bearing mice and IL-10KO mice exhibited a further growth inhibition. Our data indicate that targeting IL-10 signaling with neutralizing antibodies to IL-10 or its receptor may have a great potential for advanced CTCL therapy.",2018-10-02,https://www.semanticscholar.org/paper/95747452c8cff59dde8b737fd33fd70dc12f4617,Leukemia and Lymphoma
547,The Complexity of the Travelling Repairman Problem,"Le probleme du reparateur itinerant consiste en la donnee d'un ensemble fini de points, et des temps de parcours entre ces points. Le but est de trouver une trajectoire qui passe par tous ces points et qui minimise la duree totale du trajet. On etudie ce probleme dans le cas ou tous les points sont alignes, et on presente une solution polynomiale. Si le retard maximum pour chaque point est borne, le probleme devient NP-complet, mais il peut etre resolu avec un algorithme pseudo-polynome",,https://www.semanticscholar.org/paper/373db0e4aef0c59c634be8ec835262d0a449b607,RAIRO - Theoretical Informatics and Applications
11,k-Shape: Efficient and Accurate Clustering of Time Series,"The proliferation and ubiquity of temporal data across many disciplines has generated substantial interest in the analysis and mining of time series. Clustering is one of the most popular data mining methods, not only due to its exploratory power, but also as a preprocessing step or subroutine for other techniques. In this paper, we describe k-Shape, a novel algorithm for time-series clustering. k-Shape relies on a scalable iterative refinement procedure, which creates homogeneous and well-separated clusters. As its distance measure, k-Shape uses a normalized version of the cross-correlation measure in order to consider the shapes of time series while comparing them. Based on the properties of that distance measure, we develop a method to compute cluster centroids, which are used in every iteration to update the assignment of time series to clusters. An extensive experimental evaluation against partitional, hierarchical, and spectral clustering methods, with the most competitive distance measures, showed the robustness of k-Shape. Overall, k-Shape emerges as a domain-independent, highly accurate, and efficient clustering approach for time series with broad applications.",,https://www.semanticscholar.org/paper/229f87832bc93cc4a4ba9a228e33135799dcf4bf,SGMD
835,Multiway Cuts in Directed and Node Weighted Graphs,,1994-07-11,https://www.semanticscholar.org/paper/ce18a622d6cfc38ff739f850f01a750d33587fd4,"International Colloquium on Automata, Languages and Programming"
3279,"A Free-Ranging, Feral Mare Equus caballus Affords Similar Maternal Care to Her Genetic and Adopted Offspring","Adoption of nongenetic offspring occurs in a variety of species but is rare in equids. We report a case of adoption by a free-ranging, feral mare Equus caballus and compare the maternal care received by her genetic offspring (born 1995) to that of her adopted offspring (born 1996) for the first 30 weeks of development. We compare five measures of care: (1) total time spent suckling, (2) mare aggression during suckling, (3) number of mare-terminated suckling bouts, (4) contact maintenance, and (5) mare-foal distance. For most behaviors, we detected no difference in the mare’s treatment of the two foals; however, mare-foal distance was greater for the genetic offspring. We compare hypotheses regarding the reasons for adoption, offering postpartum physiological state as a potential driver.",2013-09-05,https://www.semanticscholar.org/paper/74a1bc5718be883764e3fd17e2d0979c5a09854f,American Naturalist
1884,Digital transformation to empower smart production for Industry 3.5 and an empirical study for textile dyeing,,2020-04-01,https://www.semanticscholar.org/paper/6105a49a9fe97438f6a86f977993c0a10ec090db,Computers & industrial engineering
332,Recent Developments in Equilibria Algorithms,,2005-12-15,https://www.semanticscholar.org/paper/64cf156a8b357aa592fc19f613470a46bb31debd,Workshop on Internet and Network Economics
2976,A nonparametric variable clustering model,"Factor analysis models effectively summarise the covariance structure of high dimensional data, but the solutions are typically hard to interpret. This motivates attempting to find a disjoint partition, i.e. a simple clustering, of observed variables into highly correlated subsets. We introduce a Bayesian non-parametric approach to this problem, and demonstrate advantages over heuristic methods proposed to date. Our Dirichlet process variable clustering (DPVC) model can discover block-diagonal covariance structures in data. We evaluate our method on both synthetic and gene expression analysis problems.",2012-12-03,https://www.semanticscholar.org/paper/8ff4f681a02be8bddcc69e831f9a7badb422a330,Neural Information Processing Systems
993,"Cross-Modal and Intra-Modal Characteristics of Visual Function and Speech Perception Performance in Postlingually Deafened, Cochlear Implant Users","Evidence of visual-auditory cross-modal plasticity in deaf individuals has been widely reported. Superior visual abilities of deaf individuals have been shown to result in enhanced reactivity to visual events and/or enhanced peripheral spatial attention. The goal of this study was to investigate the association between visual-auditory cross-modal plasticity and speech perception in post-lingually deafened, adult cochlear implant (CI) users. Post-lingually deafened adults with CIs (N = 14) and a group of normal hearing, adult controls (N = 12) participated in this study. The CI participants were divided into a good performer group (good CI, N = 7) and a poor performer group (poor CI, N = 7) based on word recognition scores. Visual evoked potentials (VEP) were recorded from the temporal and occipital cortex to assess reactivity. Visual field (VF) testing was used to assess spatial attention and Goldmann perimetry measures were analyzed to identify differences across groups in the VF. The association of the amplitude of the P1 VEP response over the right temporal or occipital cortex among three groups (control, good CI, poor CI) was analyzed. In addition, the association between VF by different stimuli and word perception score was evaluated. The P1 VEP amplitude recorded from the right temporal cortex was larger in the group of poorly performing CI users than the group of good performers. The P1 amplitude recorded from electrodes near the occipital cortex was smaller for the poor performing group. P1 VEP amplitude in right temporal lobe was negatively correlated with speech perception outcomes for the CI participants (r = -0.736, P = 0.003). However, P1 VEP amplitude measures recorded from near the occipital cortex had a positive correlation with speech perception outcome in the CI participants (r = 0.775, P = 0.001). In VF analysis, CI users showed narrowed central VF (VF to low intensity stimuli). However, their far peripheral VF (VF to high intensity stimuli) was not different from the controls. In addition, the extent of their central VF was positively correlated with speech perception outcome (r = 0.669, P = 0.009). Persistent visual activation in right temporal cortex even after CI causes negative effect on outcome in post-lingual deaf adults. We interpret these results to suggest that insufficient intra-modal (visual) compensation by the occipital cortex may cause negative effects on outcome. Based on our results, it appears that a narrowed central VF could help identify CI users with poor outcomes with their device.",2016-02-05,https://www.semanticscholar.org/paper/8db654c7045b06ad97b6763f3a308bcaf58adaa4,PLoS ONE
949,Change of peripapillary retinal nerve fiber layer and choroidal thickness during 4-year myopic progress: Boramae Myopia Cohort Study Report 4,"Aims To investigate the longitudinal changes of peripapillary retinal nerve fibre layer (RNFL) and choroidal thickness during myopic axial elongation. Methods Peripapillary RNFL and choroidal thickness were prospectively evaluated by spectral-domain optical coherence tomography (SD-OCT) in 46 eyes of 23 myopic children over the course of 4 years. Using serial OCT images acquired based on a fixed scan circle in the glaucoma progression analysis mode, general and sectoral RNFL thicknesses were acquired at the same position and the angular location of the peak was measured. The peripapillary choroidal thickness likewise was measured at eight positions in serial OCT images. Results The mean age at the baseline was 9.6±1.7 years. The mean axial length increased from 24.80±1.28 mm to 25.64±1.35 mm. The global peripapillary RNFL thickness was 98.54±12.06 µm at baseline. The global and sectoral RNFL thicknesses did not change during the 4 years. The angular location of RNFL peaks was also stable and was located in the superotemporal (64.18±10.85°) and inferotemporal (293.98±11.62°) sectors. The global peripapillary choroidal thickness was 145.40±28.67 µm at the baseline. The global and sectoral choroidal thicknesses did not change during the 4 years. Conclusions The peripapillary RNFL and choroidal thicknesses as well as the locations of the RNFL peaks had been preserved, during the 4-year follow-up on myopic children, when traced and measured from the same location.",2022-04-05,https://www.semanticscholar.org/paper/fc8b71d990189adec23fdad4ad2fb0b3274586e8,British Journal of Ophthalmology
212,On the optimality of grid cells,"Grid cells, discovered more than a decade ago [5], are neurons in the brain of mammals that fire when the animal is located near certain specific points in its familiar terrain. Intriguingly, these points form, for a single cell, a two-dimensional triangular grid, not unlike our Figure 3. Grid cells are widely believed to be involved in path integration, that is, the maintenance of a location state through the summation of small displacements. We provide theoretical evidence for this assertion by showing that cells with grid-like tuning curves are indeed well adapted for the path integration task. In particular we prove that, in one dimension under Gaussian noise, the sensitivity of measuring small displacements is maximized by a population of neurons whose tuning curves are near-sinusoids -- that is to say, with peaks forming a one-dimensional grid. We also show that effective computation of the displacement is possible through a second population of cells whose sinusoid tuning curves are in phase difference from the first. In two dimensions, under additional assumptions it can be shown that measurement sensitivity is optimized by the product of two sinusoids, again yielding a grid-like pattern. We discuss the connection of our results to the triangular grid pattern observed in animals.",2016-06-15,https://www.semanticscholar.org/paper/e3df557d0e40e2464773960a81bc12ec914b6b8f,arXiv.org
788,AMC: An Adaptive Model Checker,,2002-07-27,https://www.semanticscholar.org/paper/e4d5c4ce1760cf2f0746178e7b18feb5ab240b11,International Conference on Computer Aided Verification
1035,Snakeboard motion planning with viscous friction and skidding,"The snakeboard is a well-studied example for mechanical systems analysis, largely because of its simultaneous richness in behavior and simplicity in design. However, few snakeboard models incorporate dissipative friction in the traveling direction and skidding as a violation of the rigid nonholonomic constraints. In this paper we investigate these effects on trajectory planning by evaluating a previously proposed friction model as well as a novel skidding model based on the addition of Rayleigh dissipation functions. We show how these additions change the usual behavior of gaits in the forward planning problem, and incorporate the changes into the solutions of the inverse planning problem by utilizing body coordinates along with a curvature parameterization for trajectories.",2015-05-26,https://www.semanticscholar.org/paper/b890772a9b44fc9f153c63e7046f427067613c6d,IEEE International Conference on Robotics and Automation
865,High-Probability Parallel Transitive-Closure Algorithms,"There is a straightforward algorithm for computing the transitive-closure of an n-node graph in $O(\log ^2 n)$ time on an EREW-PRAM, using $n^3 / \log n$ processors, or indeed with $M(n) / \log n$ processors if serial matrix multiplication in $M(n)$ time can be done. This algorithm is within a log factor of optimal in work (processor-time product), for solving the all-pairs transitive-closure problem for dense graphs. However, this algorithm is far from optimal when either (a) the graph is sparse, or (b) we want to solve the single-source transitive-closure problem. It would be ideal to have an $\mathcal{NC}$ algorithm for transitive-closure that took about e processors for the single-source problem on a graph with n nodes and $e \geqq n$ arcs, or about $en$ processors for the all-pairs problem on the same graph. While an algorithm that good cannot be offered, algorithms with the following performance can be offered. (1) For single-source, $\tilde{O}(n^\varepsilon )$ time with $\tilde O(en^{1 - 2\varepsil...",1991-02-01,https://www.semanticscholar.org/paper/db9d3796a8b0543c442127f164efd193ef3853fb,SIAM journal on computing (Print)
451,Database metatheory: asking the big queries,"Is “database theory” an oxymoron? Or is ata platitude? What is the fitness measure that decides the surviva! of ideas (and areas) in mathematics, in applted science, and in computer science? Which ideas from database theory during the past twenty-five years have influenced research in other fields of computer science? How many were encapsulated in actual products? Was the relational model the on[y true paradigm sh@ m computer science ? Is applicability the only and ultimate justification of theoretical research in an applied science? Are applicability pressures rea!ly exogenous and unwelcome? Are negattve results appropriate goals of theoretical research in an appiied science —or are they the on[y possibie such research goals? If scientific theories must be refutab!e, what are the “hard facts” that provide the possibility of refutation in the case of database theory?",1995-05-22,https://www.semanticscholar.org/paper/0e76f58b096f7d9a3e04e762d87fe879f1feac85,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems
1755,Scalable Inference of Overlapping Communities,"We develop a scalable algorithm for posterior inference of overlapping communities in large networks. Our algorithm is based on stochastic variational inference in the mixed-membership stochastic blockmodel (MMSB). It naturally interleaves subsampling the network with estimating its community structure. We apply our algorithm on ten large, real-world networks with up to 60,000 nodes. It converges several orders of magnitude faster than the state-of-the-art algorithm for MMSB, finds hundreds of communities in large real-world networks, and detects the true communities in 280 benchmark networks with equal or better accuracy compared to other scalable algorithms.",2012-12-03,https://www.semanticscholar.org/paper/9a5eb8fa8df5d3311b27b895ea1af67b5e35f7ff,Neural Information Processing Systems
2952,Pitman Yor Diffusion Trees for Bayesian Hierarchical Clustering,"In this paper we introduce the Pitman Yor Diffusion Tree (PYDT), a Bayesian non-parametric prior over tree structures which generalises the Dirichlet Diffusion Tree [30] and removes the restriction to binary branching structure. The generative process is described and shown to result in an exchangeable distribution over data points. We prove some theoretical properties of the model including showing its construction as the continuum limit of a nested Chinese restaurant process model. We then present two alternative MCMC samplers which allow us to model uncertainty over tree structures, and a computationally efficient greedy Bayesian EM search algorithm. Both algorithms use message passing on the tree structure. The utility of the model and algorithms is demonstrated on synthetic and real world data, both continuous and binary.",2015-02-01,https://www.semanticscholar.org/paper/04981e6b2063168038c1c1aa3cc4f009892a8829,IEEE Transactions on Pattern Analysis and Machine Intelligence
3117,Improving web browsing performance on wireless pdas using thin-client computing,"Web applications are becoming increasingly popular for mobile wireless PDAs. However, web browsing on these systems can be quite slow. An alternative approach is handheld thin-client computing, in which the web browser and associated application logic run on a server, which then sends simple screen updates to thePDA for display. To assess the viability of this thin-client approach, we compare the web browsing performance of thin clients against fat clients that run the web browser locally on a PDA. Our results show that thin clients can provide better web browsing performance compared to fat clients, both in terms of speed and ability to correctly display web content. Surprisingly, thin clients are faster even when having to send more data over the network. We characterize and analyze different design choices in various thin-client systems and explain why these approaches can yield superior web browsing performance on mobile wireless PDAs.",2004-05-17,https://www.semanticscholar.org/paper/1a2ff6a4c321174d26eacbd63c8b67f48077077c,The Web Conference
679,Efficient energy management and data recovery in sensor networks using latent variables based tensor factorization,"A key factor in a successful sensor network deployment is finding a good balance between maximizing the number of measurements taken (to maintain a good sampling rate) and minimizing the overall energy consumption (to extend the network lifetime). In this work, we present a data-driven statistical model to optimize this tradeoff. Our approach takes advantage of the multivariate nature of the data collected by a heterogeneous sensor network to learn spatio-temporal patterns. These patterns enable us to employ an aggressive duty cycling policy on the individual sensor nodes, thereby reducing the overall energy consumption. Our experiments with the OMNeT++ network simulator using realistic wireless channel conditions, on data collected from two real-world sensor networks, show that we can sample just 20% of the data and can reconstruct the remaining 80% of the data with less than 9% mean error, outperforming similar techniques such is distributed compressive sampling. In addition, energy savings ranging up to 76%, depending on the sampling rate and the hardware configuration of the node.",2013-11-03,https://www.semanticscholar.org/paper/118d0304b459b4c338f5c7368187f0d7c8638a62,"International Conference on Modeling, Analysis and Simulation of Wireless and Mobile Systems"
118,STARTS: Stanford Proposal for Internet Meta-Searching (Experience Paper),,,https://www.semanticscholar.org/paper/e23529a0bf1a475db93951c93524b6af427cec4c,SIGMOD Conference
797,On the approximability of trade-offs and optimal access of Web sources,"We study problems in multiobjective optimization, in which solutions to a combinatorial optimization problem are evaluated with respect to several cost criteria, and we are interested in the trade-off between these objectives (the so-called Pareto curve). We point out that, under very general conditions, there is a polynomially succinct curve that /spl epsiv/-approximates the Pareto curve, for any /spl epsiv/>0. We give a necessary and sufficient condition under which this approximate Pareto curve can be constructed in time polynomial in the size of the instance and 1//spl epsiv/. In the case of multiple linear objectives, we distinguish between two cases: when the underlying feasible region is convex, then we show that approximating the multi-objective problem is equivalent to approximating the single-objective problem. If however the feasible region is discrete, then we point out that the question reduces to an old and recurrent one: how does the complexity of a combinatorial optimization problem change when its feasible region is intersected with a hyperplane with small coefficients; we report some interesting new findings in this domain. Finally, we apply these concepts and techniques to formulate and solve approximately a cost-time-quality trade-off for optimizing access to the World-Wide Web, in a model first studied by Etzioni et al. (1996) (which was actually the original motivation for this work).",2000-11-12,https://www.semanticscholar.org/paper/87e3b80fa6a8902a1f00b336c1a2134991d2f44e,Proceedings 41st Annual Symposium on Foundations of Computer Science
3736,VideoBERT: A Joint Model for Video and Language Representation Learning,"Self-supervised learning has become increasingly important to leverage the abundance of unlabeled data available on platforms like YouTube. Whereas most existing approaches learn low-level representations, we propose a joint visual-linguistic model to learn high-level features without any explicit supervision. In particular, inspired by its recent success in language modeling, we build upon the BERT model to learn bidirectional joint distributions over sequences of visual and linguistic tokens, derived from vector quantization of video data and off-the-shelf speech recognition outputs, respectively. We use VideoBERT in numerous tasks, including action classification and video captioning. We show that it can be applied directly to open-vocabulary classification, and confirm that large amounts of training data and cross-modal information are critical to performance. Furthermore, we outperform the state-of-the-art on video captioning, and quantitative results verify that the model learns high-level semantic features.",2019-04-03,https://www.semanticscholar.org/paper/c41a11c0e9b8b92b4faaf97749841170b760760a,IEEE International Conference on Computer Vision
1014,Electrophysiological effects of phencyclidine and the sigma agonist ditolylguanidine in the cerebellum of the rat,,1992-01-31,https://www.semanticscholar.org/paper/4c8e4c401da08b25129888d3992897f744ee5ee5,Neuropharmacology
1523,A Bayesian Causal Inference Approach for Assessing Fairness in Clinical Decision-Making,"Fairness in clinical decision-making is a critical element of health equity, but assessing fairness of clinical decisions from observational data is challenging. Recently, many fairness notions have been proposed to quantify fairness in decision-making, among which causality-based fairness notions have gained increasing attention due to its potential in adjusting for confounding and reasoning about bias. However, causal fairness notions remain under-explored in the context of clinical decision-making with large-scale healthcare data. In this work, we propose a Bayesian causal inference approach for assessing a causal fairness notion called principal fairness in clinical settings. We demonstrate our approach using both simulated data and electronic health records (EHR) data.",2022-11-21,https://www.semanticscholar.org/paper/60c6c0fbae81caee615f328cbc46c4a9a6f0cd22,arXiv.org
3226,An Animal Detection Pipeline for Identification,"This paper proposes a 5-component detection pipeline for use in a computer vision-based animal recognition system. The end result of our proposed pipeline is a collection of novel annotations of interest (AoI) with species and view-point labels. These AoIs, for example, could be fed as the focused input data into an appearance-based animal identification system. The goal of our method is to increase the reliability and automation of animal censusing studies and to provide better ecological information to conservationists. Our method is able to achieve a localization mAP of 81.67%, a species and viewpoint annotation classification accuracy of 94.28% and 87.11%, respectively, and an AoI accuracy of 72.75% across 6 animal species of interest. We also introduce the Wildlife Image and Localization Dataset (WILD), which contains 5,784 images and 12,007 labeled annotations across 28 classification species and a variety of challenging, real-world detection scenarios.",2018-05-03,https://www.semanticscholar.org/paper/6f3b9616e883b67debf4748473c58161b43cd667,IEEE Workshop/Winter Conference on Applications of Computer Vision
168,Game dynamics as the meaning of a game,"Learning dynamics have traditionally taken a secondary role to Nash equilibria in game theory. We propose a new approach that places the understanding of game dynamics over mixed strategy profiles as the central object of inquiry. We focus on the stable recurrent points of the dynamics, i.e. states which are likely to be revisited infinitely often; obviously, pure Nash equilibria are a special case of such behavior. We propose a new solution concept, the Markov-Conley Chain (MCC), which has several favorable properties: It is a simple randomized generalization of the pure Nash equilibrium, just like the mixed Nash equilibrium; every game has at least one MCC; an MCC is invariant under additive constants and positive multipliers of the players' utilities; there is a polynomial number of MCCs in any game, and they can be all computed in polynomial time; the MCCs can be shown to be, in a well defined sense, surrogates or traces of an important but elusive topological object called the sink chain component of the dynamics; finally, it can be shown that a natural game dynamics surely ends up at one of the MCCs of the game.",2019-05-07,https://www.semanticscholar.org/paper/2151cc3fcf890ee6b04dd529f0d5a6bcb1237294,SeCO Workshops
869,Graph-theoretic methods in database theory,"As in many areas of computer science and other disciplines, graph theoretic tools play an important role also in databases. Many concepts are best captured in terms of graphs or hypergraphs, and problems can then be formulated and solved using graph theoretic algorithms. There is a great number of such examples from schema design, dependency theory, transaction processing, query optimization, data distribution, and a host of other areas. We will not attempt to touch on the wide range of all these applications. Rather, we will concentrate on a particular, basic type of problems that has attracted a great deal of attention in the database literature over the last few years and has come to play a central role: techniques for searching graphs and computing transitive closure, and some of the applications and related problems in query processing. There is an extensive literature on these types of problems, which we cannot reasonably hope to cover in this space, but we shall give a flavour of the issues that arise in solving these problems in various frameworks.",1990-04-02,https://www.semanticscholar.org/paper/62fa835ba18dfac1e5ee801da1e27634cc5370e7,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems
1816,Relative Performance Guarantees for Approximate Inference in Latent Dirichlet Allocation,"Hierarchical probabilistic modeling of discrete data has emerged as a powerful tool for text analysis. Posterior inference in such models is intractable, and practitioners rely on approximate posterior inference methods such as variational inference or Gibbs sampling. There has been much research in designing better approximations, but there is yet little theoretical understanding of which of the available techniques are appropriate, and in which data analysis settings. In this paper we provide the beginnings of such understanding. We analyze the improvement that the recently proposed collapsed variational inference (CVB) provides over mean field variational inference (VB) in latent Dirichlet allocation. We prove that the difference in the tightness of the bound on the likelihood of a document decreases as O(k - 1) + √log m/m, where k is the number of topics in the model and m is the number of words in a document. As a consequence, the advantage of CVB over VB is lost for long documents but increases with the number of topics. We demonstrate empirically that the theory holds, using simulated text data and two text corpora. We provide practical guidelines for choosing an approximation.",2008-12-08,https://www.semanticscholar.org/paper/497e22323137e909f64dccd8695dcfb72e82e032,Neural Information Processing Systems
2910,Faithful Heteroscedastic Regression with Neural Networks,"Heteroscedastic regression models a Gaussian variable's mean and variance as a function of covariates. Parametric methods that employ neural networks for these parameter maps can capture complex relationships in the data. Yet, optimizing network parameters via log likelihood gradients can yield suboptimal mean and uncalibrated variance estimates. Current solutions side-step this optimization problem with surrogate objectives or Bayesian treatments. Instead, we make two simple modifications to optimization. Notably, their combination produces a heteroscedastic model with mean estimates that are provably as accurate as those from its homoscedastic counterpart (i.e.~fitting the mean under squared error loss). For a wide variety of network and task complexities, we find that mean estimates from existing heteroscedastic solutions can be significantly less accurate than those from an equivalently expressive mean-only model. Our approach provably retains the accuracy of an equally flexible mean-only model while also offering best-in-class variance calibration. Lastly, we show how to leverage our method to recover the underlying heteroscedastic noise variance.",2022-12-18,https://www.semanticscholar.org/paper/934ef7a5f1e876ceb4bcbf5c59cbacd7e12e4733,International Conference on Artificial Intelligence and Statistics
1982,Stochastic programming for vendor portfolio selection and order allocation under delivery uncertainty,,2013-09-03,https://www.semanticscholar.org/paper/c96a2ace9bb7c8db5eda964b694b4ebf0f844f5f,OR Spectr.
1614,The Markov link method: a nonparametric approach to combine observations from multiple experiments,"This paper studies measurement linkage. An example from cell biology helps explain the problem: imagine for a given cell we can either sequence the cell’s RNA or we can examine its morphology, but not both. Given a cell’s morphology, what do we expect to see in its RNA? Given a cell’s RNA, what do we expect in its morphology? More broadly, given a measurement of one type, can we predict measurements of the other type? This measurement linkage problem arises in many scientific and technological fields. To solve this problem, we develop a nonparametric approach we dub the “Markov link method” (MLM). The MLM makes a conditional independence assumption that holds in many multi-measurement contexts and provides a a way to estimate the link, the conditional probability of one type of measurement given the other. We derive conditions under which the MLM estimator is consistent and we use simulated data to show that it provides accurate measures of uncertainty. We evaluate the MLM on real data generated by a pair of single-cell RNA sequencing techniques. The MLM characterizes the link between them and helps connect the two notions of cell type derived from each technique. Further, the MLM reveals that some aspects of the link cannot be determined from the available data, and suggests new experiments that would allow for better estimates.",2018-10-30,https://www.semanticscholar.org/paper/f3ec477307e0ea7840debfc8826af5fe22990ca4,bioRxiv
2734,Hybrid user interfaces: breeding virtually bigger interfaces for physically smaller computers,"While virtual worlds offer a compelling alternative to conventional interfaces, the technologies these systems currently use do not provide sufficient resolution and accuracy to support detailed work such as text editing. We describe a pragmatic approach to interface design that provides users with a large virtual world in which such high-resolution work can be performed. Our approach is based on combining heterogeneous display and interaction device technologies to produce a hybrid user interface. Display and interaction technologies that have relatively low resolution, but which cover a wide (visual and interactive) field are used to form an information surround. Display and interaction technologies that have relatively high resolution over a limited visual and interaction range are used to present concentrated information in one or more selected portions of the surround. These high-resolution fields are embedded within the low-resolution surround by choosing and coordinating complementary devices that permit the user to see and interact with both simultaneously. This allows each embedded high-resolution interface to serve as a “sweet spot” within which intonation may be preferentially processed, We have developed a preliminary implementation, described in this paper, that uses a Reflection Technology Private Eye display and a Polhemus sensor to provide the secondary low-resohttion surround, and a flat-panel display and mouse to provide the primary high-resolution interface.",1991-11-11,https://www.semanticscholar.org/paper/1ae17e45e8b32969a3127400eb6525d2ccd93c3a,ACM Symposium on User Interface Software and Technology
508,On the optimal bisection of a polygon (extended abstract),We give a polynomial approximation sceme for subdividing a simple polygon into approximately equal parts by curves of the smallest possible total length. For convex polygons we show that an exact fast algorithm is possible. Several generalizations are shown NP-complete.,1990-05-01,https://www.semanticscholar.org/paper/0589ffacc424e9b8d63bcdd21d1ebf1d34f8961c,SCG '90
926,Properties of acyclic database schemes,"There is a class of database descriptions, involving one “acyclic” join dependency and a collection of functional dependencies, and nothing else, that appears powerful enough to describe most any real-world body of data in relational database terms. Further, this class has many desirable properties. Some properties make operations like updates and the selection of joins to implement a query over a universal relation especially easy. Other properties of interest were studied by other researchers who described the same class in radically different terms, and found desirable properties in their own contexts. It is the purpose of this paper to define the class formally, to give its important properties and the equivalences with the other classes mentioned, and to explain the importance of each property. This paper is intended to summarize the results that will appear in more detail in [FMU] and [BFMY].",1981-05-11,https://www.semanticscholar.org/paper/a758b4bbd0e58f30d18eb8c7cb092e7853c900a3,Symposium on the Theory of Computing
3658,A Set of C++ Classes,,,https://www.semanticscholar.org/paper/4102972639ba54dd75ccfcd626bcb9cfcaa68d14,C++ Workshop
2309,Regulation of neutrophil apoptosis by sodium butyrate.,"Neutrophils have a very short half life because they constitutively undergo apoptosis. Granulocyte-macrophage colony-stimulating factor (GM-CSF) can delay apoptosis, but this agent also primes functions such as the respiratory burst and receptor upregulation. Here, we show that sodium butyrate, which has been shown to increase gene expression and differentiation in a variety of cell types, is more effective than GM-CSF in delaying neutrophil apoptosis. Thus, sodium butyrate preserves cell morphology and function, and butyrate-treated cells express high levels of CD16 after overnight culture. However, neither GM-CSF nor sodium butyrate appear to affect mRNA levels for CD16.",1996-12-01,https://www.semanticscholar.org/paper/bc0ea61e6519e443569b5b2127eccf6dc95dfb46,Biologicals (Print)
881,"Proceedings of the Seventh ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems, March 21-23, 1988, Austin, Texas, USA",,1988-03-01,https://www.semanticscholar.org/paper/21ea1e67b96ebef895a6b89dd0c76b4619af0aaf,ACM SIGMOD Conference
671,Metric Learning on Manifolds,"Recent literature has shown that symbolic data, such as text and graphs, is often better represented by points on a curved manifold, rather than in Euclidean space. However, geometrical operations on manifolds are generally more complicated than in Euclidean space, and thus many techniques for processing and analysis taken for granted in Euclidean space are difficult on manifolds. A priori, it is not obvious how we may generalize such methods to manifolds. We consider specifically the problem of distance metric learning, and present a framework that solves it on a large class of manifolds, such that similar data are located in closer proximity with respect to the manifold distance function. In particular, we extend the existing metric learning algorithms, and derive the corresponding sample complexity rates for the case of manifolds. Additionally, we demonstrate an improvement of performance in $k$-means clustering and $k$-nearest neighbor classification on real-world complex networks using our methods.",2019-02-05,https://www.semanticscholar.org/paper/4ab0129dd9d579bc8e9cbd46325559fa9e06e44d,arXiv.org
2949,RNA splicing is a primary link between genetic variation and disease,"RNA splicing links genetics to disease Many genetic variants associated with disease have no apparent effect on any specific protein coding sequence. Li et al. systematically analyzed the effects of DNA variants on the main steps of gene regulation, from the chromatin state through protein function. One-third of expression quantitative train loci (QTLs) are mediated through transcriptional processes, not chromatin. Splice QTLs and expression QTLs are about comparable in their complex disease risk. Posttranscriptional mechanisms therefore play a large role in translating genotype to phenotype. Science, this issue p. 600 Phenotype is most affected by genetic variants that influence gene expression and transcript splicing. Noncoding variants play a central role in the genetics of complex traits, but we still lack a full understanding of the molecular pathways through which they act. We quantified the contribution of cis-acting genetic effects at all major stages of gene regulation from chromatin to proteins, in Yoruba lymphoblastoid cell lines (LCLs). About ~65% of expression quantitative trait loci (eQTLs) have primary effects on chromatin, whereas the remaining eQTLs are enriched in transcribed regions. Using a novel method, we also detected 2893 splicing QTLs, most of which have little or no effect on gene-level expression. These splicing QTLs are major contributors to complex traits, roughly on a par with variants that affect gene expression levels. Our study provides a comprehensive view of the mechanisms linking genetic variation to variation in human gene regulation.",2016-04-29,https://www.semanticscholar.org/paper/726c847bcddca4d1be6cde804c35eb5af14999fa,Science
895,Deadlock-freedom (and saftey) of transactions in a distributed database,,1985-03-25,https://www.semanticscholar.org/paper/50d420f40db135a231cb28b4636e4f15e814512a,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems
669,An analysis of document graph construction methods for AMR summarization,"Meaning Representation (AMR) is a graph-based semantic representation for sentences, composed of collections of concepts linked by semantic relations. AMR-based approaches have found success in a variety of applications, but a challenge to using it in tasks that require document-level context is that it only represents individual sentences. Prior work in AMR-based summarization has automatically merged the individual sentence graphs into a document graph, but the method of merging and its effects on summary content selection have not been independently evaluated. In this paper, we present a novel dataset consisting of human-annotated alignments between the nodes of paired documents and summaries which may be used to evaluate (1) merge strategies; and (2) the performance of content selection methods over nodes of a merged or unmerged AMR graph. We apply these two forms of evaluation to prior work as well as a new method for node merging and show that our new method has significantly better performance than prior work.",2021-11-27,https://www.semanticscholar.org/paper/a6d91db7ec0c3c4c48e66dbf679f7f98386349a4,arXiv.org
954,Development of a novel hyaluronic acid membrane for the treatment of ocular surface diseases,,2021-01-27,https://www.semanticscholar.org/paper/8f649a805d6ebe1249e0d077a4546e96d7e5182e,Scientific Reports
2097,Using process experienced correlation table to improve the accuracy and reliability of data mining for yield improvement,"The rapid innovation of new process technologies in the semiconductor industry, especially 12 inches Fab, along with continuously growing amounts of data, it is difficult to find root cause when problems occur in some process steps. It causes large amount of wafer scrapping. The analysis methods of traditional EDA system rely on experience of senior engineers. They need to define the suspected process step by their experience and then perform analysis. The analysis methods consume large amounts of human resources in order to determine the root cause of process and yield excursions. Hence, it is important that a knowledge retention method be incorporated to improve the efficiency of root cause analysis. Data mining, a new data analysis method that combines information science and technology of statistical analysis, is developed recently. The new generation data analysis method includes statistical, information science and mathematical calculation to find correlation between the target parameter, for example yield and other parameters. It will provide important clue to the analyzer. In addition, it also provides a direction to find root cause rapidly. It is difficult to find the correlation between the target parameter and other parameters by traditional statistical analysis method, and data mining can solve the blind point of the traditional method. This article discusses the design of how to define the relation between all data sources of semiconductor industry based on the experience of senior engineers. And it installs the relation to data mining analysis, it performs the analysis to identify relationship among all data sources. So, engineers can find the root cause of process issue in a short period of time",2004-09-09,https://www.semanticscholar.org/paper/ced06d058175269b386cf4305f63645e0373c334,2004 Semiconductor Manufacturing Technology Workshop Proceedings (IEEE Cat. No.04EX846)
368,Understanding the Internet,,2002-04-11,https://www.semanticscholar.org/paper/3bd83c8e7884150adff21831e93ca09357269b61,Hellenic Conference on Artificial Intelligence
3465,Scheduling Algorithms,,2004-12-01,https://www.semanticscholar.org/paper/f78cab3af5b4a54135e8bb56de9acc7108753e41,Algorithms and Theory of Computation Handbook
142,Data acquisition and view planning for 3-D modeling tasks,"In this paper we address the joint problems of automated data acquisition and view planning for large-scale indoor and outdoor sites. Our method proceeds in two distinct stages. In the initial stage, the system is given a 2-D map with which it plans a minimal set of sufficient covering views. We then use a 3-D laser scanner to take scans at each of these views. When this planning system is combined with our mobile robot, it automatically computes and executes a tour of these viewing locations and acquires the views with the robot's onboard laser scanner. These initial scans serve as an approximate 3-D model of the site. The planning software then enters a second stage in which it updates this model by using a voxel-based occupancy procedure to plan the next best view. This next best view is acquired, and further next best views are sequentially computed and acquired until a complete 3-D model is obtained. Results are shown for Fort Jay on Governors Island in the City of New York and for the church of Saint Menoux in the Bourbonnais region of France.",2007-12-10,https://www.semanticscholar.org/paper/034bd82b3c3928facbe0f3a2fd6f3fb07d46ceed,2007 IEEE/RSJ International Conference on Intelligent Robots and Systems
3674,On unifying module interfaces,"This paper presents the outline of a uniform interface mechanism for activating different kinds of modules, e.g. processes, monitors, and procedures. The usefulness of such an interface in the design of modules and in the tuning of a system is discussed. The overheads involved in using it are explained, together with some implications that its general use has on the system structure.",,https://www.semanticscholar.org/paper/29650ba4640ce69c50ed601039beebd668ce051a,OPSR
3551,Foundations of C++,,2012-03-24,https://www.semanticscholar.org/paper/c53e9984c66db460c2005256a1028117ca00faef,European Symposium on Programming
1553,"Causal Inference from Observational Healthcare Data: Implications, Impacts and Innovations",,,https://www.semanticscholar.org/paper/33b7237bd3e9477e2432ce7210e45cfb5866e9f5,American Medical Informatics Association Annual Symposium
2070,Structuring Manufacturing Strategy,"Manufacturing strategy comprises decisionmaking problems in terms of manufacturing practices to achieve manufacturing objectives through linkages of performance measurement. It is pervasively influential, long-term, and dynamic owing to the conformance with corporate strategy, business strategy, and marketing strategy. Therefore, there are many underlying sub-problems which can be expressed in a wide spectrum of forms. In this study, we will firstly define the spectrum of problem-structuring. After that, a decision analysis framework as a guide for modeling problems in different forms is introduced. Finally, a realistic problem is illustrated for discussions.",2007-10-08,https://www.semanticscholar.org/paper/8a55fb217fbe8318e00cade34f41fbf2ff0335a7,IEEE International Conference on Automation Science and Engineering
1888,UNISON data-driven intermittent demand forecast framework to empower supply chain resilience and an empirical study in electronics distribution,,2019-09-01,https://www.semanticscholar.org/paper/0af15eaf9abb97e1e758a3a4d8a23fa138a11904,Computers & industrial engineering
388,Algorithmic problems related to the Internet,,,https://www.semanticscholar.org/paper/ea066608655bfd11ec64efbbb7ce63cfea4581a3,Hellenic-European Conference on Computer Mathematics and its Applications
32,Join Optimization of Information Extraction Output: Quality Matters!,"Information extraction (IE) systems are trained to extract specific relations from text databases. Real-world applications often require that the output of multiple IE systems be joined to produce the data of interest. To optimize the execution of a join of multiple extracted relations, it is not sufficient to consider only execution time. In fact, the quality of the join output is of critical importance: unlike in the relational world, different join execution plans can produce join results of widely different quality whenever IE systems are involved. In this paper, we develop a principled approach to understand, estimate, and incorporate output quality into the join optimization process over extracted relations. We argue that the output quality is affected by (a) the configuration of the IE systems used to process documents, (b) the document retrieval strategies used to retrieve documents, and (c) the actual join algorithm used. Our analysis considers several alternatives for these factors, and predicts the output quality---and, of course, the execution time---of the alternate execution plans. We establish the accuracy of our analytical models, as well as study the effectiveness of a quality-aware join optimizer, with a large-scale experimental evaluation over real-world text collections and state-of-the-art IE systems.",2009-03-29,https://www.semanticscholar.org/paper/ba44bd05bff54484803abd54f20625b0e8c07847,IEEE International Conference on Data Engineering
3089,Using Rescue Points to Navigate Software Recovery,"We present a new technique that enables software recovery in legacy applications by retrofitting exception-handling capabilities, error virtualization using rescue points. We introduce the idea of ""rescue points"" as program locations to which an application can recover its execution in the presence of failures. The use of rescue points reduces the chance of unanticipated execution paths thereby making recovery more robust by mimicking system behavior under controlled error conditions. These controlled error conditions can be thought of as a set erroneous inputs, like the ones used by most quality-assurance teams during software development, designed to stress-test an application. To discover rescue points applications are profiled and monitored during tests that bombard the program with bad/random inputs. The intuition is that by monitoring application behavior during these runs, we gain insight into how programmer-tested program points are used to propagate faults gracefully.",2007-05-20,https://www.semanticscholar.org/paper/8ca3c3eecfd92d3b5cf5a77f6a5051404d12ef9e,IEEE Symposium on Security and Privacy
1872,A Two-stage Multi-population Genetic Algorithm with Heuristics for Workflow Scheduling in Heterogeneous Distributed Computing Environments,"Workflow scheduling in Heterogeneous Distributed Computing Environments (HDCEs) is a NP-hard problem. Although a number of scheduling approaches have been proposed for workflow scheduling in HDCEs, there is still a room and need for improvement. To fill the gaps, this study formulates workflow scheduling problem in HDCEs as a complete, solvable and extensible integer programming mathematical model with precedence and resource constraints that provides a theoretical foundation for developing workflow scheduling strategy. Then, this study develops a novel two-stage multi-population genetic algorithm with heuristics for workflow scheduling. In particular, two-stage multi-population coevolution strategy is employed with designed novel methods for population initialization, genetic operation, individual decoding and improvement. To estimate the validity, extensive experiments are designed and conducted on various scenarios based on real and random workflow applications. The results have shown the practical viability of the proposed algorithm outperforming conventional approaches.",2023-04-01,https://www.semanticscholar.org/paper/df1f1fe84ef007f4c7a1f3d4177c1dd0f2a576fb,IEEE Transactions on Cloud Computing
3770,Learning visual biases from human imagination,"Although the human visual system can recognize many concepts under challenging conditions, it still has some biases. In this paper, we investigate whether we can extract these biases and transfer them into a machine recognition system. We introduce a novel method that, inspired by well-known tools in human psychophysics, estimates the biases that the human visual system might use for recognition, but in computer vision feature spaces. Our experiments are surprising, and suggest that classifiers from the human visual system can be transferred into a machine with some success. Since these classifiers seem to capture favorable biases in the human visual system, we further present an SVM formulation that constrains the orientation of the SVM hyperplane to agree with the bias from human visual system. Our results suggest that transferring this human bias into machines may help object recognition systems generalize across datasets and perform better when very little training data is available.",2014-10-17,https://www.semanticscholar.org/paper/480f8aa54b19e7b6b31be09aa2124cb0f159ae9b,Neural Information Processing Systems
1966,Similarity Searching for Defective Wafer Bin Maps in Semiconductor Manufacturing,"Because high-dimensional wafer bin maps (WBMs) cause various features, it is difficult to search the similarity among WBMs via conventional pattern recognition methods. This study develops a novel morphology-based support vector machine for defective wafer detection. The experimental results demonstrate its usefulness in yield improvements on precision and computation cost.",2014-07-01,https://www.semanticscholar.org/paper/9590da007734661cace2b4c6d197a843a92784cc,IEEE Transactions on Automation Science and Engineering
2956,Relational Learning and Network Modelling Using Infinite Latent Attribute Models,"Latent variable models for network data extract a summary of the relational structure underlying an observed network. The simplest possible models subdivide nodes of the network into clusters; the probability of a link between any two nodes then depends only on their cluster assignment. Currently available models can be classified by whether clusters are disjoint or are allowed to overlap. These models can explain a “flat” clustering structure. Hierarchical Bayesian models provide a natural approach to capture more complex dependencies. We propose a model in which objects are characterised by a latent feature vector. Each feature is itself partitioned into disjoint groups (subclusters), corresponding to a second layer of hierarchy. In experimental comparisons, the model achieves significantly improved predictive performance on social and biological link prediction tasks. The results indicate that models with a single layer hierarchy over-simplify real networks.",2015-02-01,https://www.semanticscholar.org/paper/2f458534cd1fdf467888b599b961566ba59a3401,IEEE Transactions on Pattern Analysis and Machine Intelligence
329,Approximately dominating representatives,,2005-01-05,https://www.semanticscholar.org/paper/2def02ffe888c52cbfe1a02ab143b856fdcf6033,Theoretical Computer Science
1700,Bayesian Nonparametric Poisson Factorization for Recommendation Systems,"We develop a Bayesian nonparametric Poisson factorization model for recommendation systems. Poisson factorization implicitly models each user’s limited budget of attention (or money) that allows consumption of only a small subset of the available items. In our Bayesian nonparametric variant, the number of latent components is theoretically unbounded and eectively estimated when computing a posterior with observed user behavior data. To approximate the posterior, we develop an ecient variational inference algorithm. It adapts the dimensionality of the latent components to the data, only requires iteration over the user/item pairs that have been rated, and has computational complexity on the same order as for a parametric model with xed dimensionality. We studied our model and algorithm with large realworld data sets of user-movie preferences. Our model eases the computational burden of searching for the number of latent components and gives better predictive performance than its parametric counterpart.",2014-04-02,https://www.semanticscholar.org/paper/74771dad32d0b2ab67d901ddcb74d48d6649139b,International Conference on Artificial Intelligence and Statistics
183,The EATCS Award 2019 - Call for Nominations,,,https://www.semanticscholar.org/paper/c55dc19a0370d17c162fbd01bd0a2bd40e6bd716,Bull. EATCS
2948,Batch effects and the effective design of single-cell gene expression studies,,2016-07-08,https://www.semanticscholar.org/paper/61bc0578a8d2e94f745aa9c26501872e3ab51281,Scientific Reports
134,Routing techniques for massively parallel communication,"A survey of some packet-switched routing methods for massively parallel computers is presented. Some of the techniques are applicable to both shared-memory and message-passing architectures. These routing methods are compared in terms of their efficiency in supporting programming models, efficiency in mapping to parallel machines, and practicality. Among the outlined methods, three nonadaptive techniques and some adaptive routing algorithms are discussed. >",1991-04-01,https://www.semanticscholar.org/paper/286c8b5d5b12e043954eb145eef8330f3276cc5f,Proceedings of the IEEE
2668,Enveloping users and computers in a collaborative 3D augmented reality,"We present EMMIE (Environment Management for Multiuser Information Environments), a prototype experimental user interface to a collaborative augmented environment. Users share a 3D virtual space and manipulate virtual objects that represent information to be discussed. We refer to EMMIE as a hybrid user interface because it combines a variety of different technologies and techniques, including virtual elements such as 3D widgets, and physical objects such as tracked displays and input devices. See-through head-worn displays overlay the virtual environment on the physical environment, visualizing the pervasive ""virtual ether"" within which all interaction occurs. Our prototype includes additional 2D and 3D displays, ranging from palm-sized to wall-sized, allowing the most appropriate one to be used for any task. Objects can be moved among displays (including across dimensionalities) through drag-and-drop. In analogy to 2D window managers, we describe a prototype implementation of a shared 3D environment manager that is distributed across displays, machines, and operating systems. We also discuss two methods we are exploring for handling information privacy in such an environment.",1999-10-20,https://www.semanticscholar.org/paper/cebb6a4d86ff6d0158c30823c38c4d22ea6a232f,International Workshop on Automated Reasoning
400,Optimization problems in congestion control,"One of the crucial elements in the Internet's success is its ability to adequately control congestion. The paper defines and solves several optimization problems related to Internet congestion control, as a step toward understanding the virtues of the TCP congestion control algorithm currently used and comparing it with alternative algorithms. We focus on regulating the rate of a single unicast flow when the bandwidth available to it is unknown and may change over time. We determine near-optimal policies when the available bandwidth is unchanging, and near-optimal competitive policies when the available bandwidth is changing in a restricted manner under the control of an adversary.",2000-11-12,https://www.semanticscholar.org/paper/c0b9f1f946f92e3cfd6fd63f88b11d810c06866f,Proceedings 41st Annual Symposium on Foundations of Computer Science
2684,An analysis of COMET and MAGIC using the standard reference model for intelligent multimedia presentation systems,,1997-12-01,https://www.semanticscholar.org/paper/205405a599d124ac61946d47f83efa3d2a99ed98,Comput. Stand. Interfaces
1649,Robust Probabilistic Modeling with Bayesian Data Reweighting,"Probabilistic models analyze data by relying on a set of assumptions. Data that exhibit deviations from these assumptions can undermine inference and prediction quality. Robust models offer protection against mismatch between a model's assumptions and reality. We propose a way to systematically detect and mitigate mismatch of a large class of probabilistic models. The idea is to raise the likelihood of each observation to a weight and then to infer both the latent variables and the weights from data. Inferring the weights allows a model to identify observations that match its assumptions and down-weight others. This enables robust inference and improves predictive accuracy. We study four different forms of mismatch with reality, ranging from missing latent groups to structure misspecification. A Poisson factorization analysis of the Movielens 1M dataset shows the benefits of this approach in a practical scenario.",2016-06-13,https://www.semanticscholar.org/paper/434954d40776c87d8b354677ac393cb121f5c80b,International Conference on Machine Learning
3275,Reciprocal insurance among Kenyan pastoralists,,2013-05-01,https://www.semanticscholar.org/paper/0038c26dabb61e926bd18d59502b0a3e845ebbad,Theoretical Ecology
1841,Nonparametric empirical Bayes for the Dirichlet process mixture model,,2006-03-01,https://www.semanticscholar.org/paper/31cbc51a4a441d2ca6668e0e0614f5abfcb88194,Statistics and computing
217,Cortical Computation,"A computational theory of cortex necessitates a novel paradigm of exquisitely distributed computation. Here we review recent work on a primitive called Predictive Join, or PJoin, which is both plausible and useful in regards to cortical computation, and which enables a spontaneous form of unsupervised learning exhibiting many of the characteristics of brain activity. We also outline several immediate goals of a computational research program on the brain.",2015-07-21,https://www.semanticscholar.org/paper/3047903cb1007e97589a5b8cdbb1af338075f314,ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing
3732,Bringing Engineering Rigor to Deep Learning,"Deep learning (DL) systems are increasingly deployed in safety- and security-critical domains including autonomous driving, robotics, and malware detection, where the correctness and predictability of a system on corner-case inputs are of great importance. Unfortunately, the common practice to validating a deep neural network (DNN) - measuring overall accuracy on a randomly selected test set - is not designed to surface corner-case errors. As recent work shows, even DNNs with state-of-the-art accuracy are easily fooled by human-imperceptible, adversarial perturbations to the inputs. Questions such as how to test corner-case behaviors more thoroughly and whether all adversarial samples have been found remain unanswered. In the last few years, we have been working on bringing more engineering rigor into deep learning. Towards this goal, we have built five systems to test DNNs more thoroughly and verify the absence of adversarial samples for given datasets. These systems check a broad spectrum of properties (e.g., rotating an image should never change its classification) and find thousands of error-inducing samples for popular DNNs in critical domains (e.g., ImageNet, autonomous driving, and malware detection). Our DNN verifiers are also orders of magnitude (e.g., 5,000×) faster than similar tools. This article overviews our systems and discusses three open research challenges to hopefully inspire more future research towards testing and verifying DNNs.",2019-07-25,https://www.semanticscholar.org/paper/1657065a6f3933a8fcfa7ab99b8bcafb0e610e2d,ACM SIGOPS Operating Systems Review
3214,Reciprocity and rotating social advantage among females in egalitarian primate societies,,2019-11-01,https://www.semanticscholar.org/paper/0ff2cb4b1efb2e211a76952cedbe1b5d18ce8443,Animal Behaviour
3635,Foundations for native C++ styles,"Over the past decade, C++ has become the most commonly used language for introducing object‐oriented programming and other abstraction techniques into production software. During this period, C++ has evolved to meet the challenges of production systems. In this, C++ differs radically from languages that come primarily from academic or research environments, and from less widely used languages. Although C++ has also been extensively used in academia and for research, its evolution was driven primarily by feedback from its use in industrial applications.",1995-12-01,https://www.semanticscholar.org/paper/6df7bdb60f18b033930a69f934174d28e42de301,"Software, Practice & Experience"
2217,Inhibition of pre‐B cell colony‐enhancing factor (PBEF/NAMPT/visfatin) decreases the ability of human neutrophils to generate reactive oxidants but does not impair bacterial killing,"NAMPT, also known as PBEF and visfatin, can act extracellularly as a cytokine‐like molecule or intracellularly as a NAMPT, regulating NAD biosynthesis in the NAD salvage pathway. Inhibitors of NAMPT have anti‐inflammatory and anticancer activity and are finding use as therapeutic agents. In view of the importance of NAD metabolism in neutrophil function, we determined the effects of NAMPT inhibition on a variety of neutrophil functions associated with their role in host protection against infections. Incubation of human neutrophils with the NAMPT inhibitor APO866 decreased neutrophil NAD(P)/H levels in a dose‐ and time‐dependent manner but without a concomitant change in cell viability. NAMPT inhibition did not affect the expression of a number of cell‐surface receptors involved in adhesion and opsono‐phagocytosis, but the respiratory burst was decreased significantly. Whereas opsono‐phagocytosis of Staphylococcus aureus was unaffected by NAMPT inhibition, intraphagosomal oxidant production was decreased. However, the killing efficiency of neutrophils was unaffected. These data indicate that therapeutic NAMPT inhibition is unlikely to have deleterious effects on host protection against infections, in spite of this ability to down‐regulate neutrophil respiratory burst activity significantly.",2013-09-01,https://www.semanticscholar.org/paper/ed69abace0d4e8a2918f69502ee336d083c24263,Journal of Leukocyte Biology
1609,Equation Embeddings,"We present an unsupervised approach for discovering semantic representations of mathematical equations. Equations are challenging to analyze because each is unique, or nearly unique. Our method, which we call equation embeddings, finds good representations of equations by using the representations of their surrounding words. We used equation embeddings to analyze four collections of scientific articles from the arXiv, covering four computer science domains (NLP, IR, AI, and ML) and $\sim$98.5k equations. Quantitatively, we found that equation embeddings provide better models when compared to existing word embedding approaches. Qualitatively, we found that equation embeddings provide coherent semantic representations of equations and can capture semantic similarity to other equations and to words.",2018-03-24,https://www.semanticscholar.org/paper/b495ea6d5bcabdb757371a7cb4ce6bfda4df93c6,arXiv.org
2869,Critical role of galectin-3 in phagocytosis by macrophages.,"Galectin-3 is a member of a large family of animal lectins. This protein is expressed abundantly by macrophages, but its function in this cell type is not well understood. We have studied the effect of galectin-3 gene targeting on phagocytosis, a major function of macrophages. Compared with wild-type macrophages, galectin-3-deficient (gal3-/-) cells exhibited reduced phagocytosis of IgG-opsonized erythrocytes and apoptotic thymocytes in vitro. In addition, gal3-/- mice showed attenuated phagocytic clearance of apoptotic thymocytes by peritoneal macrophages in vivo. These mice also exhibited reduced IgG-mediated phagocytosis of erythrocytes by Kupffer cells in a murine model of autoimmune hemolytic anemia. Additional experiments indicate that extracellular galectin-3 does not contribute appreciably to the phagocytosis-promoting function of this protein. Confocal microscopic analysis of macrophages containing phagocytosed erythrocytes revealed localization of galectin-3 in phagocytic cups and phagosomes. Furthermore, gal3-/- macrophages exhibited a lower degree of actin rearrangement upon Fcgamma receptor crosslinkage. These results indicate that galectin-3 contributes to macrophage phagocytosis through an intracellular mechanism. Thus, galectin-3 may play an important role in both innate and adaptive immunity by contributing to phagocytic clearance of microorganisms and apoptotic cells.",2003-08-01,https://www.semanticscholar.org/paper/f5b1a2f3d2fa0dc01004d9f35721b85df1c1d1d5,Journal of Clinical Investigation
1278,Observation and properties of the orbitally excited B_(s2)(*) meson.,"We report the direct observation of the excited L=1 state B_(s2)(*) in fully reconstructed decays to B+K-. The mass of the B_(s2)(*) meson is measured to be 5839.6+/-1.1(stat)+/-0.7(syst) MeV/c(2), and its production rate relative to the B+ meson is measured to be [1.15+/-0.23(stat)+/-0.13(syst)]%.",2007-11-02,https://www.semanticscholar.org/paper/5328b853d31513c7289d442590e2801bad2af0e6,Physical Review Letters
357,MythematiCS: in praise of storytelling in the teaching of computer science and math,"t is usefully humbling to remember that cultural transmission via formal education at universities and schools is a very recent experiment. During almost all of the history of Humankind, cultural knowledge on all subjects (morality, religion, civics, technology, agriculture, etc.) has been passed on by storytelling. We do use storytelling in the contemporary teaching of technical subjects, even though I believe that much more is possible and needed. My purpose here is to identify, justify, salute, dissect, and propagandize such use.",2003-12-01,https://www.semanticscholar.org/paper/a1c3b5f42e907f1efdc428098f290e28395504d0,SGCS
2073,Hybrid data mining approach for pattern extraction from wafer bin map to improve yield in semiconductor manufacturing,,2007-05-01,https://www.semanticscholar.org/paper/d3e035961e87bc6bf923f6f04ad3aacd4c33f9f4,International Journal of Production Economics
2456,Keynote speaker: Getting real,"Wide-FOV, 6DOF-tracked, consumer head-worn displays are no longer just dev kits, bimanual 3D input devices can be found at Best Buy, and full-fledged graphics packages are built into web browsers. Meanwhile, augmented reality is poised to make the return trip from hand-held phones and tablets, back to the head-worn displays in which it was born. 3D is here for real. Yet, we all know how difficult it is to create effective 3D user interfaces. In this talk, I will discuss my thoughts about why this is so, and what we can do about it. I will present some of the research directions that my lab has been exploring, and suggest where I think our field may be headed next.",2016-03-19,https://www.semanticscholar.org/paper/594d3cac72485a1e6a594bb3b77e1fa33ed841e7,IEEE Symposium on 3D User Interfaces
216,Locally Adaptive Optimization: Adaptive Seeding for Monotone Submodular Functions,"The Adaptive Seeding problem is an algorithmic challenge motivated by influence maximization in social networks: One seeks to select among certain accessible nodes in a network, and then select, adaptively, among neighbors of those nodes as they become accessible in order to maximize a global objective function. More generally, adaptive seeding is a stochastic optimization framework where the choices in the first stage affect the realizations in the second stage, over which we aim to optimize. Our main result is a (1 -- 1/e)2-approximation for the adaptive seeding problem for any monotone submodular function. While adaptive policies are often approximated via non-adaptive policies, our algorithm is based on a novel method we call locally-adaptive policies. These policies combine a non-adaptive global structure, with local adaptive optimizations. This method enables the (1 -- 1/e)2-approximation for general monotone submodular functions and circumvents some of the impossibilities associated with non-adaptive policies. We also introduce a fundamental problem in submodular optimization that may be of independent interest: given a ground set of elements where every element appears with some small probability, find a set of expected size at most k that has the highest expected value over the realization of the elements. We show a surprising result: there are classes of monotone submodular functions (including coverage) that can be approximated almost optimally as the probability vanishes. For general monotone submodular functions we show via a reduction from P lanted -C lique that approximations for this problem are not likely to be obtainable. This optimization problem is an important tool for adaptive seeding via non-adaptive policies, and its hardness motivates the introduction of locally-adaptive policies we use in the main result.",2015-07-09,https://www.semanticscholar.org/paper/2d0a8afb099cbd1afb8d4635d4fd0813f94cf4f8,ACM-SIAM Symposium on Discrete Algorithms
3180,Effects of a grazing permit market on pastoralist behavior and overgrazing in Kenya,"The success of market-based mechanisms in reducing conflicts and internalizing externalities depends on their ability to clarify property rights amongst heterogenous resource users. We investigate the effectiveness of novel markets in achieving their goals using the case study of grazing markets in Laikipia County, Kenya. In this system, sheep- and goat (shoat)- and cattle-rearing pastoralists negotiate land access for cattle with neighboring cattle ranchers. Using data on pastoralists’ livestock and contracting preferences and a model of pastoral herd management, we show that contracting for cattle grazing access on private property alters relative input shadow prices for grazing resources in communal pastoral lands, ultimately resulting in relieved cattle grazing pressure. However, the permitting process is less attractive to pastoralists who prefer rearing shoats instead of cattle. These shoat-rearing pastoralists instead fill some of the vacated space with shoats instead of purchasing permits themselves. This leakage offsets some of the conservation benefits arising from the contracting program and results in a greater share of shoats in the communal herd mix. Approximately 0.59 cows’ worth of free space persists on the commons per permit sold, indicating reduced grazing pressure, but this represents a small proportion (3.8%) of the total livestock in the system. The narrow introduction of the cattle-focused permit market and lack of strong management institutions on the commons dampen the permitting program’s conservation benefits, necessitating further interventions. Alleviating these factors and dramatically scaling up the program has the potential to turn the permitting system into a successful conservation tool.",2022-02-14,https://www.semanticscholar.org/paper/9757b6e25950c970ab1bbe8098901ff228dc24c7,Environmental Research Letters
3300,A rare fight in female plains zebra,,,https://www.semanticscholar.org/paper/96d278fe951923abec07e5d02da3219d1919672d,Journal of ethology
1025,Kinematic Cartography and the Efficiency of Viscous Swimming,"The apparent “distance” between two configurations of a system and the “length” of trajectories through its configuration space can be significantly distorted by plots that use “natural” or intuitively selected coordinates. This effect is similar to the way that a latitude–longitude plot of the Earth distorts the size and shape of the continents. In this paper, we explore how ideas from cartography can be used to identify system parameterizations that better reflect the effort costs of changing configuration. We then apply these new parameters to provide geometric insight about two aspects of moving in dissipative environments such as low Reynolds number fluids: The shape of the optimal gait cycle for a three-link swimmer and the fundamentally superior efficiency of a serpenoid swimmer as compared to the classic three-link system.",2017-02-20,https://www.semanticscholar.org/paper/0292514b7d432eb14af33a6e5673c3b6aaf1a07b,IEEE Transactions on robotics
3121,Session details: Optimizing encoding,,2004-05-17,https://www.semanticscholar.org/paper/4fec3da202175274700f119b453d8174cd987df9,Proceedings of the 13th international conference on World Wide Web
1596,Dynamic Embeddings for Language Evolution,"Word embeddings are a powerful approach for unsupervised analysis of language. Recently, Rudolph et al. developed exponential family embeddings, which cast word embeddings in a probabilistic framework. Here, we develop dynamic embeddings, building on exponential family embeddings to capture how the meanings of words change over time. We use dynamic embeddings to analyze three large collections of historical texts: the U.S. Senate speeches from 1858 to 2009, the history of computer science ACM abstracts from 1951 to 2014, and machine learning papers on the ArXiv from 2007 to 2015. We find dynamic embeddings provide better fits than classical embeddings and capture interesting patterns about how language changes.",2018-04-10,https://www.semanticscholar.org/paper/417acb6b197d8f81368e67b63a9e280288f30599,The Web Conference
726,Analysis of Boolean Programs,,2013-03-16,https://www.semanticscholar.org/paper/122a1d55daa1d0f721cd04ab371193b9d596b6ab,International Conference on Tools and Algorithms for Construction and Analysis of Systems
3577,Model-Based Product-Oriented Certification,"Future space missions such as the Mars Science Laboratory and Project Constellation suggest the engineering of some of the most complex man-rated software systems. The present process-oriented certification methodologies employed by NASA are becoming prohibitively expensive when applied to systems of such complexity. The process of software certification establishes the level of confidence in a software system in the context of its functional and safety requirements. Providing such certification evidence may require the application of a number of software development, analysis, and validation techniques. We define product-oriented certification as the process of measuring the system's reliability and efficiency based on the analysis of its design (expressed in models) and implementation (expressed in source code). In this work we introduce a framework for model-based product-oriented certification founded on the concept of source code enhancement and analysis. We describe a classification of the certification artifact types, the development and validation tools and techniques, the application domain-specific factors, and the levels of abstraction. We demonstrate the application of our certification platform by analyzing the process of model-based development of the parallel autonomic goals network, a critical component of the Jet Propulsion Laboratory's Mission Data System (MDS). We describe how we identify and satisfy seven critical certification artifacts in the process of model-driven development and validation of the MDS goal network. In the analysis of this process, we establish the relationship among the seven certification artifacts, the applied development and validation techniques and tools, and the level of abstraction of system design and development.",2009-04-14,https://www.semanticscholar.org/paper/e2e096fad3938c426a886c32853e04f24fbcdf55,European Conference on the Engineering of Computer-Based Systems
3459,An O(n5/2log n) Algorithm for the Rectilinear Minimum Link-Distance Problem,"In this paper we consider the Rectilinear Minimum Link-Distance Problem in Three Dimensions. The problem is well studied in two dimensions, but is relatively unexplored in higher dimensions. We solve the problem in O(βn log n) time, where n is the number of corners among all obstacles, and β is the size of a BSP decomposition of the space containing the obstacles. It has been shown that in the worst case β = Θ(n), giving us an overall worst case time of O(n log n). Previously known algorithms have had worst-case running times of Ω(n).",,https://www.semanticscholar.org/paper/dc090a82008c8cacf08ce67142a0f875e7fd484d,Canadian Conference on Computational Geometry
589,Symmetric Space-Bounded Computation,,1982-08-01,https://www.semanticscholar.org/paper/3739fc0150b4dcd7d826d2eef807f1a5693e5a34,Theoretical Computer Science
3216,"Behavioral and Ecological Implications of Bunched, Rotational Cattle Grazing in East African Savanna Ecosystem",,2019-01-01,https://www.semanticscholar.org/paper/bd79d787459160239a954332450aaa95edbefeb4,Rangeland Ecology & Management
2243,The dual effects of TNFalpha on neutrophil apoptosis are mediated via differential effects on expression of Mcl-1 and Bfl-1.,"Neutrophils have a very short half-life in the circulation, undergoing rapid death by apoptosis, but a number of agents can either delay or accelerate the rate at which these cells undergo death. TNFalpha can exert opposing, concentration-dependent effects on neutrophils to either accelerate their apoptosis or enhance their survival. We show that TNFalpha greatly increases the rate of turnover of Mcl-1, an antiapoptotic protein that plays a key role in neutrophil survival. In contrast to Mcl-1 turnover in control- or granulocyte-macrophage colony-stimulating factor (GM-CSF)-treated neutrophils that occurs via the proteasome, TNFalpha-accelerated Mcl-1 turnover occurs via activation of caspases. Mcl-1-depleted cells thus have accelerated rates of apoptosis. While TNFalpha had no effect on MCL-1 transcription, it induced expression of another antiapoptotic molecule, BFL-1. Low concentrations of TNFalpha (<or=1 ng/mL) stimulated BFL-1 expression, whereas higher concentrations (>or=10 ng/mL) triggered caspase-dependent acceleration of Mcl-1 turnover. These opposing effects on 2 separate antiapoptotic systems of neutrophils explain the divergent effects of TNFalpha on neutrophil apoptosis and have important implications for understanding how TNFalpha may affect immune function in inflammatory diseases.",2008-01-15,https://www.semanticscholar.org/paper/bde83e31c323c2206ab39a28d8c60c8fa2b910fc,Blood
823,On Datalog vs. Polynomial Time,"We show that certain monotonic polynomial time queries are not expressible in variants of Datalog. The proof techniques include lower bounds for monotone circuit size and a ""Pumping Lemma"" for Datalog queries.",1995-10-01,https://www.semanticscholar.org/paper/1a089b08363fdc66b90a71d63395ebb4bacd6da6,Journal of computer and system sciences (Print)
1280,Properties of L=1 B(1) and B(2)* mesons.,"This Letter presents the first strong evidence for the resolution of the excited B mesons B(1) and B(2)* as two separate states in fully reconstructed decays to B(+)(*)pi(-). The mass of B(1) is measured to be 5720.6+/-2.4+/-1.4 MeV/c(2) and the mass difference DeltaM between B(2)* and B(1) is 26.2+/-3.1+/-0.9 MeV/c;{2}, giving the mass of the B(2)* as 5746.8+/-2.4+/-1.7 MeV/c(2). The production rate for B(1) and B(2)* mesons is determined to be a fraction (13.9+/-1.9+/-3.2)% of the production rate of the B+ meson.",2007-05-01,https://www.semanticscholar.org/paper/65945c04eb3333a8993a10bda4a2b185cca376b5,Physical Review Letters
950,Evaluation of moxifloxacin-induced cytotoxicity on human corneal endothelial cells,,2021-03-18,https://www.semanticscholar.org/paper/0ef7639aed47d8344938085055db9c0f71377deb,Scientific Reports
2671,A distributed 3D graphics library,"We present Repo-3D, a general-purpose, object-oriented library for developing distributed, interactive 3D graphics applications across a range of heterogeneous workstations. Repo-3D is designed to make it easy for programmers to rapidly build prototypes using a familiar multi-threaded, object-oriented programming paradigm. All data sharing of both graphical and non-graphical data is done via general-purpose remote and replicated objects, presenting the illusion of a single distributed shared memory. Graphical objects are directly distributed, circumventing the “duplicate database” problem and allowing programmers to focus on the application details. Repo-3D is embedded in Repo, an interpreted, lexically-scoped, distributed programming language, allowing entire applications to be rapidly prototyped. We discuss Repo-3D’s design, and introduce the notion of local variations to the graphical objects, which allow local changes to be applied to shared graphical structures. Local variations are needed to support transient local changes, such as highlighting, and responsive local editing operations. Finally, we discuss how our approach could be applied using other programming languages, such as Java.",1998-07-24,https://www.semanticscholar.org/paper/4d8b2b12b9cebe0fead1fde1ff2efb1bff41c2dc,International Conference on Computer Graphics and Interactive Techniques
1749,A Split-Merge MCMC Algorithm for the Hierarchical Dirichlet Process,"The hierarchical Dirichlet process (HDP) has become an important Bayesian nonparametric model for grouped data, such as document collections. The HDP is used to construct a flexible mixed-membership model where the number of components is determined by the data. As for most Bayesian nonparametric models, exact posterior inference is intractable---practitioners use Markov chain Monte Carlo (MCMC) or variational inference. Inspired by the split-merge MCMC algorithm for the Dirichlet process (DP) mixture model, we describe a novel split-merge MCMC sampling algorithm for posterior inference in the HDP. We study its properties on both synthetic data and text corpora. We find that split-merge MCMC for the HDP can provide significant improvements over traditional Gibbs sampling, and we give some understanding of the data properties that give rise to larger improvements.",2012-01-08,https://www.semanticscholar.org/paper/6a4a39c40e2e54395ab69977331ccfcff11afa87,arXiv.org
1781,Online Learning for Latent Dirichlet Allocation,"We develop an online variational Bayes (VB) algorithm for Latent Dirichlet Allocation (LDA). Online LDA is based on online stochastic optimization with a natural gradient step, which we show converges to a local optimum of the VB objective function. It can handily analyze massive document collections, including those arriving in a stream. We study the performance of online LDA in several ways, including by fitting a 100-topic topic model to 3.3M articles from Wikipedia in a single pass. We demonstrate that online LDA finds topic models as good or better than those found with batch VB, and in a fraction of the time.",2010-12-06,https://www.semanticscholar.org/paper/2d8cbd7370b4ce666edd864e66f83ebf20963516,Neural Information Processing Systems
1856,Sharing Clusters among Related Groups: Hierarchical Dirichlet Processes,"We propose the hierarchical Dirichlet process (HDP), a nonparametric Bayesian model for clustering problems involving multiple groups of data. Each group of data is modeled with a mixture, with the number of components being open-ended and inferred automatically by the model. Further, components can be shared across groups, allowing dependencies across groups to be modeled effectively as well as conferring generalization to new groups. Such grouped clustering problems occur often in practice, e.g. in the problem of topic discovery in document corpora. We report experimental results on three text corpora showing the effective and superior performance of the HDP over previous models.",2004-12-01,https://www.semanticscholar.org/paper/adcec120a442af0d58f7a1d2dc175b58ff5a32e2,Neural Information Processing Systems
2288,"Functional analysis of the human MCL-1 gene
",,2000-04-01,https://www.semanticscholar.org/paper/a57cebce14108badaa01a2c598d3539ee7981ee7,Cellular and Molecular Life Sciences (CMLS)
3116,SWAP: A Scheduler with Automatic Process Dependency Detection,"We have developed SWAP, a system that automatically detects process dependencies and accounts for such dependencies in scheduling. SWAP uses system call history to determine possible resource dependencies among processes in an automatic and fully transparent fashion. Because some dependencies cannot be precisely determined, SWAP associates confidence levels with dependency information that are dynamically adjusted using feedback from process blocking behavior. SWAP can schedule processes using this imprecise dependency information in a manner that is compatible with existing scheduling mechanisms and ensures that actual scheduling behavior corresponds to the desired scheduling policy in the presence of process dependencies. We have implemented SWAP in Linux and measured its effectiveness on microbenchmarks and real applications. Our results show that SWAP has low overhead, effectively solves the priority inversion problem and can provide substantial improvements in system performance in scheduling processes with dependencies.",2004-03-29,https://www.semanticscholar.org/paper/007d530ea71264fd36323ebcadf8ecafe20ac578,Symposium on Networked Systems Design and Implementation
3512,Faster Approximation Algorithms for the Unit Capacity Concurrent Flow Problem with Applications to Routing and Finding Sparse Cuts,"This paper describes new algorithms for approximately solving the concurrent multicommodity flow problem with uniform capacities. These algorithms are much faster than algorithms discovered previously. Besides being an important problem in its own right, the uniform-capacity concurrent flow problem has many interesting applications. Leighton and Rao used uniform-capacity concurrent flow to find an approximately ""sparsest cut"" in a graph and thereby approximately solve a wide variety of graph problems, including minimum feedback arc set, minimum cut linear arrangement, and minimum area layout. However, their method appeared to be impractical as it required solving a large linear program. This paper shows that their method might be practical by giving an $O(m^2 \log m)$ expected-time randomized algorithm for their concurrent flow problem on an $m$-edge graph. Raghavan and Thompson used uniform-capacity concurrent flow to solve approximately a channel width minimization problem in very large scale integration. An $O(k^{3/2} (m + n \log n))$ expected-time randomized algorithm and an $O(k\min{n,k} (m+n\log n)\log k)$ deterministic algorithm is given for this problem when the channel width is $\Omega(\log n)$, where $k$ denotes the number of wires to be routed in an $n$-node, $m$-edge network.",1994-06-01,https://www.semanticscholar.org/paper/274968b9c2467687624e2c01269a8063d8e5fc2c,SIAM journal on computing (Print)
2842,Galectin-3 Is a Negative Regulator of Lipopolysaccharide-Mediated Inflammation1,"Galectin-3 is a β-galactoside-binding lectin that plays an important role in inflammatory diseases. It also interacts with the surface carbohydrates of many pathogens, including LPS. However, its role in infection is not fully understood. Data presented herein demonstrate for the first time that galectin-3 is a negative regulator of LPS-induced inflammation. Galectin-3 is constitutively produced by macrophages and directly binds to LPS. Galectin-3-deficient macrophages had markedly elevated LPS-induced signaling and inflammatory cytokine production compared with wild-type cells, which was specifically inhibited by the addition of recombinant galectin-3 protein. In contrast, blocking galectin-3 binding sites by using a neutralizing Ab or its ligand, β-lactose, enhanced LPS-induced inflammatory cytokine expression by wild-type macrophages. In vivo, mice lacking galectin-3 were more susceptible to LPS shock associated with excessive induction of inflammatory cytokines and NO production. However, these changes conferred greater resistance to Salmonella infection. Thus, galectin-3 is a previously unrecognized, naturally occurring, negative regulator of LPS function, which protects the host from endotoxin shock but, conversely, favors Salmonella survival.",2008-08-15,https://www.semanticscholar.org/paper/8a5a8e701c5a64c790b4b42166a75a6856f65170,Journal of Immunology
1914,A bi-objective genetic algorithm for intelligent rehabilitation scheduling considering therapy precedence constraints,,2018-06-01,https://www.semanticscholar.org/paper/aada62d312f8a0c9915da3099334983035b62f21,Journal of Intelligent Manufacturing
3063,Teaching operating systems using virtual appliances and distributed version control,"Students learn more through hands-on project experience for computer science courses such as operating systems, but providing the infrastructure support for a large class to learn by doing can be hard. To address this issue, we introduce a new approach to managing and grading operating system homework assignments based on virtual appliances, a distributed version control system, and live demonstrations. Our solution is easy to deploy and use with students' personal computers, and obviates the need to provide a computer laboratory for teaching purposes. It supports the most demanding course projects, such as those that involve operating system kernel development, and can be used by both on-campus and remote distance learning students even with intermittent network connectivity. Our experiences deploying and using this solution to teach operating systems at Columbia University show that it is easier to use, more flexible, and more pedagogically effective than other approaches.",2010-03-10,https://www.semanticscholar.org/paper/8e63c7f5e88d1bd1cc7facd44611b1b2d6901469,Technical Symposium on Computer Science Education
398,Sharing the cost of muliticast transmissions (preliminary version),,,https://www.semanticscholar.org/paper/9f067319d7a709a6dbebbc9941c5cdc81e9c2417,Symposium on the Theory of Computing
3773,Acquiring Visual Classifiers from Human Imagination,"Abstract : The human mind can remarkably imagine objects that it has never seen, touched, or heard, all in vivid detail. Motivated by the desire to harness this rich source of information from the human mind, this paper investigates how to extract classifiers from the human visual system and leverage them in a machine. We introduce a method that, inspired by wellknown tools in human psychophysics, estimates the classifier that the human visual system might use for recognition but in computer vision feature spaces. Our experiments are surprising, and suggest that classifiers from the human visual system can be transferred into a machine with some success. Since these classifiers seem to capture favorable biases in the human visual system, we present a novel SVM formulation that constrains the orientation of the SVM hyperplane to agree with the human visual system. Our results suggest that transferring this human bias into machines can help object recognition systems generalize across datasets. Moreover, we found that people's culture may subtly vary the objects that people imagine, which influences this bias. Overall, human imagination can be an interesting resource for future visual recognition systems.",2014-10-16,https://www.semanticscholar.org/paper/e30a0e7a798831f8ab8b82ea3c51ba68201b27d1,arXiv.org
2270,Purification of ovine neutrophils and eosinophils: Anaplasma phagocytophilum affects neutrophil density.,"Studies on the functions of ovine granulocytes require pure and functionally active populations of neutrophils and eosinophils. This report describes an improved technique for the separation of neutrophils and eosinophils from the peripheral blood of sheep infected with Anaplasma phagocytophilum and from normal sheep. After centrifugation and discarding the buffy coat layer, which contains the bulk of mononuclear cells, neutrophils with a high degree of purity (94.87 [+/-1.7]%, n=9) and good yield (69 [+/-9]%, n=9) were obtained by density gradient centrifugation on Percoll with a density of 1.09 g/ml (65%). However, this density was not suitable for neutrophils obtained from sheep during the peak period of A. phagocytophilum bacteraemia. Improved purity of infected neutrophils was obtained when the leucocytes were separated on Percoll with a density of 1.08 g/ml (55%). Relatively good purity of eosinophils was obtained when leucocytes from normal sheep were separated on Percoll with a density of 1.10 g/ml (70%). Ovine eosinophils formed a distinct band just below the band of mononuclear cells when a continuous Percoll gradient with a density of 1.10 g/ml was used. The purity of the eosinophils obtained was 87.7 (+/-12.5)% (n=6; range 64.1-97.6%), with a mean recovery rate of 61.9 (+/-20.3)%.",2003-05-01,https://www.semanticscholar.org/paper/5bf0e64e088785dab6b13b75fe9f4dabe3f6f986,Journal of Comparative Pathology
519,A linear programming approach to reasoning about probabilities,,1990-09-01,https://www.semanticscholar.org/paper/ed253d8aa572b36ee25ea2e6a9c034a8ac14231e,Annals of Mathematics and Artificial Intelligence
1758,Stick-Breaking Beta Processes and the Poisson Process,"We show that the stick-breaking construction of the beta process due to Paisley et al. (2010) can be obtained from the characterization of the beta process as a Poisson process. Specifically, we show that the mean measure of the underlying Poisson process is equal to that of the beta process. We use this underlying representation to derive error bounds on truncated beta processes that are tighter than those in the literature. We also develop a new MCMC inference algorithm for beta processes, based in part on our new Poisson process construction.",2012-03-21,https://www.semanticscholar.org/paper/c95af7ce5d123e48191d5ea256029a1542558095,International Conference on Artificial Intelligence and Statistics
3078,ASSURE: automatic software self-healing using rescue points,"Software failures in server applications are a significant problem for preserving system availability. We present ASSURE, a system that introduces rescue points that recover software from unknown faults while maintaining both system integrity and availability, by mimicking system behavior under known error conditions. Rescue points are locations in existing application code for handling a given set of programmer-anticipated failures, which are automatically repurposed and tested for safely enabling fault recovery from a larger class of (unanticipated) faults. When a fault occurs at an arbitrary location in the program, ASSURE restores execution to an appropriate rescue point and induces the program to recover execution by virtualizing the program's existing error-handling facilities. Rescue points are identified using fuzzing, implemented using a fast coordinated checkpoint-restart mechanism that handles multi-process and multi-threaded applications, and, after testing, are injected into production code using binary patching. We have implemented an ASSURE Linux prototype that operates without application source code and without base operating system kernel changes. Our experimental results on a set of real-world server applications and bugs show that ASSURE enabled recovery for all of the bugs tested with fast recovery times, has modest performance overhead, and provides automatic self-healing orders of magnitude faster than current human-driven patch deployment methods.",2009-02-28,https://www.semanticscholar.org/paper/f7226ce7103c263d9c1ec2310019125aa1572369,International Conference on Architectural Support for Programming Languages and Operating Systems
3734,Visual Hide and Seek,"We train embodied agents to play Visual Hide and Seek where a prey must navigate in a simulated environment in order to avoid capture from a predator. We place a variety of obstacles in the environment for the prey to hide behind, and we only give the agents partial observations of their environment using an egocentric perspective. Although we train the model to play this game from scratch, experiments and visualizations suggest that the agent learns to predict its own visibility in the environment. Furthermore, we quantitatively analyze how agent weaknesses, such as slower speed, effect the learned policy. Our results suggest that, although agent weaknesses make the learning problem more challenging, they also cause more useful features to be learned. Our project website is available at: this http URL ~bchen/visualhideseek/.",2019-09-25,https://www.semanticscholar.org/paper/2af4c764352d911641e96c4a52093b2fdeba6a61,IEEE Symposium on Artificial Life
1586,Prescribed Generative Adversarial Networks,"Generative adversarial networks (GANs) are a powerful approach to unsupervised learning. They have achieved state-of-the-art performance in the image domain. However, GANs are limited in two ways. They often learn distributions with low support---a phenomenon known as mode collapse---and they do not guarantee the existence of a probability density, which makes evaluating generalization using predictive log-likelihood impossible. In this paper, we develop the prescribed GAN (PresGAN) to address these shortcomings. PresGANs add noise to the output of a density network and optimize an entropy-regularized adversarial loss. The added noise renders tractable approximations of the predictive log-likelihood and stabilizes the training procedure. The entropy regularizer encourages PresGANs to capture all the modes of the data distribution. Fitting PresGANs involves computing the intractable gradients of the entropy regularization term; PresGANs sidestep this intractability using unbiased stochastic estimates. We evaluate PresGANs on several datasets and found they mitigate mode collapse and generate samples with high perceptual quality. We further found that PresGANs reduce the gap in performance in terms of predictive log-likelihood between traditional GANs and variational autoencoders (VAEs).",2019-10-09,https://www.semanticscholar.org/paper/fae3d474c4d7745be06458df0c20bf837a6055ef,arXiv.org
46,Efficient summarization-aware search for online news articles,"News portals gather and organize news articles published daily on the Internet. Typically, news articles are clustered into 'events' and each cluster is displayed with a short description of its contents. A particularly interesting choice for describing the contents of a cluster is a machine-generated multi-document summary of the articles in the cluster. Such summaries are informative and help news readers to identify and explore only clusters of interest. Naturally, multi-document clusters and summaries are also valuable to help users navigate the results of keyword-search queries. Unfortunately, current document summarizers are still slow; as a result, search strategies that define document clusters and their multi-document summaries online, in a query-specific manner, are prohibitively expensive. In contrast, search strategies that only return offline, query-independent document clusters are efficient, but might return clusters whose (query-independent) summaries are of little relevance to the queries. In this paper, we present an efficient Hybrid search strategy to address the limitations of fully online and fully offline summarization-aware search approaches. Extensive experiments involving user relevance judgments and real news articles show that the quality of our Hybrid results is high, and that these results are computed in substantially less time than with the fully online strategy. We have implemented our strategy and made it available on the Newsblaster news summarization system, which crawls and summarizes news articles from a variety of web sources on a daily basis.",2007-06-18,https://www.semanticscholar.org/paper/80b259d1f832a00c3f10c1aa477be1c1f85d7786,ACM/IEEE Joint Conference on Digital Libraries
1575,The Blessings of Multiple Causes: A Reply to Ogburn et al. (2019),"Ogburn et al. (2019, arXiv:1910.05438) discuss ""The Blessings of Multiple Causes"" (Wang and Blei, 2018, arXiv:1805.06826). Many of their remarks are interesting. But they also claim that the paper has ""foundational errors"" and that its ""premise is...incorrect."" These claims are not substantiated. We correct the record here.",2019-10-15,https://www.semanticscholar.org/paper/8a514faeb037eeffbe52b20abb4e4fceb523a67e,arXiv.org
1172,Measurement of dijet angular distributions at square root(s) = 1.96 TeV and searches for quark compositeness and extra spatial dimensions.,"We present the first measurement of dijet angular distributions in pp collisions at square root(s) = 1.96 TeV at the Fermilab Tevatron Collider. The measurement is based on a dataset corresponding to an integrated luminosity of 0.7 fb(-1) collected with the D0 detector. Dijet angular distributions have been measured over a range of dijet masses, from 0.25 TeV to above 1.1 TeV. The data are in good agreement with the predictions of perturbative QCD and are used to constrain new physics models including quark compositeness, large extra dimensions, and TeV(-1) scale extra dimensions. For all models considered, we set the most stringent direct limits to date.",,https://www.semanticscholar.org/paper/9ea51e0457f41ec5d1f0e8abc75fac913207e213,Physical Review Letters
1511,Posterior Collapse and Latent Variable Non-identifiability,"Variational autoencoders model high-dimensional data by positing low-dimensional latent variables that are mapped through a flexible distribution parametrized by a neural network. Unfortunately, variational autoencoders often suffer from posterior collapse: the posterior of the latent variables is equal to its prior, rendering the variational autoencoder useless as a means to produce meaningful representations. Existing approaches to posterior collapse often attribute it to the use of neural networks or optimization issues due to variational approximation. In this paper, we consider posterior collapse as a problem of latent variable non-identifiability. We prove that the posterior collapses if and only if the latent variables are non-identifiable in the generative model. This fact implies that posterior collapse is not a phenomenon specific to the use of flexible distributions or approximate inference. Rather, it can occur in classical probabilistic models even with exact inference, which we also demonstrate. Based on these results, we propose a class of latent-identifiable variational autoencoders, deep generative models which enforce identifiability without sacrificing flexibility. This model class resolves the problem of latent variable non-identifiability by leveraging bijective Brenier maps and parameterizing them with input convex neural networks, without special variational inference objectives or optimization tricks. Across synthetic and real datasets, latent-identifiable variational autoencoders outperform existing methods in mitigating posterior collapse and providing meaningful representations of the data.",2023-01-02,https://www.semanticscholar.org/paper/0926bffd052d95e54db0ba8b38ca36dd1ec387e8,Neural Information Processing Systems
2667,Mixed reality: where real and virtual worlds meet,,1999-07-01,https://www.semanticscholar.org/paper/cce8aa4a8991fb249d2a2698ef7ae80a6f24437f,International Conference on Computer Graphics and Interactive Techniques
1639,Bayesian Learning and Inference in Recurrent Switching Linear Dynamical Systems,"Many natural systems, such as neurons firing in the brain or basketball teams traversing a court, give rise to time series data with complex, nonlinear dynamics. We can gain insight into these systems by decomposing the data into segments that are each explained by simpler dynamic units. Building on switching linear dynamical systems (SLDS), we develop a model class and Bayesian inference algorithms that not only discover these dynamical units but also, by learning how transition probabilities depend on observations or continuous latent states, explain their switching behavior. Our key innovation is to design these recurrent SLDS models to enable recent Pólya-gamma auxiliary variable techniques and thus make approximate Bayesian learning and inference in these models easy, fast, and scalable.",,https://www.semanticscholar.org/paper/fdb0543229042854dee3cfbe67e3742518bda4be,International Conference on Artificial Intelligence and Statistics
1130,Search for the standard model higgs Boson in the ZH-->nunubb channel in 5.2 fb{-1} of pp collisions at sqrt[s]=1.96 TeV.,"A search is performed for the standard model Higgs boson in 5.2 fb{-1} of pp collisions at sqrt[s]=1.96 TeV, collected with the D0 detector at the Fermilab Tevatron Collider. The final state considered is a pair of b jets and large missing transverse energy, as expected from pp-->ZH-->nunubb production. The search is also sensitive to the WH-->lnubb channel when the charged lepton is not identified. For a Higgs boson mass of 115 GeV, a limit is set at the 95% C.L. on the cross section multiplied by branching fraction for [pp-->(Z/W)H](H-->bb) that is a factor of 3.7 larger than the standard model value, consistent with the factor of 4.6 expected.",,https://www.semanticscholar.org/paper/627f7c637843a66a4fdad133cbdd5287b6be1e64,Physical Review Letters
444,The complexity of knowledge representation,Representing knowledge in forms appropriate for rapid common-sense reasoning is a challenging current problem in artificial intelligence. We review certain recent results which suggest that complexity theory has an important role to play in this field.,1996-05-24,https://www.semanticscholar.org/paper/0bbbed32a18458ad9a973ca18447f194ad618d0e,Proceedings of Computational Complexity (Formerly Structure in Complexity Theory)
959,Visual fatigue induced by watching virtual reality device and the effect of anisometropia,"Abstract The effect of small anisometropia on visual fatigue when using virtual reality (VR) devices was investigated. Participants (n = 34) visited three times. In the first visit, VR exposure (10 min) was conducted with the full correction of the refractive error of both eyes. Experimental anisometropia was induced by adding a + 1.0 dioptre spherical lens either on the dominant eyes in the second visit or on the non-dominant eyes in the third visit. At each visit, the participants played a predetermined video game using a head-mounted display VR for 10 min. Visual fatigue was assessed before and after playing VR game using the Virtual Reality Symptom Questionnaire (VRSQ) and high-frequency component of accommodative microfluctuation. Results showed that watching VR induced significant increase of VRSQ score, significant decrease in the maximum accommodation power and objective increase in visual fatigue. Experimental anisometropia induction either on the dominant or non-dominant eyes did not aggravate visual fatigue. Practitioner summary: Mild differences in refractive error (up to 1.0 dioptre) between both eyes do not significantly increase ocular fatigue by viewing virtual reality device (10 min). The impact of small anisometropia may be limited in developing a virtual reality device. Abbreviations: VR: virtual reality; VRSQ: virtual reality symptom questionnaire; HMD: head-mounted display; HFC: high-frequency component",2021-07-16,https://www.semanticscholar.org/paper/fbafc578bb16a55bb14a20a764eef99506209019,Ergonomics
2696,Research in 3D user interface design at Columbia University,The Computer Graphics and User Interfaces Laboratory at Columbia University is pursuing research in the design and development of new user interface metaphors. This overview provides a high-level description of our work and surveys projects that reflect our two key research directions: 3D user interfaces (including virtual environments and augmented reality) and knowledge-based user interfaces.,1996-04-18,https://www.semanticscholar.org/paper/c45c68b8cea5e61b41a692dc1229ffd1af897ab5,CHI Conference Companion
1539,Optimization-based Causal Estimation from Heterogenous Environments,"This paper presents a new optimization approach to causal estimation. Given data that contains covariates and an outcome, which covariates are causes of the outcome, and what is the strength of the causality? In classical machine learning (ML), the goal of optimization is to maximize predictive accuracy. However, some covariates might exhibit a non-causal association to the outcome. Such spurious associations provide predictive power for classical ML, but they prevent us from causally interpreting the result. This paper proposes CoCo, an optimization algorithm that bridges the gap between pure prediction and causal inference. CoCo leverages the recently-proposed idea of environments, datasets of covariates/response where the causal relationships remain invariant but where the distribution of the covariates changes from environment to environment. Given datasets from multiple environments -- and ones that exhibit sufficient heterogeneity -- CoCo maximizes an objective for which the only solution is the causal solution. We describe the theoretical foundations of this approach and demonstrate its effectiveness on simulated and real datasets. Compared to classical ML and existing methods, CoCo provides more accurate estimates of the causal model.",2021-09-24,https://www.semanticscholar.org/paper/34ee368fc2f5de1a95d408811b809ae234c137bc,arXiv.org
2088,Cycle time prediction and control based on production line status and manufacturing data mining,"This study aims to develop a methodology for predicting cycle time based on domain knowledge and data mining algorithms given production status including WIP, throughput. The proposed model and derived rules were validated with real data and demonstrated its practical viability for supporting production planning decisions",2005-10-03,https://www.semanticscholar.org/paper/c712e24165e0c84351292357d1e2c7e8e22a0a46,"ISSM 2005, IEEE International Symposium on Semiconductor Manufacturing, 2005."
2825,The Promigratory Activity of the Matricellular Protein Galectin-3 Depends on the Activation of PI-3 Kinase,"Expression of galectin-3 is associated with sarcoma progression, invasion and metastasis. Here we determined the role of extracellular galectin-3 on migration of sarcoma cells on laminin-111. Cell lines from methylcholanthrene-induced sarcomas from both wild type and galectin-3−/− mice were established. Despite the presence of similar levels of laminin-binding integrins on the cell surface, galectin-3−/− sarcoma cells were more adherent and less migratory than galectin-3+/+ sarcoma cells on laminin-111. When galectin-3 was transiently expressed in galectin-3−/− sarcoma cells, it inhibited cell adhesion and stimulated the migratory response to laminin in a carbohydrate-dependent manner. Extracellular galectin-3 led to the recruitment of SHP-2 phosphatase to focal adhesion plaques, followed by a decrease in the amount of phosphorylated FAK and phospho-paxillin in the lamellipodia of migrating cells. The promigratory activity of extracellular galectin-3 was inhibitable by wortmannin, implicating the activation of a PI-3 kinase dependent pathway in the galectin-3 triggered disruption of adhesion plaques, leading to sarcoma cell migration on laminin-111.",2011-12-28,https://www.semanticscholar.org/paper/f05410d0e61fa74d7aa5f588c9429098cec9ce74,PLoS ONE
1825,Nonparametric Bayes Pachinko Allocation,"Recent advances in topic models have explored complicated structured distributions to represent topic correlation. For example, the pachinko allocation model (PAM) captures arbitrary, nested, and possibly sparse correlations between topics using a directed acyclic graph (DAG). While PAM provides more flexibility and greater expressive power than previous models like latent Dirichlet allocation (LDA), it is also more difficult to determine the appropriate topic structure for a specific dataset. In this paper, we propose a nonparametric Bayesian prior for PAM based on a variant of the hierarchical Dirichlet process (HDP). Although the HDP can capture topic correlations defined by nested data structure, it does not automatically discover such correlations from unstructured data. By assuming an HDP-based prior for PAM, we are able to learn both the number of topics and how the topics are correlated. We evaluate our model on synthetic and real-world text datasets, and show that nonparametric PAM achieves performance matching the best of PAM without manually tuning the number of topics.",2007-07-19,https://www.semanticscholar.org/paper/87ce59b72873d2b77ab29611a8b448dd571f40ed,Conference on Uncertainty in Artificial Intelligence
1577,Adapting Neural Networks for the Estimation of Treatment Effects,"This paper addresses the use of neural networks for the estimation of treatment effects from observational data. Generally, estimation proceeds in two stages. First, we fit models for the expected outcome and the probability of treatment (propensity score) for each unit. Second, we plug these fitted models into a downstream estimator of the effect. Neural networks are a natural choice for the models in the first step. The question we address is: how can we adapt the design and training of the neural networks used in the first step in order to improve the quality of the final estimate of the treatment effect? We propose two adaptations based on insights from the statistical literature on the estimation of treatment effects. The first is a new architecture, the Dragonnet, that exploits the sufficiency of the propensity score for estimation adjustment. The second is a regularization procedure, targeted regularization, that induces a bias towards models that have non-parametrically optimal asymptotic properties `out-of-the-box`. Studies on benchmark datasets for causal inference show these adaptations outperform existing methods. Code is available at this http URL.",2019-06-05,https://www.semanticscholar.org/paper/a278c07c8bd2921e59dd862cd91a0540dd340030,Neural Information Processing Systems
572,The even-path problem for graphs and digraphs,On donne un simple algorithme en temps lineaire pour trouver des chemins de meme longueur entre deux nœuds specifie d'un graphe donne. On montre que le meme probleme pour des graphes orientes est NP complet,1984-12-01,https://www.semanticscholar.org/paper/2dcf9dea64f3ea4ac630fe616ee7d789755babed,Networks
998,The risk of newly developed visual impairment in treated normal‐tension glaucoma: 10‐year follow‐up,To investigate the risk and risk factors for newly developed visual impairment in treated patients with normal‐tension glaucoma (NTG) followed up on for 10 years.,2014-12-01,https://www.semanticscholar.org/paper/8a269749eb57d7232756b629316169d604bef5b2,Acta ophthalmologica
2431,SIGCHI Lifetime Research Award Talk-Seeing Past Looking Forward,"I have long been interested in how computers can help people perform skilled tasks-a theme that underlies essentially all of my research. As the intentionally convoluted title of this talk implies, I will take a look back at some of this research to explain what came later, and to speculate on what might be next. My earliest work as a graduate student was as part of a team developing novel hypermedia editing and presentation tools for technical documentation of equipment maintenance procedures. Understanding the effort involved in using these tools manually led to my dissertation, which explored rule-based techniques for the automated generation of 3D graphics that could ultimately replace the pictures found in maintenance manuals. Based on information about the task to be depicted, I designed a system that chose the objects to include, determined the level of detail at which to render them (e.g., adding more detail to disambiguate objects that could otherwise be confused with each other), highlighted important objects for emphasis, and created additional ""metaobjects"" (e.g., arrows to show actions). When I started as a faculty member, several of my students built on this research direction in both 3D and 2D domains, and we were soon collaborating with colleagues to develop generation approaches for coordinated multimedia explanations that combined graphics with text. At the same time, I was becoming excited by the potential of virtual reality (VR) to render objects stereoscopically and interact with them in 3D, in abstract domains as well as physical ones. And it was an easy jump from there to augmented reality (AR), cobbling together a simple monoscopic optical seethrough head-worn display. Our first AR system was a ""hybrid"" desktop window manager that embedded windows displayed on a flat panel display within a much larger virtual surround presented on the AR display. With that AR display now available as a tool, it became stunningly clear that so much of the effort we'd put into carefully crafting stand-alone pictures to explain tasks was unnecessary. If the user could instead look directly at the actual task domain, then a simple highlight or arrow, overlaid in situ, could show which button to push or knob to turn on a panel of controls.",2018-04-20,https://www.semanticscholar.org/paper/18ff79c3c5ca0bfcd1c23c0d01fde781097e4b6b,CHI Extended Abstracts
79,Web Mining Meets Web Search,,,https://www.semanticscholar.org/paper/8f88b41d82d65165b3b98820358e86f0b0a505fc,Workshop on Research Issues on Data Mining and Knowledge Discovery
1594,Technical perspective: Expressive probabilistic models and scalable method of moments,,2018-03-26,https://www.semanticscholar.org/paper/3c94e61000c01763e785242a199e358f7a99446a,Communications of the ACM
3205,Communication is key: Mother-offspring signaling can affect behavioral responses and offspring survival in feral horses (Equus caballus),"Acoustic signaling plays an important role in mother-offspring recognition and subsequent bond-formation. It remains unclear, however, if mothers and offspring use acoustic signaling in the same ways and for the same reasons throughout the juvenile stage, particularly after mutual recognition has been adequately established. Moreover, despite its critical role in mother-offspring bond formation, research explicitly linking mother-infant communication strategies to offspring survival are lacking. We examined the communicative patterns of mothers and offspring in the feral horse (Equus caballus) to better understand 1) the nature of mother-offspring communication throughout the first year of development; 2) the function(s) of mother- vs. offspring-initiated communication and; 3) the importance of mare and foal communication to offspring survival. We found that 1) mares and foals differ in when and how they initiate communication; 2) the outcomes of mare- vs. foal-initiated communication events consistently differ; and 3) the communicative patterns between mares and their foals can be important for offspring survival to one year of age. Moreover, given the importance of maternal activity to offspring behavior and subsequent survival, we submit that our data are uniquely positioned to address the long-debated question: do the behaviors exhibited during the juvenile stage (by both mothers and their young) confer delayed or immediate benefits to offspring? In summary, we aimed to better understand 1) the dynamics of mother-offspring communication, 2) whether mother-offspring communicative patterns were important to offspring survival, and 3) the implications of our research regarding the function of the mammalian juvenile stage. Our results demonstrate that we have achieved those aims.",2020-04-17,https://www.semanticscholar.org/paper/2fd9f055d121ed1021b613eff96a6d63d265f849,PLoS ONE
3484,Approximation schemes for minimizing average weighted completion time with release dates,"We consider the problem of scheduling n jobs with release dates on m machines so as to minimize their average weighted completion time. We present the first known polynomial time approximation schemes for several variants of this problem. Our results include PTASs for the case of identical parallel machines and a constant number of unrelated machines with and without preemption allowed. Our schemes are efficient: for all variants the running time for /spl alpha/(1+/spl epsiv/) approximation is of the form f(1//spl epsiv/, m)poly(n).",1999-10-17,https://www.semanticscholar.org/paper/604a45914d6ec955491ccf8b37ad61b1f62ff0cc,40th Annual Symposium on Foundations of Computer Science (Cat. No.99CB37039)
2303,Regulation of neutrophil apoptosis by diadenosine pentaphosphate and GM-CSF.,,1996-08-01,https://www.semanticscholar.org/paper/33a647aa953546ed5633576c5f4faa6cf328cbbf,Biochemical Society Transactions
207,Optimizing the diamond lane: A more tractable carpool problem and algorithms,"Carpooling has been long deemed a promising approach to better utilizing existing transportation infrastructure. However, there are several reasons carpooling is still not the preferred mode of commute in the United States: first, complex human factors, including time constraints and not having right incentive structures, discourage the sharing of rides; second, algorithmic and technical barriers inhibit the development of online services for matching riders. In this work, we study algorithms for 3+ high-occupancy vehicle (HOV) lanes, which permit vehicles that hold three or more people. We focus on the technical barriers but also address the aforementioned human factors. We formulate the HOV3 Carpool problem, and show that it is NP-Complete. We thus pose the relaxed problem HOV3- Carpool problem, allowing groups of up to size three, and propose several methods for solving the problem of finding globally optimal carpool groups that may utilize these 3- HOV lanes. Our methods include local search, integer programming, and dynamic programming. Our local search methods include sampling-based (hill-climbing and simulated annealing), classical neighborhood search, and a hybrid random neighborhood search. We assess the methods numerically in terms of objective value and scalability. Our findings show that our sampling-based local search methods scale up to 100K agents, thereby improving upon related previous work (which studies up to 1000 agents). The hill climbing local search method converges significantly closer and faster towards a naive lower bound on cumulative carpooling cost.",2016-11-01,https://www.semanticscholar.org/paper/a8103c9129b3f23085ab4bf6818f97ff74cf7fae,2016 IEEE 19th International Conference on Intelligent Transportation Systems (ITSC)
48,Modeling and managing changes in text databases,"Large amounts of (often valuable) information are stored in web-accessible text databases. “Metasearchers” provide unified interfaces to query multiple such databases at once. For efficiency, metasearchers rely on succinct statistical summaries of the database contents to select the best databases for each query. So far, database selection research has largely assumed that databases are static, so the associated statistical summaries do not evolve over time. However, databases are rarely static and the statistical summaries that describe their contents need to be updated periodically to reflect content changes. In this article, we first report the results of a study showing how the content summaries of 152 real web databases evolved over a period of 52 weeks. Then, we show how to use “survival analysis” techniques in general, and Cox's proportional hazards regression in particular, to model database changes over time and predict when we should update each content summary. Finally, we exploit our change model to devise update schedules that keep the summaries up to date by contacting databases only when needed, and then we evaluate the quality of our schedules experimentally over real web databases.",2007-08-01,https://www.semanticscholar.org/paper/cf3f1cb8fb7fbf9d8393c24eb20d11a585a46b65,TODS
2327,Stimulation of reactive oxidant production in neutrophils by soluble and insoluble immune complexes occurs via different receptors/signal transduction systems.,"Cell-free synovial fluid from patients with rheumatoid arthritis contains soluble and insoluble IgG-containing immune complexes which activate reactive oxidant production in human neutrophils. In this report we have measured the effects of inhibitors of signal transduction pathways on neutrophil activation by these complexes and also following activation by synthetic soluble and insoluble immune complexes made from human serum albumin (HSA) and anti-(HSA) antibodies. In all aspects studied, the soluble rheumatoid complexes and the soluble synthetic complexes were indistinguishable in the ways in which they activated neutrophils. Activation of reactive oxidant production in response to these soluble complexes was completely inhibited by pertussis toxin (indicating G-protein coupling of receptor occupancy), completely insensitive to staurosporine (indicating that oxidant production did not require protein kinase C activity), only marginally (< 30%) inhibited by butanol (indicating that dependence upon activity of phospholipase D was minimal), and completely inhibited by chloracysine, an inhibitor of phospholipase A2. In contrast, activation of reactive oxidant production in response to the insoluble rheumatoid or insoluble synthetic immune complexes was largely pertussis toxin insensitive, inhibited by > 50% by staurosporine, inhibited by > 50% by butanol, and completely inhibited by chloracysine. These results show that the receptor-mediated signal transduction systems activated by the soluble and insoluble immune complexes are different. Because the soluble complexes activate a transient burst of reactive oxidant secretion from primed neutrophils, the mechanisms regulating either the release or the intracellular production of oxidants within rheumatoid joints are distinct and hence may be pharmacologically modified independently of each other.",1994-03-01,https://www.semanticscholar.org/paper/95bb35abd40797b825f1e0d63037c0097f7f6821,FEMS Immunology & Medical Microbiology
3445,Vertex Cover Approximations on Random Graphs,,2007-06-06,https://www.semanticscholar.org/paper/d7819eedee684d961669d7623a6b81ab247653be,Workshop on Engineering Applications
1725,Topic Model,,,https://www.semanticscholar.org/paper/f47af707941a43fae8b58066eab91ee12ea7a72a,Encyclopedia of Social Network Analysis and Mining
3704,Task Bias in Vision-Language Models,"Incidental supervision from language has become a popular approach for learning generic visual representations that can be prompted to perform many recognition tasks in computer vision. We conduct an in-depth exploration of the CLIP model and show that its visual representation is often strongly biased towards solving some tasks more than others. Moreover, which task the representation will be biased towards is unpredictable, with little consistency across images. To resolve this task bias, we show how to learn a visual prompt that guides the representation towards features relevant to their task of interest. Our results show that these visual prompts can be independent of the input image and still effectively provide a conditioning mechanism to steer visual representations towards the desired task.",2022-12-08,https://www.semanticscholar.org/paper/d857fa1e47283da17b72f8e59c377c7aa72bae78,arXiv.org
1597,Measuring discursive influence across scholarship,"Significance Scientific and scholarly influence is multifaceted, shifts over time, and varies across disciplines. We present a dynamic topic model to credit documents with influence that shapes future discourse based on their content and contextual features. We trace discursive innovation in scholarship and identify the influence of particular articles along with their authors, affiliations, and journals. In collections of science, social science, and humanities research spanning over a century, our measure helps predict citations and reveals signals that recognize authors who make diverse contributions and whose contributions take longer to be appreciated, allowing us to compensate for bias in citation behavior. Assessing scholarly influence is critical for understanding the collective system of scholarship and the history of academic inquiry. Influence is multifaceted, and citations reveal only part of it. Citation counts exhibit preferential attachment and follow a rigid “news cycle” that can miss sustained and indirect forms of influence. Building on dynamic topic models that track distributional shifts in discourse over time, we introduce a variant that incorporates features, such as authorship, affiliation, and publication venue, to assess how these contexts interact with content to shape future scholarship. We perform in-depth analyses on collections of physics research (500,000 abstracts; 102 years) and scholarship generally (JSTOR repository: 2 million full-text articles; 130 years). Our measure of document influence helps predict citations and shows how outcomes, such as winning a Nobel Prize or affiliation with a highly ranked institution, boost influence. Analysis of citations alongside discursive influence reveals that citations tend to credit authors who persist in their fields over time and discount credit for works that are influential over many topics or are “ahead of their time.” In this way, our measures provide a way to acknowledge diverse contributions that take longer and travel farther to achieve scholarly appreciation, enabling us to correct citation biases and enhance sensitivity to the full spectrum of scholarly impact.",2018-03-12,https://www.semanticscholar.org/paper/4ae9c97b65ef9c816f2f88e73ecf5416863a17ad,Proceedings of the National Academy of Sciences of the United States of America
218,The Web Graph as an Equilibrium,,2015-09-28,https://www.semanticscholar.org/paper/7bc7d622b6a5ebd210ecfb00fd726598656fc867,Algorithmic Game Theory
3413,A 2-Competitive Algorithm For Online Convex Optimization With Switching Costs,"We consider a natural online optimization problem set on the real line. The state of the online algorithm at each integer time is a location on the real line. At each integer time, a convex function arrives online. In response, the online algorithm picks a new location. The cost paid by the online algorithm for this response is the distance moved plus the value of the function at the final destination. The objective is then to minimize the aggregate cost over all time. The motivating application is rightsizing power-proportional data centers. We give a 2-competitive algorithm for this problem. We also give a 3-competitive memoryless algorithm, and show that this is the best competitive ratio achievable by a deterministic memoryless algorithm. Finally we show that this online problem is strictly harder than the standard ski rental problem.",,https://www.semanticscholar.org/paper/aeba62d4a55bb17ddd1b52fd9801dd463b9e0b8d,"International Workshop and International Workshop on Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques"
990,Changes of the Macular Ganglion Cell-Inner Plexiform Layer Thickness after Cataract Surgery in Glaucoma Patients,"Purpose. To investigate the effect of uneventful cataract surgery on macular ganglion cell-inner plexiform layer (mGC-IPL) thickness in glaucoma patients. Methods. This retrospective study included 65 eyes of 65 subjects who underwent uneventful cataract surgery, including 13 glaucoma eyes and 52 normal eyes. Using spectral domain optical coherence tomography, the mGC-IPL thickness was measured and compared between glaucoma and normal eyes preoperatively as well as 1 month and 3 months postoperatively. Linear regression analysis was used to determine the factors associated with postoperative change in mGC-IPL thickness. Results. The mean mGC-IPL significantly increased in both groups 1 month and 3 months after surgery (all P values equal to or less than 0.001). The postoperative changes between groups were not significantly different (P = 0.171). In the multivariate regression analysis, preoperative mGC-IPL thickness showed a significant association with the change of average mGC-IPL thickness 1 month and 3 months after surgery (all P values < 0.001). Conclusions. The mean mGC-IPL thickness was increased after cataract surgery, and the postoperative mGC-IPL thickness changes were associated with preoperative mGC-IPL thickness in both groups and axial length in normal eye. The effects of cataract surgery on mean mGC-IPL thickness were not different in glaucomatous and normal eyes.",2016-12-22,https://www.semanticscholar.org/paper/4d2ad8e492e956c192fa837d62faa11fc3de336d,Journal of Ophthalmology
1023,Geometric Motion Planning for a Three-Link Swimmer in a Three-Dimensional low Reynolds-Number Regime,"Purcell's three-link, two-joint planar swimmer is an iconic model of a simple mechanism that can locomote in the low-Reynolds number regime. In this paper, we consider a modification to the design of the planar swimmer by allowing yaw-pitch movements at the two actuated joints as opposed to the conventional yaw-yaw movements. We demonstrate that this design with only two active inputs is capable of swimming in three dimensions unlike the planar swimmer. Using analytical and visual tools from geometric mechanics, we design motion primitives that enable the swimmer to reorient itself and swim along canonical directions in the inertial frame. We also provide experimental results on a hardware testbed to show a comparison between the trajectories derived from simulated gaits and trajectory of the robot executing those gaits.",2018-06-01,https://www.semanticscholar.org/paper/40265ddc4300cd24656d92941e1ce277d0574b9f,American Control Conference
587,On the complexity of unique solutions,We show that the problem of deciding whether an instance of the traveling salesman problem has a uniquely optimal solution is complete for Δ2P.,1982-11-03,https://www.semanticscholar.org/paper/1e86b27cc53db003e8aa43eff89407f9cc5b9e9c,23rd Annual Symposium on Foundations of Computer Science (sfcs 1982)
1890,Empirical Study of Multi-Objective Parameter Optimization in Wire Bonding Process,"Wire bonding is one of the key processes which connects the wafer die and the lead frame in the IC packaging process. It is noted that process parameters influence both the speed and quality of bonding. Parameters, such as descending speed and cycles affect the bonding speed; while parameters such as bonding force and ultrasonic energy level affect the strength of joint. Both situations will affect the output (unit per hour, UPH), and lead to the variation of the production capacity as well.Nowadays, most companies in Taiwan still follow the rule of thumb when setting the process parameters. However, the large number of parameters in the bonding process makes it difficult for engineers to find appropriate parameters at once efficiently based on their experience only. When new products are introduced into mass production, engineers must go through multiple adjustments in a repeated trial and error way to find out the appropriate parameters, which takes lots of time, manpower and material resources.Using data mining techniques to establish predictive models for process parameters can not only increase the production capacity but also reduce the cost of the operation during adjustments. However, most studies in the field of wire bonding parameter optimization focused on increasing the bonding quality, while little consider the operation time and quality simultaneously. In this study, a data mining framework is proposed to extract the relationship between the bonding speed, bonding quality and the parameters.This study aims to increase production capacity by enhancing the bonding quality and the bonding speed simultaneously via recommending optimized process parameters. The proposed framework is based on a defect classification model constructed by random forest (RF) and extreme gradient boosting (XGBoost) method, and a multi-objective process parameters optimization model applying particle swarm optimization (PSO) method.An empirical study was conducted in a leading IC packaging and assembly company in Taiwan. The empirical result on 2 testing products reveals that the proposed approach increases the bonding quality by 70%, whereas the bonding speed is enhanced by 20%; overall, the proposed approach benefits output UPH by 26.6%, 5.0% for each product. Besides improving the production capacity, the proposed framework can also recommend parameters systematically and more quickly to enhance engineers’ parameter tuning efficiency on-line and achieve the goal of reducing the introduction time of new products.",2019-10-01,https://www.semanticscholar.org/paper/184270bd767757b255573120137f351c814905a3,Impact
2812,Contribution of spinal galectin-3 to acute herpetic allodynia in mice,,2012-03-01,https://www.semanticscholar.org/paper/d29380944c7eaafecbf187535dfff30935a913bc,Pain
1814,Continuous Time Dynamic Topic Models,"In this paper, we develop the continuous time dynamic topic model (cDTM). The cDTM is a dynamic topic model that uses Brownian motion to model the latent topics through a sequential collection of documents, where a ""topic"" is a pattern of word use that we expect to evolve over the course of the collection. We derive an efficient variational approximate inference algorithm that takes advantage of the sparsity of observations in text, a property that lets us easily handle many time points. In contrast to the cDTM, the original discrete-time dynamic topic model (dDTM) requires that time be discretized. Moreover, the complexity of variational inference for the dDTM grows quickly as time granularity increases, a drawback which limits fine-grained discretization. We demonstrate the cDTM on two news corpora, reporting both predictive perplexity and the novel task of time stamp prediction.",2008-07-09,https://www.semanticscholar.org/paper/411277565d6182b40d301df1d2179f5c2b634bb3,Conference on Uncertainty in Artificial Intelligence
3701,Visual Classification via Description from Large Language Models,"Vision-language models (VLMs) such as CLIP have shown promising performance on a variety of recognition tasks using the standard zero-shot classification procedure -- computing similarity between the query image and the embedded words for each category. By only using the category name, they neglect to make use of the rich context of additional information that language affords. The procedure gives no intermediate understanding of why a category is chosen, and furthermore provides no mechanism for adjusting the criteria used towards this decision. We present an alternative framework for classification with VLMs, which we call classification by description. We ask VLMs to check for descriptive features rather than broad categories: to find a tiger, look for its stripes; its claws; and more. By basing decisions on these descriptors, we can provide additional cues that encourage using the features we want to be used. In the process, we can get a clear idea of what features the model uses to construct its decision; it gains some level of inherent explainability. We query large language models (e.g., GPT-3) for these descriptors to obtain them in a scalable way. Extensive experiments show our framework has numerous advantages past interpretability. We show improvements in accuracy on ImageNet across distribution shifts; demonstrate the ability to adapt VLMs to recognize concepts unseen during training; and illustrate how descriptors can be edited to effectively mitigate bias compared to the baseline.",2022-10-13,https://www.semanticscholar.org/paper/a42b091adaf29b06a092b67192ac07cb93312f2a,International Conference on Learning Representations
504,Designing secure communication protocols from trust specifications,,1991-12-17,https://www.semanticscholar.org/paper/c881650846db3dd471ac8f01290812c8b4dc591f,Algorithmica
1106,Search for low-mass weakly interacting massive particles using voltage-assisted calorimetric ionization detection in the SuperCDMS experiment.,"SuperCDMS is an experiment designed to directly detect weakly interacting massive particles (WIMPs), a favored candidate for dark matter ubiquitous in the Universe. In this Letter, we present WIMP-search results using a calorimetric technique we call CDMSlite, which relies on voltage-assisted Luke-Neganov amplification of the ionization energy deposited by particle interactions. The data were collected with a single 0.6 kg germanium detector running for ten live days at the Soudan Underground Laboratory. A low energy threshold of 170  eVee (electron equivalent) was obtained, which allows us to constrain new WIMP-nucleon spin-independent parameter space for WIMP masses below 6  GeV/c2.",2013-09-12,https://www.semanticscholar.org/paper/86852325718f22c201b0f867a86f5da8572eb8d6,Physical Review Letters
2305,Stimulation of primed neutrophils by soluble immune complexes.,"Soluble IgG-containing immune complexes are unable to initiate oxidant production in unprimed neutrophils. However, priming of the neutrophils with either granulocyte-macrophage colony-stimulating factor (GM-CSF) or cytochalasin B prior to exposure to these complexes results in activation of a rapid and extensive secretion of reactive oxidants. In this study, we have investigated the ability of soluble immune complexes to: (1) induce oxidant generation; (2) bind to the cell surface; and (3) induce Ca2+ transients in neutrophils primed by incubation with GM-CSF or cytochalasin B. Our findings give new insight into the molecular processes involved in the ""priming"" phenomenon.",1996-12-01,https://www.semanticscholar.org/paper/9b21b5fdc7209a894e86c5452c213875fde2758f,Biologicals (Print)
653,"Balloon angioplasty for treatment of in-stent restenosis: feasibility, safety, and efficacy.","Sixty patients with 1 or 2 stainless steel intracoronary stents (Cook, Inc.) underwent balloon angioplasty for in-stent restenosis 1.5-13.5 months after stenting. Seventy-five in-stent redilatation procedures were performed. Seventy-three restenotic lesions (97%) were successfully recrossed and dilated, reducing the mean pre-angioplasty intrastent diameter stenosis from 77 +/- 12% to 20 +/- 11% residual. Although one angioplasty (1.3%) was complicated by non-Q-wave infarction, no angioplasty-related death, acute closure, need for additional stenting, emergent coronary bypass surgery, side branch occlusion, or vascular sequelae occurred. Post-procedure heparin was not used in 83% of successful cases. Most patients were discharged the day following redilatation (mean in-hospital stay 1.7 +/- 1.3 days). At 5.4 +/- 3.4 months following in-stent angioplasty, 84% of patients were in Canadian Cardiovascular Society class 0 or I. In conclusion, balloon dilatation in this stent for restenosis appears simple and efficacious in the short term, and may entail less risk than dilatation of unprotected coronary vessels.",1994-06-01,https://www.semanticscholar.org/paper/53d92d9ec3553d72923d2884c1a71f8d58863690,Catheterization and Cardiovascular Diagnosis
2497,Using augmented snapshots for viewpoint switching and manipulation in augmented reality,"SnapAR is a magic-lens-based hand-held augmented reality application that allows its user to store snapshots of a scene and revisit them virtually at a later time. By storing a still image of the unaugmented background along with the 6DOF camera pose, this approach allows augmentations to remain dynamic and interactive. This makes it possible for the user to quickly switch between vantage points at different locations from which to view and manipulate virtual objects, without the overhead of physically traveling between those locations.",2012-05-05,https://www.semanticscholar.org/paper/825ac8d02f18c1daff59f45b5cdb86490dededab,CHI Extended Abstracts
3517,Job scheduling in rings,"We give distributed approximation algorithms for job scheduling in a ring architecture. In contrast to almost all other parallel scheduling models, the model we consider captures the influence of the underlying communications network by specifying that task migration from one processor to another takes time proportional to the distance between those two processors in the network. As a result, our algorithms must balance both computational load and communication time. The algorithms are simple, require no global control, and work in a variety of settings. All come with small constant-factor approximation guarantees; the basic algorithm yields schedules of length at most 4.22 times optimal. We also give a lower bound on the performance of any distributed algorithm some results for a simple capacitated case, and the results of simulation experiments, which give better results than our worst-case analysis.",1994-08-01,https://www.semanticscholar.org/paper/a9a951376e571894eba6f7668a727b81a2e816c1,ACM Symposium on Parallelism in Algorithms and Architectures
1676,The Population Posterior and Bayesian Modeling on Streams,"Many modern data analysis problems involve inferences from streaming data. However, streaming data is not easily amenable to the standard probabilistic modeling approaches, which require conditioning on finite data. We develop population variational Bayes, a new approach for using Bayesian modeling to analyze streams of data. It approximates a new type of distribution, the population posterior, which combines the notion of a population distribution of the data with Bayesian inference in a probabilistic model. We develop the population posterior for latent Dirichlet allocation and Dirichlet process mixtures. We study our method with several large-scale data sets.",2015-12-07,https://www.semanticscholar.org/paper/4718f38e709c8203201cf4e226519f985b3d0183,Neural Information Processing Systems
1953,Multistage semiconductor memory inventory model based on survival analysis,"Inventory management is challenging and critical problem in the semiconductor memory manufacturing industry. Rapid technological development and short product life cycle cause a high risk of product obsolescence. However, manufacturers must still hold a reasonable level of inventory to satisfy the needs of customers when demand is uncertain and lead times are long. In this study, the needs and constraints of this semiconductor manufacturing problem based on inventory days were considered and a linear programming model was constructed with the objective of determining a weighted minimal cost for resolving the problem of multistage inventory management. This study applied the concept of material requirement planning for calculating the inventory information of various stages. In addition, the shortage problem, excessive or inadequate safety stock levels, and capacity balance of technology were considered. Finally, a numerical study was conducted for evaluating the performance of the inventory model, and the results showed that the model can assist decision makers in early preparation of inventories. Under the same condition of demand fulfillment, the proposed model can be used to reduce the unnecessary inventory of downstream banks and avoid the risk of future inventory obsolescence, thereby enhancing competitiveness.",2014-08-01,https://www.semanticscholar.org/paper/1bfa3b8a5ad5473869e5f0ca7d59f2646f97d490,2014 IEEE International Conference on Automation Science and Engineering (CASE)
2611,An Evaluation of Automatically Generated Briefings of Patient Status,"We report on an evaluation of MAGIC, a system that automatically generates briefings of patient status after coronary bypass surgery, completed in the Cardio Thoracic Intensive Care Unit at New York Presbyterian Hospital. Through enhancements in system design, robustness and speed, we compared information obtained by nurses against two briefings, one automatically generated by MAGIC and one provided by physicians upon the patient's arrival to the ICU. Our results show that MAGIC and the physician briefing provide a substantial increase in the amount of information than is available prior to the patient's arrival and that the information MAGIC provides is accurate. In many aspects, MAGIC out-performs the physician briefing; information is reported earlier and is always available. We conclude that MAGIC provides the CT ICU staff early on with a better assessment of the patient's status than in current practice and allows them to better prepare for the patient's arrival.",,https://www.semanticscholar.org/paper/f1cb1e0c92dd7740e1b14fed663d99cb0b5a7b22,Medinfo
833,Testing Finite-State Machines: State Identification and Verification,"We study the complexity of two fundamental problems in the testing of finite-state machines. 1) Distinguishing sequences (state identification). We show that it is PSPACE-complete to determine whether a finite-state machine has a preset distinguishing sequence. There are machines that have distinguishing sequences, but only of exponential length. We give a polynomial time algorithm that determines whether a finite-state machine has an adaptive distinguishing sequence. (The previous classical algorithms take exponential time.) Furthermore, if there is an adaptive distinguishing sequence, then we give an efficient algorithm that constructs such a sequence of length at most n(n/spl minus/1)/2 (which is the best possible), where n is the number of states. 2) Unique input output sequences (state verification). It is PSPACE-complete to determine whether a state of a machine has a unique input output sequence. There are machines whose states have unique input output sequences but only of exponential length. >",1994-03-01,https://www.semanticscholar.org/paper/7b66fa6b859eac503906e2147980d5b47ba4d814,IEEE Trans. Computers
1361,Search for the Higgs boson in H --> WW(*) decays in pp collisions at square root of 1.96 TeV.,"We present a search for the standard model Higgs boson in H --> WW(*) decays with e+e-, e+/-mu-/+, and mu+mu- final states in pp collisions at a center-of-mass energy of square root of s = 1.96 TeV. The data, collected from April 2002 to June 2004 with the D0 detector, correspond to an integrated luminosity of 300-325 pb(-1), depending on the final state. The number of events observed is consistent with the expectation from backgrounds. Limits from the combination of all three channels on the Higgs boson production cross section times branching ratio sigma x BR(H --> WW(*) are presented.",2005-08-01,https://www.semanticscholar.org/paper/e0ad856dcd7e9e464cc577f88b0bb37671cbb690,Physical Review Letters
2971,"Gene expression changes with age in skin, adipose tissue, blood and brain",,2013-07-26,https://www.semanticscholar.org/paper/cc7224e106d5ebe2ac66444dc14056cacb27acb4,Genome Biology
3499,Improved approximation algorithms for unsplittable flow problems,"In the single-source unsplittable flow problem we are given a graph G, a source vertex s and a set of sinks t/sub 1/, ..., t/sub k/ with associated demands. We seek a single s-t/sub i/ flow path for each commodity i so that the demands are satisfied and the total flow routed across any edge e is bounded by its capacity c/sub e/. The problem is an NP-hard variant of max flow and a generalization of single-source edge-disjoint paths with applications to scheduling, load balancing and virtual-circuit routing problems. In a significant development, Kleinberg gave recently constant-factor approximation algorithms for several natural optimization versions of the problem. In this paper we give a generic framework, that yields simpler algorithms and significant improvements upon the constant factors. Our framework, with appropriate subroutines applies to all optimization versions previously considered and treats in a unified manner directed and undirected graphs.",1997-10-19,https://www.semanticscholar.org/paper/bbe009658953de1079b3f4767653b8619047908c,Proceedings 38th Annual Symposium on Foundations of Computer Science
840,Linear programming without the matrix,"We study the problem facing a set of decision-makers who must select values for the variables of a linear program, when only parts of the matrix are available to each of them. The goal is to find a feasible solution that is as close to the true optimum as possible, When each decision-maker decides one variable and knows all constraints involving this variable, we show that the worst-case ratio is related tc,the maximum number of variables appearing in each constraint, and a simple “safe” heuristic is optimal. Since this problem involves constrained optimization, there is a novel criterion, besides the competitive ratio, comparing the performance of a heuristic with the best feasible distributed algorithm, perhaps specializing on the current inst ante; we show different bounds for this parameter. When the constraint structure (the zero-nonzero pattern of the matrix) is known in advance, and the variables are partitioned bet ween decision-makers, then the optimum ratio is a complicated parameter of the associated hypergraph, which we bound from above and below in terms of variants of clique and graph coloring; but several interesting special cases are characterized completely. 1 Department of Computer Science and Engineering, University of California at San Diego. Research supported by the National Science Foundation. 2 AT&T Bell Laboratories, Murray Hill, NJ 07974. Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee arrd/or specific permission. 25th ACM STOC ‘93-51931CA, LE3A e 1993 ACM 0-89791-591-7/93/0005/0121 . ..$l .50",1993-06-01,https://www.semanticscholar.org/paper/86ce7307b8a84ae45f30a61821d40b01da2c0308,Symposium on the Theory of Computing
1034,Robot-inspired biology: The compound-wave control template,"Biologically inspired robots perform many interesting and useful behaviors, but to effectively emulate their biological counterparts, robots often need to possess many degrees of freedom, complicating their mechanical design and making it difficult to apply standard control and motion planning strategies. To address this complexity, the robotics community has derived low-dimensional parameter-based controllers that naturally coordinate many degrees of freedom such as the serpenoid curves used to control snake robots. Controllers utilizing this parameterization for snake robots have been able to induce behaviors similar to that of the robots' biological counterparts. A similar concept, called a control template, is used in the study of animal movements. However, much of the prior work on control templates has been limited to in-plane motion. In this work, we extend the usage of control templates to three dimensions to both better model and understand biology, as well as to help us gain better intuition into how we can use pre-existing control paradigms to create new behaviors for biologically inspired robots.",2015-05-26,https://www.semanticscholar.org/paper/1e1a233341ae887598f352ca328e9db2531dafa7,IEEE International Conference on Robotics and Automation
2183,"APPA, a potential new therapy for osteoarthritis, inhibits neutrophil pro-inflammatory functions without impairing host defence",,2019-04-01,https://www.semanticscholar.org/paper/e685173ebfc997c9ea7586c7b1f21ed74f3dee45,Osteoarthritis and Cartilage
311,Incentive-Compatible Interdomain Routing with Linear Utilities,"We revisit the problem of incentive-compatible interdomain routing, examining the quite realistic special case in which the utilities of autonomous systems (ASes) are linear functions of the traffic in the incident links and the traffic leaving each AS. We show that incentive-compatibility toward maximizing total welfare is achievable efficiently, and in the uncapacitated case, by an algorithm that can be easily implemented by the border gateway protocol (BGP), the standard protocol for interdomain routing.",2007-12-12,https://www.semanticscholar.org/paper/a97409a09d1907235bb8e46524d6ad6fe543978e,Internet Mathematics
492,Competitive distributed decision-making,,1992-09-07,https://www.semanticscholar.org/paper/9929c0ed18de4670216ab3743341f36e7fe85b01,Algorithmica
241,Learning and verifying quantified boolean queries by example,"To help a user specify and verify quantified queries --- a class of database queries known to be very challenging for all but the most expert users --- one can question the user on whether certain data objects are answers or non-answers to her intended query. In this paper, we analyze the number of questions needed to learn or verify qhorn queries, a special class of Boolean quantified queries whose underlying form is conjunctions of quantified Horn expressions. We provide optimal polynomial-question and polynomial-time learning and verification algorithms for two subclasses of the class qhorn with upper constant limits on a query's causal density.",2013-04-15,https://www.semanticscholar.org/paper/fc0f0316cedd8a456a11e2537455b401afa1824f,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems
3051,Cells: a virtual mobile smartphone architecture,"Smartphones are increasingly ubiquitous, and many users carry multiple phones to accommodate work, personal, and geographic mobility needs. We present Cells, a virtualization architecture for enabling multiple virtual smartphones to run simultaneously on the same physical cellphone in an isolated, secure manner. Cells introduces a usage model of having one foreground virtual phone and multiple background virtual phones. This model enables a new device namespace mechanism and novel device proxies that integrate with lightweight operating system virtualization to multiplex phone hardware across multiple virtual phones while providing native hardware device performance. Cells virtual phone features include fully accelerated 3D graphics, complete power, management features, and full telephony functionality with separately assignable telephone numbers and caller ID support. We have implemented a prototype of Cells that supports multiple Android virtual phones on the same phone. Our performance results demonstrate that Cells imposes only modest runtime and memory overhead, works seamlessly across multiple hardware devices including Google Nexus 1 and Nexus S phones, and transparently runs Android applications at native speed without any modifications.",2011-10-23,https://www.semanticscholar.org/paper/60278c1f10bf1138f7f387e76e160cf88e599d2f,Symposium on Operating Systems Principles
1671,A General Method for Robust Bayesian Modeling,"Robust Bayesian models are appealing alternatives to standard models, providing protection from data that contains outliers or other departures from the model assumptions. Historically, robust models were mostly developed on a case-by-case basis; examples include robust linear regression, robust mixture models, and bursty topic models. In this paper we develop a general approach to robust Bayesian modeling. We show how to turn an existing Bayesian model into a robust model, and then develop a generic strategy for computing with it. We use our method to study robust variants of several models, including linear regression, Poisson regression, logistic regression, and probabilistic topic models. We discuss the connections between our methods and existing approaches, especially empirical Bayes and James-Stein estimation.",2015-10-17,https://www.semanticscholar.org/paper/19de6b41be976f7035fa8927932a0a89bf359a42,Bayesian Analysis
1293,Search for W' bosons decaying to an electron and a neutrino with the D0 detector.,"This Letter describes the search for a new heavy charged gauge boson W' decaying into an electron and a neutrino. The data were collected with the D0 detector at the Fermilab Tevatron pp[over] Collider at sqrt[s]=1.96 TeV, and correspond to an integrated luminosity of about 1 fb(-1). Lacking any significant excess in the data in comparison with known processes, an upper limit is set on sigma_(W') x B(W')-->e nu), and a W' boson with mass below 1.00 TeV can be excluded at the 95% C.L., assuming standard-model-like couplings to fermions. This result significantly improves upon previous limits and is the most stringent to date.",2007-10-16,https://www.semanticscholar.org/paper/d4101c2c9c065dadd401b7c76bfd3243036feefc,Physical Review Letters
2182,The clinical significance of antinuclear antibodies and specific autoantibodies in juvenile and adult systemic lupus erythematosus patients.,"BACKGROUND
Juvenile systemic lupus erythematosus (JSLE) and adult SLE (ASLE) patients present with different clinical manifestations, but it is unknown if there are differences in their antinuclear autoantibody (ANA) profiles or if staining patterns are associated with specific autoantibodies and clinical manifestations.


OBJECTIVE
To determine whether distinct types and numbers of ANA-staining patterns are associated with specific autoantibodies and clinical manifestations in JSLE and ASLE patients.


METHODS
A retrospective study was performed in Thai children (n = 146) and adults (n = 180) diagnosed with SLE using the Systemic Lupus International Collaborating Clinics classification criteria.


RESULTS
JSLE patients with a homogeneous pattern of staining and anti-dsDNA or anti-nucleosome antibodies in serum, developed renal involvement, leukopenia and acute/subacute cutaneous LE. Coarse speckled pattern with anti-RNP or anti-Sm showed thrombocytopenia and renal involvement in JSLE patients, but leukopenia in both groups. JSLE patients with fine-coarse speckled pattern and anti-RNP, anti-Sm, anti-Ro-52 or anti-SSA developed leukopenia, thrombocytopenia and renal involvement, whilst hemolytic anemia and serositis were commonly found in those with anti-Ro-52. Median SLEDAI score was higher in JSLE than ASLE patients.


CONCLUSION
Detailed ANA-staining patterns with specific autoantibodies show particular clinical manifestations and hence prompt further clinical investigations in both JSLE and ASLE patients. Therefore, this study demonstrates that distinct patterns of ANA staining and specific autoantibodies are clinically important in both children and adults with SLE.",2019-04-23,https://www.semanticscholar.org/paper/99efac3a8d9b76843d9dc128aab60ab8eef16065,Asian Pacific Journal of Allergy and Immunology
2245,Microbial mannan inhibits bacterial killing by macrophages: a possible pathogenic mechanism for Crohn's disease.,"BACKGROUND & AIMS
Crohn's disease (CD) is mimicked by inherited phagocyte disorders and is associated with circulating antibodies against yeast mannan (anti-Saccharomyces cerevisiae antibody; ASCA). We speculated that mannans might impair phagocyte function.


METHODS
S cerevisiae mannan was assessed for its effects on human peripheral blood neutrophils, adherent monocytes, and monocyte-derived macrophages (MDM).


RESULTS
Mannan caused dose-related increased survival of CD Escherichia coli HM605 within adherent monocytes from 24% +/- 10.5% (control) to 114% +/- 22.7% with mannan 1 mg/mL at 2 hours (mean +/- SEM, n = 9; P = .0002). Electron microscopy showed E coli HM605 surviving and probably replicating within macrophage vesicles. Mannan (1 mg/mL) inhibited the respiratory burst in neutrophils and monocytes (both P = .002) and bacterial killing within MDM (P < .001). E coli survival was increased within macrophages from TLR4(-/-) (126% +/- 3.5% survival at 2 hours) and MyD88(-/-) (134.8% +/- 6.5%) mice compared with wild-type mice (both P < .0001). Mannan had no additional effect, showing that TLR4 and MyD88 are involved in bacterial killing by macrophages and its inhibition by mannan. Putative CD-associated micro-organisms were screened for the ASCA mannan epitope by Galanthus nivalis lectin (GNA) blotting. ASCA epitope was expressed by Candida albicans and Mycobacterium paratuberculosis but not by Mycobacterium tuberculosis or E coli. Supernatants from M paratuberculosis culture inhibited killing of E coli HM605 by adherent human monocytes and murine macrophages. The inhibitory activity was removed by GNA-affinity chromatography.


CONCLUSIONS
Suppression of mucosal phagocyte function by microbial mannans, possibly of Mycobacterial origin, may contribute to CD pathogenesis.",2007-11-01,https://www.semanticscholar.org/paper/142507867e5018cf96a4418b52d8a4c163e95d67,Gastroenterology
1803,Bayesian Spectral Matching: Turning Young MC into MC Hammer via MCMC Sampling,"In this paper, we introduce an audio mosaicing technique based on performing posterior inference on a probabilistic generative model. Whereas previous approaches to concatenative synthesis and audio mosaicing have mostly tried to match higher-level descriptors of audio or individual STFT frames, we try to directly match the magnitude spectrogram of a target sound by combining and overlapping a set of short samples at different times and amplitudes. Our use of the graphical modeling formalism allows us to use a standard Markov Chain Monte Carlo (MCMC) posterior inference algorithm to find a set of time shifts and amplitudes for each sample that results in a layered composite sound whose spectrogram approximately matches the target spectrogram.",,https://www.semanticscholar.org/paper/7b1d9dd5be58acb2d25d27a0dfa40c0088f3df77,International Conference on Mathematics and Computing
2854,Role of galectin-3 in mast cell functions: galectin-3-deficient mast cells exhibit impaired mediator release and defective JNK expression.,"Galectin-3 is a member of the beta-galactoside-binding animal lectin family expressed in various cell types, including mast cells. To determine the role of galectin-3 in the function of mast cells, we studied bone marrow-derived mast cells (BMMC) from wild-type (gal3(+/+)) and galectin-3-deficient (gal3(-/-)) mice. Cells from the two genotypes showed comparable expression of IgE receptor and c-Kit. However, upon activation by FcepsilonRI cross-linkage, gal3(-/-) BMMC secreted a significantly lower amount of histamine as well as the cytokine IL-4, compared with gal3(+/+) BMMC. In addition, we found significantly reduced passive cutaneous anaphylaxis reactions in gal3(-/-) mice compared with gal3(+/+) mice. These results indicate that there is a defect in the response of mast cells in gal3(-/-) mice. Unexpectedly, we found that gal3(-/-) BMMC contained a dramatically lower basal level of JNK1 protein compared with gal3(+/+) BMMC, which is probably responsible for the lower IL-4 production. The decreased JNK1 level in gal3(-/-) BMMC is accompanied by a lower JNK1 mRNA level, suggesting that galectin-3 regulates the transcription of the JNK gene or processing of its RNA. All together, these results point to an important role of galectin-3 in mast cell biology.",2006-10-15,https://www.semanticscholar.org/paper/06bf8f3000d07d272c7612104216f9fa1b173919,Journal of Immunology
1635,Science and data science,"Data science has attracted a lot of attention, promising to turn vast amounts of data into useful predictions and insights. In this article, we ask why scientists should care about data science. To answer, we discuss data science from three perspectives: statistical, computational, and human. Although each of the three is a critical component of data science, we argue that the effective combination of all three components is the essence of what data science is about.",2017-08-07,https://www.semanticscholar.org/paper/c0225f99c9b1619c3be74b63241faffe02d275d7,Proceedings of the National Academy of Sciences of the United States of America
1756,Variational Bayesian Inference with Stochastic Search,"Mean-field variational inference is a method for approximate Bayesian posterior inference. It approximates a full posterior distribution with a factorized set of distributions by maximizing a lower bound on the marginal likelihood. This requires the ability to integrate a sum of terms in the log joint likelihood using this factorized distribution. Often not all integrals are in closed form, which is typically handled by using a lower bound. We present an alternative algorithm based on stochastic optimization that allows for direct optimization of the variational lower bound. This method uses control variates to reduce the variance of the stochastic search gradient, in which existing lower bounds can play an important role. We demonstrate the approach on two non-conjugate models: logistic regression and an approximation to the HDP.",2012-06-26,https://www.semanticscholar.org/paper/9ceb1dea15ac3df3d610fd0b3cc52b9a4e9141a3,International Conference on Machine Learning
2120,A cost-based heuristic for statistically determining sampling frequency in a wafer fab,"In wafer fabrication, because of the long cycle time, the high yield uncertainty and the high manufacturing cost, earlier process monitoring and control are critical. Thus, a number of inspection and measurement stations are set in the fabrication process to assure that the wafer quality meets the specific requirements. Researchers have applied the acceptance sampling plan to determine whether a lot is accepted or not. Due to the limited capacities and costs for in-line wafer inspections, only certain wafers are inspected among a specific number of lots. Thus, it is important to determine the sampling strategy that minimizes the total expected costs, including the inspection costs, false-alarm costs, out-of-control costs, in-control costs, and the costs from the false-passed wafers. In this study, we developed a cost-based heuristic for statistically determining the sampling frequency in wafer fab based on the economics, control chart design, Bayesian decision analysis, and the acceptance sampling strategy. We aimed to determine the optimal sampling frequency that trades off the various risks (i.e. the aggregation of cost and probability).",2000-06-14,https://www.semanticscholar.org/paper/42353a948a9a25b01e0f1381f095f1184933a680,2000 Semiconductor Manufacturing Technology Workshop (Cat. No.00EX406)
3373,Migration and species diversity in the tropics.,"If the young of a dominant species are subjected to disproportionately heavy predation, this, together with a limitation on food, can promote a high species diversity. This is seen among tropical birds, which are simultaneously exposed to both conditions to a far greater degree than are Temperate Zone species. Migration to the Temperate Zones during the spring provides a release from these restraints, while also precluding breeding in the Tropics.",1974-02-01,https://www.semanticscholar.org/paper/cc71b265e4a3aec2a97b5bfb6eef31be3c08a667,Proceedings of the National Academy of Sciences of the United States of America
2835,Targeted disruption of the galectin-3 gene results in decreased susceptibility to multiple low dose streptozotocin-induced diabetes in mice.,,,https://www.semanticscholar.org/paper/d5217be83393dd66542e94042354a6cc7bdb476d,Clinical Immunology
2674,Automated Visual Presentation: From Heterogeneous Information to Coherent Visual Discourse,,1998-11-01,https://www.semanticscholar.org/paper/5d65c060e7f23f996ccabab6aae3f8a90c096015,Journal of Intelligence and Information Systems
1729,A Nested HDP for Hierarchical Topic Models,"We develop a nested hierarchical Dirichlet process (nHDP) for hierarchical topic modeling. The nHDP is a generalization of the nested Chinese restaurant process (nCRP) that allows each word to follow its own path to a topic node according to a document-specific distribution on a shared tree. This alleviates the rigid, single-path formulation of the nCRP, allowing a document to more easily express thematic borrowings as a random effect. We demonstrate our algorithm on 1.8 million documents from The New York Times.",2013-01-16,https://www.semanticscholar.org/paper/204711170720d254b19b4141ac994beeaf87f5c6,International Conference on Learning Representations
1939,A dynamic task assignment approach based on individual worklists for minimizing the cycle time of business processes,,2016-09-01,https://www.semanticscholar.org/paper/bfc3510149ceaea912167ad9d7916625b5443edd,Computers & industrial engineering
713,Power Grid State Estimation Following a Joint Cyber and Physical Attack,"This paper focuses on joint cyber and physical attacks on power grids and presents methods to retrieve the grid state information following such an attack. We consider a model where an adversary attacks a zone by physically disconnecting some of its power lines and blocking the information flow from the zone to the grid's control center. We use tools from linear algebra and graph theory and leverage the properties of the linearized power flow model to develop methods for information recovery. Using information observed outside the attacked zone, these methods recover information about the disconnected lines and the phase angles at the buses. We identify sufficient conditions on the zone structure and constraints on the attack characteristics  such that these methods can recover the information. We also show that it is NP-hard to find an approximate solution to the problem of partitioning the power grid into the minimum number of attack-resilient zones. However, since power grids can often be represented by planar graphs, we develop a constant approximation partitioning algorithm for these graphs and numerically demonstrate its performance on real power grids.",2018-03-01,https://www.semanticscholar.org/paper/f4d15393209d19b03e30c31f657a35482bc65653,IEEE Transactions on Control of Network Systems
3435,Solving maximum flow problems on real-world bipartite graphs,"In this article, we present an experimental study of several maximum-flow algorithms in the context of unbalanced bipartite networks. Our experiments are motivated by a real-world problem of managing reservation-based inventory in Google content ad systems. We are interested in observing the performance of several push-relabel algorithms on our real-world datasets and also on some generated ones. Previous work suggested an important improvement for push-relabel algorithms on unbalanced bipartite networks: the two-edge push rule. We show how the two-edge push rule improves the running time. While no single algorithm dominates the results, we show there is one that has very robust performance in practice.",2009-01-03,https://www.semanticscholar.org/paper/f888d5f268ae763a47a0fada3ad4b8d8d1f8b1ef,JEAL
952,"Offset of openings in optic nerve head canal at level of Bruch’s membrane, anterior sclera, and lamina cribrosa",,2021-04-12,https://www.semanticscholar.org/paper/4ff9886ebcb0a816da68b077ba70b279382ced7c,Scientific Reports
322,On The Approximability Of The Traveling Salesman Problem,,2006-02-01,https://www.semanticscholar.org/paper/c62bd5870663dc332993690e2cf638318539f396,Comb.
505,On selecting a satisfying truth assignment,"The complexity of certain natural generalizations of satisfiability, in which one of the possibly exponentially many satisfying truth assignments must be selected, is studied. Two natural selection criteria, default preference and minimality (circumscription), are considered. The thrust of the complexity results seems to be that hard problems become harder, while easy problems remain easy. This consideration yields as a byproduct a new and very natural polynomial-time randomized algorithm for 2SAT.<<ETX>>",1991-09-01,https://www.semanticscholar.org/paper/d32b2afda59dba352be0811c5eaef03566408fc2,[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
300,Internet and Network Economics,,,https://www.semanticscholar.org/paper/f61d66e52c4b9ed705110127e36aee6c735005bf,Lecture Notes in Computer Science
677,Time-Accuracy Tradeoffs in Kernel Prediction: Controlling Prediction Quality,"Kernel regression or classification (also referred to as weighted e-NN methods in Machine Learning) are appealing for their simplicity and therefore ubiquitous in data analysis. However, practical implementations of kernel regression or classification consist of quantizing or sub-sampling data for improving time efficiency, often at the cost of prediction quality. While such tradeoffs are necessary in practice, their statistical implications are generally not well understood, hence practical implementations come with few performance guarantees. In particular, it is unclear whether it is possible to maintain the statistical accuracy of kernel prediction--crucial in some applications--while improving prediction time. 
 
The present work provides guiding principles for combining kernel prediction with data-quantization so as to guarantee good tradeoffs between prediction time and accuracy, and in particular so as to approximately maintain the good accuracy of vanilla kernel prediction. Furthermore, our tradeoff guarantees are worked out explicitly in terms of a tuning parameter which acts as a knob that favors either time or accuracy depending on practical needs. On one end of the knob, prediction time is of the same order as that of single-nearestneighbor prediction (which is statistically inconsistent) while maintaining consistency; on the other end of the knob, the prediction risk is nearly minimax-optimal (in terms of the original data size) while still reducing time complexity. The analysis thus reveals the interaction between the data-quantization approach and the kernel prediction method, and most importantly gives explicit control of the tradeoff to the practitioner rather than fixing the tradeoff in advance or leaving it opaque. 
 
The theoretical results are validated on data from a range of real-world application domains; in particular we demonstrate that the theoretical knob performs as expected.",,https://www.semanticscholar.org/paper/f68f682f9bf329dbba8c228989162ac7b4855e2c,Journal of machine learning research
580,Two remarks on the power of counting,,1983-01-05,https://www.semanticscholar.org/paper/56c02b7ae0dbd215b0fc1cfd218ad544dc02ef7c,Theoretical Computer Science
19,Identifying content for planned events across social media sites,"User-contributed Web data contains rich and diverse information about a variety of events in the physical world, such as shows, festivals, conferences and more. This information ranges from known event features (e.g., title, time, location) posted on event aggregation platforms (e.g., Last.fm events, EventBrite, Facebook events) to discussions and reactions related to events shared on different social media sites (e.g., Twitter, YouTube, Flickr). In this paper, we focus on the challenge of automatically identifying user-contributed content for events that are planned and, therefore, known in advance, across different social media sites. We mine event aggregation platforms to extract event features, which are often noisy or missing. We use these features to develop query formulation strategies for retrieving content associated with an event on different social media sites. Further, we explore ways in which event content identified on one social media site can be used to retrieve additional relevant event content on other social media sites. We apply our strategies to a large set of user-contributed events, and analyze their effectiveness in retrieving relevant event content from Twitter, YouTube, and Flickr.",2012-02-08,https://www.semanticscholar.org/paper/4d76072a133a39144b5fe4b16611b603b641d487,Web Search and Data Mining
2285,BCL‐2 family expression in human neutrophils during delayed and accelerated apoptosis,"The human neutrophil spontaneously undergoes apoptosis, but this type of cell death can be delayed or accelerated by a wide variety of agents. There are wide discrepancies in the literature regarding the expression of the Bcl‐2 family of proteins in human neutrophils. Here, we show that A1, Mcl‐1, Bcl‐XL, and Bad are major transcripts in human neutrophils and that levels of these transcripts are cytokine regulated. However, no Bcl‐XL protein was detected in Western blots. Protein levels for the proapoptotic proteins Bad, Bax, Bak, and Bik remained constant during culture, despite changes in the levels of mRNA for these gene products. These proapoptotic proteins were extremely stable, having very long half‐lives. In contrast, A1 and Mcl‐1 transcripts were extremely unstable (with ∼3‐h half‐lives), and Mcl‐1 protein was also subject to rapid turnover. These results indicate that neutrophil survival is regulated by the inducible expression of the short‐lived Mcl‐1 and possibly the A1 gene products. In the absence of their continued expression, these prosurvival gene products are rapidly turned over, and then the activity of the stable death proteins predominates and promotes apoptosis.",2001-11-01,https://www.semanticscholar.org/paper/90865be31e0bd92e2d2866a0f3004d7be598033a,Journal of Leukocyte Biology
1584,Using Embeddings to Correct for Unobserved Confounding in Networks,"We consider causal inference in the presence of unobserved confounding. We study the case where a proxy is available for the unobserved confounding in the form of a network connecting the units. For example, the link structure of a social network carries information about its members. We show how to effectively use the proxy to do causal inference. The main idea is to reduce the causal estimation problem to a semi-supervised prediction of both the treatments and outcomes. Networks admit high-quality embedding models that can be used for this semi-supervised prediction. We show that the method yields valid inferences under suitable (weak) conditions on the quality of the predictive model. We validate the method with experiments on a semi-synthetic social network dataset. Code is available at this http URL.",2019-02-11,https://www.semanticscholar.org/paper/f432e55bfb342435da59e5c3f896778f436117f2,Neural Information Processing Systems
2966,Differential methylation of the TRPA1 promoter in pain sensitivity,,2014-02-04,https://www.semanticscholar.org/paper/a9a9ad920b21d3399a7784d4b7283a313aff5eea,Nature Communications
3487,A 2 2/3 Superstring Approximation Algorithm,,1998-11-09,https://www.semanticscholar.org/paper/02a4e481da458ab6d8e592dd2dd537f1e56a13b6,Discrete Applied Mathematics
573,A Communication-Time Tradeoff,"We show a nontrivial tradeoff between the communication c and time t required to compute a collection of values whose dependencies form a grid, i.e., value (i,j) depends on the values (i-1,j) and (i,j-1). No matter how we share the responsibility for computing the nodes of the n x n grid among processors, the law ct = /spl Omega/(n/sup 3/) must hold. Further, there must be a single path through the grid along which there are d communication steps, where dt = /spl Omega/(n/sup 2/). Depending on the machine organization, either law may be the more significant.",1984-10-24,https://www.semanticscholar.org/paper/388bcb3650a15ed973a6ad5c38d6fcda593d70bc,SIAM journal on computing (Print)
2769,An experimental system for creating and presenting interactive graphical documents,"An experimental system is described for the design, development, and presentation of computer-based documents that combine pictures and text on a high-resolution color raster display. Such documents can be used, for example, for maintenance and repair tasks, videotex databases, or computer-aided instruction. Documents are directed graphs whose nodes we refer to as pages, in analogy to the pages of a paper book. A page includes a set of simultaneously displayed pictures, actions (procedures and processes), and indexing information. Pages may be nested arbitrarily deeply in chapters that serve much the same organizing function as those of conventional books. The system is comprised of separate programs for laying out and drawing pictures, for graphically specifying the contents of pages, chapters, and their interconnections, and for displaying the document for user interaction. Examples are given from a prototype maintenance and repair manual in which emphasis was placed on designing actions that allow simple real-time animation and assist in finding one's way around the document.",,https://www.semanticscholar.org/paper/dfe3aeabf4dcf3e1e1223bb1455eb46e6a9f2f87,TOGS
3047,Capture: a desktop display-centric text recorder,"As more and more information is designed for human visual consumption through computer displays, the need to capture and process display-centric content is becoming increasingly important, especially for visually impaired users. We present Capture, a novel display-centric text recorder that facilitates real-time access to onscreen text and its structure and contextual information, including data associated with both foreground and background windows. Capture provides an intelligent caching architecture that integrates with the standard accessibility framework available on modern operating systems to continuously track onscreen text and metadata. This enables fast, semantic information recording without any modifications to applications, window systems, or operating system kernels. The recorded data is useful for a variety of problem domains, including assistive technologies, desktop search, auditing, and predictive graphical user interfaces. We have implemented a Capture prototype on Linux with the GNOME Accessibility Toolkit. Our results on real desktop applications demonstrate that Capture provides low runtime overhead and much more complete recording of onscreen text than modern desktop screen readers used for visually impaired users.",2012-10-22,https://www.semanticscholar.org/paper/fb98da66fab8a3f98294ef69875f6387704552c2,International ACM SIGACCESS Conference on Computers and Accessibility
2749,Interactive Multimedia Explanation for Equipment Maintenance and Repair,"We are developing COMET, an interactive system that generates multimedia explanations of how to operate, maintain, and repair equipment. Our research stresses the dynamic generation of the content and form of all material presented, addressing issues in the generation of text and graphics, and in coordinating text and graphics in an integrated presentation.",1990-06-24,https://www.semanticscholar.org/paper/9a1a9428af70ff258ca2ca42548d9c51f9a30a99,Human Language Technology - The Baltic Perspectiv
2858,Galectins in apoptosis.,,,https://www.semanticscholar.org/paper/af950daaaf008b4833b9a3bb15aa4bdc32d804a9,Methods in Enzymology
2933,A New Distribution on the Simplex with Auto-Encoding Applications,"We construct a new distribution for the simplex using the Kumaraswamy distribution and an ordered stick-breaking process. We explore and develop the theoretical properties of this new distribution and prove that it exhibits symmetry under the same conditions as the well-known Dirichlet. Like the Dirichlet, the new distribution is adept at capturing sparsity but, unlike the Dirichlet, has an exact and closed form reparameterization--making it well suited for deep variational Bayesian modeling. We demonstrate the distribution's utility in a variety of semi-supervised auto-encoding tasks. In all cases, the resulting models achieve competitive performance commensurate with their simplicity, use of explicit probability models, and abstinence from adversarial training.",2019-05-28,https://www.semanticscholar.org/paper/5b79f76f26cc5bdfed0c5ac5ac3f34c6426ac8d3,Neural Information Processing Systems
47,Towards a query optimizer for text-centric tasks,"Text is ubiquitous and, not surprisingly, many important applications rely on textual data for a variety of tasks. As a notable example, information extraction applications derive structured relations from unstructured text; as another example, focused crawlers explore the Web to locate pages about specific topics. Execution plans for text-centric tasks follow two general paradigms for processing a text database: either we can scan, or “crawl,” the text database or, alternatively, we can exploit search engine indexes and retrieve the documents of interest via carefully crafted queries constructed in task-specific ways. The choice between crawl- and query-based execution plans can have a substantial impact on both execution time and output “completeness” (e.g., in terms of recall). Nevertheless, this choice is typically ad hoc and based on heuristics or plain intuition. In this article, we present fundamental building blocks to make the choice of execution plans for text-centric tasks in an informed, cost-based way. Towards this goal, we show how to analyze query- and crawl-based plans in terms of both execution time and output completeness. We adapt results from random-graph theory and statistics to develop a rigorous cost model for the execution plans. Our cost model reflects the fact that the performance of the plans depends on fundamental task-specific properties of the underlying text databases. We identify these properties and present efficient techniques for estimating the associated parameters of the cost model. We also present two optimization approaches for text-centric tasks that rely on the cost-model parameters and select efficient execution plans. Overall, our optimization approaches help build efficient execution plans for a task, resulting in significant efficiency and output completeness benefits. We complement our results with a large-scale experimental evaluation for three important text-centric tasks and over multiple real-life data sets.",2007-11-01,https://www.semanticscholar.org/paper/a968bb1dea8969d062e4f68059ce94ad7ce734f9,TODS
2222,"Serine 162, an Essential Residue for the Mitochondrial Localization, Stability and Anti-Apoptotic Function of Mcl-1","Mcl-1 is an anti-apoptotic member of the Bcl-2 family that plays a key role in normal development, but also in pathologies such as cancer. It has some unusual properties compared to other anti-apoptotic members of the Bcl-2 family, and its expression and function are dynamically regulated by a variety of post-transcriptional and post-translational processes. Of note, Mcl-1 protein has a very short half life, and its stability and function may be regulated by reversible phosphorylation. There is also evidence to suggest that it may be localized to different subcellular compartments. The aim of this work was to determine whether residues within the PEST region of Mcl-1 that may undergo reversible phosphorylation, also regulate its subcellular distribution. We show that EGFP:Mcl-1 localizes mainly to the mitochondria of HeLa cells, with some additional cytoplasmic and nuclear localization. The mutations, S64A, S64E, S121A, S159A, T163A and T163E did not significantly affect the localization of Mcl-1. However, mutation of Ser162 to the phospho-null residue, Alanine resulted in an essentially nuclear localization, with some cytoplasmic but no mitochondrial localization. This mutant Mcl-1 protein, S162A, showed significantly decreased stability and it decreased the ability to protect against Bak-induced apoptosis. These data identify a new molecular determinant of Mcl-1 function, localization and stability that may be important for understanding the role of this protein in disease.",2012-09-14,https://www.semanticscholar.org/paper/51061405186d956e9b38bdc70be782ac53ab4b41,PLoS ONE
3065,Operating system virtualization: practice and experience,"Operating system (OS) virtualization can provide a number of important benefits, including transparent migration of applications, server consolidation, online OS maintenance, and enhanced system security. However, the construction of such a system presents a myriad of challenges, even for the most cautious developer, that if overlooked may result in a weak, incomplete virtualization. We present a detailed discussion of key implementation issues in providing OS virtualization in a commodity OS, including system call interposition, virtualization state management, and race conditions. We discuss our experiences in implementing such functionality across two major versions of Linux entirely in a loadable kernel module without any kernel modification. We present experimental results on both uniprocessor and multiprocessor systems that demonstrate the ability of our approach to provide fine-grain virtualization with very low overhead.",2010-05-24,https://www.semanticscholar.org/paper/ef0baa52cd817c9f5645c196d51031e67279b90c,Annual Haifa Experimental Systems Conference
3115,THINC: a virtual display architecture for thin-client computing,"Rapid improvements in network bandwidth, cost, and ubiquity combined with the security hazards and high total cost of ownership of personal computers have created a growing market for thin-client computing. We introduce THINC, a virtual display architecture for high-performance thin-client computing in both LAN and WAN environments. THINC virtualizes the display at the device driver interface to transparently intercept application display commands and translate them into a few simple low-level commands that can be easily supported by widely used client hardware. THINC's translation mechanism efficiently leverages display semantic information through novel optimizations such as offscreen drawing awareness, native video support, and server-side screen scaling. This is integrated with an update delivery architecture that uses shortest command first scheduling and non-blocking operation. THINC leverages existing display system functionality and works seamlessly with unmodified applications, window systems, and operating systems.We have implemented THINC in an X/Linux environment and compared its performance against widely used commercial approaches, including Citrix MetaFrame, Microsoft RDP, GoToMyPC, X, NX, VNC, and Sun Ray. Our experimental results on web and audio/video applications demonstrate that THINC can provide up to 4.8 times faster web browsing performance and two orders of magnitude better audio/video performance. THINC is the only thin client capable of transparently playing full-screen video and audio at full frame rate in both LAN and WAN environments. Our results also show for the first time that thin clients can even provide good performance using remote clients located in other countries around the world.",2005-10-23,https://www.semanticscholar.org/paper/fb7a756e451eb24ef74fba423bffb9b606fc44f8,Symposium on Operating Systems Principles
3053,Improving Virtual Appliance Management through Virtual Layered File Systems,"Managing many computers is difficult. Recent virtualization trends exacerbate this problem by making it easy to create and deploy multiple virtual appliances per physical machine, each of which can be configured with different applications and utilities. This results in a huge scaling problem for large organizations as management overhead grows linearly with the number of appliances. 
 
To address this problem, we introduce Strata, a system that combines unioning file system and package management semantics to enable more efficient creation, provisioning and management of virtual appliances. Unlike traditional systems that depend on monolithic file systems, Strata uses a collection of individual sotware layers that are composed together into the Virtual Layered File System (VLFS) to provide the traditional file system view. Individual layers are maintained in a central repository and shared across all file systems that use them. Layer changes and upgrades only need to be done once in the repository and are then automatically propagated to all virtual appliances, resulting in management overhead independent of the number of appliances. Our Strata Linux prototype requires only a single loadable kernel module providing the VLFS support and doesn't require any application or source code level kernel modifications. Using this prototype, we demonstrate how Strata enables fast system provisioning, simplifies system maintenance and upgrades, speeds system recovery from security exploits, and incurs only modest performance overhead.",2011-12-04,https://www.semanticscholar.org/paper/78d339b7801f593d5739cf55dde3337bbe2de7ab,LiSA
76,Distributed Search over the Hidden Web: Hierarchical Database Sampling and Selection,,2002-08-20,https://www.semanticscholar.org/paper/3240f47572a6efef76c6bb6656d4cc0bbb9eb03a,Very Large Data Bases Conference
3542,Open pattern matching for C++,"Pattern matching is an abstraction mechanism that can greatly simplify source code. We present functional-style pattern matching for C++ implemented as a library, called Mach7. All the patterns are user-definable, can be stored in variables, passed among functions, and allow the use of open class hierarchies. As an example, we implement common patterns used in functional languages.",2013-10-26,https://www.semanticscholar.org/paper/6d5d774581a74ded889569249b0fa62f07ad2e7b,"ACM SIGPLAN International Conference on Systems, Programming, Languages and Applications: Software for Humanity"
3025,Optimizing the Design and Implementation of the Linux ARM Hypervisor,"Modern hypervisor designs for both ARM and ×86 virtualization rely on running an operating system kernel, the hypervisor OS kernel, to support hypervisor functionality. While ×86 hypervisors effectively leverage architectural support to run the kernel, existing ARM hypervisors map poorly to the virtualization features of the ARM architecture, resulting in worse performance. We identify the key reason for this problem is the need to multiplex kernel mode state between the hypervisor and virtual machines, which each run their own kernel. To address this problem, we take a fundamentally different approach to hypervisor design that runs the hypervisor together with its OS kernel in a separate CPU mode from kernel mode. Using this approach, we redesign KVM/ARM to leverage a separate ARM CPU mode for running both the hypervisor and its OS kernel. We show what changes are required in Linux to implement this on current ARM hardware as well as how newer ARM architectural support can be used to support this approach without any changes to Linux other than to KVM/ARM itself. We show that our redesign and optimizations can result in an order ofmagnitude performance improvement for KVM/ARM, and can provide faster performance than x86 on key hypervisor operations. As a result, many aspects of our design have been successfully merged into mainline Linux.",2017-07-12,https://www.semanticscholar.org/paper/03a97821f3f77490f1c775501762985f10cd7be8,USENIX Annual Technical Conference
2186,Anti-neutrophil cytoplasmic antibodies and their clinical significance,,2018-03-10,https://www.semanticscholar.org/paper/80f12a848fc47b4fe690ee045d67fc319e900d86,Clinical Rheumatology
3524,Parallel algorithms for the assignment and minimum-cost flow problems,,1993-11-01,https://www.semanticscholar.org/paper/f6e94da6740eee7e2de64b69ff53a32f3f444cc3,Operations Research Letters
1697,A Filtering Approach to Stochastic Variational Inference,"Stochastic variational inference (SVI) uses stochastic optimization to scale up Bayesian computation to massive data. We present an alternative perspective on SVI as approximate parallel coordinate ascent. SVI trades-off bias and variance to step close to the unknown true coordinate optimum given by batch variational Bayes (VB). We define a model to automate this process. The model infers the location of the next VB optimum from a sequence of noisy realizations. As a consequence of this construction, we update the variational parameters using Bayes rule, rather than a hand-crafted optimization schedule. When our model is a Kalman filter this procedure can recover the original SVI algorithm and SVI with adaptive steps. We may also encode additional assumptions in the model, such as heavy-tailed noise. By doing so, our algorithm outperforms the original SVI schedule and a state-of-the-art adaptive SVI algorithm in two diverse domains.",2014-12-08,https://www.semanticscholar.org/paper/35409ac9b33f2cc074257018c47fe9c373acb55f,Neural Information Processing Systems
2221,Neutrophil-derived reactive oxygen species in SSc.,"OBJECTIVE
Reactive oxygen species (ROS) are implicated in the pathogenesis of SSc. Neutrophils constitute a major source of ROS during inflammation. Here, we examined endogenous and stimulated ex vivo ROS production of SSc neutrophils compared with control neutrophils with and without prior priming with TNF-α.


METHODS
ROS generation was measured using luminol-enhanced chemiluminescence. Neutrophils isolated from SSc patients and healthy controls were unprimed or were primed with TNF-α. ROS production was stimulated in vitro with phorbol 12-myristate 13-acetate (PMA) and formyl-met-leu-phe (fMLP). To examine the effects of serum mediators on ROS generation, control neutrophils were also stimulated with SSc or control serum.


RESULTS
Neutrophil stimulation with PMA and fMLP resulted in a greater increase in ROS generation in SSc neutrophils compared with controls. However, unstimulated SSc neutrophils generated lower levels of ROS than controls. SSc neutrophils demonstrated an increased response to fMLP in the absence of in vitro TNF-α priming indicating priming of SSc neutrophils in vivo. SSc serum did not stimulate neutrophil ROS generation in vitro.


CONCLUSION
SSc neutrophils are primed for ROS generation. Neutrophils binding to activated endothelium in SSc, may induce local production of ROS, perpetuating endothelial dysfunction and mediating fibrosis.",2012-07-01,https://www.semanticscholar.org/paper/4e7c9ab6932a8adbaafe5ecead395a07b5e3d7e8,Rheumatology
2394,Changes in cytochrome levels during growth of the yeast-like fungusAureobasidium pullulans,,1981-05-01,https://www.semanticscholar.org/paper/9709bfbadb2aa7ece3b3b0340e87fcab02f82667,Current Microbiology
2944,Annotation-free quantification of RNA splicing using LeafCutter,,2017-11-09,https://www.semanticscholar.org/paper/92b8b08b3b73070e405c3b97483abdb7c2e8676c,Nature Genetics
766,Succinct Approximation of Trade-Off Curves,,2006-12-15,https://www.semanticscholar.org/paper/f652907235d5865e840d29b7c868a87c713af6cc,Workshop on Internet and Network Economics
2176,AB0115 SECUKINUMAB THERAPY DOES NOT AFFECT NEUTROPHIL HOST DEFENCE IN PSORIATIC ARTHRITIS,"Biologic therapies have revolutionised therapy in inflammatory diseases such as psoriatic arthritis (PsA), driving major improvements in outcomes. Th17 cells appear to play a key role in the pathogenesis of PsA, and IL-17 can trigger the release of chemoattractants such as CXCL8 and CCL20, leading to the further infiltration of other immune cells including neutrophils. Infiltrating activated neutrophils can themselves generate a range of chemoattractants which may amplify and sustain the inflammatory response. Therapeutic targeting of IL-17 with biologics such as secukinumab offers great benefit in PsA by blocking this inflammatory cycle: however the interaction of this agent with neutrophils, key components of host defence as well as potential mediators of this disease, is not known.This study aimed to measure key aspects of neutrophil function to determine: a) changes in the functions of circulating neutrophils in PsA patients pre-therapy, compared to age- and sex-matched healthy controls and b) if these functions changed in PsA patients 12-weeks post-secukinumab therapy.Neutrophils were isolated from venous blood of 16 PsA patients and 10 healthy controls. Key neutrophil functions were measured at baseline and 12 weeks: reactive oxygen species (ROS) production, apoptosis (+/- TNF and GM-CSF), phagocytosis, receptor expression and chemotaxis. Changes in gene expression pre- and 12-weeks post-therapy (n=5 PsA) were measured using RNAseq.PsARC response was observed in 70.6% of participants on secukinumab therapy at 12 weeks. There were no significant differences in ROS production, phagocytosis or chemotaxis in PsA patients at baseline (compared to healthy controls) or during therapy. Chemotaxis towards IL-8 in PsA patients at baseline was decreased compared to that of healthy controls, but this difference did not reach statistical significance. Surface levels of activation markers CD11b/CD18 and CD63 were increased in PsA patients at 12-weeks compared to baseline, while surface levels of CD16 decreased. RNA-seq analysis indicated down-regulation of pathways mediated by IL-17A, oncostatin M, TWEAK (TNFSF12) and CCL2 during therapy, but up-regulated expression of pathways involvingde novoprotein biosynthesis.Therapy with secukimumab in PsA did not significantly affect neutrophil host defence functions. The changes that were seen in circulating neutrophils indicate selective up- and down-regulation of functions that may reflect potential alterations in local or systemic cytokines, and/or an increase in the circulating pool of activated neutrophils that are no longer recruited into sites of inflammation because of the down-regulation of the local IL-17/CXCL8 signalling network.Andrew Cross: None declared, Jennifer Hawkes: None declared, Helen Frankland: None declared, Ayren Mediana: None declared, Helen Wright Grant/research support from: Novartis supporting this study, Nicola Goodson Grant/research support from: Novartis supporting this research, Steven Edwards Grant/research support from: Novartis supporting this work, Robert Moots Grant/research support from: Novartis supporting this work, Consultant of: a variety of companies including Novartis, Speakers bureau: a variety of companies including Novartis",2020-06-01,https://www.semanticscholar.org/paper/c1b6742f0ed8d109a246c8cf9eeaf6004a4a4a0c,Annals of the Rheumatic Diseases
2608,"Proceedings of the 17th Annual ACM Symposium on User Interface Software and Technology, Santa Fe, NM, USA, October 24-27, 2004",,,https://www.semanticscholar.org/paper/bba9f646d92e21ddbd6870a230c394a4c4a2d886,ACM Symposium on User Interface Software and Technology
1560,Towards Clarifying the Theory of the Deconfounder,"Wang and Blei (2019) studies multiple causal inference and proposes the deconfounder algorithm. The paper discusses theoretical requirements and presents empirical studies. Several refinements have been suggested around the theory of the deconfounder. Among these, Imai and Jiang clarified the assumption of ""no unobserved single-cause confounders."" Using their assumption, this paper clarifies the theory. Furthermore, Ogburn et al. (2020) proposes counterexamples to the theory. But the proposed counterexamples do not satisfy the required assumptions.",2020-03-10,https://www.semanticscholar.org/paper/85a988216a1c6590fc762050eb24ff69cf732a6e,arXiv.org
1031,Locomotive analysis of a single-input three-link snake robot,"When commanding gaits for snake robots and other articulated systems, direct control of all possible joint inputs may not always be necessary or optimal to achieve a locomotive goal. Here we consider a three-link nonholonomic snake robot-an already underactuated system with locomotive capabilities in SE(2)-and reduce its input space to a single actuated joint, replacing the other joint's motor with a passive mass-spring-damper system. We show that the modified system can operate dynamically in addition to kinematically, and that it is possible to find gaits that produces locomotion similar to a fully actuated system. In particular, we describe the emergence of a new type of gait that incorporates the system's singular configurations to produce high locomotive efficiency without incurring unbounded constraint forces.",2016-12-01,https://www.semanticscholar.org/paper/68d0fc8c081630b4473d1b7a12d890ff64761dee,IEEE Conference on Decision and Control
771,Recursive Markov Decision Processes and Recursive Stochastic Games,"We introduce Recursive Markov Decision Processes (RMDPs) and Recursive Simple Stochastic Games (RSSGs), which are classes of (finitely presented) countable-state MDPs and zero-sum turn-based (perfect information) stochastic games. They extend standard finite-state MDPs and stochastic games with a recursion feature. We study the decidability and computational complexity of these games under termination objectives for the two players: one player's goal is to maximize the probability of termination at a given exit, while the other player's goal is to minimize this probability. In the quantitative termination problems, given an RMDP (or RSSG) and probability p, we wish to decide whether the value of such a termination game is at least p (or at most p); in the qualitative termination problem we wish to decide whether the value is 1. The important 1-exit subclasses of these models, 1-RMDPs and 1-RSSGs, correspond in a precise sense to controlled and game versions of classic stochastic models, including multitype Branching Processes and Stochastic Context-Free Grammars, where the objective of the players is to maximize or minimize the probability of termination (extinction). We provide a number of upper and lower bounds for qualitative and quantitative termination problems for RMDPs and RSSGs. We show both problems are undecidable for multi-exit RMDPs, but are decidable for 1-RMDPs and 1-RSSGs. Specifically, the quantitative termination problem is decidable in PSPACE for both 1-RMDPs and 1-RSSGs, and is at least as hard as the square root sum problem, a well-known open problem in numerical computation. We show that the qualitative termination problem for 1-RMDPs (i.e., a controlled version of branching processes) can be solved in polynomial time both for maximizing and minimizing 1-RMDPs. The qualitative problem for 1-RSSGs is in NP ∩ coNP, and is at least as hard as the quantitative termination problem for Condon's finite-state simple stochastic games, whose complexity remains a well known open problem. Finally, we show that even for 1-RMDPs, more general (qualitative and quantitative) model-checking problems with respect to linear-time temporal properties are undecidable even for a fixed property.",2005-07-11,https://www.semanticscholar.org/paper/8e08c0fa120d02db8516ae669c4583d0ad93de49,"International Colloquium on Automata, Languages and Programming"
16,REEL: A Relation Extraction Learning framework,"We introduce the REEL (RElation Extraction Learning) framework, an open source framework that facilitates the development and evaluation of relation extraction systems over text collections. To define a relation extraction system for a new relation and text collection, users only need to specify the parsers to load the collection, the relation and its constraints, and the learning and extraction techniques to be used. This makes REEL a powerful framework to enable the deployment and evaluation of relation extraction systems for both application building and research.",2014-09-08,https://www.semanticscholar.org/paper/f28f1dc630b73175c331aa8857dd32fd8737c2a8,IEEE/ACM Joint Conference on Digital Libraries
1641,A probabilistic approach to discovering dynamic full-brain functional connectivity patterns,,2017-02-07,https://www.semanticscholar.org/paper/ff543c45096d817b748e29874317b31ec766dd33,NeuroImage
413,On the complexity of single-rule datalog queries,,1999-09-06,https://www.semanticscholar.org/paper/cd0b508cc8a127cffab0d010b0bafdb360fa77c8,Information and Computation
1213,Precise measurement of the top-quark mass from lepton + jets events.,We measure the mass of the top quark using top-quark pair candidate events in the lepton+jets channel from data corresponding to 1 fb;{-1} of integrated luminosity collected by the D0 experiment at the Fermilab Tevatron collider. We use a likelihood technique that reduces the jet energy scale uncertainty by combining an in situ jet energy calibration with the independent constraint on the jet energy scale (JES) from the calibration derived using photon+jets and dijet samples. We find the mass of the top quark to be 171.5+/-1.8(stat.+JES)+/-1.1(syst.) GeV.,2008-07-14,https://www.semanticscholar.org/paper/4a55811ac821b9bbc90b535f284457fd8745ca04,Physical Review Letters
170,Optimal Strategies of Blotto Games: Beyond Convexity,"The Colonel Blotto game, first introduced by Borel in 1921, is a well-studied game theory classic. Two colonels each have a pool of troops that they divide simultaneously among a set of battlefields. The winner of each battlefield is the colonel who puts more troops in it and the overall utility of each colonel is the sum of weights of the battlefields that s/he wins. Over the past century, the Colonel Blotto game has found applications in many different forms of competition from advertisements to politics to sports. Two main objectives have been proposed for this game in the literature: (i) maximizing the guaranteed expected payoff, and (ii) maximizing the probability of obtaining a minimum payoff u. The former corresponds to the conventional utility maximization and the latter concerns scenarios such as elections where the candidates' goal is to maximize the probability of getting at least half of the votes (rather than the expected number of votes). In this paper, we consider both of these objectives and show how it is possible to obtain (almost) optimal solutions that have few strategies in their support. One of the main technical challenges in obtaining bounded support strategies for the Colonel Blotto game is that the solution space becomes non-convex. This prevents us from using convex programming techniques in finding optimal strategies which are essentially the main tools that are used in the literature. However, we show through a set of structural results that the solution space can, interestingly, be partitioned into polynomially many disjoint convex polytopes that can be considered independently. Coupled with a number of other combinatorial observations, this leads to polynomial time approximation schemes for both of the aforementioned objectives. We also provide the first complexity result for finding the maximin of Blotto-like games: we show that computing the maximin of a generalization of the Colonel Blotto game that we call General Colonel Blotto is exponential time-complete.",2019-01-14,https://www.semanticscholar.org/paper/2a8393c972ad75e77f34367240d890ecd14a4424,ACM Conference on Economics and Computation
2967,Beta Diffusion Trees,"We define the beta diffusion tree, a random tree structure with a set of leaves that defines a collection of overlapping subsets of objects, known as a feature allocation. The generative process for the tree is defined in terms of particles (representing the objects) diffusing in some continuous space, analogously to the Dirichlet and Pitman-Yor diffusion trees (Neal, 2003b; Knowles & Ghahramani, 2011), both of which define tree structures over clusters of the particles. With the beta diffusion tree, however, multiple copies of a particle may exist and diffuse to multiple locations in the continuous space, resulting in (a random number of) possibly overlapping clusters of the objects. We demonstrate how to build a hierarchically-clustered factor analysis model with the beta diffusion tree and how to perform inference over the random tree structures with a Markov chain Monte Carlo algorithm. We conclude with several numerical experiments on missing data problems with data sets of gene expression arrays, international development statistics, and intranational socioeconomic measurements.",2014-06-21,https://www.semanticscholar.org/paper/b1637dbe60de0193d5680811506fb210507fcb02,International Conference on Machine Learning
711,Reachability for Branching Concurrent Stochastic Games,"We give polynomial time algorithms for deciding almost-sure and limit-sure reachability in Branching Concurrent Stochastic Games (BCSGs). These are a class of infinite-state imperfect-information stochastic games that generalize both finite-state concurrent stochastic reachability games, as well as branching simple stochastic reachability games.",2018-06-01,https://www.semanticscholar.org/paper/738aa8b51fc76c3a2ae9048e37e38fcc4bc3e511,"International Colloquium on Automata, Languages and Programming"
2447,Travel in large-scale head-worn VR: Pre-oriented teleportation with WIMs and previews,"We demonstrate an interaction technique that allows a user to point at a world-in-miniature representation of a city-scale virtual environment and perform efficient and precise teleportation by pre-orienting an avatar. A preview of the post-teleport view of the full-scale virtual environment updates interactively as the user adjusts the position, yaw, and pitch of the avatar's head with a pair of 6DoF-tracked controllers. We describe design decisions and contrast with alternative approaches to virtual travel.",2017-03-18,https://www.semanticscholar.org/paper/904f9c396af32e8d415989b51af5aeb2a17d29f5,IEEE Conference on Virtual Reality and 3D User Interfaces
1555,The Multi-Outcome Medical Deconfounder: Assessing Treatment Effect on Multiple Renal Measures,,,https://www.semanticscholar.org/paper/4438483026e4f34b3b6349554543447991f4eef1,American Medical Informatics Association Annual Symposium
1258,Study of direct CP violation in B(+/-)-->J/psiK(+/-)(pi(+/-)) decays.,"We present a search for direct CP violation in B(+/-)-->J/psiK(+/-)(pi(+/-)) decays. The event sample is selected from 2.8 fb(-1) of pp collisions recorded by D0 experiment in run II of the Fermilab Tevatron Collider. The charge asymmetry A_(CP)(B(+)-->J/psiK(+))= + 0.0075 +/- 0.0061(stat)+/-0.0030(syst) is obtained using a sample of approximately 40, 000 B(+/-)-->J/psiK(+/-) decays. The achieved precision is of the same level as the expected deviation predicted by some extensions of the standard model. We also measured the charge asymmetry A(CP)(B(+)-->J/psipi(+))=-0.09+/-0.08(stat)+/-0.03(syst).",,https://www.semanticscholar.org/paper/dbefd8ed8837a9e07cb17720044a71ae7562fdff,Physical Review Letters
1887,Deep reinforcement learning for selecting demand forecast models to empower Industry 3.5 and an empirical study for a semiconductor component distributor,"A semiconductor distributor that plays a third-party role in the supply chain will buy diverse components from different suppliers, warehouse and resell them to a number of electronics manufacturers with vendor-managed inventories, while suffering both risks of oversupply and shortage due to demand uncertainty. However, demand fluctuation and supply chain complexity are increasing due to shortening product life cycle in the consumer electronics era and long lead time for capacity expansion for high-tech manufacturing. Focusing realistic needs of a leading distributor for semiconductor components and modules, this study aims to construct a UNISON framework based on deep reinforcement learning (RL) for dynamically selecting the optimal demand forecast model for each of the products with the corresponding demand patterns to empower smart production for Industry 3.5. Deep RL that integrates deep learning architecture and RL algorithm can learn successful policies from the dynamic and complex real world. The reward function mechanism of deep RL can reduce negative impact of demand uncertainty. An empirical study was conducted for validation showing practical viability of the proposed approach. Indeed, the developed solution has been in real settings.",2020-03-03,https://www.semanticscholar.org/paper/b745b89e1412516e6e6799367214b8c8bdcaae8d,International Journal of Production Research
3595,Fast dynamic casting,"We describe a scheme for implementing dynamic casts suitable for systems where the performance and predictability of performance is essential. A dynamic cast from a base class to a derived class in an object‐oriented language can be performed quickly by having the linker assign an integer type ID to each class. A simple integer arithmetic operation verifies whether the cast is legal at run time. The type ID scheme presented uses the modulo function to check that one class derives from another. A 64‐bit type ID is sufficient to handle class hierarchies of large size at least nine levels of derivation deep. We also discuss the pointer adjustments required for a C++ dynamic_cast. All examples will be drawn from the C++ language. Copyright © 2005 John Wiley & Sons, Ltd.",2006-02-01,https://www.semanticscholar.org/paper/3f566ae993364bfc435e9ef34e995af8a1a079e8,"Software, Practice & Experience"
2911,A crystal plasticity model that accounts for grain size effects and slip system interactions on the deformation of austenitic stainless steels,,2022-02-01,https://www.semanticscholar.org/paper/952738cac8c557ed5b2eb5af1e9ac7eb3bd87d68,International journal of plasticity
563,"Correction to ""A Theorem in Database Concurrency Control""",,,https://www.semanticscholar.org/paper/bc636f317b59675609e69586d29522b87f68068a,Journal of the ACM
2808,Galectin-3 regulates intracellular trafficking of epidermal growth factor receptor through Alix and promotes keratinocyte migration,,2012-07-06,https://www.semanticscholar.org/paper/7dab6ea0ddf6f7655ac9be008f73a9c36b520399,Journal of Investigative Dermatology
2164,Upregulation of p16INK4A in Peripheral White Blood Cells as a Novel Screening Marker for Colorectal Carcinoma,"Objective: Screening of colorectal cancer (CRC) is important for the early detection. CRC is relating to aging and immuno-senescence. One such senescent marker is p16INK4A expression in immune cells. The objective of the study is to investigate the protein expression of p16INK4A in peripheral white blood cells as a screening marker for colorectal cancer. Methods: A case-control studies were conducted. Cases were patients with colorectal cancer and controls were matched with cases based on age and sex. Peripheral blood was collected from patients and controls and the protein p16INK4A was measured with immunofluorescent techniques. The p16INK4A levels from cases and controls were evaluated using ROC analysis to be used as a screening marker in CRC patients. Mean fluorescent intensity of p16INK4A of cases and controls were analyzed in CD45+, CD3+ or CD14+ cells. The p16INK4A levels of cases were also correlated with clinical data. Result: Statistically significant increased expression of p16INK4A levels were found in cases compared to controls. p16INK4A in peripheral immune cells had 78% sensitivity and 71% specificity which can possibly be used as a diagnosis tool for colorectal cancer. P16INK4A-positive cell percentage and mean florescent intensity were significantly higher in CD45+ cells, CD3 positive cells and CD14 positive cells. No significant correlation was observed with the clinical data and p16INK4A level of CRC patients. Conclusion: The significant increase of p16 INK4A expression level in peripheral immune cells represents potential for use as a CRC screening marker.",2022-11-01,https://www.semanticscholar.org/paper/a010912da96094fc1182703dd5b7315df99a145c,Asian Pacific Journal of Cancer Prevention
2938,Landscape of stimulation-responsive chromatin across diverse human immune cells,,2018-09-05,https://www.semanticscholar.org/paper/28e2a40e8863ccd08bc8336a91df11a892d4ea02,Nature Genetics
1691,Hierarchical Variational Models,"Black box variational inference allows researchers to easily prototype and evaluate an array of models. Recent advances allow such algorithms to scale to high dimensions. However, a central question remains: How to specify an expressive variational distribution that maintains efficient computation? To address this, we develop hierarchical variational models (HVMs). HVMs augment a variational approximation with a prior on its parameters, which allows it to capture complex structure for both discrete and continuous latent variables. The algorithm we develop is black box, can be used for any HVM, and has the same computational efficiency as the original approximation. We study HVMs on a variety of deep discrete latent variable models. HVMs generalize other expressive variational distributions and maintains higher fidelity to the posterior.",2015-11-07,https://www.semanticscholar.org/paper/f31ac36adbd24c43dcd28397081702e98e026b34,International Conference on Machine Learning
914,Independent database schemas,"Abstract A database schema is independent with respect to a given set of constraints if the constraints can be enforced separately in the relations. A polynomial time algorithm is presented that recognizes independent schemas, when the given constraints consist of functional dependencies and the join dependency of the database schema.",1982-03-29,https://www.semanticscholar.org/paper/79452415c4785939e214ea25e4a4eab1c6bc4704,Journal of computer and system sciences (Print)
1954,An effective Markov network based EDA for flexible job shop scheduling problems under uncertainty,"This paper presents a min-max regret version programming model for the stochastic flexible job shop scheduling problem (S-FJSP) with the uncertainty of processing time. An effective Markov network based estimation of distribution algorithm (EDA) is proposed to solve S-FJSP to minimize its maximum regret. The proposal employs Markov network modeling machine assignment where the effects between decision variables are represented as an undirected graph model. Furthermore, min-max regret metric based assessing algorithm is used to measure the robustness, where a critical path-based local search method is adopted to achieve better performance. We present an empirical validation for the proposal by applying it to solve various benchmark flexible job shop problems.",2014-10-30,https://www.semanticscholar.org/paper/29580ab60eecf85a53f2137490ce0523e03a313e,2014 IEEE International Conference on Automation Science and Engineering (CASE)
2353,Role of myeloperoxidase in intracellular and extracellular chemiluminescence of neutrophils.,"Activated polymorphonuclear leucocytes (neutrophils) can generate both intracellular and extracellular luminol dependent chemiluminescence. As luminol dependent chemiluminescence largely measures the activity of the myeloperoxidase-H2O2 system, and as the extracellular activity of this enzyme may be responsible for the tissue damage associated with inflammatory conditions such as rheumatoid arthritis, the aim of this work was to distinguish between intracellular and extracellular chemiluminescence so that the extracellular activity of this enzyme could be evaluated. Azide was used as a non-specific inhibitor of both intracellular and extracellular chemiluminescence, whereas anti-(human myeloperoxidase) IgG was used to inhibit specifically the extracellular activity of myeloperoxidase. Thus this IgG is a useful analytical tool for studying the extracellular activity of the myeloperoxidase-H2O2 system in the pathology of rheumatoid arthritis.",1989-01-01,https://www.semanticscholar.org/paper/4cfa5d1488717e4fbc3ce88c1afb3402919af596,Annals of the Rheumatic Diseases
489,On Finding Extensions of Default Theories,,1992-10-14,https://www.semanticscholar.org/paper/50edbb4e99d724ea1a2a57c61e8a759a94fa5c28,International Conference on Database Theory
1218,Search for anomalous Wtb couplings in single top quark production.,"In 0.9 fb(-1) of pp[over] collisions, the D0 Collaboration presented evidence for single top quark production in events with an isolated lepton, missing transverse momentum, and two to four jets. We examine these data to study the Lorentz structure of the Wtb coupling. The standard model predicts a left-handed vector coupling at the Wtb vertex. The most general lowest dimension, CP-conserving Lagrangian admits right-handed vector and left- or right-handed tensor couplings as well. We find that the data prefer the left-handed vector coupling and set upper limits on the anomalous couplings. These are the first direct constraints on a general Wtb interaction and the first direct limits on left- and right-handed tensor couplings.",2008-07-10,https://www.semanticscholar.org/paper/4d06eb19a518cd34052cbbff035ecd9e6d3633e3,Physical Review Letters
3085,DejaView: a personal virtual computer recorder,"As users interact with the world and their peers through their computers, it is becoming important to archive and later search the information that they have viewed. We present DejaView, a personal virtual computer recorder that provides a complete record of a desktop computing experience that a user can playback, browse, search, and revive seamlessly. DejaView records visual output, checkpoints corresponding application and file system state, and captures displayed text with contextual information to index the record. A user can then browse and search the record for any visual information that has been displayed on the desktop, and revive and interact with the desktop computing state corresponding to any point in the record. DejaView combines display, operating system, and file system virtualization to provide its functionality transparently without any modifications to applications, window systems, or operating system kernels. We have implemented DejaView and evaluated its performance on real-world desktop applications. Our results demonstrate that DejaView can provide continuous low-overhead recording without any user noticeable performance degradation, and allows browsing, search and playback of records fast enough for interactive use.",2007-10-14,https://www.semanticscholar.org/paper/18efc6ae1f845e710b256bdcc39ffd1fa6a79661,Symposium on Operating Systems Principles
594,A theorem in database concurrency control,"Consider two straight-line programs a and b, and let h be a set of sequences of steps of a and b, possibly interleaved, but each containing all steps of a and b in the right order. A necessary and sufficient condition is given for h to be realisable as the set of all sequences of steps which are legal under some insertion of lock-unlock steps between the steps of a and b. 13 references.",1982-10-01,https://www.semanticscholar.org/paper/8a76ccc9eb307722bf4dc977be450ed0016d4818,JACM
565,A note the expressive power of Prolog,,,https://www.semanticscholar.org/paper/e4faadf6a6b0523482c1bfff53c4c5cfd1f8225f,Bull. EATCS
1716,Content-based recommendations with Poisson factorization,"We develop collaborative topic Poisson factorization (CTPF), a generative model of articles and reader preferences. CTPF can be used to build recommender systems by learning from reader histories and content to recommend personalized articles of interest. In detail, CTPF models both reader behavior and article texts with Poisson distributions, connecting the latent topics that represent the texts with the latent preferences that represent the readers. This provides better recommendations than competing methods and gives an interpretable latent space for understanding patterns of readership. Further, we exploit stochastic variational inference to model massive real-world datasets. For example, we can fit CPTF to the full arXiv usage dataset, which contains over 43 million ratings and 42 million word counts, within a day. We demonstrate empirically that our model outperforms several baselines, including the previous state-of-the art approach.",2014-12-08,https://www.semanticscholar.org/paper/d01508b7f10468b47712cfdc19c028997b541dd1,Neural Information Processing Systems
2011,Manufacturing intelligence for determining machine subgroups to enhance yield in semiconductor manufacturning,"Linewidth control is a critical issue for yield enhancement in semiconductor manufacturing. Most of the existing techniques such as run-to-run control have been developed to control the critical dimension (CD) in photolithography and etching process. However, few studies have addressed the tool behavior that would also affect the result of CD in etching process and the etch bias that is the CD difference between photolithograph and etching process. This study aims to propose a manufacturing intelligence (MI) approach to develop dispatching rules for etching tool in order to reduce the variation of critical dimension measured after etching process and determine the machine subgroups for compensating the etching bias. An empirical study was conducted to estimate the validity of proposed approach and the results showed practical viability of this approach.",2011-12-11,https://www.semanticscholar.org/paper/30208f4b99577c16b6b6dad7d4a25d51808324e8,Online World Conference on Soft Computing in Industrial Applications
1880,Energy-Efficient Unmanned Aerial Vehicle (UAV) Surveillance Utilizing Artificial Intelligence (AI),"Recently, unmanned aerial vehicles (UAVs) enhance connectivity and accessibility for civilian and military applications. A group of UAVs with on-board cameras usually monitors or collects information about designated areas. The UAVs can build a distributed network to share/exchange and to process collected sensing data before sending to a data processing center. A huge data transmission among them may cause latency and high-energy consumption. This paper deploys artificial intelligent (AI) techniques to process the video data streaming among the UAVs. Thus, each distributed UAV only needs to send a certain required information to each other. Each UAV processes data utilizing AI and only sends the data that matters to the others. The UAVs, formed as a connected network, communicate within a short communication range and share their own data to each other. Convolution neural network (CNN) technique extracts feature from images automatically that the UAVs only send the moving objects instead of the whole frames. This significantly reduces redundant information for either each UAV or the whole network and saves a huge energy consumption for the network. The UAVs can also save energy for their motion in the sensing field. In addition, a flocking control algorithm is deployed to lead the group of UAVs in the working fields and to avoid obstacles if needed. Simulation and experimental results are provided to verify the proposed algorithms in either AI-based data processing or controlling the UAVs. The results show promising points to save energy for the networks.",2021-10-13,https://www.semanticscholar.org/paper/9406d652e434772e5e451b931526e90e386449c3,Wireless Communications and Mobile Computing
2487,Session details: Papers: manipulating video,,2013-04-27,https://www.semanticscholar.org/paper/b133b7ad22025357acf935dd5668df6c0810fc91,International Conference on Human Factors in Computing Systems
731,"A Note on the Complexity of Comparing Succinctly Represented Integers, with an Application to Maximum Probability Parsing","The following two decision problems capture the complexity of comparing integers or rationals that are succinctly represented in product-of-exponentials notation, or equivalently, via arithmetic circuits using only multiplication and division gates, and integer inputs. <i>Input instance:</i> Four lists of positive integers:
 <i>a</i>1,..., <i>an</i>∈N+<i>n</i>; <i>b</i>1,...,<i>bn</i>∈N+<i>n</i>; <i>c</i>1,...,<i>cm</i>∈N+<i>m</i>; <i>d</i>1, ..., <i>dm</i>∈N+<i>m</i>;
 where each of the integers is represented in binary.
 <i>Problem 1 (equality testing):</i> Decide whether <i>a</i>1<i>b</i>1 <i>a</i>2<i>b</i>2⋯<i>anbn</i>=<i>c</i>1<i>d</i>1 <i>c</i>2<i>d</i>2⋯<i>cmdm</i>.
 <i>Problem 2 (inequality testing):</i> Decide whether <i>a</i>1<i>b</i>1 <i>a</i>2<i>b</i>2⋯<i>anbn</i>≥<i>c</i>1<i>d</i>1 <i>c</i>2<i>d</i>2⋯<i>cmdm</i>.
 Problem 1 is easily decidable in polynomial time using a simple iterative algorithm. Problem 2 is much harder. We observe that the complexity of Problem 2 is intimately connected to deep conjectures and results in number theory. In particular, if a refined form of the <i>ABC conjecture</i> formulated by Baker in 1998 holds, or if the older <i>Lang-Waldschmidt conjecture</i> (formulated in 1978) on linear forms in logarithms holds, then Problem 2 is decidable in P-time (in the standard Turing model of computation). Moreover, it follows from the best available quantitative bounds on linear forms in logarithms, namely, by Baker and Wüstholz [1993] or Matveev [2000], that if <i>m</i> and <i>n</i> are fixed universal constants then Problem 2 is decidable in P-time (without relying on any conjectures). This latter fact was observed earlier by Shub [1993].
 We describe one application: P-time maximum probability parsing for arbitrary stochastic context-free grammars (where <i>ε</i>-rules are allowed).",2013-04-19,https://www.semanticscholar.org/paper/e1f40198836f804b0efb555968c4c357736b7d25,TOCT
1345,Measurement of semileptonic branching fractions of B mesons to narrow D** states.,"Using the data accumulated in 2002-2004 with the D0 detector in proton-antiproton collisions at the Fermilab Tevatron collider with a center-of-mass energy of 1.96 TeV, the branching fractions of the decays B --> D0(1)(2420)mu+ vmuX and B --> D2(*0)(2460)mu+ vmuX and their ratio have been measured: B(b --> B) x B(B --> D0(1)mu+ vmuX) x B(D0(*0) --> D*- pi+) = [0.087 +/- 0.007(stat) +/- 0.014(syst)]%; B(b --> B) x B(B --> D2(*0) mu+ vmuX) x B(D2(*0) --> D*- pi+) = [0.035 +/- 0.007(stat) +/- 0.008(syst)]% and [B(B --> D2(*0)mu+ vmuX) x B(D2(*0) --> D*- pi+)]/[B(B --> D0(1)mu+ vmuX) x B(D0(1) --> D*- pi+)] = 0.39 +/- 0.09(stat) +/- 0.12 (syst), where the charge conjugated states are always implied.",2005-07-10,https://www.semanticscholar.org/paper/7954a4e30eda0139630cecf7da2aff734e821197,Physical Review Letters
2279,Bile acids inhibit Mcl-1 protein turnover via an epidermal growth factor receptor/Raf-1-dependent mechanism.,"Bile acids have been implicated in biliary tract carcinogenesis, in part, by activating the epidermal growth factor receptor (EGFR). Overexpression of Mcl-1, a potent antiapoptotic protein of the Bcl-2 family, has also been reported in cholangiocarcinomas. Because receptor tyrosine kinases like EGFR may modulate antiapoptotic protein expression, we examined the hypothesis that bile acids modulate Mcl-1 expression levels via EGFR. Deoxycholate increased cellular Mcl-1 protein in a concentration-dependent manner. The deoxycholate-mediated increase of cellular Mcl-1 protein was blocked equally by EGFR tyrosine kinase inhibitors or an EGFR-neutralizing antibody. Although inhibition of mitogen-activated protein kinases did not attenuate the deoxycholate-associated increase in Mcl-1 protein, the Raf-1 inhibitor, BAY 37-9751, effectively blocked the cellular increase of this protein. Neither Mcl-1 transcriptional activity nor its mRNA stability was altered by deoxycholate treatment. However, Mcl-1 protein stability was increased by bile acid treatment, an effect duplicated by proteasome inhibition. Deoxycholate prolongation of Mcl-1 turnover was blocked by either EGFR inhibitors or the Raf-1 inhibitor. Whereas the deoxycholate-induced increase in Mcl-1 reduced Fas-mediated apoptosis, the Raf-1 inhibitor potentiated Fas apoptosis. Our results demonstrate that bile acids block Mcl-1 protein degradation via activation of an EGFR/Raf-1 cascade resulting in its cellular accumulation. Raf-1 inhibitors block this increase of Mcl-1 and render the cells more susceptible to apoptosis, a potential therapeutic strategy for cholangiocarcinomas.",2002-11-15,https://www.semanticscholar.org/paper/6f4fcaed2a15555d952a43ee20586be2f095f7d7,Cancer Research
1917,Soft computing for smart production to empower industry 4.0,,2018-07-01,https://www.semanticscholar.org/paper/da180a0159e7bf7860fefc23ff036ed5c912f888,Applied Soft Computing
2728,Interactive constraint-based search and replace,"We describe enhancements to graphical search and replace that allow users to extend the capabilities of a graphical editor. Interactive constraint-based search and replace can search for objects that obey user-specified sets of constraints and automatically apply other constraints to modify these objects. We show how an interactive tool that employs this technique makes it possible for users to define sets of constraints graphically that modify existing illustrations or control the creation of new illustrations. The interace uses the same visual language as the editor and allows users to understand and create powerful rules without conventional programming. Rules can be saved and retrieved for use alone or in combination. Examples, generated with a working implementation, demonstrate applications to drawing beautification and transformation.",1992-06-01,https://www.semanticscholar.org/paper/ad78505acffdd64e206f27c19f93e20140622a65,International Conference on Human Factors in Computing Systems
2824,Galectin-3 modulates T helper responses by regulating dendritic cell cytokine expression (103.11),"
 Dendritic cells play a critical role in the initiation and maintenance of inflammatory responses. Galectin-3, a protein found in DCs, is a carbohydrate-binding protein implicated in several cellular processes. In mouse models of asthma and atopic dermatitis, galectin-3-deficient (gal3-/-) mice had significantly fewer infiltrating eosinophils and displayed lower Th2 but higher Th1 responses compared to wild-type mice, suggesting that galectin-3 plays a key role in allergic inflammation. Given the ability of DCs to direct the T lymphocyte response, we hypothesized that galectin-3 may affect immune responses by altering DC functions and tested the hypothesis by comparing wild-type and gal3-/- DCs. OT-II CD4+ cells cultured with OVA-pulsed gal3-/- DCs generated higher Th1 responses relative to gal3+/+ DCs, and the differences were diminished following IL-12 neutralization. Moreover, gal3-/- DCs expressed more IL-12p35 mRNA than gal3+/+ DCs, indicating that gal3 may modulate IL-12 at the transcriptional level or through upstream signaling pathways. T cells co-cultured with gal3-/- DCs also secreted more IL-17 than cells cultured with gal3+/+ DCs, suggesting that gal3 may also negatively regulate Th17 responses. Furthermore, we observed higher IL-6 secretion and IL-23p19 expression in gal3-/- DCs, which may contribute to the enhanced Th17 polarization induced by these cells. We conclude that gal3 may regulate DC cytokine expression, thereby modulating T helper responses.",2011-04-01,https://www.semanticscholar.org/paper/c7439e84e1508a25fd07036a915a3bbf910e699e,Journal of Immunology
841,Approximate max-flow min-(multi)cut theorems and their applications,"Consider the multicommodity flow problem in which the object is to maximize the sum of commodities routed. We prove the following approximate max-flow min-multicut theorem: $$ \dst \frac{\mbox{\rm min multicut}}{O(\log k)} \leq \mbox{ \rm max flow } \leq \mbox{ \rm min multicut}, $$ \noindent where $k$ is the number of commodities. Our proof is constructive; it enables us to find a multicut within $O(\log k)$ of the max flow (and hence also the optimal multicut). In addition, the proof technique provides a unified framework in which one can also analyse the case of flows with specified demands of Leighton and Rao and Klein et al. and thereby obtain an improved bound for the latter problem.",1993-06-01,https://www.semanticscholar.org/paper/a2ad294dced8fe080cf9eb9a2aabd7cb4a5eb74a,SIAM journal on computing (Print)
3772,Inferring the Why in Images,"Abstract : Humans have the remarkable capability to infer the motivations of other people's actions, likely due to cognitive skills known in psychophysics as the theory of mind. In this paper, we strive to build a computational model that predicts the motivation behind the actions of people from images. To our knowledge, this challenging problem has not yet been extensively explored in computer vision. We present a novel learning based framework that uses high-level visual recognition to infer why people are performing an actions in images. However, the information in an image alone may not be sufficient to automatically solve this task. Since humans can rely on their own experiences to infer motivation, we propose to give computer vision systems access to some of these experiences by using recently developed natural language models to mine knowledge stored in massive amounts of text. While we are still far away from automatically inferring motivation, our results suggest that transferring knowledge from language into vision can help machines understand why a person might be performing an action in an image.",2014-06-20,https://www.semanticscholar.org/paper/dfe448d6297ea0a3d4deba21fbf1006bc35877d7,arXiv.org
3048,Pervasive detection of process races in deployed systems,"Process races occur when multiple processes access shared operating system resources, such as files, without proper synchronization. We present the first study of real process races and the first system designed to detect them. Our study of hundreds of applications shows that process races are numerous, difficult to debug, and a real threat to reliability. To address this problem, we created RacePro, a system for automatically detecting these races. RacePro checks deployed systems in-vivo by recording live executions then deterministically replaying and checking them later. This approach increases checking coverage beyond the configurations or executions covered by software vendors or beta testing sites. RacePro records multiple processes, detects races in the recording among system calls that may concurrently access shared kernel objects, then tries different execution orderings of such system calls to determine which races are harmful and result in failures. To simplify race detection, RacePro models under-specified system calls based on load and store micro-operations. To reduce false positives and negatives, RacePro uses a replay and go-live mechanism to distill harmful races from benign ones. We have implemented RacePro in Linux, shown that it imposes only modest recording overhead, and used it to detect a number of previously unknown bugs in real applications caused by process races.",2011-10-23,https://www.semanticscholar.org/paper/388794ec80deec7754c65a4c2fb3f12eb7b3bac6,Symposium on Operating Systems Principles
2381,The cytochromes of Dictyostelium discoideum.,,,https://www.semanticscholar.org/paper/1c6ba45b6b7345bc32df8de227c4beafd987cb95,"Comparative biochemistry and physiology. B, Comparative biochemistry"
3421,FairTorrent: A Deficit-Based Distributed Algorithm to Ensure Fairness in Peer-to-Peer Systems,"Peer-to-peer file-sharing applications suffer from a fundamental problem of unfairness. Free-riders cause slower download times for others by contributing little or no upload bandwidth while consuming much download bandwidth. Previous attempts to address this fair bandwidth allocation problem suffer from slow peer discovery, inaccurate predictions of neighboring peers' bandwidth allocations, underutilization of bandwidth, and complex parameter tuning. We present FairTorrent, a new deficit-based distributed algorithm that accurately rewards peers in accordance with their contribution. A FairTorrent peer simply uploads the next data block to a peer to whom it owes the most data as measured by a deficit counter. FairTorrent is resilient to exploitation by free-riders and strategic peers, is simple to implement, requires no bandwidth overallocation, no prediction of peers' rates, no centralized control, and no parameter tuning. We implemented FairTorrent in a BitTorrent client without modifications to the BitTorrent protocol and evaluated its performance against other widely used BitTorrent clients. Our results show that FairTorrent provides up to two orders of magnitude better fairness, up to five times better download times for contributing peers, and 60%-100% better performance on average in live BitTorrent swarms.",2012-10-01,https://www.semanticscholar.org/paper/b7f32327cef986b3572a3b1bc625ec6c1dfd9390,IEEE/ACM Transactions on Networking
752,Recursive Stochastic Games with Positive Rewards,,2008-07-07,https://www.semanticscholar.org/paper/53b764aaadae945a515fc1676433ad4021769ab6,"International Colloquium on Automata, Languages and Programming"
2174,The Inhibitory Effect of Validamycin A on Aspergillus flavus,"Aspergillus flavus is one of the most common isolates from patients with fungal infections. Aspergillus infection is usually treated with antifungal agents, but side effects of these agents are common. Trehalase is an essential enzyme involved in fungal metabolism, and the trehalase inhibitor, validamycin A, has been used to prevent fungal infections in agricultural products. In this study, we observed that validamycin A significantly increased trehalose levels in A. flavus conidia and delayed germination, including decreased fungal adherence. In addition, validamycin A and amphotericin B showed a combinatorial effect on A. flavus ATCC204304 and clinical isolates with high minimum inhibitory concentrations (MICs) of amphotericin B using checkerboard assays. We observed that validamycin A and amphotericin B had a synergistic effect on A. flavus strains resistant to amphotericin B. The MICs in the combination of validamycin A and amphotericin B were at 0.125 μg/mL and 2 μg/mL, respectively. The FICI of validamycin A and amphotericin B of these clinical isolates was about 0.25–0.28 with synergistic effects. No drug cytotoxicity was observed in human bronchial epithelial cells treated with validamycin A using LDH-cytotoxicity assays. In conclusion, this study demonstrated that validamycin A inhibited the growth of A. flavus and delayed conidial germination. Furthermore, the combined effect of validamycin A with amphotericin B increased A. flavus killing, without significant cytotoxicity to human bronchial epithelial cells. We propose that validamycin A could potentially be used in vivo as an alternative treatment for A. flavus infections.",2020-06-27,https://www.semanticscholar.org/paper/97f8e1a67f475b5ce4f41eb0a0674c8e0230f8a8,International Journal of Microbiology
127,Adaptive Deadlock- and Livelock-Free Routing in the Hypercube Network,"This paper consists of two parts. In the first one, two new algorithms for wormhole routing on the hypercube network are presented. These techniques are adaptive and are ensured to be deadlock- and livelock-free. These properties are guaranteed by using a small number of resources in the routing node. The first algorithm is adaptive and nonminimal and will be referred to as Nonminimal. In this technique, some moderate derouting is allowed in order to alleviate the potential congestion arising from highly structured communication patterns. The second algorithm, dubbed Subcubes, is adaptive and minimal, and is based on partitioning the hypercube into subcubes of smaller dimension; This technique requires only two virtual channels per physical link of the node. In the second part of the paper, a wide variety of techniques for wormhole routing in the hypercube are evaluated from an algorithmic point of view. Five partially adaptive algorithms are considered: the Hanging algorithm, the Zenith algorithm, the Hanging-Order algorithm, the Nonminimal algorithm; and the Subcubes algorithm. One oblivious algorithm, the Dimension-Order, or E-Cube routing algorithm, is also used. Finally, a Fully Adaptive Minimal algorithm is tried. A simple node model was designed and adapted to all the algorithms. >",1994-11-01,https://www.semanticscholar.org/paper/90242db49d74e120e96ef1a66ecda0f0a92e1faa,IEEE Trans. Parallel Distributed Syst.
2266,Oscillations in NF-κB Signaling Control the Dynamics of Gene Expression,"Signaling by the transcription factor nuclear factor kappa B (NF-κB) involves its release from inhibitor kappa B (IκB) in the cytosol, followed by translocation into the nucleus. NF-κB regulation of IκBα transcription represents a delayed negative feedback loop that drives oscillations in NF-κB translocation. Single-cell time-lapse imaging and computational modeling of NF-κB (RelA) localization showed asynchronous oscillations following cell stimulation that decreased in frequency with increased IκBα transcription. Transcription of target genes depended on oscillation persistence, involving cycles of RelA phosphorylation and dephosphorylation. The functional consequences of NF-κB signaling may thus depend on number, period, and amplitude of oscillations.",2004-10-22,https://www.semanticscholar.org/paper/f524615f85ad6c945ddcf885ba3a05ad3048abc4,Science
189,The EATCS Award 2017 - Laudatio for Eva Tardos,,2017-02-08,https://www.semanticscholar.org/paper/acb69b14902a3872dfd9fa76d2b415c7a6309251,Bull. EATCS
1015,Effects of pertussis toxin on caudate neuron electrophysiology: studies with dopamine D1 and D2 agonists,,1990-11-19,https://www.semanticscholar.org/paper/b30552656a12b5d8760996e43a9b6ee9705a9e25,Brain Research
2768,Apex: An Experiment in the Automated Creation of Pictorial Explanations,How might we automate the design of pictures that are intended to show the viewer how to perform a series of tasks?,1985-11-01,https://www.semanticscholar.org/paper/21132fc605f9326efbcdf47149a9ab7fb157473d,IEEE Computer Graphics and Applications
1169,Search for the standard model Higgs boson in tau final states.,"We present a search for the standard model Higgs boson using hadronically decaying tau leptons, in 1 fb(-1) of data collected with the D0 detector at the Fermilab Tevatron pp collider. We select two final states: tau+/- plus missing transverse energy and b jets, and tau+ tau- plus jets. These final states are sensitive to a combination of associated W/Z boson plus Higgs boson, vector boson fusion, and gluon-gluon fusion production processes. The observed ratio of the combined limit on the Higgs production cross section at the 95% C.L. to the standard model expectation is 29 for a Higgs boson mass of 115 GeV.",2009-03-27,https://www.semanticscholar.org/paper/924f20f4176a4dd1946950a1d410f842e747e754,Physical Review Letters
2473,"WeARHand: Head-worn, RGB-D camera-based, bare-hand user interface with visually enhanced depth perception","We introduce WeARHand, which allows a user to manipulate virtual 3D objects with a bare hand in a wearable augmented reality (AR) environment. Our method uses no environmentally tethered tracking devices and localizes a pair of near-range and far-range RGB-D cameras mounted on a head-worn display and a moving bare hand in 3D space by exploiting depth input data. Depth perception is enhanced through egocentric visual feedback, including a semi-transparent proxy hand. We implement a virtual hand interaction technique and feedback approaches, and evaluate their performance and usability. The proposed method can apply to many 3D interaction scenarios using hands in a wearable AR environment, such as AR information browsing, maintenance, design, and games.",2014-11-06,https://www.semanticscholar.org/paper/58b5fea06a70ec4acacc3d2f7e36f9a5b9120270,International Symposium on Mixed and Augmented Reality
3067,MediaPod: A Personalized Multimedia Desktop in Your Pocket,"We present MediaPod, a portable system that allows mobile users to maintain the same persistent, personalized multimedia desktop environment on any available computer. Regardless of which computer is being used, MediaPod provides a consistent multimedia desktop session, maintaining all of a user's applications, documents and configuration settings. This is achieved by leveraging rapid improvements in capacity, cost, and size of portable storage devices. MediaPod provides a virtualization and checkpoint-restart mechanism that decouples a desktop environment and its applications from the host, enabling multimedia desktop sessions to be suspended to portable storage, carried around, and resumed from the storage device on another computer. MediaPod virtualization also isolates desktop sessions from the host, protecting the privacy of the user and preventing malicious applications from damaging the host. We have implemented a Linux MediaPod prototype and demonstrate its ability to quickly suspend and resume multimedia desktop sessions, enabling a seamless computing experience for mobile users as they move among computers.",2009-12-14,https://www.semanticscholar.org/paper/0f35a41f62bdb35b9ec3c64e13990a4ede1d70ae,IEEE International Symposium on Multimedia
3159,SMART: a processor scheduler for multimedia applications,"Applications that manipulate digital audio and video represent a growing class of computations executed by workstation users. This class of computations is known as continuous media. Their distinguishing characteristic is that they must process and transport media samples within application-specific time constraints. To support these real-time activities, the operating system must manage resources so that their time constraints can be satisfied whenever possible.",1995-12-03,https://www.semanticscholar.org/paper/d31ace6a28ebbb10d09db1a0868f1d6596f15f49,Symposium on Operating Systems Principles
882,Expressing Combinatorial Optimization Problems by Linear Programs (Extended Abstract),,,https://www.semanticscholar.org/paper/37f59817e1bff35eb8c68f8b73a9781746075efb,Symposium on the Theory of Computing
3436,On distributing symmetric streaming computations,"A common approach for dealing with large data sets is to stream over the input in one pass, and perform computations using sublinear resources. For truly massive data sets, however, even making a single pass over the data is prohibitive. Therefore, streaming computations must be distribued over many machines. In practice, obtaining significant speedups using distributed computations has numerous challenges including synchronization, load balancing, overcoming processor failures, and data distribution. Successful Systems in practice such as Google's MapReduce and Apache's Hadoop address these problems by only allowing a certain class of highly distributable tasks defined by local computations that can be applied in any order to the input.
 The fundamental question that arises is: How does the class of computational tasks supported by these systems differ from the class for which streaming solutions exist?
 We introduce a simple algorithmic model for massive, unordered, distributed (mud) computation, as implemented by these systems. We show that in principle, mud algorithms are equivalent in power to symmetric streaming algorithms. More precisely, we show that any symmetric (order-invariant) function that can be computed by a steraming algorithm can also be computed by a mud algorithym, with comparable space and communication complexity. Our simulation uses Savitch's theorem and therefore has superpolynomial time complexity. We extend our simulation result to some natural classes of approximate and randomized steraming algorithms. We also give negative results, using communication complexity arguments to prove that extensions to private randomness, promise problems and indeterminate functions are impossible. We also introduce an extension of the mud model to multiple keys and multiple rounds.",2008-01-20,https://www.semanticscholar.org/paper/09d2f2521cff8614f5508f95db94356fe453cc5d,ACM-SIAM Symposium on Discrete Algorithms
2744,Generating coordinated multimedia explanations,"The coordinated multimedia explanation testbed (COMET) is a research system being developed to explore the coordinated generation of multimedia explanations of equipment maintenance and repair procedures. The form and content of all material presented is generated interactively, allowing explanations to be customized for the individual user and situation. COMET's architecture includes multiple static and dynamic knowledge sources, a content planner, a media coordinator, media generators and a media layout manager. Coordinating multiple media through the development of a single content planner that generates a common content description for both media is emphasized. This content description is annotated by the media coordinator to indicate the assignment of the concepts to be expressed to the media generators. This will ultimately allow cross-references between media and coordinated display layouts that reflect fine-grain relationships among the material presented.<<ETX>>",,https://www.semanticscholar.org/paper/069dc786d8a814286e32678a51d63ca88a08bbca,Sixth Conference on Artificial Intelligence for Applications
2834,Galectin-3 is critical for the development of the allergic inflammatory response in a mouse model of atopic dermatitis.,"Galectin-3 belongs to a family of beta-galactoside-binding animal lectins expressed in several cell types, including epithelial and immune cells. To establish the role of galectin-3 in the development of allergic skin inflammation, we compared inflammatory skin responses of galectin-3-deficient (gal3(-/-)) and wild-type (gal3(+/+)) mice to epicutaneous sensitization with ovalbumin (OVA). OVA-treated gal3(-/-) mice exhibited markedly reduced epidermal thickening, lower eosinophil infiltration, and lower serum IgE levels compared with gal3(+/+) mice. The former evoked lower interleukin-4, but higher interferon-gamma, mRNA expression at OVA-treated skin sites. Moreover, gal3(-/-) splenocytes from OVA-sensitized mice secreted more interleukin-12 compared with gal3(+/+) splenocytes. In addition, antigen presentation by gal3(-/-) dendritic cells to T cells in vitro were T helper cell (Th1)-polarized relative to presentation by gal3(+/+) dendritic cells. When exposed to OVA, recipients engrafted with T cells from gal3(-/-) OVA-specific T cell receptor transgenic mice developed significantly reduced dermatitis and a markedly lower Th2 response compared with recipients of comparable gal3(+/+) T cells. We conclude that galectin-3 is critical for the development of inflammatory Th2 responses to epicutaneously administered antigens; in its absence, mice develop a Th1-polarized response. This regulatory effect of galectin-3 on Th development is exerted at both the dendritic cell and T cell levels. Our studies suggest that galectin-3 may play an important role in the acute phase of human atopic dermatitis.",2009-03-01,https://www.semanticscholar.org/paper/aae0cd8b144dff9e0192a3a405f644c32b3da7c0,American Journal of Pathology
3640,Possible Directions for C++,,1993-09-07,https://www.semanticscholar.org/paper/37898643665993ab75c50e6e06c2cebe6626aa68,C++ Workshop
2701,Introduction to the special issue on virtual reality software and technology,"We are delighted to present this special issue containing extended versions of four papers selected from the 1994 Virtual Reality Software & Technology (VRST) conference, held August 23-26, 1994, in Singapore. Techniques and tools for interacting with virtual environments are at the core of research and development efforts around the world, and these articles provide a sampling of the latest advances. (A companion issue of Communications of the ACM, containing articles that address a broader-based audience, is also being edited.) This issue begins with Alan Wexelblat’s article on the use of gestures in multimodal applications, “An Approach to Natural Gesture in Virtual Environments.” His work uses empty-hand, continuous gestures, much like the gestures we use in our everyday communication with people, for interaction with virtual worlds. These gestures are decomposed into feature-based descriptions that serve as an intermediate language for gestural input. Wexelblat’s prototype system uses several tracking devices attached to various parts of the user’s body to collect raw data about body movement. The data are filtered and converted into body-relative information and sent to a gesture analyzer. The analyzer converts the input to a higher-level description and forwards it to the gesture interpreter, which assigns meaning to gestures based on the application and the interaction context. “Taking Steps: The Influence of a Walking Technique on Presence in Virtual Reality,” by Mel Slater, Martin Usoh, and Anthony Steed, presents an interactive technique for moving through immersive virtual worlds. When within the range of the electromagnetic tracking device used, the user moves around by actually walking. To cover a virtual distance that is larger than the tracker’s physical range, the user walks in place. Walking in place causes the user to move forward in the direction of gaze. The authors also apply a modified version of this interaction technique to climbing up and down steps and ladders. The implementation uses a feed-forward neural network to construct a pattern recognize that detects whether the user is walking in place or doing something else. User trials show that the walking technique leads to a greater sense of presence, even though the users preferred pointing-based navigation techniques. In “HoloSketch: A Virtual Reality Sketching/Animation Tool,” Michael Deering discusses the user interface of a true 3D drawing system based on a “fish-tank VR” model: a high-resolution CRT, head-tracked, liquid-crystal stereo eyewear and a hand-held 3D wand interaction device. Unlike the majority of virtual environments, Deering’s software and hardware infrastructure allows virtual objects to be positioned with subcentimeter accuracy, which is maintained even as the user’s head moves. This makes it possible for the 3D wand to be used as a direct pointer, rather than as an indirect 3D",1995-09-01,https://www.semanticscholar.org/paper/9581b6e532ca8f59a1c71f5f0ab84d63e098a71f,TCHI
987,An Evaluation of the in vivo Safety of Nonporous Silica Nanoparticles: Ocular Topical Administration versus Oral Administration,,2017-08-15,https://www.semanticscholar.org/paper/b8b76a3e45aa224944b8010a3054ebb0c1ec4297,Scientific Reports
171,An Axiomatic Approach to Block Rewards,"Proof-of-work blockchains reward each miner for one completed block by an amount that is, in expectation, proportional to the number of hashes the miner contributed to the mining of the block. Is this proportional allocation rule optimal? And in what sense? And what other rules are possible? In particular, what are the desirable properties that any ""good"" allocation rule should satisfy? To answer these questions, we embark on an axiomatic theory of incentives in proof-of-work blockchains at the time scale of a single block. We consider desirable properties of allocation rules including: symmetry; budget balance (weak or strong); sybil-proofness; and various grades of collusion-proofness. We show that Bitcoin's proportional allocation rule is the unique allocation rule satisfying a certain system of properties, but this does not hold for slightly weaker sets of properties, or when the miners are not risk-neutral. We also point out that a rich class of allocation rules can be approximately implemented in a proof-of-work blockchain.",2019-09-23,https://www.semanticscholar.org/paper/55e75c05da175841cdf096d0584a6202ccf619ee,Conference on Advances in Financial Technologies
2020,Manufacturing Intelligence to Exploit the Value of Production and Tool Data to Reduce Cycle Time,"Cycle time reduction is crucial for semiconductor wafer fabrication companies to maintain competitive advantages as the semiconductor industry is becoming more dynamic and changing faster. According to Little's Law, while maintaining the same throughput level, the reduction in Work-in-Process (WIP) will result in cycle time reduction. On one hand, the existing queueing models for predicting the WIP of tool sets in wafer fabrication facilities (fab) have limitations in real settings. On the other hand, little research has been done to predict the WIP of tool sets with tool dedication and waiting time constraint so as to control the corresponding WIP levels of various tool sets to reduce cycle time without affecting throughput. This study aims to fill the gap by proposing a manufacturing intelligence (MI) approach based on neural networks (NNs) to exploit the value of the wealthy production data and tool data for predicting the WIP levels of the tool sets for cycle time reduction. To validate this approach, empirical data were collected and analyzed in a leading semiconductor company. The comparison results have shown practical viability of this approach. Furthermore, the proposed approach can identify and improve the critical input factors for reducing the WIP to reduce cycle time in a fab.",,https://www.semanticscholar.org/paper/e5627f0efb618d7fb2766135c6fed27a7bef3b41,IEEE Transactions on Automation Science and Engineering
660,The Gianturco-Roubin flexible intracoronary stent: clinical application and initial results.,,1991-11-01,https://www.semanticscholar.org/paper/ab876e43c6d76bf171ae9163e8c36b8e529b3add,Indian Heart Journal
560,An Algorithm for Shortest-Path Motion in Three Dimensions,,1985-06-12,https://www.semanticscholar.org/paper/867bb6f7ab1728d76eeb6d9974d6d2f3a96466e9,Information Processing Letters
3490,Minimizing average completion time in the presence of release dates,,1998-06-01,https://www.semanticscholar.org/paper/6ba26104c8c73f384b614fcdada123de4778af8f,Mathematical programming
1970,Hierarchical indices to detect equipment condition changes with high dimensional data for semiconductor manufacturing,,2014-10-01,https://www.semanticscholar.org/paper/debebc5cfdfeb3232bbf633eb352432c7478b53a,Journal of Intelligent Manufacturing
3037,Teaching operating systems using code review,"Learning about operating systems often involves modifying a large and complex code base. Grading student projects can be difficult and time consuming, yet students often do not learn from their programming errors and struggle to understand core operating system concepts. We present GradeBoard, a code review system designed to simplify grading for instructors and enable students to understand and learn from their errors. GradeBoard provides an easy-to-use Web interface that allows instructors to annotate student code submissions with grading comments and scores, and students to discuss the comments and scores with instructors. GradeBoard presents student code changes with syntax highlighting and lets users collapse or expand code sections to provide a desired level of context, making it easier to read and understand student programming project submissions. Comments and scores are easily identifiable by visual cues, improving interaction between instructors and students. We have deployed and used GradeBoard in a large operating systems course involving Linux kernel programming projects. GradeBoard provided robust, easy-to-use functionality for reviewing Linux kernel code changes, improved the instructional staff grading experience, and over 90% of students surveyed indicated that GradeBoard improved their understanding of the kernel programming projects better than other alternatives.",2014-03-05,https://www.semanticscholar.org/paper/419bcc6cde93454cf38d641990b816e3ef075b14,Technical Symposium on Computer Science Education
1163,Search for associated W and Higgs Boson production in pp[over ] collisions at sqrt[s]=1.96 TeV.,"We present results of a search for WH-->lnubb[over ] production in pp[over ] collisions based on the analysis of 1.05 fb;{-1} of data collected by the D0 experiment at the Fermilab Tevatron, using a neural network for separating the signal from backgrounds. No signal-like excess is observed, and we set 95% C.L. upper limits on the WH production cross section multiplied by the branching ratio for H-->bb[over ] for Higgs boson masses between 100 and 150 GeV. For a mass of 115 GeV, we obtain an observed (expected) limit of 1.5 (1.4) pb, a factor of 11.4 (10.7) times larger than the standard model prediction.",,https://www.semanticscholar.org/paper/81a3201a97071bf492a880aa49bd34b155cc5f53,Physical Review Letters
9,Fast and Accurate Time-Series Clustering,"The proliferation and ubiquity of temporal data across many disciplines has generated substantial interest in the analysis and mining of time series. Clustering is one of the most popular data-mining methods, not only due to its exploratory power but also because it is often a preprocessing step or subroutine for other techniques. In this article, we present k-Shape and k-MultiShapes (k-MS), two novel algorithms for time-series clustering. k-Shape and k-MS rely on a scalable iterative refinement procedure. As their distance measure, k-Shape and k-MS use shape-based distance (SBD), a normalized version of the cross-correlation measure, to consider the shapes of time series while comparing them. Based on the properties of SBD, we develop two new methods, namely ShapeExtraction (SE) and MultiShapesExtraction (MSE), to compute cluster centroids that are used in every iteration to update the assignment of time series to clusters. k-Shape relies on SE to compute a single centroid per cluster based on all time series in each cluster. In contrast, k-MS relies on MSE to compute multiple centroids per cluster to account for the proximity and spatial distribution of time series in each cluster. To demonstrate the robustness of SBD, k-Shape, and k-MS, we perform an extensive experimental evaluation on 85 datasets against state-of-the-art distance measures and clustering methods for time series using rigorous statistical analysis. SBD, our efficient and parameter-free distance measure, achieves similar accuracy to Dynamic Time Warping (DTW), a highly accurate but computationally expensive distance measure that requires parameter tuning. For clustering, we compare k-Shape and k-MS against scalable and non-scalable partitional, hierarchical, spectral, density-based, and shapelet-based methods, with combinations of the most competitive distance measures. k-Shape outperforms all scalable methods in terms of accuracy. Furthermore, k-Shape also outperforms all non-scalable approaches, with one exception, namely k-medoids with DTW, which achieves similar accuracy. However, unlike k-Shape, this approach requires tuning of its distance measure and is significantly slower than k-Shape. k-MS performs similarly to k-Shape in comparison to rival methods, but k-MS is significantly more accurate than k-Shape. Beyond clustering, we demonstrate the effectiveness of k-Shape to reduce the search space of one-nearest-neighbor classifiers for time series. Overall, SBD, k-Shape, and k-MS emerge as domain-independent, highly accurate, and efficient methods for time-series comparison and clustering with broad applications.",2017-06-01,https://www.semanticscholar.org/paper/5242b843cd4543b575bd093d63711ef6cc8a9bad,ACM Transactions on Database Systems
3386,Queuing Safely for Elevator Systems Amidst a Pandemic,"The requirement of social distancing during the COVID-19 pandemic has presented significant challenges for high-rise buildings, which heavily rely on elevators for vertical transportation. In particular, the need for social distancing has reduced elevator capacity by by at least half and as much as two-thirds the normal amount. This reduction is a serious concern, as reduced elevator capacities cause large queues to build up in lobbies, which makes social distancing a challenge. The objective of this study was to propose simple interventions for safely managing the elevator queues that drastically reduce the waiting time and length of lobby queues. We use mathematical modeling, epidemiological principles, and simulation to design and evaluate our interventions. The key idea is to explicitly or implicitly group passengers that are going to the same floor into the same elevator as much as possible. In the Cohorting intervention, we attempt to find passengers going to the same floor as the first person in the queue. In the Queue Splitting intervention, we create a different queue for different groups of floors. Based on simulation studies, Cohorting and Queue Splitting can significantly reduce queue length and wait time, while also maintaining safety from viral transmission in otherwise crowded elevators, building lobbies, and entrances. The interventions we propose do not require programming the elevators, and rely on using signage and/or a queue manager to guide passengers.",2020-12-21,https://www.semanticscholar.org/paper/cb2bd44198aff5dcdf0fbf037c2261ccc7d831c7,Social Science Research Network
3693,Muscles in Action,"Human motion is created by, and constrained by, our muscles. We take a first step at building computer vision methods that represent the internal muscle activity that causes motion. We present a new dataset, Muscles in Action (MIA), to learn to incorporate muscle activity into human motion representations. The dataset consists of 12.5 hours of synchronized video and surface electromyography (sEMG) data of 10 subjects performing various exercises. Using this dataset, we learn a bidirectional representation that predicts muscle activation from video, and conversely, reconstructs motion from muscle activation. We evaluate our model on in-distribution subjects and exercises, as well as on out-of-distribution subjects and exercises. We demonstrate how advances in modeling both modalities jointly can serve as conditioning for muscularly consistent motion generation. Putting muscles into computer vision systems will enable richer models of virtual humans, with applications in sports, fitness, and AR/VR.",2022-12-05,https://www.semanticscholar.org/paper/326cbf7c9891dff720bd1f57bcbe6dc0b5d327a1,arXiv.org
2883,Modulation of functional properties of galectin-3 by monoclonal antibodies binding to the non-lectin domains.,"Galectin-3 is a member of a newly defined family of animal lectins, which is composed of three domains: a small amino-terminal domain, a domain containing repeating elements, and a carboxyl-terminal domain containing the carbohydrate-recognition site. Various functions have been described or proposed for this lectin, and it appears that galectin-3 has diverse roles. Murine monoclonal antibodies (MAbs) have been generated from mice hyperimmunized with recombinant human galectin-3 or galectin-3C (the carboxyl-terminal domain), and seven MAbs have been characterized in detail. All MAbs generated against the intact galectin-3 recognize the amino-terminal region of the molecule, as demonstrated by ELISA and immunoblotting using recombinant galectin-3C and galectin-3NR, which contains the amino-terminal domain and all the repeating elements. Their epitopes were all found to be within the first 45 amino acids of galectin-3, as determined by using galectin-3 mutants with a truncated amino-terminal region. However, these MAbs were found to profoundly modulate the lectin activities of galectin-3. The MAb B2C10 inhibited (i) the binding of 125I-labeled galectin-3 to IgE coated on microtiter plates; (ii) the galectin-3's hemagglutination activity; and (iii) galectin-3-induced superoxide production by human neutrophils. Other MAbs, especially A3A12, caused marked potentiation of these activities. The results support our model that the lectin function of galectin-3 is influenced by protein homodimerization resulting from self-association of the amino-terminal region of the molecule. The potentiating activities of some MAbs are probably due to facilitation of dimerization galectin-3, and the inhibitory activity of MAb B2C10 is probably the result of its disruption of the self-association process.",1996-05-14,https://www.semanticscholar.org/paper/afa5ad4f05c9b1c75e4556ca7586f0fe15e004b3,Biochemistry
1482,Progress report on the SLD cerenkov ring imaging detector,"We describe test beam results from a prototype cerenkov Ring Imaging Detector (CRID) for the SLD experiment at the SLAC Linear Collider (SLC). The system includes both liquid and gas radiators, a long drift box containing gaseous TMAE and a proportional wire chamber with charge division readout. Measurements of the multiplicity and detection resolution of cerenkov photons, from both radiators are presented. Various design aspects of a new engineering prototype, currently under construction, are discussed and recent R&D results relevant to this effort are reported.",1987-02-01,https://www.semanticscholar.org/paper/a0b7120ba33a8d780a39ded7caee1176329a9702,IEEE Transactions on Nuclear Science
27,Popularity-guided top-k extraction of entity attributes,"Recent progress in information extraction technology has enabled a vast array of applications that rely on structured data that is embedded in natural-language text. In particular, the extraction of concepts from the Web---with their desired attributes---is important to provide applications with rich, structured access to information. In this paper, we focus on an important family of concepts, namely, entities (e.g., people or organizations) and their attributes, and study how to efficiently and effectively extract them from Web-accessible text documents. Unfortunately, information extraction over the Web is challenging for both quality and efficiency reasons. Regarding quality, many sources on the Web contain misleading or invalid information; furthermore, extraction systems often return incorrect data. Regarding efficiency, information extraction is a time-consuming process, often involving expensive text-processing steps. We present a top-k extraction processing approach that addresses both the quality and efficiency challenges: for each entity and attribute of interest, we return the top-k values of the attribute for the entity according to a scoring function for extracted attribute values. This scoring function weighs the extraction confidence from individual documents, as well as the ""importance"" of the documents where the information originates. We define the document importance in terms of entity-specific document ""popularity"" statistics from a major search engine. Overall, our top-k extraction processing approach manages to identify the top attribute values for the entities of interest efficiently, as we demonstrate with a large-scale experimental evaluation over real-life data.",2010-06-06,https://www.semanticscholar.org/paper/d55e2998548a8fc202413ebe473f5e74ad69e800,International Workshop on the Web and Databases
3223,"Citizen Science in Schools: Students Collect Valuable Mammal Data for Science, Conservation, and Community Engagement","Citizen science has been touted as an effective means to collect large-scale data while engaging the public. We demonstrate that children as young as 9 years old can collect valuable mammal monitoring data using camera traps while connecting with nature and learning through their own scientific discoveries. Indian, Kenyan, Mexican, and American students used camera traps near their schools and detected 13–37 species, all of which were verified by professionals. These data describe rich mammal faunas near schools, sometimes surpassing nearby protected areas, and included five endangered species. Ninety-four percent of the camera traps were set in accordance with scientific protocols, and the teachers reported the experience as highly engaging for their students. Furthermore, the generated photos and results had community-wide impacts involving local politicians, community members, and the media. We show that children can run sensors to contribute valid scientific data important for conservation and research.",2018-12-12,https://www.semanticscholar.org/paper/35c7d3265161ac30cb359d0ca243b66593656544,BioScience
1889,Pathways and barriers to circularity in food systems,,2019-04-01,https://www.semanticscholar.org/paper/0c4a6610a5f68f2d582cf9367ff5c3ba5c4f8386,"Resources, Conservation and Recycling"
3690,Private Multiparty Perception for Navigation,"We introduce a framework for navigating through cluttered environments by connecting multiple cameras together while simultaneously preserving privacy. Occlusions and obstacles in large environments are often challenging situations for navigation agents because the environment is not fully observable from a single camera view. Given multiple camera views of an environment, our approach learns to produce a multiview scene representation that can only be used for navigation, provably preventing one party from inferring anything beyond the output task. On a new navigation dataset that we will publicly release, experiments show that private multiparty representations allow navigation through complex scenes and around obstacles while jointly preserving privacy. Our approach scales to an arbitrary number of camera viewpoints. We believe developing visual representations that preserve privacy is increasingly important for many applications such as navigation.",2022-12-02,https://www.semanticscholar.org/paper/17aaad12138347e3f1a40b6be69b892ad455a00c,Neural Information Processing Systems
2326,Identification of a subgroup of myelodysplastic patients with a neutrophil stimulation‐signalling defect,"Summary. f‐Met‐Leu‐Phe‐stimulated luminol‐enhanced chemiluminescence was found to be repeatedly defective in some MDS patients. This defect was not attributed to myeloperoxidase deficiency, nor to a defect in NADPH oxidase function, because PMA chemiluminescence was found to be normal in these individuals. An arbitrary value of 7 mV (half the mean control value) was chosen to subdivide the group: MDS patients with values <7 mV had a mean f‐Met‐Leu‐Phe chemiluminescence response of 2·5±0·5 compared to MDS patients with values <7 mV who had a mean response of 15·6±1·6mV, P<0·01 (healthy controls 14±2 mV). The characteristics of the f‐Met‐Leu‐Phe receptor and initial calcium flux results suggested that the receptor itself was normal in number and function in low f‐Met‐Leu‐Phe responders. The rate of superoxide generation, which is calcium‐dependent, was also found to be in the normal range in low f‐Met‐Leu‐Phe responders, although total superoxide production was reduced in some of these patients.",1994-04-01,https://www.semanticscholar.org/paper/8baadb7929f6e6ebef1706026240028618ee38be,British Journal of Haematology
3110,Breaking the Ties That Bind: Application Isolation and Migration,,,https://www.semanticscholar.org/paper/c66d9503817cacab0238d71b73adacb17f9d82bd,Login: The Usenix Magazine
3055,Record and transplay: partial checkpointing for replay debugging across heterogeneous systems,"Software bugs that occur in production are often difficult to reproduce in the lab due to subtle differences in the application environment and nondeterminism. To address this problem, we present Transplay, a system that captures production software bugs into small per-bug recordings which are used to reproduce the bugs on a completely different operating system without access to any of the original software used in the production environment. Transplay introduces partial checkpointing, a new mechanism that efficiently captures the partial state necessary to reexecute just the last few moments of the application before it encountered a failure. The recorded state, which typically consists of a few megabytes of data, is used to replay the application without requiring the specific application binaries, libraries, support data, or the original execution environment. Transplay integrates with existing debuggers to provide standard debugging facilities to allow the user to examine the contents of variables and other program state at each source line of the application's replayed execution. We have implemented a Transplay prototype that can record unmodified Linux applications and replay them on different versions of Linux as well as Windows. Experiments with several applications including Apache and MySQL show that Transplay can reproduce real bugs and be used in production with modest recording overhead.",,https://www.semanticscholar.org/paper/ebdbd137674e8f1d077228c6cb9be23c213295aa,PERV
3406,"Simultaneously Load Balancing for Every p-norm, With Reassignments","This paper investigates the task of load balancing where the objective function is to minimize the p-norm of loads, for p\geq 1, in both static and incremental settings. We consider two closely related load balancing problems. In the bipartite matching problem we are given a bipartite graph G=(C\cup S, E) and the goal is to assign each client c\in C to a server s\in S so that the p-norm of assignment loads on S is minimized. 
In the graph orientation problem the goal is to orient (direct) the edges of a given undirected graph while minimizing the p-norm of the out-degrees. The graph orientation problem is a special case of the bipartite matching problem, but less complex, which leads to simpler algorithms. 
 
For the graph orientation problem we show that the celebrated Chiba-Nishizeki peeling algorithm provides a simple linear time load balancing scheme whose output is an orientation that is 2-competitive, in a p-norm sense, for all p\geq 1. For the bipartite matching problem we first provide an offline algorithm that computes an optimal assignment. We then extend this solution to the online bipartite matching problem with reassignments, where vertices from C arrive in an online fashion together with their corresponding edges, and we are allowed to reassign an amortized O(1) vertices from C each time a new vertex arrives. In this online scenario we show how to maintain a single assignment that is 8-competitive, in a p-norm sense, for all p\geq 1.",2017-11-01,https://www.semanticscholar.org/paper/fdda90a3b02a54439a1354676f28d64b6872d9f5,Information Technology Convergence and Services
2756,Expert systems and hypertext,"The relationships between expert systems and hypertext are many and varied. Expert systems have been proposed as authoring environments and navigational aids for hypertext systems and hypertext systems have been proposed as knowledge representation vehicles and rule editors for expert systems. These interrelationships should come as no surprise given the similarity of intellectual challenges confronting investigators in the two respective disciplines. In each, issues of knowledge representation, control of inference, and computational complexity are central. This panel will attempt to explore some of these overlapping issues from the perspective of both basic research and commercial applications. Although some primary data will be presented, the session will be more one of making links between other Hypertext 89 Proceedings presentations and a larger body of work. Audience participation will be strongly encouraged.",1989-11-01,https://www.semanticscholar.org/paper/989cce99996943f66eb0aa2ce390974fe591ff6b,UK Conference on Hypertext
1882,Minimax Optimization for Recipe Management in High-Mixed Semiconductor Lithography Process,"This article addresses the application of minimax optimization in the control design of complex dynamic systems of the semiconductor manufacturing. We highlight the main challenge in the control system of high-mixed wafer fabrication during the photolithography process called overlay control. In the semiconductor photolithography process, the sophisticated and high-mixed setting is generated by multiple recipe adjustments for the single scanner device. The high complexity will be moderated if there is a communication interface among the process variables. We design a communication protocol for the high-mixed photolithography process for overlay control. The proposed system is designed on the basis of the recipe management system for a distinct batches of recipes. The focal point of switching recipes performs as a communication hop, where aligning recipes together make a multihop communication system for recipe management. The proposed multihop communication system is optimized by the minimax decision rule to select the best parameter setting for each recipe and boost the overlay compensation.",2020-08-01,https://www.semanticscholar.org/paper/3832008bdcec73b20b1779faa9d5900981b22375,IEEE Transactions on Industrial Informatics
2830,Lack of Galectin-3 Drives Response to Paracoccidioides brasiliensis toward a Th2-Biased Immunity,"There is recent evidence that galectin-3 participates in immunity to infections, mostly by tuning cytokine production. We studied the balance of Th1/Th2 responses to P. brasiliensis experimental infection in the absence of galectin-3. The intermediate resistance to the fungal infection presented by C57BL/6 mice, associated with the development of a mixed type of immunity, was replaced with susceptibility to infection and a Th2-polarized immune response, in galectin-3-deficient (gal3−/−) mice. Such a response was associated with defective inflammatory and delayed type hypersensitivity (DTH) reactions, high IL-4 and GATA-3 expression and low nitric oxide production in the organs of infected animals. Gal3−/− macrophages exhibited higher TLR2 transcript levels and IL-10 production compared to wild-type macrophages after stimulation with P. brasiliensis antigens. We hypothesize that, during an in vivo P. brasiliensis infection, galectin-3 exerts its tuning role on immunity by interfering with the generation of regulatory macrophages, thus hindering the consequent Th2-polarized type of response.",2009-02-20,https://www.semanticscholar.org/paper/34c11fdb303daf9abe3228921665b3429b11a90f,PLoS ONE
3447,"Grouped distributed queues: distributed queue, proportional share multiprocessor scheduling","We present Grouped Distributed Queues (GDQ), the first proportional share scheduler for multiprocessor systems that scales well with a large number of processors and processes. GDQ uses a distributed queue architecture, and achieves accurate proportional fairness scheduling with only O(1) scheduling overhead. GDQ takes a novel approach to distributed queuing: instead of creating per-processor queues that need to be constantly balanced to achieve any measure of proportional sharing fairness, GDQ uses a simple grouping strategy to organize processes into groups based on similar processor time allocation rights, and then assigns processors to groups based on aggregate group shares. Group membership of processes is static, and fairness is achieved by dynamically migrating processors among groups. The set of processors working on a group use simple, low-overhead round-robin queues, while processor reallocation among groups is achieved using a new multiprocessor adaptation of Weighted Fair Queuing. By commoditizing processors and decoupling their allocation from process scheduling, GDQ provides, with only constant scheduling overhead, fairness within a constant of the ideal generalized processor sharing model for process weights with a fixed upper bound. We have implemented GDQ in Linux and measured its performance. Our experimental results show that GDQ has low overhead and scales well with the number of processors and processes.",2006-07-23,https://www.semanticscholar.org/paper/0e3f0e1278b314a2f5e23ef06c71c0e295e32b48,ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing
3099,Highly Reliable Mobile Desktop Computing in Your Pocket,"We present DeskPod, a portable system that provides a highly reliable desktop computing environment for mobile users by leveraging rapid improvements in capacity, cost, and size of portable storage devices. DeskPod enables a user's live computing environment to be suspended to portable storage, carried around, easily copied for fault-resilience, and resumed from the storage device to provide the user with the same persistent, personalized computing environment on another computer DeskPod achieves this by providing a virtualization and checkpoint/restart mechanism that decouples a desktop computing environment from any single hardware device so that it can be stored and executed anywhere, improving desktop computing reliability by eliminating a potential single point of failure. We have implemented a Linux DeskPod prototype and demonstrate its ability to quickly suspend and resume desktop sessions, enabling a seamless mobile experience",2006-09-17,https://www.semanticscholar.org/paper/add36c24029f050c751cb7f030de802e89795d09,Annual International Computer Software and Applications Conference
2368,Luminol- and lucigenin-dependent chemiluminescence of neutrophils: role of degranulation.,"The role of myeloperoxidase in luminol- and lucigenin-dependent chemiluminescence of stimulated human neutrophils has been investigated using purified myeloperoxidase and anti-(human myeloperoxidase) antiserum. This antiserum has been used as a specific enzyme inhibitor to assess myeloperoxidase-dependent neutrophil functions in single preparations of cells, thus overcoming the limitations inherent in other approaches using non-specific haem-inhibitors or myeloperoxidase-deficient neutrophils. The results show that luminol-dependent chemiluminescence is largely dependent on both oxidase activity and degranulation (of myeloperoxidase), while lucigenin monitors oxidase activity independently of the extent of degranulation. Since oxidase activation can occur in the absence of degranulation, assays utilizing luminol-dependent chemiluminescence to measure oxidant generation by stimulated neutrophils should include saturating levels of exogenous myeloperoxidase to overcome this problem.",,https://www.semanticscholar.org/paper/06b0010fc3b5eed7c3dd4eb3847c16c9bc07c588,Journal of clinical & laboratory immunology
2828,Lack of galectin-3 alleviates trypanosomiasis-associated anemia of inflammation.,,2010-09-01,https://www.semanticscholar.org/paper/89e04f8a4851ef9c52e9d15f4a5905c1abcace45,Immunobiology
3583,Practical and Verifiable C++ Dynamic Cast for Hard Real-Time Systems,"The dynamic cast operation allows flexibility in the design and use of data management facilities in object-oriented programs. Dynamic cast has an important role in the implementation of the Data Management Services (DMS) of the Mission Data System Project (MDS), the Jet Propulsion Laboratory’s experimental work for providing a state-based and goal-oriented unified architecture for testing and development of mission software. DMS is responsible for the storage and transport of control and scientific data in a remote autonomous spacecraft. Like similar operators in other languages, the C++ dynamic cast operator does not provide the timing guarantees needed for hard real-time embedded systems. In a recent study, Gibbs and Stroustrup (G&S) devised a dynamic cast implementation strategy that guarantees fast constant-time performance. This paper presents the definition and application of a cosimulation framework to formally verify and evaluate the G&S fast dynamic casting scheme and its applicability in the Mission Data System DMS application. We describe the systematic process of model-based simulation and analysis that has led to performance improvement of the G&S algorithm’s heuristics by about a factor of 2. In this work we introduce and apply a library for extracting semantic information from C++ source code that helps us deliver a practical and verifiable implementation of the fast dynamic casting algorithm.",2008-12-31,https://www.semanticscholar.org/paper/86e1515db53dcca67d1d9f62f26b8d5449ab0d60,Journal of Computing Science and Engineering
259,Modeling Social Networks through User Background and Behavior,,2011-05-27,https://www.semanticscholar.org/paper/d41417d03bc623c50f5c2bd7887eb4377ea9622a,Workshop on Algorithms and Models for the Web-Graph
3139,Optimal linear interpolation coding for server-based computing,"Due to its reduced administrative costs and better resource utilization, server-based computing (SBC) is becoming a popular approach for delivering computational services across a network. In SBC, all application processing is done on servers while only screen updates are sent to clients. While many SBC encoding techniques have been explored for transmitting screen updates efficiently, existing approaches do not effectively support multimedia applications. To address this problem, we propose optimal linear interpolation (OLI), a new pixel-based SBC screen update coding algorithm. With OLI, the server selects and transmits only a small sample of pixels to represent a screen update. The client recovers the complete screen update from these samples using piecewise linear interpolation to achieve the best visual quality. OLI can be used to provide lossless or lossy compression for an adaptive trade-off between network bandwidth and processing time requirements. We further propose and evaluate 2D lossless linear interpolation (2DLI), which is based on OLI but additionally provides lower encoding complexity for lossless compression. Our experimental results show that when compared with other compression methods, 2DLI provides good data compression ratio with modest computational overhead, for both servers and clients.",2002-08-07,https://www.semanticscholar.org/paper/450fff5448f51244a5a95c854036d486afd28e20,2002 IEEE International Conference on Communications. Conference Proceedings. ICC 2002 (Cat. No.02CH37333)
1742,Truncation-free Online Variational Inference for Bayesian Nonparametric Models,"We present a truncation-free online variational inference algorithm for Bayesian nonparametric models. Unlike traditional (online) variational inference algorithms that require truncations for the model or the variational distribution, our method adapts model complexity on the fly. Our experiments for Dirichlet process mixture models and hierarchical Dirichlet process topic models on two large-scale data sets show better performance than previous online variational inference algorithms.",,https://www.semanticscholar.org/paper/05a55d4d3935c9517c91deef8df76447cb93b7f4,Neural Information Processing Systems
3503,Improved Scheduling Algorithms for Minsum Criteria,,1996-07-08,https://www.semanticscholar.org/paper/1b45bdcb7cb3a51dd3fc952fc1795270a9b176b1,"International Colloquium on Automata, Languages and Programming"
3369,The Socially Constructive Aspects of Outside Agents in Community Decision-Making in a Rural Area,,1975-07-01,https://www.semanticscholar.org/paper/2cbb9391b9b157ecb835ece6634aab5fa2f2f9f7,The Journal of Sociology &amp; Social Welfare
1556,Markovian Score Climbing: Variational Inference with KL(p||q),"Modern variational inference (VI) uses stochastic gradients to avoid intractable expectations, enabling large-scale probabilistic inference in complex models. VI posits a family of approximating distributions $q$ and then finds the member of that family that is closest to the exact posterior $p$. Traditionally, VI algorithms minimize the ""exclusive KL"" KL$(q\|p)$, often for computational convenience. Recent research, however, has also focused on the ""inclusive KL"" KL$(p\|q)$, which has good statistical properties that makes it more appropriate for certain inference problems. This paper develops a simple algorithm for reliably minimizing the inclusive KL. Consider a valid MCMC method, a Markov chain whose stationary distribution is $p$. The algorithm we develop iteratively samples the chain $z[k]$, and then uses those samples to follow the score function of the variational approximation, $\nabla \log q(z[k])$ with a Robbins-Monro step-size schedule. This method, which we call Markovian score climbing (MSC), converges to a local optimum of the inclusive KL. It does not suffer from the systematic errors inherent in existing methods, such as Reweighted Wake-Sleep and Neural Adaptive Sequential Monte Carlo, which lead to bias in their final estimates. In a variant that ties the variational approximation directly to the Markov chain, MSC further provides a new algorithm that melds VI and MCMC. We illustrate convergence on a toy model and demonstrate the utility of MSC on Bayesian probit regression for classification as well as a stochastic volatility model for financial data.",2020-03-23,https://www.semanticscholar.org/paper/464c366ed20aa4c3fa57d71332dceb460586ed40,Neural Information Processing Systems
2934,Clinically-relevant cell type cross-talk identified from a human lung tumor microenvironment interactome,"Tumors comprise a complex microenvironment of interacting malignant and stromal cell types. Much of our understanding of the tumor microenvironment comes from in vitro studies isolating the interactions between malignant cells and a single stromal cell type, often along a single pathway. To develop a deeper understanding of the interactions between cells within human lung tumors we performed RNA-seq profiling of flow-sorted malignant cells, endothelial cells, immune cells, fibroblasts, and bulk cells from freshly resected human primary non-small-cell lung tumors. We mapped the cell-specific differential expression of prognostically-associated secreted factors and cell surface genes, and computationally reconstructed cross-talk between these cell types to generate a novel resource we call the Lung Tumor Microenvironment Interactome (LTMI). Using this resource, we identified and validated a prognostically unfavorable influence of Gremlin-1 production by fibroblasts on proliferation of malignant lung adenocarcinoma cells. We also found a prognostically favorable association between infiltration of mast cells and less aggressive tumor cell behavior. These results illustrate the utility of the LTMI as a resource for generating hypotheses concerning tumor-microenvironment interactions that may have prognostic and therapeutic relevance. Summary RNA-seq profiling of sorted populations from primary lung cancer samples identifies prognostically relevant cross-talk between cell types in the tumor microenvironment.",2019-05-30,https://www.semanticscholar.org/paper/5d03bd170267fc0623335423822701667f8ad853,bioRxiv
1075,Nuclear-recoil energy scale in CDMS II silicon dark-matter detectors,,2018-03-07,https://www.semanticscholar.org/paper/c5e52a8c04b1636375263c9bf74e90a4789ea80f,"Nuclear Instruments and Methods in Physics Research Section A : Accelerators, Spectrometers, Detectors and Associated Equipment"
1821,"Statistical Network Analysis: Models, Issues, and New Directions - ICML 2006 Workshop on Statistical Network Analysis, Pittsburgh, PA, USA, June 29, 2006, Revised Selected Papers",,,https://www.semanticscholar.org/paper/46c825c9c5c6ce278671419ce76ea9c71d4dcbc7,SNA@ICML
374,A BGP-based mechanism for lowest-cost routing,,2002-07-21,https://www.semanticscholar.org/paper/9697f782ee1fb60ec9c5030422eea7f7b5f8f628,ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing
3008,A Secure and Formally Verified Linux KVM Hypervisor,"Commodity hypervisors are widely deployed to support virtual machines (VMs) on multiprocessor hardware. Their growing complexity poses a security risk. To enable formal verification over such a large codebase, we introduce microverification, a new approach that decomposes a commodity hypervisor into a small core and a set of untrusted services so that we can prove security properties of the entire hypervisor by verifying the core alone. To verify the multiprocessor hypervisor core, we introduce security-preserving layers to modularize the proof without hiding information leakage so we can prove each layer of the implementation refines its specification, and the top layer specification is refined by all layers of the core implementation. To verify commodity hypervisor features that require dynamically changing information flow, we introduce data oracles to mask intentional information flow. We can then prove noninterference at the top layer specification and guarantee the resulting security properties hold for the entire hypervisor implementation. Using microverification, we retrofitted the Linux KVM hypervisor with only modest modifications to its codebase. Using Coq, we proved that the hypervisor protects the confidentiality and integrity of VM data, while retaining KVM’s functionality and performance. Our work is the first machine-checked security proof for a commodity multiprocessor hypervisor.",2021-05-01,https://www.semanticscholar.org/paper/66537bf2100089307bb0baa0dc6df11a04c62531,IEEE Symposium on Security and Privacy
1840,Dynamic topic models,"A family of probabilistic time series models is developed to analyze the time evolution of topics in large document collections. The approach is to use state space models on the natural parameters of the multinomial distributions that represent the topics. Variational approximations based on Kalman filters and nonparametric wavelet regression are developed to carry out approximate posterior inference over the latent topics. In addition to giving quantitative, predictive models of a sequential corpus, dynamic topic models provide a qualitative window into the contents of a large document collection. The models are demonstrated by analyzing the OCR'ed archives of the journal Science from 1880 through 2000.",2006-06-25,https://www.semanticscholar.org/paper/2dd53efa74850c6b60bc7bb0dcfca049a4a71474,International Conference on Machine Learning
123,Precision and recall of GlOSS estimators for database discovery,"Online information vendors and the Internet together offer thousands of text databases from which a user may choose for a given information need. This paper presents a framework for and analyses a solution to this problem, which we call the text-database discovery problem. Our solution is to build a service that can suggest potentially good databases to search. A user's query goes through two steps: first, the query is presented to the GlOSS server (Glossary-Of-Servers Server) to select a set of promising databases to search. Secondly, the query is actually evaluated in the chosen databases. GlOSS gives a hint of what databases might be useful for the user's query, based on word-frequency information for each database. This information indicates how many documents in each database actually contain a keyword, for each field designator. To evaluate the set of databases that GlOSS returns for a given query, we present a framework based on the precision and recall metrics of information retrieval theory. We define metrics for the text-database discovery problem. We further extend our framework by offering different definitions for a ""relevant database"". We have performed experiments using query traces from the FOLIO library information retrieval system, involving six databases available through FOLIO. The results obtained for different variants of GlOSS are very promising. Even though GlOSS keeps a small amount of information about the contents of the available databases, this information proved to be sufficient to produce very useful hints on where to search.<<ETX>>",1994-10-01,https://www.semanticscholar.org/paper/3c0c8394cbbc2fd71e3de082b2f676f45ee3f95d,Proceedings of 3rd International Conference on Parallel and Distributed Information Systems
2322,"Priming of the respiratory burst of human neutrophils by the diadenosine polyphosphates, AP4A and AP3A: role of intracellular calcium.","The diadenosine polyphosphates, Ap3A and Ap4A, prime the respiratory burst of human neutrophils after stimulation with fMet-Leu-Phe. Maximal priming of oxidase activity occurred at 600-800 microM Ap3A and Ap4A, compared with maximal priming observed at 200 microM ATP. The time course of priming of the oxidase by all 3 nucleotides was very rapid, being detectable if added within 10 s of fMet-Leu-Phe. All 3 nucleotides also elicited increases in intracellular Ca2+ levels and there was a close concentration-dependency between the extent of priming and the increase in intracellular Ca2+. However, at low concentrations of nucleotides (< 50 microM Ap3A and Ap4A and < 0.1 microM ATP) priming of the oxidase was observed without detectable increases in intracellular Ca2+. These observations indicate that diadenosine polyphosphates may be novel regulators of neutrophil function and that priming of oxidase activity may occur via mechanisms that are either dependent or independent of increases in intracellular Ca2+.",1994-07-15,https://www.semanticscholar.org/paper/47e429d6c9381ecd1bc5efa22e7d2ffa3927eccd,Biochemical and Biophysical Research Communications - BBRC
1486,Experimental limit on the decay tau ---> nu tau K-K0.,,1987-08-01,https://www.semanticscholar.org/paper/c872f94a6f1b4cd6ac6b688b56a9460434012d05,Physical Review Letters
439,NP-Completeness: A Retrospective,,1997-07-07,https://www.semanticscholar.org/paper/ca6e78bc7b3e942750b2c52f00b13c1030d6e557,"International Colloquium on Automata, Languages and Programming"
280,On oblivious PTAS's for nash equilibrium,"If a class of games is known to have a Nash equilibrium with probability values that are either zero or Ω(1) -- and thus with support of bounded size -- then obviously this equilibrium can be found exhaustively in polynomial time. Somewhat surprisingly, we show that there is a PTAS for the class of games whose equilibria are guaranteed to have small --- O(1/n) -- values, and therefore large -- Ω(n) -- supports. We also point out that there is a PTAS for games with sparse payoff matrices, a family for which the exact problem is known to be PPAD-complete [Chen, Deng, Teng 2006]. Both algorithms are of a special kind that we call oblivious: The algorithm just samples a fixed distribution on pairs of mixed strategies, and the game is only used to determine whether the sampled strategies comprise an ε-Nash equilibrium; the answer is ""yes"" with inverse polynomial probability (in the second case, the algorithm is actually deterministic). These results bring about the question: Is there an oblivious PTAS for finding a Nash equilibrium in general games? We answer this question in the negative; our lower bound comes close to the quasi-polynomial upper bound of [Lipton, Markakis, Mehta 2003]. Another recent PTAS for anonymous games [Daskalakis, Papadimitriou 2007 and 2008, Daskalakis 2008] is also oblivious in a weaker sense appropriate for this class of games (it samples from a fixed distribution on unordered collections of mixed strategies), but its running time is exponential in 1/ε. We prove that any oblivious PTAS for anonymous games with two strategies and three player types must have 1/εα in the exponent of the running time for some α ≥ 1/3, rendering the algorithm in [Daskalakis 2008] (which works with any bounded number of player types) essentially optimal within oblivious algorithms. In contrast, we devise a poly n • (1/ε)O(\log2(1/ε)) non-oblivious PTAS for anonymous games with two strategies and any bounded number of player types. The key idea of our algorithm is to search not over unordered sets of mixed strategies, but over a carefully crafted set of collections of the first O(log 1/ε) moments of the distribution of the number of players playing strategy 1 at equilibrium. The algorithm works because of a probabilistic result of more general interest that we prove: the total variation distance between two sums of independent indicator random variables decreases exponentially with the number of moments of the two sums that are equal, independent of the number of indicators.",2009-05-31,https://www.semanticscholar.org/paper/d41b5308fe2cbbd3e653951c3362e7b32841c5a1,Symposium on the Theory of Computing
330,Experiments with an Economic Model of the Worldwide Web,,2005-12-15,https://www.semanticscholar.org/paper/4443b04a11215ff5867596ceb9b5a25dc239f1ad,Workshop on Internet and Network Economics
2110,Using DEA to Measure the Relative Efficiency of the Service Center and Improve Operation Efficiency through Reorganization,"Data envelopment analysis (DEA) has become a practicable approach to evaluate the relative efficiencies of decision-making units in various contexts. A DEA study was conducted to measure the relative efficiencies of 17 service centers of the NAN-TOU electricity distribution district of Taiwan Power Company (TPC). Altematives for reorganizing the service centers via efficiency measurement were investigated. The results showed that the proposed reorganization alternatives have better efficiency scores. Based on DEA evaluations, we provide specific directions for the inefficient service centers to improve their operation efficiencies and thus maintain the competitive advantage of TPC in facing power market liberalization.",2002-11-01,https://www.semanticscholar.org/paper/04186f47f51401b1203f10f91c6ebc8cedb0139a,IEEE Power Engineering Review
655,Predictors of thrombotic complications after placement of the flexible coil stent.,,1994-06-15,https://www.semanticscholar.org/paper/f72a198d0c5eae595e07dc8648d91c5b26c8424c,American Journal of Cardiology
1585,Multiple Causes: A Causal Graphical View,"Unobserved confounding is a major hurdle for causal inference from observational data. Confounders---the variables that affect both the causes and the outcome---induce spurious non-causal correlations between the two. Wang & Blei (2018) lower this hurdle with ""the blessings of multiple causes,"" where the correlation structure of multiple causes provides indirect evidence for unobserved confounding. They leverage these blessings with an algorithm, called the deconfounder, that uses probabilistic factor models to correct for the confounders. In this paper, we take a causal graphical view of the deconfounder. In a graph that encodes shared confounding, we show how the multiplicity of causes can help identify intervention distributions. We then justify the deconfounder, showing that it makes valid inferences of the intervention. Finally, we expand the class of graphs, and its theory, to those that include other confounders and selection variables. Our results expand the theory in Wang & Blei (2018), justify the deconfounder for causal graphs, and extend the settings where it can be used.",2019-05-30,https://www.semanticscholar.org/paper/f84ca302bd7ff93fd706c9937676535887a4aef6,arXiv.org
1927,Big data analytic for multivariate fault detection and classification in semiconductor manufacturing,"Nowadays, there are more attentions on cost control and yield enhancement in the semiconductor industry. Many manufacturers have the ability to collect the physical data called Status Variables Identification (SVID) by sensors embedded in the advanced machines during the manufacturing process. To maintain the competitive advantages, process monitoring and quick response to yield problem are pivotal in detecting the cause of the faults with the help of the sensor data. To state the physical nature of certain SVID, we usually transform SVID into Fault Detection and Classification parameters (FDC parameters) using statistical indicators. The data containing FDC parameters is called FDC data. This study aims to develop a multivariate analysis model to find out the crucial factors which may lead to process excursion among a large amount of FDC data. We proposed a 2-phase multivariate analysis framework: (1) the Least Absolute Shrinkage and Selection Operator (LASSO) is applied for key operation screening. (2) And Random Forest (RF) is used to rank the FDC parameters based on the key operations. Based on the results, domain engineers can quickly take actions responding to low yield problems.",2017-08-01,https://www.semanticscholar.org/paper/fd43561839625a9fda3e77201547a05adf6e5188,CASE
3722,Generative Interventions for Causal Learning,"We introduce a framework for learning robust visual representations that generalize to new viewpoints, backgrounds, and scene contexts. Discriminative models often learn naturally occurring spurious correlations, which cause them to fail on images outside of the training distribution. In this paper, we show that we can steer generative models to manufacture interventions on features caused by confounding factors. Experiments, visualizations, and theoretical results show this method learns robust representations more consistent with the underlying causal relationships. Our approach improves performance on multiple datasets demanding out-of-distribution generalization, and we demonstrate state-of-the-art performance generalizing from ImageNet to ObjectNet dataset.",2020-12-22,https://www.semanticscholar.org/paper/945aa2eb4b7ceecebf0562dfc12fcadb8fd38970,Computer Vision and Pattern Recognition
922,Node-Deletion Problems on Bipartite Graphs,"A set of problems which has attracted considerable interest recently is the set of node-deletion problems. The general node-deletion problem can be stated as follows: Given a graph, find the minimum number of nodes whose deletion results in a subgraph satisfying property $\pi $. In [LY] this problem was shown to be NP-complete for a large class of properties (the class of properties that are hereditary on induced subgraphs) using a small number of reduction schemes from the node cover problem. Since the node cover problem becomes polynomial on bipartite graphs, it might be hoped that this is the case with other node-deletion problems too.In this paper we characterize those properties for which the bipartite restriction of the node-deletion problem is polynomial and those for which it remains NP-complete. Similar results follow for analogous problems on other structures such as families of sets, hypergraphs and 0,1 matrices. For example, in the case of matrices, our result states that if M is a class of 0,...",1981-05-01,https://www.semanticscholar.org/paper/72ce49a2fb801d583ed1074625ce826c17609664,SIAM journal on computing (Print)
2748,Visualizing n-dimensional virtual worlds with n-vision,"There are many applications in science, mathematics , statistics, and business, in which it is important to explore an d manipulate clam in more than three dimensions . In thes e applications, data can be defined by points in Euclidean n-space . A point's position is then specified with it coordinates . each o f which determines its position relative to one of It mutuall y perpendicular axes . We describe here research that has as its goa l the development of interaction techniques and metaphors for th e 4D and higher-dimensional worlds that this data represents .",1990-02-01,https://www.semanticscholar.org/paper/8f09a0a105a36997179c9f4a36b3acab522e46e6,ACM Symposium on Interactive 3D Graphics and Games
2900,Prediction of on-target and off-target activity of CRISPR-Cas13d guide RNAs using deep learning.,,2023-07-03,https://www.semanticscholar.org/paper/f4b1a3cf74e8128f5d9c3b7f64666135ffe3e2fd,Nature Biotechnology
716,On the Complexity of Simple and Optimal Deterministic Mechanisms for an Additive Buyer,"We show that the Revenue-Optimal Deterministic Mechanism Design problem for a single additive buyer is #P-hard, even when the distributions have support size 2 for each item and, more importantly, even when the optimal solution is guaranteed to be of a very simple kind: the seller picks a price for each individual item and a price for the grand bundle of all the items; the buyer can purchase either the grand bundle at its given price or any subset of items at their total individual prices. The following problems are also #P-hard, as immediate corollaries of the proof: 
1. determining if individual item pricing is optimal for a given instance, 
2. determining if grand bundle pricing is optimal, and 
3. computing the optimal (deterministic) revenue. 
On the positive side, we show that when the distributions are i.i.d. with support size 2, the optimal revenue obtainable by any mechanism, even a randomized one, can be achieved by a simple solution of the above kind (individual item pricing with a discounted price for the grand bundle) and furthermore, it can be computed in polynomial time. The problem can be solved in polynomial time too when the number of items is constant.",2017-02-22,https://www.semanticscholar.org/paper/4bc3c3a49403ca19f67cbda2dac6850eaea4ac62,ACM-SIAM Symposium on Discrete Algorithms
1522,Transport Score Climbing: Variational Inference Using Forward KL and Adaptive Neural Transport,"Variational inference often minimizes the""reverse""Kullbeck-Leibler (KL) KL(q||p) from the approximate distribution q to the posterior p. Recent work studies the""forward""KL KL(p||q), which unlike reverse KL does not lead to variational approximations that underestimate uncertainty. This paper introduces Transport Score Climbing (TSC), a method that optimizes KL(p||q) by using Hamiltonian Monte Carlo (HMC) and a novel adaptive transport map. The transport map improves the trajectory of HMC by acting as a change of variable between the latent variable space and a warped space. TSC uses HMC samples to dynamically train the transport map while optimizing KL(p||q). TSC leverages synergies, where better transport maps lead to better HMC sampling, which then leads to better transport maps. We demonstrate TSC on synthetic and real data. We find that TSC achieves competitive performance when training variational autoencoders on large-scale data.",2022-02-03,https://www.semanticscholar.org/paper/5de7813ce5bc9f361f2b6fd09f884b468dcb43bf,arXiv.org
1537,Hierarchical Inducing Point Gaussian Process for Inter-domain Observations,"We examine the general problem of inter-domain Gaussian Processes (GPs): problems where the GP realization and the noisy observations of that realization lie on different domains. When the mapping between those domains is linear, such as integration or differentiation, inference is still closed form. However, many of the scaling and approximation techniques that our community has developed do not apply to this setting. In this work, we introduce the hierarchical inducing point GP (HIP-GP), a scalable inter-domain GP inference method that enables us to improve the approximation accuracy by increasing the number of inducing points to the millions. HIP-GP, which relies on inducing points with grid structure and a stationary kernel assumption, is suitable for low-dimensional problems. In developing HIP-GP, we introduce (1) a fast whitening strategy, and (2) a novel preconditioner for conjugate gradients which can be helpful in general GP settings. Our code is available at https: //github.com/cunningham-lab/hipgp.",2021-02-28,https://www.semanticscholar.org/paper/1006312979506add9e3e1dadd012b0fb858ca40d,International Conference on Artificial Intelligence and Statistics
2770,An integrated system for creating and presenting complex computer-based documents,"An experimental system is described for the design, development, and presentation of computer-based documents that combine pictures and text on a high-resolution raster color display. Such documents can be used, for example, for maintenance and repair tasks or computer-aided instruction.
 Documents are directed graphs whose nodes we refer to as pages, in analogy to the pages of a paper book. A page includes a set of simultaneously displayed pictures, actions (procedures and processes) triggered when the page is accessed or when pickable picture elements on it are selected, and indexing information. Pages may be nested arbitrarily deeply in chapters that serve much the same organizing function as those of conventional books.
 The system is comprised of separate programs for laying out and drawing pictures, for graphically specifying the contents of pages, chapters, and their interconnections, and for displaying the document for user interaction.
 Examples are given from a prototype document for the maintenance and repair of computerized numerical control equipment. Emphasis was placed on designing actions for simple realtime animation (both by color table techniques and by transforming named primitives and manipulating their attributes), and for finding one's way around the document (displays include: a “timeline” of recently visited pages, immediate predecessor and successor pages, sibling pages and their interconnections, and those pages satisfying key-word retrieval requests).",1981-08-03,https://www.semanticscholar.org/paper/763fef1efe5bba96020e784657423a0a0a198875,International Conference on Computer Graphics and Interactive Techniques
2207,The multifactorial role of neutrophils in rheumatoid arthritis,,2014-10-01,https://www.semanticscholar.org/paper/6f27acb18f2b6c4484740c05fe872e8fa13f1f85,Nature Reviews Rheumatology
940,The Complexity of Restricted Minimum Spanning Tree Problems (Extended Abstract),,1979-07-16,https://www.semanticscholar.org/paper/939910f3e1a823a8b3b88854f151cd8579a0913e,"International Colloquium on Automata, Languages and Programming"
2218,Analysis of SF and plasma cytokines provides insights into the mechanisms of inflammatory arthritis and may predict response to therapy.,"OBJECTIVES
Biologic drugs have revolutionized the care of RA, but are expensive and not universally effective. To further understand the inflammatory mechanisms underlying RA and identify potential biomarkers predicting response to therapy, we measured multiple cytokine concentrations in SF of patients with inflammatory arthritides (IAs) and, in a subset of patients with RA, correlated this with response to TNF-α inhibition.


METHODS
SF from 42 RA patients and 19 non-RA IA patients were analysed for 12 cytokines using a multiplex cytokine assay. Cytokines were also measured in the plasma of 16 RA patients before and following treatment with anti-TNF-α. Data were analysed using Mann-Whitney U-test, Spearman's rank correlation and cluster analysis with the Kruskal-Wallis test with Dunn's post-test analysis.


RESULTS
RA SF contained significantly elevated levels of IL-1β, IL-1ra, IL-2, IL-4, IL-8, IL-10, IL-17, IFN-γ, G-CSF, GM-CSF and TNF-α compared with other IA SF. RA patients who did not respond to anti-TNF therapy had elevated IL-6 in their SF pre-therapy (P < 0.05), whereas responders had elevated IL-2 and G-CSF (P < 0.05). Plasma cytokine concentrations were not significantly modulated by TNF inhibitors, with the exception of IL-6, which decreased after 12 weeks (P < 0.05).


CONCLUSIONS
Cytokine profiles in RA SF vary with treatment and response to therapy. Cytokine concentrations are significantly lower in plasma than in SF and relatively unchanged by TNF inhibitor therapy. Concentrations of IL-6, IL-2 and G-CSF in SF may predict response to TNF inhibitors.",2012-03-01,https://www.semanticscholar.org/paper/04673c6ede6b8c18a81ad3f99582b194b0327a16,Rheumatology
2012,A two-stage stochastic programming approach for new tape-out allocation decisions for demand fulfillment planning in semiconductor manufacturing,,2011-07-26,https://www.semanticscholar.org/paper/3c93ca2e846a676f2de656ac98ba5f977bfb7820,Flexible Services and Manufacturing Journal
3182,Towards Individual Grevy's Zebra Identification via Deep 3D Fitting and Metric Learning,"This paper combines deep learning techniques for species detection, 3D model fitting, and metric learning in one pipeline to perform individual animal identification from photographs by exploiting unique coat patterns. This is the first work to attempt this and, compared to traditional 2D bounding box or segmentation based CNN identification pipelines, the approach provides effective and explicit view-point normalisation and allows for a straight forward visualisation of the learned biometric population space. Note that due to the use of metric learning the pipeline is also readily applicable to open set and zero shot re-identification scenarios. We apply the proposed approach to individual Grevy's zebra (Equus grevyi) identification and show in a small study on the SMALST dataset that the use of 3D model fitting can indeed benefit performance. In particular, back-projected textures from 3D fitted models improve identification accuracy from 48.0% to 56.8% compared to 2D bounding box approaches for the dataset. Whilst the study is far too small accurately to estimate the full performance potential achievable in larger-scale real-world application settings and in comparisons against polished tools, our work lays the conceptual and practical foundations for a next step in animal biometrics towards deep metric learning driven, fully 3D-aware animal identification in open population settings. We publish network weights and relevant facilitating source code with this paper for full reproducibility and as inspiration for further research.",2022-06-05,https://www.semanticscholar.org/paper/a6b0984cdca0fb6612cd19a4bd4b34717c52249e,arXiv.org
2851,Galectin‐3 Stimulates Preadipocyte Proliferation and Is Up‐regulated in Growing Adipose Tissue,,2007-01-01,https://www.semanticscholar.org/paper/844010c5c73ad83c601bebddc752c3d0cf2be3fd,Obesity
1244,Measurement of Bs0 mixing parameters from the flavor-tagged decay Bs0-->J/psiphi.,"From an analysis of the flavor-tagged decay Bs0-->J/psiphi we obtain the width difference between the Bs0 light and heavy mass eigenstates, DeltaGammas = 0.19+/-0.07(stat)(-0.01)+0.02(syst) ps(-1), and the CP-violating phase, phi s= -0.57(-0.30)+0.24(stat)(-0.02)+0.08(syst). The allowed 90% CL intervals of DeltaGammas and phi s are 0.06 < DeltaGammas < 0.30 ps(-1) and -1.20 < phi s < 0.06, respectively. The data sample corresponds to an integrated luminosity of 2.8 fb(-1) accumulated with the D0 detector at the Fermilab Tevatron collider.",2008-12-08,https://www.semanticscholar.org/paper/a25538a1bed116f6995a49d0b7b402493a3e9aae,Physical Review Letters
800,Model Checking of Message Sequence Charts,,1999-08-24,https://www.semanticscholar.org/paper/38505f1e1b063724a6c23c11ebeee2ec1d81ff95,International Conference on Concurrency Theory
1911,Multi-objective demand fulfillment problem for solar cell industry,,2018-11-01,https://www.semanticscholar.org/paper/808dcbb9267f5c36eacc3d6caea84e5a5b2fdc66,Computers & industrial engineering
899,"Simple Linear-Time Algorithms to Test Chordality of Graphs, Test Acyclicity of Hypergraphs, and Selectively Reduce Acyclic Hypergraphs","An article of golfing equipment has a golf tee attached to a spring-biassed reel by a length of string. The reel is mounted in a casing which receives the tee when the spring rotates the reel to wind the spring onto it. The reel is normally locked by a one-way ratchet but is released to wind in the string by a push-button which has a spike and is detachable from the casing so as to be usable as a ball marker. When practising, the cord can be aligned with the green or hole and used as an aid in swinging the club face in the correct direction. The casing has a spring-clip so that the article can be clipped into the golfer's pocker when he is not using it.",1984-07-27,https://www.semanticscholar.org/paper/1ffc977d82798cfab971e4abdb46ae7b707c57c0,SIAM journal on computing (Print)
153,The Computational Complexity of Multi-player Concave Games and Kakutani Fixed Points,"Kakutani's Fixed Point theorem is a fundamental theorem in topology with numerous applications in game theory and economics. Formally, Kakutani's theorem states that for any set-valued function mapping F, also known as correspondence, from a compact, convex set to itself in a locally convex topological vector space, if the function is upper hemicontinuous, has a closed graph, and its output at any given point is a non-empty and convex set, then there exists a fixed point x, namely a point in the domain which is mapped to itself by the function x ∈ F(x). Interestingly, computational formulations of Kakutani exist only in special cases and are too restrictive to be useful in reductions.",2022-07-15,https://www.semanticscholar.org/paper/c54611e2a4f60862b3c3e9075cbf289164387d7c,ACM Conference on Economics and Computation
3420,Online scheduling of packets with agreeable deadlines,"This article concerns an online packet scheduling problem that arises as a natural model for buffer management at a network router. Packets arrive at a router at integer time steps, and are buffered upon arrival. Packets have non-negative weights and integer deadlines that are (weakly) increasing in their arrival times. In each integer time step, at most one packet can be sent. The objective is to maximize the sum of the weights of the packets that are sent by their deadlines. The main results include an optimal (φ := (1 + √ 5)/2 ≈ 1.618)-competitive deterministic online algorithm, a (4/3 ≈ 1.33)-competitive randomized online algorithm against an oblivious adversary, and a 2-speed 1-competitive deterministic online algorithm. The analysis does not use a potential function explicitly, but instead modifies the adversary's buffer and credits the adversary to account for these modifications.",2012-12-01,https://www.semanticscholar.org/paper/b1cce74220e7eb9d79bf0d48f2844f09066160ed,TALG
3765,Visualizing Object Detection Features,,2015-02-18,https://www.semanticscholar.org/paper/54bf134b47bdadbf2dd04956cf51076d4f26ce01,International Journal of Computer Vision
3699,Landscape Learning for Neural Network Inversion,"Many machine learning methods operate by inverting a neural network at inference time, which has become a popular technique for solving inverse problems in computer vision, robotics, and graphics. However, these methods often involve gradient descent through a highly non-convex loss landscape, causing the optimization process to be unstable and slow. We introduce a method that learns a loss landscape where gradient descent is efficient, bringing massive improvement and acceleration to the inversion process. We demonstrate this advantage on a number of methods for both generative and discriminative tasks, including GAN inversion, adversarial defense, and 3D human pose reconstruction.",2022-06-17,https://www.semanticscholar.org/paper/5dad3748e8d4d8c659005903062e5d8e855fa86c,arXiv.org
2202,Heparin derivatives for the targeting of multiple activities in the inflammatory response.,,2015-03-06,https://www.semanticscholar.org/paper/ed2c34ed843411b08525a33ad8333fbc40eb16fa,Carbohydrate Polymers
2596,"Digitally modeling, visualizing and preserving archaeological sites","Preserving cultural heritage and historic sites is an important problem. These sites are subject to erosion and vandalism, and, as long-lived artifacts, they have gone through many phases of construction, damage and repair. We believe that it is important to use 3D model building technology to create an accurate record of these sites, so preservationists can track changes and foresee structural problems. From a digital libraries perspective, 3D models also allow a much wider audience to ""virtually"" see and tour these sites. We have developed a suite of new methods that can reduce the time to build a model through automation. Our methods utilize range image segmentation and feature extraction algorithms. Once the 3D model is constructed, it is necessary to texture map it with imagery to create a geometrically and photometrically correct model. We are developing a 3D visualization environment to aid archaeologists in their post-excavation interpretation and analysis.",2004-06-07,https://www.semanticscholar.org/paper/36a37ccab70a12901e557193b61c91a5a753aa69,"Proceedings of the 2004 Joint ACM/IEEE Conference on Digital Libraries, 2004."
67,Text joins for data cleansing and integration in an RDBMS,"An organization's data records are often noisy because of transcription errors, incomplete information, lack of standard formats for textual data or combinations thereof. A fundamental task in a data cleaning system is matching textual attributes that refer to the same entity (e.g., organization name or address). This matching is effectively performed via the cosine similarity metric from the information retrieval field. For robustness and scalability, these ""text joins"" are best done inside an RDBMS, which is where the data is likely to reside. Unfortunately, computing an exact answer to a text join can be expensive. We propose an approximate, sampling-based text join execution strategy that can be robustly executed in a standard, unmodified RDBMS.",2003-03-05,https://www.semanticscholar.org/paper/667f5206d5afab49f830a24849bf14d5a6ad5564,Proceedings / International Conference on Data Engineering
929,Issues of correctness in database concurrency control by locking,"Our aim in this paper is to show that there is a mathematically inherent reason why existing systems enforce D-serializability (rather than just because of its simplicity): it is because they are based on locking. Our main result is a characterization of the power of locking which states that if a locking policy is safe then it must allow only D-serializable schedules. Furthermore any such schedule can be produced by some safe locking policy.
 The rest of the paper is organized as follows. In Section 2 we formalize our concepts and describe the model. In Section 3 we characterize D-serializability in semantic terms. In Section 4 we examine when a set of transactions can be let to run safely by themselves without locking or any intervention from the scheduler. Section 5 is concerned with locking policies and in Section 6 we discuss some implications of our results.",1981-05-11,https://www.semanticscholar.org/paper/fa87e502eb71cf480a25602354704c89af8d6ce1,Symposium on the Theory of Computing
603,On the power of locking,"We study the expressive power of locking primitives, as measured by ther ability to implement different concurrency control principles. We give a necessary and sufficient condition for a concurrency control principle (abstractly, a set of histories) to be implementable by binary semaphores. Also, we characterize exactly those sets of locking primitives that are no more powerful than binary semaphores.",1981-04-29,https://www.semanticscholar.org/paper/6d57730605f25a80e0c1dd3f9b6263d4eb393b47,ACM SIGMOD Conference
3628,Language-technical aspects of reuse,"Reuse happens only when a variety of social conditions are favorable. However, social conditions, development processes, and design methods alone cannot guarantee success. In the end, working code must be produced, and at this stage programming languages and programming styles can make a critical difference. The paper focuses on C++ and its use because that's where the author's experience is, and because reuse in the abstract is a sterile exercise. To yield its obvious benefits, reuse must be practised. Reuse is a result of good design; it is not something you get from simple minded use of special language features. The paper presents examples of how designs of components for use in several contexts can be expressed directly and efficiently in C++. Concrete types, abstract classes, class hierarchies, and generic programming using templates are mentioned.",1996-04-23,https://www.semanticscholar.org/paper/80f17151348d458fef29b2b2a54c90381caeba1d,Proceedings of Fourth IEEE International Conference on Software Reuse
1195,Search for ZZ and Zgamma* production in pp[over ] collisions at square root s=1.96 TeV and limits on anomalous ZZZ and ZZgamma* couplings.,"We present a study of micro micro micro micro, eeee, and micro micro ee events using 1 fb(-1) of data collected with the D0 detector at the Fermilab Tevatron pp[over ] Collider at square root s=1.96 TeV. Requiring the lepton pair masses to be greater than 30 GeV, we observe one event, consistent with the expected background of 0.13+/-0.03 events and with the predicted standard model ZZ and Zgamma* production of 1.71+/-0.15 events. We set an upper limit on the ZZ and Zgamma* cross section of 4.4 pb at the 95% C.L. We also derive limits on anomalous neutral trilinear ZZZ and ZZgamma* gauge couplings. The one-parameter 95% C.L. coupling limits with a form-factor scale Lambda=1.2 TeV are -0.28<f(40)(Z)<0.28, -0.31<f(50)(Z)<0.29, -0.26<f(40)(gamma)<0.26, and -0.30<f(50)(gamma)<0.28.",,https://www.semanticscholar.org/paper/02f6d1f23c3552cb23c49da51fb76df3512b005e,Physical Review Letters
2075,Overall Wafer Effectiveness (OWE): A Novel Industry Standard for Wafer Productivity,"Overall equipment efficiency (OEE) is an index that is widely used to measure equipment performance for semiconductor manufacturing. However, little research has been done to address productivity from the perspective of wafer exposure performance. This study aims to propose a novel standard, overall wafer effectiveness (OWE), to evaluate the effectiveness of wafer exposure rather than only considering tool productivity. Furthermore, the proposed OWE can be easily extended to incorporate additional attributes such as throughput, yield, and price. In particular, a weighted OWE that integrates mask field utilization and OWE is illustrated with numerical example to show the practical viability of OWE as a semiconductor industry standard to drive collaborative efforts among IC designers, equipment vendors, and manufacturers for enhancing total wafer effectiveness.",2006-09-01,https://www.semanticscholar.org/paper/14c4c5b34aef3d2edada63e27334aecb6328123c,2006 IEEE International Symposium on Semiconductor Manufacturing
1660,Operator Variational Inference,"Variational inference is an umbrella term for algorithms which cast Bayesian inference as optimization. Classically, variational inference uses the Kullback-Leibler divergence to define the optimization. Though this divergence has been widely used, the resultant posterior approximation can suffer from undesirable statistical properties. To address this, we reexamine variational inference from its roots as an optimization problem. We use operators, or functions of functions, to design variational objectives. As one example, we design a variational objective with a Langevin-Stein operator. We develop a black box algorithm, operator variational inference (OPVI), for optimizing any operator objective. Importantly, operators enable us to make explicit the statistical and computational tradeoffs for variational inference. We can characterize different properties of variational objectives, such as objectives that admit data subsampling---allowing inference to scale to massive data---as well as objectives that admit variational programs---a rich class of posterior approximations that does not require a tractable density. We illustrate the benefits of OPVI on a mixture model and a generative model of images.",2016-10-27,https://www.semanticscholar.org/paper/97fcee01d9777372a3d8966ab7cd4c17e3a6a5ca,Neural Information Processing Systems
3189,The gastrointestinal nematodes of plains and Grevy's zebras: Phylogenetic relationships and host specificity,,2021-10-01,https://www.semanticscholar.org/paper/1ee13f528e15fd694856a9629bd36b134fee1967,International Journal for Parasitology: Parasites and Wildlife
3191,Moving through the mosaic: identifying critical linkage zones for large herbivores across a multiple‐use African landscape,,2021-03-14,https://www.semanticscholar.org/paper/4a6a1676680c2aa84c55b811a09a68c907325b23,Landscape Ecology
3251,"Correction for Giuggioli et al., Stigmergy, collective actions, and animal social spacing","ECOLOGY, APPLIED MATHEMATICS Correction for “Stigmergy, collective actions, and animal social spacing,” by Luca Giuggioli, Jonathan R. Potts, Daniel I. Rubenstein, and Simon A. Levin, which appeared in issue 42, October 15, 2013, of Proc Natl Acad Sci USA (110:16904–16909; first published September 30, 2013; 10.1073/pnas.1307071110). The authors note that Fig. 3 appeared incorrectly. The corrected figure and its legend appear below. The authors thank Dr. Jannis Uhlendorf for calling their attention to this error.",2016-02-01,https://www.semanticscholar.org/paper/e6c4e9cf919627b282589f08f1041ee975c554af,Proceedings of the National Academy of Sciences of the United States of America
564,The Complexity of Cubical Graphs,,1985-07-01,https://www.semanticscholar.org/paper/e09d85193f17a9810a40840444611f12f690821d,Information and Control
1682,Variational inference with copula augmentation,"We develop a general methodology for variational inference which preserves dependency among the latent variables. This is done by augmenting the families of distributions used in mean-field and structured approximation with copulas. Copulas allow one to separately model the dependency given a factorization of the variational distribution, and can guarantee us better approximations to the posterior as measured by KL divergence. We show that inference on the augmented distribution is highly scalable using stochastic optimization. Furthermore, the addition of a copula is generic and can be applied straightforwardly to any inference procedure using the original meanfield or structured approach. This reduces bias, sensitivity to local optima, sensitivity to hyperparameters, and significantly helps characterize and interpret the dependency among the latent variables.",2015-06-10,https://www.semanticscholar.org/paper/7f50ccd131867f65f3044d20db0a90d6374f7a12,arXiv.org
789,Perfect Packing Theorems and the Average-Case Behavior of Optimal and Online Bin Packing,"A process for producing alkoxy ketones of formula I wherein R1 represents an alkyl group having 1 to 8 carbon atoms, R2 represents hydrogen or an alkyl group having 1 to 4 carbon atoms, and n represents 1 or 2, by dehydrogenation of a corresponding alkoxyalkanol on a copper-containing catalyst which has been activated by treatment with hydrogen at 120 DEG to 450 DEG C., in which process the activated catalyst is firstly brought into contact at 230 DEG to 350 DEG C. with the vapor of the alkoxyalkanol to be dehydrogenated; hydrogen is subsequently passed over the catalyst at 250 DEG to 450 DEG C.; and then the dehydrogenation is performed at 150 DEG to 450 DEG C. on the catalyst pretreated in this manner, is disclosed.",,https://www.semanticscholar.org/paper/ed92c20debeca111fdea9b2c2bf864d8753735d6,SIAM Review
3050,Record and transplay: partial checkpointing for replay debugging across heterogeneous systems,"Software bugs that occur in production are often difficult to reproduce in the lab due to subtle differences in the application environment and nondeterminism. To address this problem, we present Transplay, a system that captures production software bugs into small per-bug recordings which are used to reproduce the bugs on a completely different operating system without access to any of the original software used in the production environment. Transplay introduces partial checkpointing, a new mechanism that efficiently captures the partial state necessary to reexecute just the last few moments of the application before it encountered a failure. The recorded state, which typically consists of a few megabytes of data, is used to replay the application without requiring the specific application binaries, libraries, support data, or the original execution environment. Transplay integrates with existing debuggers to provide standard debugging facilities to allow the user to examine the contents of variables and other program state at each source line of the application's replayed execution. We have implemented a Transplay prototype that can record unmodified Linux applications and replay them on different versions of Linux as well as Windows. Experiments with several applications including Apache and MySQL show that Transplay can reproduce real bugs and be used in production with modest recording overhead.",2011-06-07,https://www.semanticscholar.org/paper/4bec0240809c506f45411a43e16c901c131c481d,Measurement and Modeling of Computer Systems
1935,Building energy saving performance indices for cleaner semiconductor manufacturing and an empirical study,,2016-09-01,https://www.semanticscholar.org/paper/80a14d259da55e24c85beab4f711923b9b40f01b,Computers & industrial engineering
2151,A Simple Lower Bound for the Capacity of the Deletion Channel,"We present a simple proof that the capacity of the binary independent and identically distributed (i.i.d.) deletion channel, where each bit is deleted independently with probability d, is at least (1-d)/9, by developing a correspondence between the deletion channel and an insertion/deletion channel that we call a Poisson-repeat channel",2006-10-01,https://www.semanticscholar.org/paper/9a267dcf9a400f9ad02c6f72b68a2a44a54985a6,IEEE Transactions on Information Theory
1344,Measurement of the B0s lifetime in the exclusive decay channel B0s-->J/psiphi.,"Using the exclusive decay B0s-->J/psi(mu+mu-)phi(K+K-), we report the most precise single measurement of the B0s lifetime. The data sample corresponds to an integrated luminosity of approximately 220 pb(-1) collected with the D0 detector at the Fermilab Tevatron Collider in 2002-2004. We reconstruct 337 signal candidates, from which we extract the B0s lifetime, tau(B0s)=1.444(+0.098)(-0.090)(stat)+/-0.020(sys) ps. We also report a measurement for the lifetime of the B0 meson using the exclusive decay B0-->J/psi(mu+mu-)K*0(892)(K+pi-). We reconstruct 1370 signal candidates, obtaining tau(B0)=1.473(+0.052)(-0.050)(stat)+/-0.023(sys) ps, and the ratio of lifetimes, tau(B0s)/tau(B0)=0.980(+0.076)(-0.071)(stat)+/-0.003(sys).",,https://www.semanticscholar.org/paper/76abbf33e4a610a0dd467047dd70e24154f46391,Physical Review Letters
2579,Interaction techniques using prosodic features of speech and audio localization,"We describe several approaches for using prosodic features of speech and audio localization to control interactive applications. This information can be applied to parameter control, as well as to speech disambiguation. We discuss how characteristics of spoken sentences can be exploited in the user interface; for example, by considering the speed with which a sentence is spoken and the presence of extraneous utterances. We also show how coarse audio localization can be used for low-fidelity gesture tracking, by inferring the speaker's head position.",2005-01-10,https://www.semanticscholar.org/paper/23062112a90cf95c4743d9db7a9dbe073aa123b4,International Conference on Intelligent User Interfaces
2160,Platelet TLR7 is essential for the formation of platelet-neutrophil complexes and low-density neutrophils in lupus nephritis.,"OBJECTIVES
Platelets and low-density neutrophils (LDNs) are major players in the immunopathogenesis of systemic lupus erythematosus (SLE). Despite evidence showing the importance of platelet neutrophil complexes (PNCs) in inflammation, little is known about the relationship between LDNs and platelets in SLE. We sought to characterize the role of LDNs and TLR7 in clinical disease.


METHODS
Flow cytometry was used to immunophenotype LDNs from SLE patients and controls. The association of LDNs with organ damage was investigated in a cohort of 290 SLE patients. TLR7mRNA expression was assessed in LDNs and high-density neutrophils (HDNs) using publicly available mRNA sequencing datasets, and our own cohort using RT-PCR. The role of TLR7 in platelet binding was evaluated in platelet: HDN mixing studies using TLR7 deficient mice and Klinefelter syndrome patients.


RESULTS
SLE patients with active disease have more LDNs, which are heterogeneous and more immature in patients with evidence of kidney dysfunction. LDNs are platelet bound, in contrast to HDNs. LDNs settle in the PBMC layer due to the increased buoyancy and neutrophil degranulation from platelet binding. Mixing studies demonstrated that this PNC formation was dependent on platelet-TLR7, and that the association results in increased NETosis. The neutrophil-to-platelet ratio (NPR), is a useful clinical correlate for LDNs, and a higher NPR is associated with past and current flares of lupus nephritis.


CONCLUSIONS
LDNs sediment in the upper PBMC fraction due to PNC formation, which is dependent on the expression of TLR7 in platelets. Collectively, our results reveal a novel TLR7-dependent crosstalk between platelets and neutrophils, which may be an important therapeutic opportunity for lupus nephritis.",2023-06-21,https://www.semanticscholar.org/paper/fe8bcb06e0cd3686854c95a5134cab25647f61f8,Rheumatology
1400,Search for leptoquark pairs decaying into nunu+jets in pp collisions at square root[s] = 1.8 TeV.,"We present the results of a search for leptoquark (LQ) pairs in (85.2+/-3.7) pb(-1) of pp* collider data collected by the D0 experiment at the Fermilab Tevatron. We observe no evidence for leptoquark production and set a limit on sigma(pp*-->LQLQ-->nunu+jets) as a function of the mass of the leptoquark (m(LQ)). Assuming the decay LQ-->nuq, we exclude scalar leptoquarks for m(LQ) < 98 GeV/c(2), and vector leptoquarks for m(LQ) < 200 GeV/c(2) and coupling which produces the minimum cross section, at a 95% confidence level.",,https://www.semanticscholar.org/paper/ca31f931e15f2a28f26f600a6a39de9ed95fd05d,Physical Review Letters
1224,Search for Higgs bosons decaying to tau pairs in pp over collisions with the D0 detector.,"We present a search for the production of neutral Higgs bosons varphi decaying into tau+tau - final states in pp over collisions at a center-of-mass energy of 1.96 TeV. The data, corresponding to an integrated luminosity of approximately 1 fb(-1), were collected by the D0 experiment at the Fermilab Tevatron Collider. Limits on the production cross section times branching ratio are set. The results are interpreted in the minimal supersymmetric standard model yielding limits that are the most stringent to date at hadron colliders.",2008-05-16,https://www.semanticscholar.org/paper/611313076a36d38b6f0c9f6c25dd199dca383d92,Physical Review Letters
3364,On Being Socialized Out of the Human Sexual Response in the Later Years,,1978-11-01,https://www.semanticscholar.org/paper/8471a6d262a6fb6c90ad391336b709ccf8d57665,The Journal of Sociology &amp; Social Welfare
1748,Visualizing Topic Models,"
 
 Managing large collections of documents is an important problem for many areas of science, industry, and culture. Probabilistic topic modeling offers a promising solution. Topic modeling is an unsupervised machine learning method that learns the underlying themes in a large collection of otherwise unorganized documents. This discovered structure summarizes and organizes the documents. However, topic models are high-level statistical tools—a user must scrutinize numerical distributions to understand and explore their results. In this paper, we present a method for visualizing topic models. Our method creates a navigator of the documents, allowing users to explore the hidden structure that a topic model discovers. These browsing interfaces reveal meaningful patterns in a collection, helping end-users explore and understand its contents in new ways. We provide open source software of our method.
 
",2012-05-20,https://www.semanticscholar.org/paper/59d7d8415dacd300eb4d98b0da3cb32d27503b36,International Conference on Web and Social Media
2818,Absence of galectin-3 does not affect the development of experimental tongue carcinomas in mice.,,2011-04-01,https://www.semanticscholar.org/paper/4b0e1bfdc885514f973dfb7394c7833d95eebece,Experimental and molecular pathology (Print)
715,On the Complexity of Bundle-Pricing and Simple Mechanisms,"We show that the problem of finding an optimal bundle-pricing for a single additive buyer is #P-hard, even when the distributions have support size 2 for each item and the optimal solution is guaranteed to be a simple one: the seller picks a price for the grand bundle and a price for each individual item; the buyer can purchase either the grand bundle at the given price or any bundle of items at their total individual prices. We refer to this simple and natural family of pricing schemes as discounted item-pricings. In addition to the hardness result, we show that when the distributions are i.i.d. with support size 2, a discounted item-pricing can achieve the optimal revenue obtainable by lottery-pricings and it can be found in polynomial time.",2017-02-22,https://www.semanticscholar.org/paper/230253dbefc3e959ee9072bb824a5f39e3c6d407,arXiv.org
1978,A method for estimating the cycle time of business processes with many-to-many relationships among the resources and activities based on individual worklists,,2013-06-01,https://www.semanticscholar.org/paper/70d59bd941a83e89964d74626231424b2bb0172b,Computers & industrial engineering
2171,"Type I Interferon regulates cytokine-delayed neutrophil apoptosis, reactive oxygen species production and chemokine expression via activation of p38 MAPK","Interferons (IFNs) are key regulators of a number of inflammatory conditions in which neutrophils play an important role in pathology, such as rheumatoid arthritis (RA) and systemic lupus erythematosus (SLE), where Type I IFNs are implicated in disease pathology. However, IFNs are usually generated in vivo together with other cytokines that also have immunoregulatory functions but such interactions are poorly-defined experimentally. We measured the effects of Type-I IFN (IFNα), elevated in both RA and SLE, on the functions of healthy neutrophils incubated in vitro in the absence and presence of pro-inflammatory cytokines typically elevated in inflammatory diseases (TNFα, GM-CSF). IFNα alone had no effect on neutrophil apoptosis, however it did abrogate the anti-apoptotic effect of GM-CSF (18h, p< 0.01). The enhanced stabilty of the anti-apoptotic protein Mcl-1 and delayed activation of caspase activation normally regulated by GM-CSF were blocked by IFNα: this effect was mediated, in part, by activation of p38 MAPK, increased turnover of the anti-apoptotic protein Mcl-1 and cleavage of caspases. IFNα alone also primed ROS production alone and maintained the transient priming effect of TNF for up to 4h: it also down-regulated GM-CSF and TNFα-activated expression of CXCL1, CXCL2, CXCL3, CXCL8, CCL3 and CCL4, but in contrast increased the expression of CXCL10. These novel data identify complex regulatory signalling networks in which Type I IFNs profoundly alter the response of neutrophils to inflammatory cytokines. This is likely to have important consequences in vivo and may explain the complexity and heterogeneity of inflammatory diseases such as RA, in which multiple cytokine cascades have been activated.",2020-07-01,https://www.semanticscholar.org/paper/57cb689cfbde6994af023f0d9a89abafb661504e,bioRxiv
2195,Wolbachia endosymbionts induce neutrophil extracellular trap formation in human onchocerciasis,,2016-10-18,https://www.semanticscholar.org/paper/9cee1f7d2ff2c942392bcb854d481ecde2266974,Scientific Reports
3685,ViperGPT: Visual Inference via Python Execution for Reasoning,"Answering visual queries is a complex task that requires both visual processing and reasoning. End-to-end models, the dominant approach for this task, do not explicitly differentiate between the two, limiting interpretability and generalization. Learning modular programs presents a promising alternative, but has proven challenging due to the difficulty of learning both the programs and modules simultaneously. We introduce ViperGPT, a framework that leverages code-generation models to compose vision-and-language models into subroutines to produce a result for any query. ViperGPT utilizes a provided API to access the available modules, and composes them by generating Python code that is later executed. This simple approach requires no further training, and achieves state-of-the-art results across various complex visual tasks.",2023-03-14,https://www.semanticscholar.org/paper/6e754273d54a91371efbc928cd6b156364d517da,arXiv.org
3000,BlackBox: A Container Security Monitor for Protecting Containers on Untrusted Operating Systems,"Containers are widely deployed to package, isolate, and multiplex applications on shared computing infrastructure, but rely on the operating system to enforce their security guarantees. This poses a signiﬁcant security risk as large operating system codebases contain many vulnerabilities. We have created BlackBox, a new container architecture that provides ﬁne-grain protection of application data conﬁdentiality and integrity without trusting the operating system. BlackBox introduces a container security monitor, a small trusted computing base that creates protected physical address spaces (PPASes) for each container such that there is no direct information ﬂow from container to operating system or other container PPASes. Indirect information ﬂow can only happen through the monitor, which only copies data between container PPASes and the operating system as system call arguments, encrypting data as needed to protect interprocess communication through the operating system. Containerized applications do not need to be modiﬁed, can still make use of operating system services via system calls, yet their CPU and memory state are isolated and protected from other containers and the operating system. We have implemented BlackBox by leveraging Arm hardware virtualization support, using nested paging to enforce PPASes. The trusted computing base is a few thousand lines of code, many orders of magnitude less than Linux, yet supports widely-used Linux containers with only modest modiﬁcations to the Linux kernel. We show that BlackBox provides superior security guarantees over traditional hypervisor and container architectures with only modest performance overhead on real application workloads.",,https://www.semanticscholar.org/paper/3af13b96191578ccb41388b3017829246859ea00,USENIX Symposium on Operating Systems Design and Implementation
3692,FLEX: Full-Body Grasping Without Full-Body Grasps,"Synthesizing 3D human avatars interacting realistically with a scene is an important problem with applications in AR/VR, video games, and robotics. Towards this goal, we address the task of generating a virtual human – hands and full body – grasping everyday objects. Existing methods approach this problem by collecting a 3D dataset of humans interacting with objects and training on this data. However, 1) these methods do not generalize to different object positions and orientations or to the presence of furniture in the scene, and 2) the diversity of their generated full-body poses is very limited. In this work, we address all the above challenges to generate realistic, diverse full-body grasps in everyday scenes without requiring any 3D full-body grasping data. Our key insight is to leverage the existence of both full-body pose and hand-grasping priors, composing them using 3D geometrical constraints to obtain full-body grasps. We empirically validate that these constraints can generate a variety of feasible human grasps that are superior to baselines both quantitatively and qualitatively. See our webpage for more details: flex.cs.columbia.edu.",2022-11-22,https://www.semanticscholar.org/paper/1f1fd049a174e521e417596946e64a37290ec251,Computer Vision and Pattern Recognition
346,On a conjecture related to geometric routing,,2004-07-16,https://www.semanticscholar.org/paper/90300646c319d9a798ab55a07562b17610c0e269,Theoretical Computer Science
939,Topological Characterization of Families of Graphs Generated by Certain Types of Graph Grammars,,1979-07-01,https://www.semanticscholar.org/paper/658fd83c4c1aa045745284e09275629c7ec28d8b,Information and Control
34,Event Identification in Social Media,"Social media sites such as Flickr, YouTube, and Facebook host substantial amounts of user-contributed materials (e.g., photographs, videos, and textual content) for a wide variety of real-world events. These range from widely known events, such as the presidential inauguration, to smaller, community-specific events, such as annual conventions and local gatherings. By identifying these events and their associated user-contributed social media documents, which is the focus of this paper, we can greatly improve local event browsing and search in state-of-the-art search engines. To address our problem of focus, we exploit the rich “context” associated with social media content, including user-provided annotations (e.g., title, tags) and automatically generated information (e.g., content creation time). We form a variety of representations of social media documents using different context dimensions, and combine these dimensions in a principled way into a single clustering solution—where each document cluster ideally corresponds to one event—using a weighted ensemble approach. We evaluate our approach on a large-scale, real-world dataset of event images, and report promising performance with respect to several baseline approaches. Our preliminary experiments suggest that our ensemble approach identifies events, and their associated images, more effectively than the state-of-the-art strategies on which we build.",,https://www.semanticscholar.org/paper/f3a6725548c22d5bea6a0cb0b0a705d2e81475c9,International Workshop on the Web and Databases
1030,Optimal control for geometric motion planning of a robot diver,"Inertial reorientation of airborne articulated bodies has been an active area of research in the robotics community, as this behavior can help guide dynamic robots to a safe landing with minimal damage. The main objective of this work is emulating the aggressive and large angle correction maneuvers, like somersaults, that are performed by human divers. To this end, a planar three link robot, called DiverBot, is proposed. By considering a gravity-free scenario, a local connection is obtained between joint angles and the body orientation, resulting in a reduction in the system dynamics. An optimal control policy applied on this reduced configuration space yielded diving maneuvers that are dynamically feasible. Numerical results show that the DiverBot can execute one somersault without drift and multiple somersaults with minimal drift.",2016-10-01,https://www.semanticscholar.org/paper/4e8a39940432895a0123b79c09b6cd879649dca8,IEEE/RJS International Conference on Intelligent RObots and Systems
786,Testing and Checking of Finite State Systems,,2002-04-03,https://www.semanticscholar.org/paper/4ca49b0258077b8317a2ede39ca899112ab2510b,Latin American Symposium on Theoretical Informatics
2461,Patient Experiences Using an Inpatient Personal Health Record,"Summary Objective To investigate patients’ experience using an inpatient personal health record (PHR) on a tablet computer to increase engagement in their hospital care. Methods We performed observations and conducted semi-structured interviews with 14 post-operative cardiac surgical patients and their family members who received an inpatient PHR. Themes were identified using an inductive coding scheme. Results All participants responded favorably to having access to view their clinical information. A majority (85.7%) of participants used the application following an initial training session. Patients reported high satisfaction with being able to view their hospital medications and access educational materials related to their medical conditions. Patients reported a desire to view daily progress reports about their hospital stay and have access to educational information about their postacute recovery. In addition, patients expressed a common desire to view their diagnoses, laboratory test results, radiology reports, and procedure notes in language that is patient-friendly. Conclusion Patients have unmet information needs in the hospital setting. Our findings suggest that for some inpatients and their family members, providing personalized health information through a tablet computer may improve satisfaction, decrease anxiety, increase understanding of their health conditions, and improve safety and quality of care.",2016-04-01,https://www.semanticscholar.org/paper/beed87bdd7f68523c3d57623267d25bd34da3bb4,Applied Clinical Informatics
3749,Following Gaze in Video,"Following the gaze of people inside videos is an important signal for understanding people and their actions. In this paper, we present an approach for following gaze in video by predicting where a person (in the video) is looking even when the object is in a different frame. We collect VideoGaze, a new dataset which we use as a benchmark to both train and evaluate models. Given one frame with a person in it, our model estimates a density for gaze location in every frame and the probability that the person is looking in that particular frame. A key aspect of our approach is an end-to-end model that jointly estimates: saliency, gaze pose, and geometric relationships between views while only using gaze as supervision. Visualizations suggest that the model learns to internally solve these intermediate tasks automatically without additional supervision. Experiments show that our approach follows gaze in video better than existing approaches, enabling a richer understanding of human activities in video.",2017-10-01,https://www.semanticscholar.org/paper/241b86d3c71d14b8cc6044a425b047a0724cfdc9,IEEE International Conference on Computer Vision
2178,"Type I interferon regulates cytokine‐delayed neutrophil apoptosis, reactive oxygen species production and chemokine expression","Interferons (IFNs) are key regulators of a number of inflammatory conditions in which neutrophils play an important role in pathology, such as rheumatoid arthritis (RA) and systemic lupus erythematosus (SLE), where type I IFNs are implicated in disease pathology. However, IFNs are usually generated in vivo together with other cytokines that also have immunoregulatory functions, but such interactions are poorly defined experimentally. We measured the effects of type I (IFN‐α) IFN, elevated in both RA and SLE, on the functions of healthy neutrophils incubated in vitro in the absence and presence of proinflammatory cytokines typically elevated in inflammatory diseases [tumour necrosis factor (TNF‐α), granulocyte–macrophage colony‐stimulating factor (GM‐CSF)]. IFN‐α alone had no effect on neutrophil apoptosis; however, it abrogated the anti‐apoptotic effect of GM‐CSF (18 h, P < 0·01). The enhanced stability of the anti‐apoptotic protein myeloid cell leukaemia 1 (Mcl‐1) and delayed activation of caspase activation normally regulated by GM‐CSF were blocked by IFN‐α: this effect was mediated, in part, by activation of p38 mitogen‐activated protein kinase (MAPK). IFN‐α alone also primed reactive oxygen species (ROS) production and maintained the transient priming effect of TNF‐α for up to 4 h: it also down‐regulated GM‐CSF‐ and TNF‐α‐activated expression of chemokine (C‐X‐C motif) ligand (CXCL)1, CXCL2, CXCL3, CXCL8, CCL3 and CCL4 but, in contrast, increased the expression of CXCL10. These novel data identify complex regulatory signalling networks in which type I IFNs profoundly alter the response of neutrophils to inflammatory cytokines. This is likely to have important consequences in vivo and may explain the complexity and heterogeneity of inflammatory diseases such as RA, in which multiple cytokine cascades have been activated.",2020-09-29,https://www.semanticscholar.org/paper/e4023a06fbb929f6eebe9719d3822ff5451d0b29,Clinical and Experimental Immunology
2521,Exploring interfaces to botanical species classification,"We have developed several prototype user interfaces for botanical species identification and data collection across a diversity of platforms including Tablet PC, Ultra Mobile PC (UMPC), Apple iPhone, Augmented Reality, and Microsoft Surface. In our demonstration, we show UMPC and iPhone user interfaces, discuss the commonalities and distinctions across the different interfaces, and invite visitors to explore these differences. Our prototypes address several issues of interest to the CHI community including mobile interfaces, interfaces to object recognition, and visualization.",2010-04-09,https://www.semanticscholar.org/paper/7e5ef85047567f9f86fb6a19e79b750abb78d136,CHI Extended Abstracts
1967,Manufacturing intelligence and innovation for digital manufacturing and operational excellence,,2014-10-01,https://www.semanticscholar.org/paper/9de0053e27006d20c99be1ea158db6592447beec,Journal of Intelligent Manufacturing
631,The adjacency relation on the traveling salesman polytope is NP-Complete,,1978-12-01,https://www.semanticscholar.org/paper/2578271780622b98d9a9316e927781db3a1942ea,Mathematical programming
2525,Session details: Graphs,,2010-04-10,https://www.semanticscholar.org/paper/bdcd648da25f4758b72c14f0e0bffbdc1ff9a742,Proceedings of the SIGCHI Conference on Human Factors in Computing Systems
3694,Adversarially Robust Video Perception by Seeing Motion,"Despite their excellent performance, state-of-the-art computer vision models often fail when they encounter adversarial examples. Video perception models tend to be more fragile under attacks, because the adversary has more places to manipulate in high-dimensional data. In this paper, we find one reason for video models' vulnerability is that they fail to perceive the correct motion under adversarial perturbations. Inspired by the extensive evidence that motion is a key factor for the human visual system, we propose to correct what the model sees by restoring the perceived motion information. Since motion information is an intrinsic structure of the video data, recovering motion signals can be done at inference time without any human annotation, which allows the model to adapt to unforeseen, worst-case inputs. Visualizations and empirical experiments on UCF-101 and HMDB-51 datasets show that restoring motion information in deep vision models improves adversarial robustness. Even under adaptive attacks where the adversary knows our defense, our algorithm is still effective. Our work provides new insight into robust video perception algorithms by using intrinsic structures from the data. Our webpage is available at https://motion4robust.cs.columbia.edu.",2022-12-13,https://www.semanticscholar.org/paper/384d527447f96c7efec56a9bd0a352f9a764ed63,arXiv.org
372,"The Internet, the Web, and Algorithms",,2002-04-03,https://www.semanticscholar.org/paper/7addbcc81bdaf134cee73cd945465d52e9961376,Latin American Symposium on Theoretical Informatics
871,ON THE COMPLEXITY OF LOCAL SEARCH (Extended Abstract),,,https://www.semanticscholar.org/paper/86e638c69caad65dbdc93690c8ee1be7f00ef528,Symposium on the Theory of Computing
2962,Transcriptome Sequencing of a Large Human Family Identifies the Impact of Rare Noncoding Variants,,2014-09-01,https://www.semanticscholar.org/paper/22477e3148a4d3959ee681f19f74b85be5fe15c1,American Journal of Human Genetics
2064,Economic analysis of 450mm wafer migration,"To achieve the required continuous cost reduction driven by Moore's Law, both miniaturization through technology advances and wafer size increase have been employed in order to maintain the growth and profitability of semiconductor industry. Although some technical analyses have been done for 450 mm migration, little research has been done on economic analysis to justify the decisions and thus suggest appropriate timing for 450 mm migration. This study aims to fill the gap by proposing a preliminary economic analysis to clarify some myths and facilitate further discussions concerning collaborations among the stakeholders including equipment vendors, customers, and chipmakers.",2007-10-01,https://www.semanticscholar.org/paper/68e6c40bd71c6615ce9045830562d376f00f9e5c,International Symposium on Semiconductor Manufacturing
3259,From Pleistocene to trophic rewilding: A wolf in sheep’s clothing,"Nearly 10 y ago, we (1) critiqued the idea of Pleistocene rewilding (2), a misguided attempt to resurrect bygone ecosystems. Much has happened to the Earth’s biodiversity over the decade since the term “Pleistocene rewilding” was coined, most of it bad. More than half a billion people have been added to the world’s population, and ecosystems continue to be degraded at an alarming rate. A sixth mass extinction is underway, and poaching of megafauna has increased across sub-Saharan Africa. Unfortunately, one thing that has not happened is any serious attempt to scientifically study Pleistocene rewilding. Despite a number of publicized Pleistocene rewilding projects (Oostvaardersplassen in The Netherlands and Pleistocene Park in Siberia), we have yet to see any quantitative data concerning the impacts of megafauna reintroductions.",2015-12-16,https://www.semanticscholar.org/paper/8803ad1b73328267d1b087f4af48fa5ef7c2baba,Proceedings of the National Academy of Sciences of the United States of America
3567,Reliable and Efficient Concurrent Synchronization for Embedded Real-Time Software,"The high degree of autonomy and increased complexity of future robotic spacecraft pose significant challenges in assuring their reliability and efficiency. To achieve fast and safe concurrent interactions in mission critical code, we survey the practical state-of-the-art nonblocking programming techniques. We study in detail two nonblocking approaches: (1) CAS-based algorithms and (2) Software Transactional Memory. We evaluate the strengths and weaknesses of each approach by applying each methodology for engineering the design and implementation of a nonblocking shared vector. Our study investigates how the application of nonblocking synchronization can help eliminate the problems of deadlock, livelock, and priority inversion and at the same time deliver a performance improvement in embedded real-time software.",2009-07-19,https://www.semanticscholar.org/paper/0b553242f822c4a7443bc3fd0d6b378276465f7c,2009 Third IEEE International Conference on Space Mission Challenges for Information Technology
2880,Expression of galectin-3 modulates T-cell growth and apoptosis.,"Galectin-3 is a member (if a large family of beta-galactoside-binding animal lectins. It has been shown that the expression of galectin-3 is upregulated in proliferating cells, suggesting a possible role for this lectin in regulation of cell growth. Previously, we have shown that T cells infected with human T-cell leukemia virus type I express high levels of galectin-3, in contrast to uninfected cells, which do not express detectable amounts of this protein. In this study, we examined growth properties of human leukemia T cells transfected with galectin-3 cDNA, and thus constitutively overexpressing this lectin. Transfectants expressing galectin-3 displayed higher growth rates than control transfectants, which do not express this lectin. Furthermore, galectin-3 expression in these cells confers resistance to apoptosis induced by anti-Fas antibody and staurosporine. Galectin-3 was found to have significant sequence similarity with Bcl-2, a well-characterized suppressor of apoptosis. In particular, the lectin contains the NWGR motif that is highly conserved among members of the Bcl-2 family and shown to be critical for the apoptosis-suppressing activity. We further demonstrated that galectin-3 interacts with Bc1-2 in a lactose-inhibitable manner. We conclude that galectin-3 is a regulator of cell growth and apoptosis and it may function through a cell death inhibition pathway that involves Bcl-2.",1996-06-25,https://www.semanticscholar.org/paper/33f780243e1364011d53512bab723e0c78209561,Proceedings of the National Academy of Sciences of the United States of America
431,Incremental Recompilation of Knowledge,"Approximating a general formula from above and below by Horn formulas (its Horn envelope and Horn core, respectively) was proposed by Selman and Kautz (1991, 1996) as a form of ""knowledge compilation,"" supporting rapid approximate reasoning; on the negative side, this scheme is static in that it supports no updates, and has certain complexity drawbacks pointed out by Kavvadias, Papadimitriou and Sideri (1993). On the other hand, the many frameworks and schemes proposed in the literature for theory update and revision are plagued by serious complexity-theoretic impediments, even in the Horn case, as was pointed out by Eiter and Gottlob (1992), and is further demonstrated in the present paper. More fundamentally, these schemes are not inductive, in that they may lose in a single update any positive properties of the represented sets of formulas (small size, Horn structure, etc.). In this paper we propose a new scheme, incremental recompilation, which combines Horn approximation and model-based updates; this scheme is inductive and very efficient, free of the problems facing its constituents. A set of formulas is represented by an upper and lower Horn approximation. To update, we replace the upper Horn formula by the Horn envelope of its minimum-change update, and similarly the lower one by the Horn core of its update; the key fact which enables this scheme is that Horn envelopes and cores are easy to compute when the underlying formula is the result of a minimum-change update of a Horn formula by a clause. We conjecture that efficient algorithms are possible for more complex updates.",1997-12-31,https://www.semanticscholar.org/paper/3a12e22b1ca1fe84c475b4c2f8e52ce8582f2f21,Journal of Artificial Intelligence Research
535,The parallel complexity of simple chain queries,"They are written m a notation called DATALOG, that IS, PROLOG without function symbols and other lmpurities (an orthogonal way of vlewmg DATALOG 1s as Relatlonal Calculus with the additional power of recurslon) For more on DATALOG see, for example, [UV] Both queries above define a view S m terms of the database relations a and b We are Interested m the parallel complexJtyof these and similar queries, that is, the degree to which such queries are amenable to rapid parallel evaluation by the cooperation of many processors Recently, m view of the projected avallablhty of multlprocessmg systems with a very large number of processors, there has been much Interest m such a classlficatlon of computational problems In particular, it has been proposed that a problem be considered satlsfactorlly solved m parallel if there IS an algorithm for it which can be rendered as a circuit with a polynomial number of gates, andpoiyiogarlthmlc depth (that IS, of depth O(logk n), where n IS the length of the input) The class of all problems thus solvable 1s called NC [Co] Obviously, NC 1s a subset of P, the class of all problems solvable m polynomial sequentd time (It IS perhaps amusing that, m response to a sequence of lmpresslve breakthroughs m computmg technology,",1987-06-01,https://www.semanticscholar.org/paper/052106387e31bc2996721a168027ad86b8b0ac00,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems
3505,Finding Real-Valued Single-Source Shortest Paths,,1996-06-03,https://www.semanticscholar.org/paper/2c7317e7bce5fd473b5eea232d1ffdd27429b0a5,Conference on Integer Programming and Combinatorial Optimization
3707,RESIN: A Dockerized Schema-Guided Cross-document Cross-lingual Cross-media Information Extraction and Event Tracking System,"We present a new information extraction system that can automatically construct temporal event graphs from a collection of news documents from multiple sources, multiple languages (English and Spanish for our experiment), and multiple data modalities (speech, text, image and video). The system advances state-of-the-art from two aspects: (1) extending from sentence-level event extraction to cross-document cross-lingual cross-media event extraction, coreference resolution and temporal event tracking; (2) using human curated event schema library to match and enhance the extraction output. We have made the dockerlized system publicly available for research purpose at GitHub, with a demo video.",,https://www.semanticscholar.org/paper/039ce73659332c12168de439e3f79e7039b636af,North American Chapter of the Association for Computational Linguistics
1828,PUTOP: Turning Predominant Senses into a Topic Model for Word Sense Disambiguation,"We extend on McCarthy et al.'s predominant sense method to create an unsupervised method of word sense disambiguation that uses automatically derived topics using Latent Dirichlet allocation. Using topic-specific synset similarity measures, we create predictions for each word in each document using only word frequency information. It is hoped that this procedure can improve upon the method for larger numbers of topics by providing more relevant training corpora for the individual topics. This method is evaluated on SemEval-2007 Task 1 and Task 17.",2007-06-23,https://www.semanticscholar.org/paper/ad3ca6979f82d7926dfcae949b099b2aa36f3df1,International Workshop on Semantic Evaluation
2147,Online Broadcasting with Network Coding,"Consider a source broadcasting M packets to N receivers over independent erasure channels, where perfect feedback is available from the receivers to the source, and the source is allowed to use coding. We investigate offline and online algorithms that optimize delay, both through theoretical analysis as well as simulation results.",2008-03-31,https://www.semanticscholar.org/paper/b8c9dd37746f10f466f0cf59afd29a7b7e09decd,"2008 Fourth Workshop on Network Coding, Theory and Applications"
2106,Managing technologies to enhance and enrich services in high-tech industry,"New information technologies and business models have had a profound effect on how services are created and delivered, in particular with regard to the innovation, contents, delivery methods, systems, enabling technologies and management. This special issue of the International Journal of Services Technology and Management (IJSTM) focuses on all aspects of managing the hard and soft technologies to enhance and enrich services in high-tech industries, e.g., semiconductor manufacturing. This introduction gives an overview of this special issue from different perspectives.",,https://www.semanticscholar.org/paper/4595c2abff770a6d774a320762e6576610c3726f,International Journal of Services Technology and Management
2685,Negotiation for automated generation of temporal multimedia presentations,"Creating high-quality multimedia presentations requires much skill, time, and effort. This is particularly true when temporal media, such as speech and animation, are involved. We describe the design and implementation of a knowledge-based system that generates customized temporal multimedia presentations. We provide an overview of the system’s architecture, and explain how speech, written text, and graphics are generated and coordinated. Our emphasis is on how temporal media are coordinated by the system through a multi-stage negotiation process. In negotiation, media-specific generation components interact with a novel coordination component that solves temporal constraints provided by the generators. We illustrate our work with a set of examples generated by the system in a testbed application intended to update hospital caregivers on the status of patients who have undergone a cardiac bypass operation.",1997-02-01,https://www.semanticscholar.org/paper/2297f1fef37313a79800b2783315bf8c63263c1b,MULTIMEDIA '96
1418,Exclusion limits on the WIMP-nucleon cross section from the cryogenic dark matter search.,"The Cryogenic Dark Matter Search (CDMS) employs Ge and Si detectors to search for weakly interacting massive particles (WIMPs) via their elastic-scattering interactions with nuclei while discriminating against interactions of background particles. CDMS data, accounting for the neutron background, give limits on the spin-independent WIMP-nucleon elastic-scattering cross section that exclude unexplored parameter space above 10 GeV/c2 WIMP mass and, at >75% C.L., the entire 3sigma allowed region for the WIMP signal reported by the DAMA experiment.",2000-06-19,https://www.semanticscholar.org/paper/0030c4844265dfeb1a3c04540dbb58c039993639,Physical Review Letters
2199,Mucocutaneous manifestations in juvenile-onset systemic lupus erythematosus: a review of literature,,2015-01-05,https://www.semanticscholar.org/paper/958ec48f86a4515de982a824a33b6ebb7beb307d,Pediatric Rheumatology Online Journal
380,Selfish behavior and stability of the internet: a game-theoretic analysis of TCP,"For years, the conventional wisdom [7, 22] has been that the continued stability of the Internet depends on the widespread deployment of ""socially responsible"" congestion control. In this paper, we seek to answer the following fundamental question: If network end-points behaved in a selfish manner, would the stability of the Internet be endangered?.We evaluate the impact of greedy end-point behavior through a game-theoretic analysis of TCP. In this ""TCP Game"" each flowattempts to maximize the throughput it achieves by modifying its congestion control behavior. We use a combination of analysis and simulation to determine the Nash Equilibrium of this game. Our question then reduces to whether the network operates efficiently at these Nash equilibria.Our findings are twofold. First, in more traditional environments -- where end-points use TCP Reno-style loss recovery and routers use drop-tail queues -- the Nash Equilibria are reasonably efficient. However, when endpoints use more recent variations of TCP (e.g., SACK) and routers employ either RED or drop-tail queues, the Nash equilibria are very inefficient. This suggests that the Internet of the past could remain stable in the face of greedy end-user behavior, but the Internet of today is vulnerable to such behavior. Second, we find that restoring the efficiency of the Nash equilibria in these settings does not require heavy-weight packet scheduling techniques (e.g., Fair Queuing) but instead can be done with a very simple stateless mechanism based on CHOKe [21].",2002-10-01,https://www.semanticscholar.org/paper/fe856288df47d00bb48748dc43231e7582341045,"Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication"
2,Detecting Foodborne Illness Complaints in Multiple Languages Using English Annotations Only,"Health departments have been deploying text classification systems for the early detection of foodborne illness complaints in social media documents such as Yelp restaurant reviews. Current systems have been successfully applied for documents in English and, as a result, a promising direction is to increase coverage and recall by considering documents in additional languages, such as Spanish or Chinese. Training previous systems for more languages, however, would be expensive, as it would require the manual annotation of many documents for each new target language. To address this challenge, we consider cross-lingual learning and train multilingual classifiers using only the annotations for English-language reviews. Recent zero-shot approaches based on pre-trained multi-lingual BERT (mBERT) have been shown to effectively align languages for aspects such as sentiment. Interestingly, we show that those approaches are less effective for capturing the nuances of foodborne illness, our public health application of interest. To improve performance without extra annotations, we create artificial training documents in the target language through machine translation and train mBERT jointly for the source (English) and target language. Furthermore, we show that translating labeled documents to multiple languages leads to additional performance improvements for some target languages. We demonstrate the benefits of our approach through extensive experiments with Yelp restaurant reviews in seven languages. Our classifiers identify foodborne illness complaints in multilingual reviews from the Yelp Challenge dataset, which highlights the potential of our general approach for deployment in health departments.",2020-10-11,https://www.semanticscholar.org/paper/10b7e11424bd8778d4def8f0a4b2c5f2eff9a632,International Workshop on Health Text Mining and Information Analysis
3146,Fast Indexing: Support for Size-Changing Algorithms in Stackable File Systems,"Stackable file systems can provide extensible file system functionality with minimal performance overhead and development cost. However, previous approaches provide only limited functionality. In particular, they do not support size-changing algorithms (SCAs), which are important and useful for many applications such as compression and encryption. We propose fast indexing, a technique for efficient support of SCAs in stackable file systems. Fast indexing provides a page mapping between file system layers in a way that can be used with any SCA. We use index files to store this mapping. Index files are designed to be recoverable if lost and add less than 0.1% disk space overhead. We have implemented fast indexing using portable stackable templates, and we have used this system to build several example file systems with SCAs. We demonstrate that fast index files have low overhead for typical user workloads such as large compilations, only 2.3% over other stacked file systems and 4.7%over non-stackable file systems. Our system can deliver better performance with SCAs than userlevel applications, as much as five times faster.",2001-06-25,https://www.semanticscholar.org/paper/535ffd4979373706dc7d4cc6ca670f518fce3f2d,"USENIX Annual Technical Conference, General Track"
1630,Implicit Causal Models for Genome-wide Association Studies,"Progress in probabilistic generative models has accelerated, developing richer models with neural architectures, implicit densities, and with scalable algorithms for their Bayesian inference. However, there has been limited progress in models that capture causal relationships, for example, how individual genetic factors cause major human diseases. In this work, we focus on two challenges in particular: How do we build richer causal models, which can capture highly nonlinear relationships and interactions between multiple causes? How do we adjust for latent confounders, which are variables influencing both cause and effect and which prevent learning of causal relationships? To address these challenges, we synthesize ideas from causality and modern probabilistic modeling. For the first, we describe implicit causal models, a class of causal models that leverages neural architectures with an implicit density. For the second, we describe an implicit causal model that adjusts for confounders by sharing strength across examples. In experiments, we scale Bayesian inference on up to a billion genetic measurements. We achieve state of the art accuracy for identifying causal factors: we significantly outperform existing genetics methods by an absolute difference of 15-45.3%.",2017-10-01,https://www.semanticscholar.org/paper/83cffda7d9b47d0927d03fdc574a019487a3d5d8,International Conference on Learning Representations
2336,"Changes in mechanisms of monocyte/macrophage-mediated cytotoxicity during culture. Reactive oxygen intermediates are involved in monocyte-mediated cytotoxicity, whereas reactive nitrogen intermediates are employed by macrophages in tumor cell killing.","Freshly isolated human blood monocytes were spontaneously cytotoxic toward K562 tumor cells. During culture of the monocytes in vitro cytotoxicity decreased during the first 48 h but tumoricidal competence was restored after 3 to 4 days in vitro. These changes were accompanied by changes in both reactive oxygen intermediate generating capacity and reactive nitrogen intermediate production. Lucigenin-dependent chemiluminescence stimulated with either FMLP or PMA declined during the first 2 days in culture and was negligible during the later days in culture. Superoxide radical production in response to either FMLP or PMA remained fairly constant for the first few days in vitro and then declined. NO2- concentration in monocyte-conditioned medium was fairly constant during the first few days in vitro but increased after 6 days. The return to tumoricidal competence after 3 to 4 days in culture was decreased by the addition of NG-monomethyl-L-arginine. These results indicate that reactive oxygen intermediates are employed by monocytes in the killing of tumor cells. However, after maturation of monocytes to macrophages, this mechanism becomes less important and reactive nitrogen intermediates are employed in mediating macrophage cytotoxicity.",1993-04-15,https://www.semanticscholar.org/paper/87227c615cc851d553e5114c1aab74ec6daa37e1,Journal of Immunology
2142,Real-time delay with network coding and feedback,,2013-03-01,https://www.semanticscholar.org/paper/eb3b4f85af38bc9f855cea4d67dae5cfcd8fe5a2,Physical Communication
880,Shortest Paths Without a Map,,1989-07-11,https://www.semanticscholar.org/paper/ff187225ff569e01e751ebf004076350d4456a51,Theoretical Computer Science
903,On monotone formulae with restricted depth,"We prove a hierarchy theorem for the representation of monotone Boolean functions by monotone formulae with restricted depth. Specifically, we show that there are functions with π<subscrpt>k</subscrpt>-formula of size n for which every &sgr;<subscrpt>k</subscrpt>-formula has size exp ω(n<supscrpt>1/(k−1)</supscrpt>). A similar lower bound applies to concrete functions such as transitive closure and clique. We also show that any function with a formula of size n (and any depth) has a &sgr;<subscrpt>k</subscrpt>-formula of size exp o(n<supscrpt>1/(k−1)</supscrpt>). Thus our hierarchy theorem is the best possible.",1984-12-01,https://www.semanticscholar.org/paper/cbb6dd042a1eee8fcc608354fa5e86a1384b4af2,Symposium on the Theory of Computing
894,How Easy Is Local Search? (Extended Abstract),,,https://www.semanticscholar.org/paper/285711d00040658337aa0ae7abe5ff0c05d70d43,IEEE Annual Symposium on Foundations of Computer Science
3706,Forget-me-not! Contrastive Critics for Mitigating Posterior Collapse,"Variational autoencoders (VAEs) suffer from posterior collapse, where the powerful neural networks used for modeling and inference optimize the objective without meaningfully using the latent representation. We introduce inference critics that detect and incentivize against posterior collapse by requiring correspondence between latent variables and the observations. By connecting the critic's objective to the literature in self-supervised contrastive representation learning, we show both theoretically and empirically that optimizing inference critics increases the mutual information between observations and latents, mitigating posterior collapse. This approach is straightforward to implement and requires significantly less training time than prior methods, yet obtains competitive results on three established datasets. Overall, the approach lays the foundation to bridge the previously disconnected frameworks of contrastive learning and probabilistic modeling with variational autoencoders, underscoring the benefits both communities may find at their intersection.",2022-07-19,https://www.semanticscholar.org/paper/f354354d32e820ce55f26f2cb6508599df8cc698,Conference on Uncertainty in Artificial Intelligence
325,The complexity of low-distortion embeddings between point sets,We prove that it is NP-hard to approximate by a ratio better than 3 the minimum distortion of a bijection between two given finite three-dimensional sets of points.,2005-01-23,https://www.semanticscholar.org/paper/0e007f56b766cd711c321af344b1156480074933,ACM-SIAM Symposium on Discrete Algorithms
527,Probabilistic satisfiability,,1988-03-01,https://www.semanticscholar.org/paper/4005f6485750a2b6b922a6af9d2be6aa3c42b40d,Journal of Complexity
161,A Biologically Plausible Parser,"Abstract We describe a parser of English effectuated by biologically plausible neurons and synapses, and implemented through the Assembly Calculus, a recently proposed computational framework for cognitive function. We demonstrate that this device is capable of correctly parsing reasonably nontrivial sentences.1 While our experiments entail rather simple sentences in English, our results suggest that the parser can be extended beyond what we have implemented, to several directions encompassing much of language. For example, we present a simple Russian version of the parser, and discuss how to handle recursion, embedding, and polysemy.",2021-08-04,https://www.semanticscholar.org/paper/e1a6360d8694f3b7e0fbc27b7e0464a91a97d644,Transactions of the Association for Computational Linguistics
3297,Reproductive status influences group size and persistence of bonds in male plains zebra (Equus burchelli),,2009-04-02,https://www.semanticscholar.org/paper/46402a7ecb8ebfdef6553ff07082a82a6f637259,Behavioral Ecology and Sociobiology
1581,Topic Modeling in Embedding Spaces,"Abstract Topic modeling analyzes documents to learn meaningful patterns of words. However, existing topic models fail to learn interpretable topics when working with large and heavy-tailed vocabularies. To this end, we develop the embedded topic model (etm), a generative model of documents that marries traditional topic models with word embeddings. More specifically, the etm models each word with a categorical distribution whose natural parameter is the inner product between the word’s embedding and an embedding of its assigned topic. To fit the etm, we develop an efficient amortized variational inference algorithm. The etm discovers interpretable topics even with large vocabularies that include rare words and stop words. It outperforms existing document models, such as latent Dirichlet allocation, in terms of both topic quality and predictive performance.",2019-07-08,https://www.semanticscholar.org/paper/d5f47453a6d00ede2881dfb65fc7ea141a50deeb,Transactions of the Association for Computational Linguistics
3144,The Performance of Remote Display Mechanisms for Thin-Client Computing,"The growing popularity of thin-client systems makes it important to determine the factors that govern the performance of these thin-client architectures. To assess the viability of the thin-client computing model, we measured the performance of six popular thin-client platforms—Citrix MetaFrame, Microsoft Terminal Services, Sun Ray, Tarantella, VNC, and X—running over a wide range of network access bandwidths. We find that thinclient systems can perform well on web and multimedia applications in LAN environments, but the efficiency of the thin-client protocols varies widely. We analyze the differences in the various approaches and explain the impact of the underlying remote display protocols on overall performance. Our results quantify the impact of different approaches in display encoding primitives, display update policies, and display caching and compression techniques across a broad range of thin-client systems.",2002-06-10,https://www.semanticscholar.org/paper/e60a892427b03bfbcbbcfb058ef310708006276c,"USENIX Annual Technical Conference, General Track"
3652,Panel: Object-Oriented Languages: Premises and Promises,,,https://www.semanticscholar.org/paper/7a6b0bee8b14bb22464c5fd7098cf41c9fe4d778,"Conference on Object-Oriented Programming Systems, Languages, and Applications"
3475,Scheduling Multi-task Agents,,2001-12-02,https://www.semanticscholar.org/paper/25d78fba4f9f4b5543f3ff1298ed10e4a0baf9af,Mobile Agents
888,A Note on Succinct Representations of Graphs,,1986-12-01,https://www.semanticscholar.org/paper/087f7f09020cb5f65b990b9c7d6fca85b13f59c1,Information and Control
244,Multiplicative updates in coordination games and the theory of evolution,"In this paper we point out a new and unexpected connection between three fields: Evolution Theory, Game Theory, and Algorithms.
 In particular, we study the standard equations of population genetics for Evolution, in the presence of recombination (sex), focusing on the important special case of weak selection [1,2] in which all fitness values are assumed to be close to one another. Weak selection is the mathematical regime capturing the widely accepted Neutral Theory proposed by Kimura in the 1970s [3], hypothesizing that evolution proceeds for the most part not by substantial increases in fitness but by essentially random drift. We show that in this regime evolution through natural selection and sex is tantamount to a game played through the multiplicative weight updates game dynamics [4]. The players of the game are the genes (genetic loci), the strategies available to each player are the alleles of the gene, and the probabilities whereby a player plays a strategy is the strategy's frequency in the population. The utility to each player/gene of each strategy profile is the fitness of the corresponding genotype (organism). That is, the game is a coordination game between genes, in which the players' interests are perfectly aligned. Importantly, the utility maximized in this game, as well as the amount by which each allele is boosted, is precisely the allele's mixability, or average fitness, a quantity recently proposed in [5] as a novel concept that is crucial in understanding natural selection under sex, thus providing a rigorous demonstration of that insight.
 We also establish a result regarding the maintenance of genetic diversity (multiplicity of alleles per gene). We prove that the equilibria in two-person coordination games are likely to have large supports, and thus genetic diversity need not suffer much at equilibrium. Establishing large supports involves answering through a novel technique the following question: what is the probability that for a random square matrix $A$ (with entries drawn independently from smooth distributions that are symmetric around zero) both systems Ax=1 and ATy=1 have positive solutions? The proof is through a simple potential function argument. Both the question and the technique may be of broader interest.
 It has often seemed astonishing --- even to experienced students of Evolution, Darwin included --- that the crude mechanism of natural selection is responsible for producing the dazzling variety of Life around us. The present mathematical connection of Evolution with the multiplicative weight updates algorithm --- a technique that has surprised our field time and again with its fantastic effectiveness and versatility --- may carry some explanatory force in this regard.",2012-08-15,https://www.semanticscholar.org/paper/3ae474d4ad8ed141cb5d5a3ad48588544496bba1,Information Technology Convergence and Services
149,Topological mobile robot localization using fast vision techniques,"We present a system for topologically localizing a mobile robot using color histogram matching of omnidirectional images. The system is intended for use as a navigational tool for the autonomous vehicle for exploration and navigation of urban environments (AVENUE) mobile robot. Our method makes use of omnidirectional images which are acquired from the robot's on-board camera. The method is fast and rotation invariant. Our tests have indicated that normalized color histograms are best for an outdoor environment while normalization is not required for indoor work. The system quickly narrows down the robot's location to one or two regions within the much larger test environment. Using this regional localization information, other vision systems that we have developed can further localize the robot.",2002-08-07,https://www.semanticscholar.org/paper/06e7faf4adc8be14a7259616141b867f6a2482b8,Proceedings 2002 IEEE International Conference on Robotics and Automation (Cat. No.02CH37292)
520,The bisection width of grid graphs,,,https://www.semanticscholar.org/paper/ff4c5479bfc075b6829a9d5f2005cf26763af099,ACM-SIAM Symposium on Discrete Algorithms
1240,Evidence for the decay Bs0-->Ds(*)Ds(*) and a measurement of DeltaGammasCP/Gammas.,"We search for the semi-inclusive process Bs0-->Ds(*)Ds(*) using 2.8 fb(-1) of pp collisions at sqrt[s]=1.96 TeV recorded by the D0 detector operating at the Fermilab Tevatron Collider. We observe 26.6+/-8.4 signal events with a significance above background of 3.2 standard deviations yielding a branching ratio of B(Bs0-->Ds(*)Ds(*))=0.035+/-0.010(stat.)+/-0.011(syst.). Under certain theoretical assumptions, these double-charm final states saturate CP-even eigenstates in the Bs0 decays resulting in a width difference of DeltaGammasCP/Gammas=0.072+/-0.021(stat.)+/-0.022(syst.).",2008-11-13,https://www.semanticscholar.org/paper/968a8a289dfa1e1c1033140ba281fdb3d0464b56,Physical Review Letters
3170,Emergent Network Patterns of Internal Displacement in Somalia Driven by Natural Disasters and Conflicts,,,https://www.semanticscholar.org/paper/edc96599f2c3846b0d05ad1335d1f7d3e7b779ee,Social Science Research Network
501,"On Total Functions, Existence Theorems and Computational Complexity",,1991-04-30,https://www.semanticscholar.org/paper/96c51fb7999f560e8f28db772603fb6541560d25,Theoretical Computer Science
91,Using q-grams in a DBMS for Approximate String Processing,"String data is ubiquitous, and its management has taken on particular importance in the past few years. Approximate queries are very important on string data. This is due, for example, to the prevalence of typographical errors in data, and multiple conventions for recording attributes such as name and address. Commercial databases do not support approximate string queries directly, and it is a challenge to implement this functionality efficiently with user-defined functions (UDFs). In this paper, we develop a technique for building approximate string processing capabilities on top of commercial databases by exploiting facilities already available in them. At the core, our technique relies on generating short substrings of length q, called q-grams, and processing them using standard methods available in the DBMS. The proposed technique enables various approximate string processing methods in a DBMS, for example approximate (sub)string selections and joins, and can even be used with a variety of possible edit distance functions. The approximate string match predicate, with a suitable edit distance threshold, can be mapped into a vanilla relational expression and optimized by conventional relational optimizers.",,https://www.semanticscholar.org/paper/72d116438a2e7ab0902e3b5fe9fa4cf37c18f5a5,IEEE Data Engineering Bulletin
1624,SHOPPER: A Probabilistic Model of Consumer Choice with Substitutes and Complements,"We develop SHOPPER, a sequential probabilistic model of market baskets. SHOPPER uses interpretable components to model the forces that drive how a customer chooses products; in particular, we designed SHOPPER to capture how items interact with other items. We develop an efficient posterior inference algorithm to estimate these forces from large-scale data, and we analyze a large dataset from a major chain grocery store. We are interested in answering counterfactual queries about changes in prices. We found that SHOPPER provides accurate predictions even under price interventions, and that it helps identify complementary and substitutable pairs of products.",2017-11-09,https://www.semanticscholar.org/paper/3a4bea2033c86ac1d26f76d1d9c07216c9a1a541,Annals of Applied Statistics
3618,Exception Safety: Concepts and Techniques,,,https://www.semanticscholar.org/paper/d43340c4b07b7f15b81ebae919a9daa86fa800d5,Advances in Exception Handling Techniques
886,On Generating All Maximal Independent Sets,,1988-03-25,https://www.semanticscholar.org/paper/bf6af5a2d45964c7daf45a22976d6f2ffe205c16,Information Processing Letters
1638,Variational Sequential Monte Carlo,"Many recent advances in large scale probabilistic inference rely on variational methods. The success of variational approaches depends on (i) formulating a flexible parametric family of distributions, and (ii) optimizing the parameters to find the member of this family that most closely approximates the exact posterior. In this paper we present a new approximating family of distributions, the variational sequential Monte Carlo (VSMC) family, and show how to optimize it in variational inference. VSMC melds variational inference (VI) and sequential Monte Carlo (SMC), providing practitioners with flexible, accurate, and powerful Bayesian inference. The VSMC family is a variational family that can approximate the posterior arbitrarily well, while still allowing for efficient optimization of its parameters. We demonstrate its utility on state space models, stochastic volatility models for financial data, and deep Markov models of brain neural circuits.",2017-05-31,https://www.semanticscholar.org/paper/fbb8aaac1b6bc7326fad06c4bb8ef10c61c8f1d2,International Conference on Artificial Intelligence and Statistics
2520,Physician-driven management of patient progress notes in an intensive care unit,"We describe fieldwork in which we studied hospital ICU physicians and their strategies and documentation aids for composing patient progress notes. We then present a clinical documentation prototype, activeNotes, that supports the creation of these notes, using techniques designed based on our fieldwork. ActiveNotes integrates automated, context-sensitive patient data retrieval, and user control of automated data updates and alerts via tagging, into the documentation process. We performed a qualitative study of activeNotes with 15 physicians at the hospital to explore the utility of our information retrieval and tagging techniques. The physicians indicated their desire to use tags for a number of purposes, some of them extensions to what we intended, and others new to us and unexplored in other systems of which we are aware. We discuss the physicians' responses to our prototype and distill several of their proposed uses of tags: to assist in note content management, communication with other clinicians, and care delivery.",2010-04-10,https://www.semanticscholar.org/paper/656b7bd6457bc04070f750462337ba868e8d72c7,International Conference on Human Factors in Computing Systems
1534,Causal inference from text: A commentary,Statistical and machine learning methods help social scientists and other researchers make causal inferences from texts.,2022-10-01,https://www.semanticscholar.org/paper/f924b383cc7e83e454f8a43e7ff60355539e2110,Science Advances
673,Model-Agnostic Meta-Learning using Runge-Kutta Methods,"Meta-learning has emerged as an important framework for learning new tasks from just a few examples. The success of any meta-learning model depends on (i) its fast adaptation to new tasks, as well as (ii) having a shared representation across similar tasks. Here we extend the model-agnostic meta-learning (MAML) framework introduced by Finn et al. (2017) to achieve improved performance by analyzing the temporal dynamics of the optimization procedure via the Runge-Kutta method. This method enables us to gain fine-grained control over the optimization and helps us achieve both the adaptation and representation goals across tasks. By leveraging this refined control, we demonstrate that there are multiple principled ways to update MAML and show that the classic MAML optimization is simply a special case of second-order Runge-Kutta method that mainly focuses on fast-adaptation. Experiments on benchmark classification, regression and reinforcement learning tasks show that this refined control helps attain improved results.",2019-10-16,https://www.semanticscholar.org/paper/b2fe8068465a01c9d030869ad3576a2760b6110f,arXiv.org
3217,"Apparent Competition, Lion Predation, and Managed Livestock Grazing: Can Conservation Value Be Enhanced?","Predator restorations often result in apparent competition, where co-occurring prey populations experience asymmetric predation pressure driven by predator preferences. In many rangeland ecosystems, livestock share the landscape with wildlife, including ungulates and the large carnivores that consume them. We examined whether apparent competition reorganized prey communities following restoration of lions (Panthera leo) to a savanna ecosystem, and whether and how livestock management could alter this indirect interaction between lions and their prey. Three lines of evidence supported the hypothesis that Jackson’s hartebeest (Alcelaphus bucelaphus lelwel; an ungulate of conservation concern) are suppressed via lion-mediated apparent competition. First, hartebeest exhibited an Allee effect where they were exposed to lions, but displayed negative density-dependent population growth where they were protected from lions. Second, spatial overlap between plains zebra (Equus burchelli; the primary prey of lions) and hartebeest further exacerbated lion predation on hartebeest. Finally, hartebeest were killed selectively by lions, whereas zebra were killed by lions in proportion to their abundance. We then tested whether glades (nutrient-rich hotspots created by abandoned cattle [Bos indicus] corrals) could be used to manipulate top-down control of hartebeest via their influence on the spatial distribution of zebra. Zebra aggregated at glades, and survival of hartebeest increased with increasing distance from glades, suggesting that corrals may be placed on the landscape away from hartebeest to create spatial refuges from lions. Our findings demonstrate how informed placement of livestock corrals can be used to manipulate the spatial distribution of primary prey (zebra), thereby reducing apparent competition suffered by hartebeest. Our work further provides an example of how integrating apparent competition theory with proactive livestock management can improve conservation efforts in multiple-use landscapes.",,https://www.semanticscholar.org/paper/eda4a9915eaced140eb4376ef59c5b161091720a,Frontiers in Ecology and Evolution
1793,Building and using a semantivisual image hierarchy,"A semantically meaningful image hierarchy can ease the human effort in organizing thousands and millions of pictures (e.g., personal albums), and help to improve performance of end tasks such as image annotation and classification. Previous work has focused on using either low-level image features or textual tags to build image hierarchies, resulting in limited success in their general usage. In this paper, we propose a method to automatically discover the “semantivisual” image hierarchy by incorporating both image and tag information. This hierarchy encodes a general-to-specific image relationship. We pay particular attention to quantifying the effectiveness of the learned hierarchy, as well as comparing our method with others in the end-task applications. Our experiments show that humans find our semantivisual image hierarchy more effective than those solely based on texts or low-level visual features. And using the constructed image hierarchy as a knowledge ontology, our algorithm can perform challenging image classification and annotation tasks more accurately.",2010-06-13,https://www.semanticscholar.org/paper/fbac75009db521a7dbe08344316bb538f039ac00,2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition
3578,Runtime concepts for the C++ standard template library,"A key benefit of generic programming is its support for producing modules with clean separation. In particular, generic algorithms are written to work with a wide variety of unmodified types. The Runtime concept idiom extends this support by allowing unmodified concrete types to behave in a runtime polymorphic manner. In this paper, we describe one implementation of the runtime concept idiom, in the domain of the C++ standard template library (STL). We describe and measure the performance of runtime-polymorphic analogs of several STL algorithms. We augment the runtime concept idiom by employing a dispatch mechanism that considers both type and concept information to maximize performance when selecting algorithm implementations. We use our implementation to demonstrate the effects of different compile-time vs. run-time algorithm selection choices, and we indicate where improved language and compiler support would be useful.",2008-03-16,https://www.semanticscholar.org/paper/2184f48ae1ae03292aab3ad069fdf5af266ec5e5,ACM Symposium on Applied Computing
45,SQL Queries Over Unstructured Text Databases,"Text documents often embed data that is structured in nature. By processing a text database with information extraction systems, we can define a variety of structured ""relations"" over which we can then issue SQL queries. Processing SQL queries in this text-based scenario presents multiple challenges. One key challenge is efficiency: information extraction is a time-consuming process, so query processing strategies should pick efficient extraction systems whenever possible, and also minimize the number of documents that they process. Another key challenge is result quality: extraction systems might output erroneous information or miss information that they should capture; also, efficiency-related query processing decisions (e.g., to avoid processing large numbers of useless documents) may compromise result completeness. To address these challenges, we characterize SQL query processing strategies in terms of their efficiency and result quality, and discuss the (user-specific) tradeoff between these two properties.",2007-04-15,https://www.semanticscholar.org/paper/560877311db77387ce1e822e3374f43eb41be692,IEEE International Conference on Data Engineering
272,The ACM PODS Alberto O. Mendelzon test-of-time-award 2009,"In 2007, the PODS Executive Committee decided to establish a Test-of-Time Award, named after the late Alberto O. Mendelzon, in recognition of his scientific legacy, and his service and dedication to the database community. Mendelzon was an international leader in database theory, whose pioneering and fundamental work has inspired and influenced both database theoreticians and practitioners, and continues to be applied in a variety of advanced settings. He served the database community in many ways; in particular, he served as the General Chair of the PODS conference, and was instrumental in bringing together the PODS and SIGMOD conferences. He also was an outstanding educator, who guided the research of numerous doctoral students and postdoctoral fellows. The Award is to be awarded each year to a paper or a small number of papers published in the PODS proceedings ten years prior, that had the most impact (in terms of research, methodology, or transfer to practice) over the intervening decade. The decision was approved by SIGMOD and the ACM. The funds for the Award were contributed by IBM Toronto.",2009-06-29,https://www.semanticscholar.org/paper/10add187b910d4df9340cc0e23282ad657dfa8b3,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems
2553,Content-aware layout,"We describe content-aware layout (CAL), a technique that automatically arranges windows on a user.s desktop. Unlike conventional window managers that automatically cascade or tile each window without regard to its content, CAL uses information about the contents of windows to help decide if and where they should be placed. We present the approach to designing CAL, as well as its implementation. We then conclude with a discussion about future work and CAL.s potential use in large display environments.",2007-04-28,https://www.semanticscholar.org/paper/17997b04f3c02db4d634fd2418812db0c2a22f92,CHI Extended Abstracts
460,On the k-server conjecture,"We prove that the <italic>work function algorithm</italic> for the <italic>k</italic>-server problem has a competitive ratio at most 2<italic>k</italic>−1. Manasse et al. [1988] conjectured that the competitive ratio for the <italic>k</italic>-server problem is exactly <italic>k</italic> (it is trivially at least <italic>k</italic>); previously the best-known upper bound was exponential in <italic>k</italic>. Our proof involves three crucial ingredients: A <italic>quasiconvexity property</italic> of work functions, a <italic>duality lemma</italic> that uses quasiconvexity to characterize the configuration that achieve maximum increase of the work function, and a <italic>potential function</italic> that exploits the duality lemma.",1995-09-01,https://www.semanticscholar.org/paper/fff192ac4261f92383529b6405213430492758af,JACM
1871,Advanced quality control for probe precision forming to empower virtual vertical integration for semiconductor manufacturing,,2023-07-01,https://www.semanticscholar.org/paper/7cdb583b78ecf19f8c766822387b1747102e0dde,Computers & industrial engineering
1567,Poisson-Randomized Gamma Dynamical Systems,"This paper presents the Poisson-randomized gamma dynamical system (PRGDS), a model for sequentially observed count tensors that encodes a strong inductive bias toward sparsity and burstiness. The PRGDS is based on a new motif in Bayesian latent variable modeling, an alternating chain of discrete Poisson and continuous gamma latent states that is analytically convenient and computationally tractable. This motif yields closed-form complete conditionals for all variables by way of the Bessel distribution and a novel discrete distribution that we call the shifted confluent hypergeometric distribution. We draw connections to closely related models and compare the PRGDS to these models in studies of real-world count data sets of text, international events, and neural spike trains. We find that a sparse variant of the PRGDS, which allows the continuous gamma latent states to take values of exactly zero, often obtains better predictive performance than other models and is uniquely capable of inferring latent structures that are highly localized in time.",2019-10-28,https://www.semanticscholar.org/paper/0ee3535732c7fe44ae43e1a6d77ce1ff770bf208,Neural Information Processing Systems
1646,A Variational Analysis of Stochastic Gradient Algorithms,"Stochastic Gradient Descent (SGD) is an important algorithm in machine learning. With constant learning rates, it is a stochastic process that, after an initial phase of convergence, generates samples from a stationary distribution. We show that SGD with constant rates can be effectively used as an approximate posterior inference algorithm for probabilistic modeling. Specifically, we show how to adjust the tuning parameters of SGD such as to match the resulting stationary distribution to the posterior. This analysis rests on interpreting SGD as a continuous-time stochastic process and then minimizing the Kullback-Leibler divergence between its stationary distribution and the target posterior. (This is in the spirit of variational inference.) In more detail, we model SGD as a multivariate Ornstein-Uhlenbeck process and then use properties of this process to derive the optimal parameters. This theoretical framework also connects SGD to modern scalable inference algorithms; we analyze the recently proposed stochastic gradient Fisher scoring under this perspective. We demonstrate that SGD with properly chosen constant rates gives a new way to optimize hyperparameters in probabilistic models.",2016-02-08,https://www.semanticscholar.org/paper/2a91c778288f67c856041447f350a1a022fc6554,International Conference on Machine Learning
1368,First results from the Cryogenic Dark Matter Search in the Soudan Underground Laboratory.,"We report the first results from a search for weakly interacting massive particles (WIMPs) in the Cryogenic Dark Matter Search experiment at the Soudan Underground Laboratory. Four Ge and two Si detectors were operated for 52.6 live days, providing 19.4 kg d of Ge net exposure after cuts for recoil energies between 10 and 100 keV. A blind analysis was performed using only calibration data to define the energy threshold and selection criteria for nuclear-recoil candidates. Using the standard dark-matter halo and nuclear-physics WIMP model, these data set the world's lowest exclusion limits on the coherent WIMP-nucleon scalar cross section for all WIMP masses above 15 GeV/c2, ruling out a significant range of neutralino supersymmetric models. The minimum of this limit curve at the 90% C.L. is 4 x 10(-43) cm2 at a WIMP mass of 60 GeV/c2.",2004-11-19,https://www.semanticscholar.org/paper/4cbcf135a2adddc02645223fc22c05e631692ea0,Physical Review Letters
1488,Pion and kaon pair production in photon-photon collisions.,"We report measurements of the two-photon processes e-italic/sup +/e/sup -/..-->..e/sup +/e/sup -/..pi../sup +/..pi../sup -/ and e-italic/sup +/e/sup -/..-->..e/sup +/e/sup -/K/sup +/K/sup -/, at an e-italic/sup +/e/sup -/ center-of-mass energy of 29 GeV. In the ..pi../sup +/..pi../sup -/ data a high-statistics analysis of the f-italic(1270) results in a ..gamma gamma.. width GAMMA(..gamma gamma -->..f-italic) = 3.2 +- 0.4 keV. The ..pi../sup +/..pi../sup -/ continuum below the f-italic mass is well described by a QED Born approximation, whereas above the f-italic mass it is consistent with a QCD-model calculation if a large contribution from the f-italic is assumed. For the K-italic/sup +/K/sup -/ data we find agreement of high-mass continuum with the QCD prediction; limits on f-italic'(1520) and t-italich-italice-italict-italica-italic(1720) formation are presented.",1986-07-28,https://www.semanticscholar.org/paper/116d85e7bc8ea1690264329fa8dc044e47db2254,Physical Review Letters
1490,Development of the cerenkov ring imaging detector for the SLD,"Results of recent beam tests of a physics prototype cerenkov Ring Imaging Detector (CRID) for the SLD are reported. The system includes both liquid (C6F14) and gas (isobutane) radiators and an 80 cm quartz TPC with a gaseous TMAE photocathode and proportional wire readout. Measurements of the quality factor (N0) and cerenkov angles of both radiators at various TMAE concentrations and beam momenta are presented. Other system characteristics, including electron lifetimes, spatial resolution, ""photon feedback"" and preliminary results from third coordinate charge division readout are discussed.",1986-02-01,https://www.semanticscholar.org/paper/dc0e3fa947631ca98cff8c211d52162930c090a7,IEEE Transactions on Nuclear Science
1791,Nonparametric Density Estimation for Stochastic Optimization with an Observable State Variable,"In this paper we study convex stochastic optimization problems where a noisy objective function value is observed after a decision is made. There are many stochastic optimization problems whose behavior depends on an exogenous state variable which affects the shape of the objective function. Currently, there is no general purpose algorithm to solve this class of problems. We use nonparametric density estimation to take observations from the joint state-outcome distribution and use them to infer the optimal decision for a given query state s. We propose two solution methods that depend on the problem characteristics: function-based and gradient-based optimization. We examine two weighting schemes, kernel based weights and Dirichlet process based weights, for use with the solution methods. The weights and solution methods are tested on a synthetic multi-product newsvendor problem and the hour ahead wind commitment problem. Our results show that in some cases Dirichlet process weights offer substantial benefits over kernel based weights and more generally that nonparametric estimation methods provide good solutions to otherwise intractable problems.",2010-12-06,https://www.semanticscholar.org/paper/e700bd326cd50ac0154f6e58f2adab9fa51b3128,Neural Information Processing Systems
2437,Hands-free augmented reality for vascular interventions,"During a vascular intervention (a type of minimally invasive surgical procedure), physicians maneuver catheters and wires through a patient's blood vessels to reach a desired location in the body. Since the relevant anatomy is typically not directly visible in these procedures, virtual reality and augmented reality systems have been developed to assist in 3D navigation. Because both of a physician's hands may already be occupied, we developed an augmented reality system supporting hands-free interaction techniques that use voice and head tracking to enable the physician to interact with 3D virtual content on a head-worn display while leaving both hands available intraoperatively. We demonstrate how a virtual 3D anatomical model can be rotated and scaled using small head rotations through first-order (rate) control, and can be rigidly coupled to the head for combined translation and rotation through zero-order control. This enables easy manipulation of a model while it stays close to the center of the physician's field of view.",2018-08-12,https://www.semanticscholar.org/paper/8a15675fb927224db6c465ef36037f0ad4a6c343,SIGGRAPH Emerging Technologies
65,Modeling Query-Based Access to Text Databases,"Searchable text databases abound on the web. Applications that require access to such databases often resort to querying to extract relevant documents because of two main reasons. First, some text databases on the web are not “crawlable,” and hence the only way to retrieve their documents is via querying. Second, applications often require only a small fraction of a database’s contents, so retrieving relevant documents via querying is an attractive choice from an efficiency viewpoint, even for crawlable databases. Often an application’s query-based strategy starts with a small number of user-provided queries. Then, new queries are extracted ‐in an application-dependent way‐ from the documents in the initial query results, and the process iterates. The success of this common type of strategy relies on retrieved documents “contributing” new queries. If new documents fail to produce new queries, then the process might stall before all relevant documents are retrieved. In this paper, we develop a graph-based “reachability” metric that allows to characterize when an application’s query-based strategy will successfully “reach” all documents that the application needs. We complement our metric with an efficient sampling-based technique that accurately estimates the reachability associated with a text database and an application’s query-based strategy. We report preliminary experiments backing the usefulness of our metric and the accuracy of the associated estimation technique over real text databases and for two applications.",,https://www.semanticscholar.org/paper/1c474ca9904e9915a85a18683c6be1aa86631375,International Workshop on the Web and Databases
2833,Endogenous galectin-3 is localized in membrane lipid rafts and regulates migration of dendritic cells.,,2009-03-01,https://www.semanticscholar.org/paper/a6afe20d538480398bc3b077885eb312fc5d6573,Journal of Investigative Dermatology
3768,Anticipating Visual Representations from Unlabeled Video,"Anticipating actions and objects before they start or appear is a difficult problem in computer vision with several real-world applications. This task is challenging partly because it requires leveraging extensive knowledge of the world that is difficult to write down. We believe that a promising resource for efficiently learning this knowledge is through readily available unlabeled video. We present a framework that capitalizes on temporal structure in unlabeled video to learn to anticipate human actions and objects. The key idea behind our approach is that we can train deep networks to predict the visual representation of images in the future. Visual representations are a promising prediction target because they encode images at a higher semantic level than pixels yet are automatic to compute. We then apply recognition algorithms on our predicted representation to anticipate objects and actions. We experimentally validate this idea on two datasets, anticipating actions one second in the future and objects five seconds in the future.",2015-04-29,https://www.semanticscholar.org/paper/932ac3707e1ed84ab67526692a1ef8f064f24ab5,Computer Vision and Pattern Recognition
1582,Model-based Classification,"• A probability model is a joint distribution of a set of observations. • Often, a model is indexed by a parameter. Each value of the parameter gives a different distribution of the data. – The parameter of a Bernoulli is the probability of heads. – The parameters of a Gaussian are its mean and variance. • Many models (but not all) assume the data are independent and identically distributed. N n=1 p (x i | π). (1) Each term is a Bernoulli, p (x n | π) = π 1(x n =h) (1 − π) 1(x n =t) (2) • Suppose we flip a coin N times and record the outcomes. • Further suppose that we think that the probability of heads is π. (This is distinct from whatever the probability of heads "" really "" is.)",2019-07-31,https://www.semanticscholar.org/paper/e1a868d2782fa8756433661e21771517f7960e5c,Model-Based Clustering and Classification for Data Science
2201,Interferon gene expression signature in rheumatoid arthritis neutrophils correlates with a good response to TNFi therapy.,"OBJECTIVE
The aim of this study was to use whole transcriptome sequencing (RNA-Seq) of RA neutrophils to identify pre-therapy gene expression signatures that correlate with disease activity or response to TNF inhibitor (TNFi) therapy.


METHODS
Neutrophils were isolated from the venous blood of RA patients (n = 20) pre-TNFi therapy and from healthy controls (n = 6). RNA was poly(A) selected and sequenced on the Illumina HiSeq 2000 platform. Reads were mapped to the human genome (hg19) using TopHat and differential expression analysis was carried out using edgeR (5% false discovery rate). Signalling pathway analysis was carried out using Ingenuity Pathway Analysis (IPA) software. IFN signalling was confirmed by western blotting for phosphorylated signal transducer and activator of transcription (STAT) proteins. Response to TNFi was measured at 12 weeks using change in the 28-item DAS (DAS28).


RESULTS
Pathway analysis with IPA predicted activation of IFN signalling in RA neutrophils, identifying 178 IFN-response genes regulated by IFN-α, IFN-β or IFN-γ (P < 0.01). IPA also predicted activation of STAT1, STAT2 and STAT3 transcription factors in RA neutrophils (P < 0.01), which was confirmed by western blotting. Expression of IFN-response genes was heterogeneous and patients could be categorized as IFN-high or IFN-low. Patients in the IFN-high group achieved a better response to TNFi therapy [ΔDAS28, P = 0.05, odds ratio (OR) 1.4 (95% CI 1.005, 1.950)] than patients in the IFN-low group. The level of expression of IFN-response genes (IFN score) predicted a good response [European League Against Rheumatism (EULAR) criteria] to TNFi using receiver operating characteristic curve analysis (area under the curve 0.76).


CONCLUSION
IFN-response genes are significantly up-regulated in RA neutrophils compared with healthy controls. Higher IFN-response gene expression in RA neutrophils correlates with a good response to TNFi therapy.",,https://www.semanticscholar.org/paper/e315426625933e9b8a140832936abcd8dd79b8c3,Rheumatology
798,"Bin Packing with Discrete Item Sizes, Part I: Perfect Packing Theorems and the Average Case Behavior of Optimal Packings","We consider the one-dimensional bin packing problem with unit-capacity bins and item sizes chosen according to the discrete uniform distribution U{j,k}, $1 < j \leq k,$ where each item size in {1/k,2/k,. . .,j/k} has probability 1/j of being chosen. Note that for fixed j,k as $m\rightarrow\infty$ the discrete distributions U{mj,mk} approach the continuous distribution U(0,j/k], where the item sizes are chosen uniformly from the interval (0,j/k]. We show that average-case behavior can differ substantially between the two types of distributions. In particular, for all j,k with j<k-1, there exist on-line algorithms that have constant expected wasted space under U{j,k}, whereas no on-line algorithm has even o(n1/2) expected waste under U(0,u] for any $0 < u \leq 1$. Our U{j,k} result is an application of a general theorem of Courcoubetis and Weber [C. Courcoubetis and R. R. Weber, Probab. Engrg. Inform. Sci., 4 (1990), pp. 447--460] that covers all discrete distributions. Under each such distribution, the optimal expected waste for a random list of n items must be either $\Theta (n)$, $\Theta (n^{1/2} )$, or O(1), depending on whether certain ""perfect"" packings exist. The perfect packing theorem needed for the U{j,k} distributions is an intriguing result of independent combinatorial interest, and its proof is a cornerstone of the paper.",2000-05-01,https://www.semanticscholar.org/paper/93cafcd104cbc206b259e60a1b799e4632b9dfaf,SIAM Journal on Discrete Mathematics
278,On a Network Generalization of the Minmax Theorem,,2009-07-03,https://www.semanticscholar.org/paper/a0feab11362fb168fa4408f00b5c5a2fbbf029aa,"International Colloquium on Automata, Languages and Programming"
757,On the Complexity of Nash Equilibria and Other Fixed Points (Extended Abstract),"We reexamine, what it means to compute Nash equilibria and, more, generally, what it means to compute a fixed point of a given Brouwer function, and we investigate the complexity of the associated problems. Specifically, we study the complexity of the following problem: given a finite game, Gamma, with 3 or more players, and given epsiv > 0, compute a vector x' (a mixed strategy profile) that is within distance e (say in t^) of some (exact) Nash equilibrium. We show that approximation of an (actual) Nash equilibrium for games with 3 players, even to within any non-trivial constant additive factor epsiv < 1/2 in just one desired coordinate, is at least as hard as the long standing square-root sum problem, as well as more general arithmetic circuit decision problems, and thus that even placing the approximation problem in NP would-resolve a major open problem in the complexity of numerical computation. Furthermore, we show that the (exact or approximate) computation of Nash equilibria for 3 or more players is complete for the class of search problems, which we call FIXP, that can be cast as fixed point computation problems for functions represented by algebraic circuits (straight line programs) over basis {+, *, -, /, max, min}, with rational constants. We show that the linear fragment of FIXP equals PPAD. Many problems in game theory, economics, and probability theory, can be cast as fixed point problems for such algebraic functions. We discuss several important such problems: computing the value of Shapley's stochastic games, and the simpler games of Condon, extinction probabilities of branching processes, termination probabilities of stochastic context-free grammars, and of Recursive Markov Chains. We show that for some of them, the approximation, or even exact computation, problem can be placed-in PPAD, while for others, they are at least as hard as the square-root sum and arithmetic circuit decision problems.",2007-10-21,https://www.semanticscholar.org/paper/38e35793b0a3de5bb8e72489c66375557e3325d4,IEEE Annual Symposium on Foundations of Computer Science
2795,Chlamydophila psittaci-negative ocular adnexal marginal zone lymphomas express self polyreactive B-cell receptors,,2015-02-13,https://www.semanticscholar.org/paper/67ab55b75ab2b9473d89a6bbe739af90455972f9,Leukemia
685,Latent variables based data estimation for sensing applications,"Recovering missing sensor data is a critical problem for sensor networks, especially when nodes duty cycle their activity or may experience periodic downtimes due to limited energy. Fortunately, sensor readings are often correlated across different nodes and sensor types. Among state-of-the-art statistical data estimation techniques, latent variable based factor models have emerged as a powerful framework for recovering missing data. In this paper we propose the use of latent variable models to estimate missing data in heterogeneous sensor networks. Our model not only correlates data across different sensor locations and types, but also takes advantage of the temporal structure that is often present in sensor readings. We analyze how this model can effectively reconstruct missing sensor data when the individual sensor nodes have to duty-cycle their activity in order to extend network lifetime. We evaluate our model on a real life sensor network consisting of 122 environmental monitoring stations that periodically collect data from 13 different sensors. Results show that our proposed model can effectively reconstruct over 50% of missing data with less than 10% error.",2011-12-01,https://www.semanticscholar.org/paper/c43ec6a8ddefb32a703a71eb4316ad64476d59ec,"2011 Seventh International Conference on Intelligent Sensors, Sensor Networks and Information Processing"
1521,Learning Transferrable Representations of Career Trajectories for Economic Prediction,"Understanding career trajectories—the sequences of jobs that individuals hold over their working lives—is important to economists for studying labor markets. In the past, economists have estimated relevant quantities by ﬁtting predictive models to small surveys, but in recent years large datasets of online resumes have also become available. These new datasets provide job sequences of many more individuals, but they are too large and complex for standard econometric modeling. To this end, we adapt ideas from modern language modeling to the analysis of large-scale job sequence data. We develop CAREER, a transformer-based model that learns a low-dimensional representation of an individual’s job history. This representation can be used to predict jobs directly on a large dataset, or can be “transferred” to represent jobs in smaller and better-curated datasets. We ﬁt the model to a large dataset of resumes, 24 million people who are involved in more than a thousand unique occupations. It forms accurate predictions on held-out data, and it learns useful career representations that can be ﬁne-tuned to make accurate predictions on common economics datasets. for predicting and forecasting career",,https://www.semanticscholar.org/paper/57ceadbb37da24ce24b3ab8ff826ddcae717c0e2,arXiv.org
3414,Minimizing the Total Weighted Completion Time of Coflows in Datacenter Networks,"Communications in datacenter jobs (such as the shuffle operations in MapReduce applications) often involve many parallel flows, which may be processed simultaneously. This highly parallel structure presents new scheduling challenges in optimizing job-level performance objectives in data centers. Chowdhury and Stoica introduced the coflow abstraction to capture these communication patterns, and recently Chowdhury et al. developed effective heuristics to schedule coflows. In this paper, we consider the problem of efficiently scheduling coflows with release dates so as to minimize the total weighted completion time, which has been shown to be strongly NP-hard. Our main result is the first polynomial-time deterministic approximation algorithm for this problem, with an approximation ratio of 67/3, and a randomized version of the algorithm, with a ratio of 9+16√2/3. Our results use techniques from both combinatorial scheduling and matching theory, and rely on a clever grouping of coflows. We also run experiments on a Facebook trace to test the practical performance of several algorithms, including our deterministic algorithm. Our experiments suggest that simple algorithms provide effective approximations of the optimal, and that our deterministic algorithm has near-optimal performance.",2015-06-13,https://www.semanticscholar.org/paper/bd87cc38abc992be2d154a522729cddfa90dc4fc,ACM Symposium on Parallelism in Algorithms and Architectures
3044,VMTorrent: scalable P2P virtual machine streaming,"Clouds commonly store Virtual Machine (VM) images on networked storage. This poses a serious potential scalability bottleneck as launching a single fresh VM instance requires, at minimum, several hundred MB of network reads. As this bottleneck occurs most severely during read-intensive launching of new VMs, we focus on scalably minimizing time to boot a VM and load its critical applications.
 While effective scalable P2P streaming techniques for Video on Demand (VOD) scenarios where blocks arrive in-order and at constant rate are available, no techniques address scalable large-executable streaming. VM execution is non-deterministic, divergent, variable rate, and cannot miss blocks. VMTORRENT introduces a novel combination of block prioritization, profile-based execution prefetch, on-demand fetch, and decoupling of VM image presentation from underlying data-stream. VMTORRENT provides the first complete and effective solution to this growing scalability problem that is based on making better use of existing capacity, instead of throwing more hardware at it.
 Supported by analytic modeling, we present comprehensive experimental evaluation of VMTORRENT on real systems at scale, demonstrating the effectiveness of VMTORRENT. We find that VMTORRENT supports comparable execution time to that achieved using local disk. VMTORRENT maintains this performance while scaling to 100 instances, providing up to 11x speedup over current state-of-the-art and 30x over traditional network storage.",2012-12-10,https://www.semanticscholar.org/paper/bf9c8100d311af457c347d0ec6fcdf507fb64f85,Conference on Emerging Network Experiment and Technology
484,On Horn Envelopes and Hypergraph Transversals,,1993-12-15,https://www.semanticscholar.org/paper/eb16b514a506159032277e045f8bd906db88cb00,International Symposium on Algorithms and Computation
3715,Visual behavior modelling for robotic theory of mind,,2021-01-11,https://www.semanticscholar.org/paper/9fab147d338fe2b7fa3ffb6ecefed75bed7cf824,Scientific Reports
2653,Dynamic space management for user interfaces,"We present a general approach to the dynamic representation of 2D space that is well suited for user-interface layout. We partition space into two distinct categories: full and empty. The user can explicitly specify a set of possibly overlapping upright rectangles that represent the objects of interest. These full-space rectangles are processed by the system to create a representation of the remaining empty space. This representation makes it easy for users to develop customized spatial allocation strategies that avoid overlapping the full-space rectangles. We describe the representation; provide efficient incremental algorithms for adding and deleting full-space rectangles, and for querying the empty-space representation; and show several allocation strategies that the representation makes possible. We present two testbed applications that incorporate an implementation of the algorithm; one shows the utility of our representation for window management tasks; the other applies it to the layout of components in a 3D user interface, based on the upright 2D bounding boxes of their projections.",2000-11-01,https://www.semanticscholar.org/paper/92f4b67419139aeb6fdc6929a51348e9c58a8b2e,ACM Symposium on User Interface Software and Technology
126,"Storage-Efficient, Deadlock-Free Packet Routing Algorithms for Torus Networks","We present two new packet routing algorithms for parallel computers with torus interconnection networks of arbitrary size and dimension. Both algorithms use only minimal length paths, are fully adaptive in the sense that all minimal length paths may be used to avoid congestion, and are free of deadlock, livelock and starvation. Algorithm 1 requires only three central queues per routing node. It is the first known minimal length packet routing algorithm for torus networks which requires a constant number of queues per node, regardless of the size and dimension of the torus. In fact, the requirement of three queues per node is optimal, as no such algorithm is possible when all nodes have two or fewer queues. Algorithm 2 requires only that each node have two input buffers per edge. It is the first known minimal-fully-adaptive packet routing algorithm for torus networks which does not require central queues and which does not require any node to have more than two input or two output buffers per edge. Both algorithms are simple and appear to be well-suited to VLSI implementation. They can be used with either store-and-forward or virtual cut-through routing. >",1994-12-01,https://www.semanticscholar.org/paper/8beea0917ac99f0dcd98f9d5cc56a76c1809df22,IEEE Trans. Computers
1878,Taiwan Drought was a Microcosm of Climate Change Adaptation Challenges in Complex Island Economies,,2021-08-31,https://www.semanticscholar.org/paper/3a208df7c868ec7c05f4eea733c6407a8bdf3282,Process Integration and Optimization for Sustainability
2427,Manipulating 3D Anatomic Models in Augmented Reality: Comparing a Hands-Free Approach and a Manual Approach,"Many AR and VR task domains involve manipulating virtual objects; for example, to perform 3D geometric transformations. These operations are typically accomplished with tracked hands or hand-held controllers. However, there are some activities in which the user's hands are already busy with another task, requiring the user to temporarily stop what they are doing to perform the second task, while also taking time to disengage and reengage with the original task (e.g., putting down and picking up tools). To avoid the need to overload the user's hands this way in an AR system for guiding a physician performing a surgical procedure, we developed a hands-free approach to performing 3D transformations on patient-specific virtual organ models. Our approach uses small head motions to accomplish first-order and zero-order control, in conjunction with voice commands to establish the type of transformation. To show the effectiveness of this approach for translating, scaling, and rotating 3D virtual models, we conducted a within-subject study comparing the hands-free approach with one based on conventional manual techniques, both running on a Microsoft HoloLens and using the same voice commands to specify transformation type. Independent of any additional time to transition between tasks, users were significantly faster overall using the hands-free approach, significantly faster for hands-free translation and scaling, and faster (although not significantly) for hands-free rotation.",2019-10-01,https://www.semanticscholar.org/paper/434d802aeaf56dc659f5c153b2ebc49f3e4e020a,International Symposium on Mixed and Augmented Reality
313,Balancing traffic load in wireless networks with curveball routing,"We address the problem of balancing the traffic load in multi-hop wireless networks. We consider a point-to-point communicating network with a uniform distribution of source-sink pairs. When routing along shortest paths, the nodes that are centrally located forward a disproportionate amount of traffic. This translates into increased congestion and energy consumption. However, the maximum load can be decreased if the packets follow curved paths. We show that the optimum such routing scheme can be expressed in terms of geometric optics and computed by linear programming. We then propose a practical solution, which we call Curveball Routing which achieves results not much worse than the optimum.
 We evaluate our solution at three levels of fidelity: a Java high-level simulator, the ns2 simulator, and the Intel Mirage Sensor Network Testbed. Simulation results using the high-level simulator show that our solution successfully avoids the crowded center of the network, and reduces the maximum load by up to 40%. At the same time, the increase of the expected path length is minimal, i.e., only 8% on average. Simulation results using the ns2 simulator show that our solution can increase throughput on moderately loaded networks by up to 15%, while testbed results show a reduction in peak energy usage by up to 25%. Our prototype suggests that our solution is easily deployable.",2007-09-09,https://www.semanticscholar.org/paper/d44805a895d8371526b226a3dbb1890d3974a03a,ACM Interational Symposium on Mobile Ad Hoc Networking and Computing
3023,ARM Virtualization,"ARM servers are becoming increasingly common, making server technologies such as virtualization for ARM of growing importance. We present the first study of ARM virtualization performance on server hardware, including multi-core measurements of two popular ARM and x86 hypervisors, KVM and Xen. We show how ARM hardware support for virtualization can enable much faster transitions between VMs and the hypervisor, a key hypervisor operation. However, current hypervisor designs, including both Type 1 hypervisors such as Xen and Type 2 hypervisors such as KVM, are not able to leverage this performance benefit for real application workloads on ARMv8.0. We discuss the reasons why and show that other factors related to hypervisor software design and implementation have a larger role in overall performance. Based on our measurements, we discuss software changes and new hardware features, the Virtualization Host Extensions (VHE), added in ARMv8.1 that bridge the gap and bring ARM's faster VM-to-hypervisor transition mechanism to modern Type 2 hypervisors running real applications.",2018-08-28,https://www.semanticscholar.org/paper/24fe584525c3307d7ffdd726b9849b8bb2aba0cd,OPSR
1520,Reconstructing the universe with variational self-boosted sampling,"Forward modeling approaches in cosmology have made it possible to reconstruct the initial conditions at the beginning of the Universe from the observed survey data. However the high dimensionality of the parameter space still poses a challenge to explore the full posterior, with traditional algorithms such as Hamiltonian Monte Carlo (HMC) being computationally inefficient due to generating correlated samples and the performance of variational inference being highly dependent on the choice of divergence (loss) function. Here we develop a hybrid scheme, called variational self-boosted sampling (VBS) to mitigate the drawbacks of both these algorithms by learning a variational approximation for the proposal distribution of Monte Carlo sampling and combine it with HMC. The variational distribution is parameterized as a normalizing flow and learnt with samples generated on the fly, while proposals drawn from it reduce auto-correlation length in MCMC chains. Our normalizing flow uses Fourier space convolutions and element-wise operations to scale to high dimensions. We show that after a short initial warm-up and training phase, VBS generates better quality of samples than simple VI approaches and in the hybrid sampling phase, reduces the correlation length in the sampling phase by a factor of 10–50 over using only HMC to explore the posterior of initial conditions in 643 and 1283 dimensional problems, with larger gains for high signal-to-noise data observations. Hybrid sampling with online training of the variational distribution violates Markov property, and to retain the asymptotic guarantees of HMC, in the final phase we use a fixed variational distribution as proposal distribution and propagate these samples to the posterior distribution.",2022-06-28,https://www.semanticscholar.org/paper/41aead90e62b111616e21b4e4eb3af38faa1310e,Journal of Cosmology and Astroparticle Physics
3126,Remote Display Performance for Wireless Healthcare Computing,"Organizations are beginning to recognize that health care providers are highly mobile and optimal care requires providing access to a large and dynamic body of information wherever the provider and patient are. Remote display protocols (RDP) are one way that organizations are using to deliver healthcare applications to mobile users. While many organizations have begun to use RDPs to deliver real-time access to health care information to clinicians, little formal work has been done to evaluate the performance or the effectiveness of thin-client computing with health care applications. This study examines the performance of wireless thin-client tablets with two web-based clinical applications, a text-centric, graphics-poor EMR and a graphic-rich image analysis program. The study compares the performance of two popular RDP implementations, Citrix and Microsoft Remote Desktop, with the performance of a traditional web browser in a wireless environment. For both applications, the RDPs demonstrated both higher speed and reduced bandwidth requirements than the web browser.",,https://www.semanticscholar.org/paper/d47995e4ce282c8a90e6ff7a32d1bf091ed63636,Medinfo
1033,The three-link nonholonomic snake as a hybrid kinodynamic system,"Motion planners often avoid the so-called singular configurations of a locomoting system because they can change a system's dynamics and cause it to incur unbounded input controller costs. However, such configurations can also allow a system to exhibit new modes of locomotion, which can be incorporated into previously established planning techniques. Here we take a nonholonomic kinematic system and present the formalism for representing it as a hybrid system taking advantage of the singular configuration and the associated dynamics. We show how to achieve the transition maps between the modes of operation by allowing actuated joints to become passive or locked, circumventing problems with unbounded constraint forces. This new model offers a new capability to take advantage of drift dynamics and rolling motions, which we demonstrate using a switching controller involving established kinematic techniques and preliminary dynamic maneuvers.",2016-07-06,https://www.semanticscholar.org/paper/d89d57cf06364b4740574c7d8b47493efffff13d,American Control Conference
3123,ksniffer: Determining the Remote Client Perceived Response Time from Live Packet Streams,"As dependence on the World Wide Web continues to grow, so does the need for businesses to have quantitative measures of the client perceived response times of their Web services. We present ksniffer, a kernel-based traffic monitor capable of determining pageview response times as perceived by remote clients, in real-time at gigabit traffic rates. ksniffer is based on novel, online mechanisms that take a ""look once, then drop"" approach to packet analysis to reconstruct TCP connections and learn client pageview activity. These mechanisms are designed to operate accurately with live network traffic even in the presence of packet loss and delay, and can be efficiently implemented in kernel space. This enables ksniffer to perform analysis that exceeds the functionality of current traffic analyzers while doing so at high bandwidth rates. ksniffer requires only to passively monitor network traffic and can be integrated with systems that perform server management to achieve specified response time goals. Our experimental results demonstrate that ksniffer can run on an inexpensive, commodity, Linux-based PC and provide online pageview response time measurements, across a wide range of operating conditions, that are within five percent of the response times measured at the client by detailed instrumentation.",2004-12-06,https://www.semanticscholar.org/paper/66e91238ed494600f18b79b94aa45acfc5650e71,USENIX Symposium on Operating Systems Design and Implementation
83,"Probe, count, and classify: categorizing hidden web databases","The contents of many valuable web-accessible databases are only accessible through search interfaces and are hence invisible to traditional web “crawlers.” Recent studies have estimated the size of this “hidden web” to be 500 billion pages, while the size of the “crawlable” web is only an estimated two billion pages. Recently, commercial web sites have started to manually organize web-accessible databases into Yahoo!-like hierarchical classification schemes. In this paper, we introduce a method for automating this classification process by using a small number of query probes. To classify a database, our algorithm does not retrieve or inspect any documents or pages from the database, but rather just exploits the number of matches that each query probe generates at the database in question. We have conducted an extensive experimental evaluation of our technique over collections of real documents, including over one hundred web-accessible databases. Our experiments show that our system has low overhead and achieves high classification accuracy across a variety of databases.",2001-05-01,https://www.semanticscholar.org/paper/1e2355cea9da54478de58efdec1e4f8e995cf231,ACM SIGMOD Conference
2741,COMET: generating coordinated multimedia explanations,"COMET (Coordinated Multimedia Explanation Testbed) is an experimental system that generates interactive multimedia instructions for equipment maintenance and repair [3, 4]. COMET combines research in natural language generation and in knowledge-based graphics. The form and content of all text and graphics is created interactively. Thus, instructions can be customized on the fly for the individual user and situation.",1991-03-01,https://www.semanticscholar.org/paper/6c177657e47f25938c05ee21e421d79c506b2154,International Conference on Human Factors in Computing Systems
125,The effectiveness of GIOSS for the text database discovery problem,"The popularity of on-line document databases has led to a new problem: finding which text databases (out of many candidate choices) are the most relevant to a user. Identifying the relevant databases for a given query is the text database discovery problem. The first part of this paper presents a practical solution based on estimating the result size of a query and a database. The method is termed GlOSS—Glossary of Servers Server. The second part of this paper evaluates the effectiveness of GlOSS based on a trace of real user queries. In addition, we analyze the storage cost of our approach.",1994-05-24,https://www.semanticscholar.org/paper/70ad64e3ee1191c858e27756c80487ea8b69d2ac,ACM SIGMOD Conference
2625,"Coarse, inexpensive, infrared tracking for wearable computing","We present a novel, inexpensive, coarse tracking systemthat determines a person's approximate 2D locationand 1D head orientation in an indoor environment. Whilethis coarse tracking cannot support precise registration ofoverlaid material, it can be used to drive user interfacesthat can adapt to the quality of tracking available.Our approach uses a set of strong infrared beacons,each of which broadcasts a unique ID. The beacons aredeployed in the environment such that their zones of influencestrategically overlap, partitioning the area of coverageinto a set of uniquely identifiable fragments. We use acompound, omnidirectional infrared receiver, composedof a set of individual, directional infrared receivers, toinfer 2D position (parallel to the ground plane) and 1Dorientation (azimuth), employing a Kalman-filter-basedarchitecture for smoothing and data integration withother tracking systems available. To test our ideas, wehave applied them to a prototype head tracker, and presentresults from our tests.",2003-10-21,https://www.semanticscholar.org/paper/bcfe1751ce0254239b93c0f5fa63fa855c66eb23,"Seventh IEEE International Symposium on Wearable Computers, 2003. Proceedings."
2024,A simulation optimization-based framework for capacity planning under uncertainty,"Capacity planning in semiconductor manufacturing industry is a challenging task due to its high capital investments, volatile demands, and the long lead time. Current approaches to handle this problem are to model it as an optimization problem where the uncertain demand is either based on a single forecast, or is decomposed via a finite-scenario structure with an assigned probability for each scenario to reflect the likelihood of occurrence. However, when the uncertainty cannot be decomposed into finite scenarios or when the number of possible scenarios is extremely large, traditional approaches such as the mathematical programming either cannot deal with the problem or may require unreasonable computing time. In this paper, we consider a multiple-period capacity planning problem where the uncertain demand is modeled as a continuous stochastic process over the planning horizon. Moreover, the long lead time and capacity migrations between different products are taken into account to accurately determine the optimal capacity plan. A new framework based on sample path method in simulation optimization is proposed to solve the problem. Comparing to traditional methods, the new framework is much more efficient in terms of the required computing time, as is demonstrated in the computational study.",2010-07-25,https://www.semanticscholar.org/paper/4e8f4e402f7a40171de71b98de8200a2ac494813,The 40th International Conference on Computers & Indutrial Engineering
192,Cycles in adversarial regularized learning,"Regularized learning is a fundamental technique in online optimization, machine learning and many other fields of computer science. A natural question that arises in these settings is how regularized learning algorithms behave when faced against each other. We study a natural formulation of this problem by coupling regularized learning dynamics in zero-sum games. We show that the system's behavior is Poincare recurrent, implying that almost every trajectory revisits any (arbitrarily small) neighborhood of its starting point infinitely often. This cycling behavior is robust to the agents' choice of regularization mechanism (each agent could be using a different regularizer), to positive-affine transformations of the agents' utilities, and it also persists in the case of networked competition, i.e., for zero-sum polymatrix games.",2017-09-08,https://www.semanticscholar.org/paper/dd3eefc22d78467971b28f0b0f9b5b09e838fd56,ACM-SIAM Symposium on Discrete Algorithms
1545,Variational Combinatorial Sequential Monte Carlo Methods for Bayesian Phylogenetic Inference,"Bayesian phylogenetic inference is often conducted via local or sequential search over topologies and branch lengths using algorithms such as random-walk Markov chain Monte Carlo (MCMC) or Combinatorial Sequential Monte Carlo (CSMC). However, when MCMC is used for evolutionary parameter learning, convergence requires long runs with inefficient exploration of the state space. We introduce Variational Combinatorial Sequential Monte Carlo (VCSMC), a powerful framework that establishes variational sequential search to learn distributions over intricate combinatorial structures. We then develop nested CSMC, an efficient proposal distribution for CSMC and prove that nested CSMC is an exact approximation to the (intractable) locally optimal proposal. We use nested CSMC to define a second objective, VNCSMC which yields tighter lower bounds than VCSMC. We show that VCSMC and VNCSMC are computationally efficient and explore higher probability spaces than existing methods on a range of tasks.",2021-05-31,https://www.semanticscholar.org/paper/89a587b76b2ad28417ec209c205848362df1ef8e,Conference on Uncertainty in Artificial Intelligence
2476,Attributes of Subtle Cues for Facilitating Visual Search in Augmented Reality,"Goal-oriented visual search is performed when a person intentionally seeks a target in the visual environment. In augmented reality (AR) environments, visual search can be facilitated by augmenting virtual cues in the person's field of view. Traditional use of explicit AR cues can potentially degrade visual search performance due to the creation of distortions in the scene. An alternative to explicit cueing, known as subtle cueing, has been proposed as a clutter-neutral method to enhance visual search in video-see-through AR. However, the effects of subtle cueing are still not well understood, and more research is required to determine the optimal methods of applying subtle cueing in AR. We performed two experiments to investigate the variables of scene clutter, subtle cue opacity, size, and shape on visual search performance. We introduce a novel method of experimentally manipulating the scene clutter variable in a natural scene while controlling for other variables. The findings provide supporting evidence for the subtlety of the cue, and show that the clutter conditions of the scene can be used both as a global classifier, as well as a local performance measure.",2014-03-01,https://www.semanticscholar.org/paper/63a3da01b70ccae417cbad77a7c1b29326b026e4,IEEE Transactions on Visualization and Computer Graphics
229,On the Complexity of Dynamic Mechanism Design,"We introduce a dynamic mechanism design problem in which the designer wants to offer for sale an item to an agent, and another item to the same agent at some point in the future. The agent's joint distribution of valuations for the two items is known, and the agent knows the valuation for the current item (but not for the one in the future). The designer seeks to maximize expected revenue, and the auction must be deterministic, truthful, and ex post individually rational. The optimum mechanism involves a protocol whereby the seller elicits the buyer's current valuation, and based on the bid makes two take-it-or-leave-it offers, one for now and one for the future. We show that finding the optimum deterministic mechanism in this situation --- arguably the simplest meaningful dynamic mechanism design problem imaginable --- is NP-hard. We also prove several positive results, among them a polynomial linear programming-based algorithm for the optimum randomized auction (even for many bidders and periods), and we show strong separations in revenue between non-adaptive, adaptive, and randomized auctions, even when the valuations in the two periods are uncorrelated. Finally, for the same problem in an environment in which contracts cannot be enforced, and thus perfection of equilibrium is necessary, we show that the optimum randomized mechanism requires multiple rounds of cheap talk-like interactions.",2014-07-21,https://www.semanticscholar.org/paper/6f2ec53689a5c3ba0ee3c148b7ac5a93093e1b0a,ACM-SIAM Symposium on Discrete Algorithms
1757,Stochastic variational inference,"We develop stochastic variational inference, a scalable algorithm for approximating posterior distributions. We develop this technique for a large class of probabilistic models and we demonstrate it with two probabilistic topic models, latent Dirichlet allocation and the hierarchical Dirichlet process topic model. Using stochastic variational inference, we analyze several large collections of documents: 300K articles from Nature, 1.8M articles from The New York Times, and 3.8M articles from Wikipedia. Stochastic inference can easily handle data sets of this size and outperforms traditional variational inference, which can only handle a smaller subset. (We also show that the Bayesian nonparametric topic model outperforms its parametric counterpart.) Stochastic variational inference lets us apply complex Bayesian models to massive data sets.",2012-06-29,https://www.semanticscholar.org/paper/bccb2f99a9d1c105699f5d88c479569085e2c7ba,Journal of machine learning research
1899,Compensating Misalignment Using Dynamic Random-Effect Control System: A Case of High-Mixed Wafer Fabrication,"It is vital to have an exclusive modification in semiconductor production process because of meeting differentiated customer demands in dynamic and competitive global minuscule semiconductor technology market and the highly complex fabrication process. In this paper, we propose a control system based on the dynamic mixed-effect least-square support vector regression (LS-SVR) control system for overlay error compensation with stochastic metrology delay to minimize the misalignment of the patterning process. Moreover, for the stability of the control system in the presence of metrology delay and to deal with nonlinearity among the overlay factors, the novel Lyapunov-based kernel function is merged with the LS-SVR controller. The proposed controller’s operation has been validated and implemented by a major semiconductor manufacturer in Taiwan. The experiments are verified that mixed-effect LS-SVR controller has the higher validity and higher efficiency in comparison with the exponentially weighted moving average (EWMA) and threaded EWMA controllers which had been previously implemented at the company or applied in similar studies. Note to Practitioners—Due to high production complexity in semiconductor manufacturing process, a meticulous and intelligent process control is needed to achieve higher throughput and customer satisfaction. Monitoring a complex system is challenging because the process components and variables operate autonomously and interoperate with other manufacturing segments. This paper proposes a novel run-to-run (R2R) control system to compensate the overlay error during the photolithography process that efficiently deals with the high-mixed manufacturing environment and metrology delay.",2019-02-19,https://www.semanticscholar.org/paper/d8389627324cdfcb66e650eb7ef8685f531fdcef,IEEE Transactions on Automation Science and Engineering
2878,Galectin‐3 expression is induced in cirrhotic liver and hepatocellular carcinoma,"Galectins are a family of β‐galactoside‐binding animal lectins. In particular, a widely studied member galectin‐3, previously designated as ϵBP, CBP35, Mac‐2, L‐29 and L‐34, has been associated with assorted processes such as cell growth, tumor transformation and metastasis. Galectin‐3 is expressed in various tissues and organs but is significantly absent in normal hepatocytes. However, evaluation of patient liver biopsies for galectin‐3 expression resulted in the finding that hepatocellular carcinoma (HCC) frequently expressed significant levels of this lectin (76% immunohistochemically positive). Further investigation revealed that galectin‐3 expression in HCC is independent of whether the patient had prior hepatitis B virus infection: 14 of 18 HCC cases from HBV+ patients, and 5 of 7 cases from HBV− patients demonstrated positive galectin‐3 immunohistochemistry. However, co‐transfection studies using a galectin‐3 promoter construct and an HBV‐X protein (HBV‐X) expression vector demonstrated that galectin‐3 expression can occur through transactivation of the lectin promoter by HBV‐X. Based on presently known properties of this lectin, it is possible that deregulated expression of galectin‐3 can result in tumor transformation and invasiveness, or confer propensity for tumor cell survival. In addition, galectin‐3 was abundantly expressed in cirrhotic liver in peripheral distribution within regenerating nodules. Such galectin‐3 expression in rapidly proliferating hepatocytes in cirrhotic liver may be a result of the high mitotic index. Alternatively, it is possible that proliferating cells expressing galectin‐3 are in the process of being transformed, thus indicating an early neoplastic event. Int. J. Cancer 81:519–526, 1999. © 1999 Wiley‐Liss, Inc.",1999-05-17,https://www.semanticscholar.org/paper/5d5caf6e11b3bc937a674dfb438b1b0c207f30cf,International Journal of Cancer
3433,An O(n5/2logn) algorithm for the Rectilinear Minimum Link-Distance Problem in three dimensions,,2009-07-01,https://www.semanticscholar.org/paper/c20973a4a4703b5cfbd9c92eca82228981bcd68e,Computational geometry
160,Self-Attention Networks Can Process Bounded Hierarchical Languages,"Despite their impressive performance in NLP, self-attention networks were recently proved to be limited for processing formal languages with hierarchical structure, such as Dyck-k, the language consisting of well-nested parentheses of k types. This suggested that natural language can be approximated well with models that are too weak for formal languages, or that the role of hierarchy and recursion in natural language might be limited. We qualify this implication by proving that self-attention networks can process Dyck-(k, D), the subset of Dyck-k with depth bounded by D, which arguably better captures the bounded hierarchical structure of natural language. Specifically, we construct a hard-attention network with D+1 layers and O(log k) memory size (per token per layer) that recognizes Dyck-(k, D), and a soft-attention network with two layers and O(log k) memory size that generates Dyck-(k, D). Experiments show that self-attention networks trained on Dyck-(k, D) generalize to longer inputs with near-perfect accuracy, and also verify the theoretical memory advantage of self-attention networks over recurrent networks.",2021-05-24,https://www.semanticscholar.org/paper/b2186dd1ccc4b7adcf70c0cf7649c2c118e4ceea,Annual Meeting of the Association for Computational Linguistics
870,The Analysis of Local Search Problems and Their Heuristics,,1990-02-01,https://www.semanticscholar.org/paper/73de51c19c60539be5aa3d96288676468607d62a,Symposium on Theoretical Aspects of Computer Science
3713,Visual behavior modelling for robotic theory of mind,,2021-01-11,https://www.semanticscholar.org/paper/868381eea54ec0b8a96fb525e22836cd756a08b7,Scientific Reports
839,The Traveling Salesman Problem with Distances One and Two,"We present a polynomial-time approximation algorithm with worst-case ratio 7/6 for the special case of the traveling salesman problem in which all distances are either one or two. We also show that this special case of the traveling salesman problem is MAX SNP-hard, and therefore it is unlikely that it has a polynomial-time approximation scheme.",1993-02-01,https://www.semanticscholar.org/paper/85502d9b5621594d6c71d01cd5cfee5f903d69cf,Mathematics of Operations Research
3565,Programming and Validation Techniques for Reliable Goal-driven Autonomic Software,,,https://www.semanticscholar.org/paper/c21a4e332b7db0494d70810744da97fff4c7bb94,Autonomic Communication
74,Navigation- vs. index-based XML multi-query processing,"XML path queries form the basis of complex filtering of XML data. Most current XML path query processing techniques can be divided in two groups. Navigation-based algorithms compute results by analyzing an input document one tag at a time. In contrast, index-based algorithms take advantage of precomputed numbering schemes over the input XML document. We introduce a new index-based technique, index-filter, to answer multiple XML path queries. Index-filter uses indexes built over the document tags to avoid processing large portions of the input document that are guaranteed not to be part of any match. We analyze index-filter and compare it against Y-filter, a state-of-the-art navigation-based technique. We show that both techniques have their advantages, and we discuss the scenarios under which each technique is superior to the other one. In particular, we show that while most XML path query processing techniques work off SAX events, in some cases it pays off to preprocess the input document, augmenting it with auxiliary information that can be used to evaluate the queries faster. We present experimental results over real and synthetic XML documents that validate our claims.",2003-03-05,https://www.semanticscholar.org/paper/f9df398e5e8e173111a27a12da5ee72f1722ad11,Proceedings / International Conference on Data Engineering
3403,An O(Log Log m)-Competitive Algorithm for Online Machine Minimization,"This paper considers the online machine minimization problem, a basic real time scheduling problem. The setting for this problem consists of n jobs that arrive over time, where each job has a deadline by which it must be completed. The goal is to design an online scheduler that feasibly schedules the jobs on a nearly minimal number of machines. An algorithm is c-machine optimal if the algorithm will feasibly schedule a collection of jobs on c ·m machines if there exists a feasible schedule on m machines. For over two decades the best known result was a O(log P)-machine optimal algorithm, where P is the ratio of the maximum to minimum job size. In a recent breakthrough, a O(log m)-machine optimal algorithm was given. In this paper, we exponentially improve on this recent result by giving a O(log log m)-machine optimal algorithm.",2017-08-29,https://www.semanticscholar.org/paper/ba3b847a21e431961ef60e466afa2df2334ef125,IEEE Real-Time Systems Symposium
2239,The role of neutrophil apoptosis in juvenile-onset systemic lupus erythematosus.,"OBJECTIVE
Accumulation of apoptotic cells may lead to the development of systemic lupus erythematosus (SLE) through a breakdown in immune tolerance. Altered neutrophil apoptosis may contribute to nuclear autoantigen exposure, ultimately leading to autoantibody generation. This study aimed to determine whether neutrophil apoptosis is altered in patients with juvenile-onset SLE as compared with controls.


METHODS
Apoptosis was measured in neutrophils from patients with juvenile-onset SLE (n=12), adult-onset SLE (n=6), and pediatric patients with inflammatory (n=12) and noninflammatory (n=12) conditions. Annexin V staining and flow cytometry were used to determine neutrophil apoptosis. Proapoptotic and antiapoptotic proteins were measured in sera and in neutrophil cell lysates.


RESULTS
Neutrophil apoptosis was significantly increased in patients with juvenile-onset SLE as compared with the noninflammatory controls at time 0. Incubation of neutrophils with sera from patients with juvenile-onset SLE further increased neutrophil apoptosis as compared with incubation with sera from pediatric controls. Concentrations of TRAIL and FasL were significantly increased in sera from patients with juvenile-onset SLE, whereas interleukin-6, tumor necrosis factor alpha, and granulocyte-macrophage colony-stimulating factor (GM-CSF) were significantly decreased. Addition of GM-CSF to sera from patients with juvenile-onset SLE significantly decreased neutrophil apoptosis as compared with juvenile-onset SLE sera alone. The expression of proapoptotic proteins (caspase 3, Fas, and FADD) was elevated in juvenile-onset SLE neutrophils, whereas the expression of antiapoptotic proteins (cellular inhibitor of apoptosis 1 and 2 and X-linked inhibitor of apoptosis) was decreased. Neutrophil apoptosis correlated with biomarkers of disease activity (erythrocyte sedimentation rate and double-stranded DNA concentration) and the British Isles Lupus Assessment Group disease activity score.


CONCLUSION
Our data demonstrate an imbalance in proapoptotic and antiapoptotic factors in both neutrophils and sera from patients with juvenile-onset SLE. This imbalance results in increased neutrophil apoptosis in these patients. Correlations with markers of disease activity indicate that altered neutrophil apoptosis in juvenile-onset SLE patients may play a pathogenic role in this condition.",2009-08-01,https://www.semanticscholar.org/paper/f64d398f6adeeaed3a15c0f7fe14ccd617ad1477,Arthritis & Rheumatism
803,Near-optimal hardness results and approximation algorithms for edge-disjoint paths and related problems,"We study the approximability of edge-disjoint paths and related problems. In the edge-disjoint paths (EDP) problem, we are given a network G with source-sink pairs (si, ti), 1 ≤i≤k, and the goal is to find a largest subset of source-sink pairs that can be simultaneously connected in an edge-disjoint manner. We show that in directed networks, for any e>0, EDP is NP-hard to approximate within m1/2-e. We also design simple approximation algorithms that achieve essentially matching approximation guarantees for some generalizations of EDP. Another related class of routing problems that we study concerns EDP with the additional constraint that the routing paths be of bounded length. We show that, for any e > 0, bounded length EDP is hard to approximate within m1/2-e even in undirected networks, and give an O(√m)-approximation algorithm for it. For directed networks, we show that even the single source-sink pair case (i.e. find the maximum number of paths of bounded length between a given source-sink pair) is hard to approximate within m1/2-e, for any e > 0.",1999-05-01,https://www.semanticscholar.org/paper/62eb3948683c5aae8f66736ed79e0675edef9d3f,Symposium on the Theory of Computing
2550,Developing an augmented reality racing game,"Augmented reality (AR) makes it possible to create games in which virtual objects are overlaid on the real world, and real objects are tracked and used to control virtual ones. We describe the development of an AR racing game created by modifying an existing racing game, using an AR infrastructure that we developed for use with the XNA game development platform. In our game, the driver wears a tracked video see-through head-worn display, and controls the car with a passive tangible controller. Other players can participate by manipulating waypoints that the car must pass and obstacles with which the car can collide. We discuss our AR infrastructure, which supports the creation of AR applications and games in a managed code environment, the user interface we developed for the AR racing game, the game's software and hardware architecture, and feedback and observations from early demonstrations.",2008-01-08,https://www.semanticscholar.org/paper/ee27785c0e7f1ddacee7edd8bd47abbda0347403,Intelligent Technologies for Interactive Entertainment
2504,A tablet computer application for patients to participate in their hospital care.,"Building on our institution's commercial electronic health record and custom personal health record Web portal, we developed a tablet computer application to provide interactive information to hospital patients. Using Apple iPad devices, the prototype application was provided to five patients in a cardiology step-down unit. We conducted detailed interviews to assess patients' knowledge of their inpatient care, as well as their perceptions of the usefulness of the application. While patients exhibited varying levels of comfort with using the tablet computer, they were highly enthusiastic about the application's ability to supply health information such as their inpatient medication histories and photographs of their care providers. Additional research is warranted to assess the benefit such applications may have for addressing inpatient information needs, enhancing patient-provider communication and improving patient satisfaction.",,https://www.semanticscholar.org/paper/13537ea0991b9b93963c71c99da9a4960220c059,AMIA ... Annual Symposium proceedings. AMIA Symposium
3400,Advance Service Reservations with Heterogeneous Customers,"We study a fundamental model of resource allocation in which a finite number of resources must be assigned in an online manner to a heterogeneous stream of customers. The customers arrive randomly over time according to known stochastic processes. Each customer requires a specific amount of capacity and has a specific preference for each of the resources with some resources being feasible for the customer and some not. The system must find a feasible assignment of each customer to a resource or must reject the customer. The aim is to maximize the total expected capacity utilization of the resources over the horizon. This model has application in services, freight transportation, and online advertising. We present online algorithms with bounded competitive ratios relative to an optimal off-line algorithm that knows all stochastic information. Our algorithms perform extremely well compared with common heuristics as demonstrated on a real data set from a large hospital system in New York City. This paper was accepted by Yinyu Ye, optimization.",2018-05-15,https://www.semanticscholar.org/paper/e24400ee3147ebcb16729a87863543de1ebf7044,Management Sciences
618,On the performance of balanced hashing functions when the keys are not equiprobable,"The cost (expected number of accesses per retrieval) of hashing functions is examined without the assumption that it is equally probable for all keys to be present in the table. It is shown that the obvious strategy—trying to balance the sums of probabilities of the keys mapped to any given address—may be suboptimal; however, the difference from the exactly optimal distribution cannot be large.",,https://www.semanticscholar.org/paper/c90ba73e111af466c4e408826cb29474672927ca,TOPL
908,Tools for Template Dependencies,"Template dependencies (TD’s) are a class of data dependencies that include multivalued and join dependencies and embedded versions of these. A collection of techniques, examples and results about TD’s are presented. The principal results are: 1) Finite implication (implication over relations with a finite number of tuples) is distinct from unrestricted implication for TD’s. 2) There are, for TD’s over three or more attributes, infinite chains of increasingly weaker and increasingly stronger full TD’s. 3) However, there are weakest (nontrivial) and strongest full TD’s over any given set of attributes. 4) Over two attributes, there are only three distinct TD’s. 5) There is no weakest (not necessarily full) TD over any set of three or more attributes. 6) There is a finite relation that obeys every strictly partial TD but no full TD. 7) The conjunction of each finite set of full TD’s is equivalent to a single full TD. However, the conjunction of a finite set of (not necessarily full) TD’s is not necessarily e...",1983-02-01,https://www.semanticscholar.org/paper/a8610247fe0d4b549394986f2e7f10c2636ebb30,SIAM journal on computing (Print)
2212,RNA-Seq Reveals Activation of Both Common and Cytokine-Specific Pathways following Neutrophil Priming,"Neutrophils are central to the pathology of inflammatory diseases, where they can damage host tissue through release of reactive oxygen metabolites and proteases, and drive inflammation via secretion of cytokines and chemokines. Many cytokines, such as those generated during inflammation, can induce a similar “primed” phenotype in neutrophils, but it is unknown if different cytokines utilise common or cytokine-specific pathways to induce these functional changes. Here, we describe the transcriptomic changes induced in control human neutrophils during priming in vitro with pro-inflammatory cytokines (TNF-α and GM-CSF) using RNA-seq. Priming led to the rapid expression of a common set of transcripts for cytokines, chemokines and cell surface receptors (CXCL1, CXCL2, IL1A, IL1B, IL1RA, ICAM1). However, 580 genes were differentially regulated by TNF-α and GM-CSF treatment, and of these 58 were directly implicated in the control of apoptosis. While these two cytokines both delayed apoptosis, they induced changes in expression of different pro- and anti-apoptotic genes. Bioinformatics analysis predicted that these genes were regulated via differential activation of transcription factors by TNF-α and GM-CSF and these predictions were confirmed using functional assays: inhibition of NF-κB signalling abrogated the protective effect of TNF-α (but not that of GM-CSF) on neutrophil apoptosis, whereas inhibition of JAK/STAT signalling abrogated the anti-apoptotic effect of GM-CSF, but not that of TNF-α (p<0.05). These data provide the first characterisation of the human neutrophil transcriptome following GM-CSF and TNF-α priming, and demonstrate the utility of this approach to define functional changes in neutrophils following cytokine exposure. This may provide an important, new approach to define the molecular properties of neutrophils after in vivo activation during inflammation.",2013-03-06,https://www.semanticscholar.org/paper/0289216b0961ad16d91f448e12840dfce8de1f63,PLoS ONE
1788,A Language-based Approach to Measuring Scholarly Impact,"Identifying the most influential documents in a corpus is an important problem in many fields, from information science and historiography to text summarization and news aggregation. Unfortunately, traditional bibliometrics such as citations are often not available. We propose using changes in the thematic content of documents over time to measure the importance of individual documents within the collection. We describe a dynamic topic model for both quantifying and qualifying the impact of these documents. We validate the model by analyzing three large corpora of scientific articles. Our measurement of a document's impact correlates significantly with its number of citations.",2010-06-21,https://www.semanticscholar.org/paper/c645d99bf9b3f496a561818b37350d7a1834b92b,International Conference on Machine Learning
755,Small Approximate Pareto Sets for Bi-objective Shortest Paths and Other Problems,,2007-08-20,https://www.semanticscholar.org/paper/151681cb721d1fffaacc852d7760a82bff40f088,"International Workshop and International Workshop on Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques"
1698,Deterministic Annealing for Stochastic Variational Inference,"Stochastic variational inference (SVI) maps posterior inference in latent variable models to nonconvex stochastic optimization. While they enable approximate posterior inference for many otherwise intractable models, variational inference methods suffer from local optima. We introduce deterministic annealing for SVI to overcome this issue. We introduce a temperature parameter that deterministically deforms the objective, and then reduce this parameter over the course of the optimization. Initially it encourages high entropy variational distributions, which we find eases convergence to better optima. We test our method with Latent Dirichlet Allocation on three large corpora. Compared to SVI, we show improved predictive likelihoods on held-out data.",2014-11-07,https://www.semanticscholar.org/paper/5cee21ae4605a330d9977164523be4b865df6ebd,arXiv.org
466,On the Complexity of Cooperative Solution Concepts,"We study from a complexity theoretic standpoint the various solution concepts arising in cooperative game theory. We use as a vehicle for this study a game in which the players are nodes of a graph with weights on the edges, and the value of a coalition is determined by the total weight of the edges contained in it. The Shapley value is always easy to compute. The core is easy to characterize when the game is convex, and is intractable (NP-complete) otherwise. Similar results are shown for the kernel, the nucleolus, the e-core, and the bargaining set. As for the von Neumann-Morgenstern solution, we point out that its existence may not even be decidable. Many of these results generalize to the case in which the game is presented by a hypergraph with edges of size k > 2.",1994-05-01,https://www.semanticscholar.org/paper/42ae40a85a2b9e41197bacfdecd1576b02ab8827,Mathematics of Operations Research
1658,Factorization Meets the Item Embedding: Regularizing Matrix Factorization with Item Co-occurrence,"Matrix factorization (MF) models and their extensions are standard in modern recommender systems. MF models decompose the observed user-item interaction matrix into user and item latent factors. In this paper, we propose a co-factorization model, CoFactor, which jointly decomposes the user-item interaction matrix and the item-item co-occurrence matrix with shared item latent factors. For each pair of items, the co-occurrence matrix encodes the number of users that have consumed both items. CoFactor is inspired by the recent success of word embedding models (e.g., word2vec) which can be interpreted as factorizing the word co-occurrence matrix. We show that this model significantly improves the performance over MF models on several datasets with little additional computational overhead. We provide qualitative results that explain how CoFactor improves the quality of the inferred factors and characterize the circumstances where it provides the most significant improvements.",2016-09-07,https://www.semanticscholar.org/paper/89a16eb847e5039fe5d9c6372ab45145400c9aa1,ACM Conference on Recommender Systems
708,Alembic: Automated Model Inference for Stateful Network Functions,"Network operators today deploy a wide range of complex, stateful network functions (NFs). Typically, they only have access to the NFs’ binary executables, configuration interfaces, and manuals from vendors. To ensure correct behavior of NFs, operators use network testing and verification tools, which often rely on models of the deployed NFs. The effectiveness of these tools depends on the fidelity of such models. Today, models are handwritten, which can be error prone, tedious, and does not account for implementation-specific artifacts. To address this gap, our goal is to automatically infer behavioral models of stateful NFs for a given configuration. The problem is challenging because NF configurations can contain diverse rule types and the space of dynamic and stateful NF behaviors is large. In this work, we present Alembic, which synthesizes NF models viewed as an ensemble of finite-state machines (FSMs). Alembic consists of an offline stage that learns symbolic FSM representations for each NF rule type and an online stage that generates a concrete behavioral model for a given configuration using these symbolic FSMs. We demonstrate that Alembic is accurate, scalable, and sheds light on subtle differences across NF implementations.",,https://www.semanticscholar.org/paper/b4295b05ed4efd680880230883fecf71c1d60059,Symposium on Networked Systems Design and Implementation
2903,Predicting residual stress in a 316L electron beam weld joint incorporating plastic properties derived from a crystal plasticity finite element model,,2022-12-01,https://www.semanticscholar.org/paper/4e90bb0f343c0b534d29c8bffc13e9d4a568f2e9,International Journal of Pressure Vessels and Piping
2376,Regulation of superoxide generation by myeloperoxidase during the respiratory burst of human neutrophils.,"The role of myeloperoxidase in the regulation of the respiratory burst of human neutrophils activated by the chemotactic peptide (N-formyl-L-methionyl-L-leucyl-L-phenylalanine) plus cytochalasin B was determined by using anti-(human myeloperoxidase) antibody. The respiratory burst activated under these conditions consisted of an initial (1-2 min) phase with high rates of O2 uptake, luminol-dependent chemiluminescence and superoxide radical (O2-.) generation and a second, more sustained, phase of lower magnitude of chemiluminescence and O2 uptake: O2-. generation did not occur during this second phase. In cell suspensions stimulated in the presence of anti-(human myeloperoxidase) antibody, the magnitude of the initial phase of both O2 uptake and O2-. generation was unaffected, but these high rates were maintained over much longer periods than in control suspensions. It is therefore proposed that a product of myeloperoxidase normally regulates the duration of O2-. generation during the respiratory burst, possibly by inhibition of NADPH oxidase.",1986-07-15,https://www.semanticscholar.org/paper/e0b40946ed93a765c96c233206711ad32d5e573c,Biochemical Journal
115,Metadata for digital libraries: architecture and design rationale,"In a distributed, heterogeneous, proxy-based digital library, autonomous services and collections are accessed indirectly via proxies. To facilitate metadata compatibility and interoperability in such a digital library, we have designed a metadata architecture that includes four basic component classes: attribute model proxies, attribute model translators, metadata facilities for search proxies, and metadata repositories. Attribute model proxies elevate both attribute sets and the attributes they define to first-class objects. They also allow relationships among attributes to be captured. Attribute model translators map attributes and attribute values from one attribute model to another (where posMetadata facilities for search proxies provide structured descriptions both of the collections to which the search proxies provide access and of the search capabilities of the proxies. Finally, metadata repositories accumulate selected metadata from local instances of the other three component classes in order to facilitate global metadata queries and local metadata caching. In this paper, we outline further the roles of these component classes, discuss our design rationale, and analyze related work. Keywords: Metadata architecture, interoperability, attribute model, attribute model translation, metadata repository, InfoBus, proxy architecture, heterogeneity, digital libraries, CORBA.",1997-07-01,https://www.semanticscholar.org/paper/273f7bc28571360deb92854c5f55f0ac8d577f34,Digital library
3424,How to Schedule When You Have to Buy Your Energy,,2010-09-01,https://www.semanticscholar.org/paper/98d9e84dde13a349b5dca5e2eb5239a5895df60a,"International Workshop and International Workshop on Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques"
692,Smoothed Complexity of SWAP in Local Graph Partitioning,"We give the first quasipolynomial upper bound $\phi n^{\text{polylog}(n)}$ for the smoothed complexity of the SWAP algorithm for local Graph Partitioning (also known as Bisection Width), where $n$ is the number of nodes in the graph and $\phi$ is a parameter that measures the magnitude of perturbations applied on its edge weights. More generally, we show that the same quasipolynomial upper bound holds for the smoothed complexity of the 2-FLIP algorithm for any binary Maximum Constraint Satisfaction Problem, including local Max-Cut, for which similar bounds were only known for $1$-FLIP. Our results are based on an analysis of cycles formed in long sequences of double flips, showing that it is unlikely for every move in a long sequence to incur a positive but small improvement in the cut weight.",2023-05-25,https://www.semanticscholar.org/paper/b1b60d6f6a6679e616c7efeaaeb2e0229cda3e8a,arXiv.org
3642,A history of C++: 1979–1991,"This paper outlines the history of the C++ programming language. The emphasis is on the ideas, constraints, and people that shaped the language, rather than the minuitiae of language features. Key design decisions relating to language features are discussed, but the focus is on the overall design goals and practical constraints. The evolution of C++ is traced from C with Classes to the current ANSI and ISO standards work and the explosion of use, interest, commercial activity, compilers, tools, environments, and libraries.",1993-03-01,https://www.semanticscholar.org/paper/e94286ad503414eb85fecd8164264ee2a0b7927e,HOPL-II
2855,Toxoplasma gondii infection reveals a novel regulatory role for galectin-3 in the interface of innate and adaptive immunity.,"In attempts to investigate the role of galectin-3 in innate immunity, we studied galectin-3-deficient (gal3-/-) mice with regard to their response to Toxoplasma gondii infection, which is characterized by inflammation in affected organs, Th-1-polarized immune response, and accumulation of cysts in the central nervous system. In wild-type (gal3+/+) mice, infected orally, galectin-3 was highly expressed in the leukocytes infiltrating the intestines, liver, lungs, and brain. Compared with gal3+/+, infected gal3-/- mice developed reduced inflammatory response in all of these organs but the lungs. Brain of gal3-/- mice displayed a significantly reduced number of infiltrating monocytes/macrophages and CD8+ cells and a higher parasite burden. Furthermore, gal3-/- mice mounted a higher Th1-polarized response and had comparable survival rates on peroral T. gondii infection, even though they were more susceptible to intraperitoneal infection. Interestingly, splenic cells and purified CD11c+ dendritic cells from gal3-/- mice produced higher amounts of interleukin-12 than cells from gal3+/+ mice, possibly explaining the higher Th1 response verified in the gal3-/- mice. We conclude that galectin-3 exerts an important role in innate immunity, including not only a pro-inflammatory effect but also a regulatory role on dendritic cells, capable of interfering in the adaptive immune response.",2006-06-01,https://www.semanticscholar.org/paper/2a9e45578b4fa9651f0ed44e4dd08120ea7cacb4,American Journal of Pathology
2619,A hypermedia authoring tool for augmented and virtual reality,"Most existing hypermedia authoring systems are intended for use on desktop computers. These systems are typically designed for the creation of 2D documents and therefore employ 2D authoring mechanisms. In contrast, authoring systems for nontraditional multimedia/hypermedia experiences for 3D virtual or augmented worlds focus mainly on creating separate media objects and embedding them within the user's surroundings. As a result, linking these media objects to create 3D hypermedia is a tedious manual task. To address this issue, we present an authoring tool for creating and editing linked 3D hypermedia narratives that are interwoven with a wearable computer user's surrounding environment. Our system is designed for use by authors who are not programmers, and allows them to preview their results on a desktop workstation, as well as with an augmented or virtual reality system.",2003-01-01,https://www.semanticscholar.org/paper/69834836ece8f5370b76507902f49356b192772f,New Rev. Hypermedia Multim.
1721,Stochastic Structured Variational Inference,,2014-04-16,https://www.semanticscholar.org/paper/e97be50cc895bc1b819fac457a6203bb066a5ae4,International Conference on Artificial Intelligence and Statistics
3389,Fully Dynamic Maximal Independent Set with Polylogarithmic Update Time,"We present the first algorithm for maintaining a maximal independent set (MIS) of a fully dynamic graph---which undergoes both edge insertions and deletions---in polylogarithmic time. Our algorithm is randomized and, per update, takes O(log^2 Δ log^2 n) expected time. Furthermore, the algorithm can be adjusted to have O(log^2 Δ log^4 n) worst-case update-time with high probability. Here, n denotes the number of vertices and Δ is the maximum degree in the graph. The MIS problem in fully dynamic graphs has attracted significant attention after a breakthrough result of Assadi, Onak, Schieber, and Solomon [STOC'18] who presented an algorithm with O(m^3/4) update-time (and thus broke the natural Ω(m) barrier) where m denotes the number of edges in the graph. This result was improved in a series of subsequent papers, though, the update-time remained polynomial. In particular, the fastest algorithm prior to our work had Õ (min{√n, m^1/3}) update-time [Assadi et al. SODA'19]. Our algorithm maintains the lexicographically first MIS over a random order of the vertices. As a result, the same algorithm also maintains a 3-approximation of correlation clustering. We also show that a simpler variant of our algorithm can be used to maintain a random-order lexicographically first maximal matching in the same update-time.",2019-09-08,https://www.semanticscholar.org/paper/502140641ab77c45002bfed620d064253a699a9a,IEEE Annual Symposium on Foundations of Computer Science
461,Beyond competitive analysis [on-line algorithms],"The competitive analysis of on-line algorithms has been criticized as being too crude and unrealistic. We propose two refinements of competitive analysis an two directions: The first restricts the power of the adversary by allowing only certain input distributions, while the other allows for comparisons between information regimes for on-line decision-making. We illustrate the first with an application to the paging problem; as a by product we characterize completely the work functions of this important special case of the k-server problem. We use the second refinement to explore the power of lookahead in server systems, and the power of visual sensors in robot navigation.<<ETX>>",1994-11-20,https://www.semanticscholar.org/paper/0596142782494dd8b45f8c7069641a10d8bd5406,Proceedings 35th Annual Symposium on Foundations of Computer Science
1932,Data-driven innovation to capture user-experience product design: An empirical study for notebook visual aesthetics design,,2016-09-01,https://www.semanticscholar.org/paper/37ddc78cc849fcefb0d6151bbe8a47d002e2851c,Computers & industrial engineering
2815,Galectin-3 promotes HIV-1 budding through association with Alix and Gag-p6 (170.25),"
 Galectin-3 (Gal3), a β-galactoside-binding lectin, has been reported to regulate the functions of a number of immune cell types. We demonstrated that Gal3 is translocated to the immunological synapse in T cells upon T cell receptors engagement and associated with ALG-2-interacting protein X (Alix). Alix is known to coordinate with endosomal sorting complex required for transport (ESCRT) to promote HIV-1 virion release through binding to the HIV-1 Gag-P6 protein. We hypothesized that Gal3 plays a role in HIV-1 viral budding. Co-transfection of Gal3 and HIV-1 plasmids in HEK293T cells indicate that endogenous Gal3 facilitates HIV-1 budding. This effect was inhibited by knocking down Gal3 expression by shRNA. Immunoblotting of trypsin-treated virions and immuno-electron microscopy indicate that Gal3 is mainly located inside the HIV-1 virions. Immunofluorescent staining and coimmunoprecipitation (Co-IP) suggest that Gal3, Alix, and Gag-p6 are colocalized in HIV-1-infected cells. Notably, Gal3 expression was found to promote the association between Alix and Gag-p6 as demonstrated by Co-IP. Co-transfection of Gal3 and HIV-1 plasmids in Alix knocked-down cells indicate that promotion of HIV-1 budding by Gal3 is mediated through Alix. Finally, knocking down Gal3 expression in primary CD4+ T cells resulted in a significant decrease of HIV-1 titer. Our results indicate that endogenous Gal3 facilitates the HIV-1 virus budding through stabilizing the association between Alix and Gag-p6.",2012-05-01,https://www.semanticscholar.org/paper/f75e32bf6a91fac67b3e4f4f016006c69df579c2,Journal of Immunology
3728,"A Unifying Framework for Formal Theories of Novelty: Framework, Examples and Discussion","Managing inputs that are novel, unknown, or out-of-distribution is critical as an agent moves from the lab to the open world. Novelty-related problems include being tolerant to novel perturbations of the normal input, detecting when the input includes novel items, and adapting to novel inputs. While significant research has been undertaken in these areas, a noticeable gap exists in the lack of a formalized definition of novelty that transcends problem domains. As a team of researchers spanning multiple research groups and different domains, we have seen, first hand, the difficulties that arise from ill-specified novelty problems, as well as inconsistent definitions and terminology. Therefore, we present the first unified framework for formal theories of novelty and use the framework to formally define a family of novelty types. Our framework can be applied across a wide range of domains, from symbolic AI to reinforcement learning, and beyond to open world image recognition. Thus, it can be used to help kick-start new research efforts and accelerate ongoing work on these important novelty-related problems. This extended version of our AAAI 2021 paper included more details and examples in multiple domains.",2020-12-08,https://www.semanticscholar.org/paper/f2d1cbb25b1c58b89b6a8140f775493817f73752,arXiv.org
1857,Hierarchical Topic Models and the Nested Chinese Restaurant Process,"We address the problem of learning topic hierarchies from data. The model selection problem in this domain is daunting—which of the large collection of possible trees to use? We take a Bayesian approach, generating an appropriate prior via a distribution on partitions that we refer to as the nested Chinese restaurant process. This nonparametric prior allows arbitrarily large branching factors and readily accommodates growing data collections. We build a hierarchical topic model by combining this prior with a likelihood that is based on a hierarchical variant of latent Dirichlet allocation. We illustrate our approach on simulated data and with an application to the modeling of NIPS abstracts.",2003-12-09,https://www.semanticscholar.org/paper/28e245ce0e06f398dd26a9d6ab6bb04ef4c016e6,Neural Information Processing Systems
3647,Exception Handling for C++,"This paper outlines a design for an exception handling mechanism for C ++. It presents the reasoning behind the major design decisions and considers their implications for implementation alternatives. The mechanism is flexible, comparatively safe and easy to use, works in a mixed language execution environment, and can be implemented to run efficiently. Two implementation strategies are described in some detail.",1990-06-01,https://www.semanticscholar.org/paper/1955673684ccef4551543c9d6c819bbb7347df32,C++ Conference
2341,"Neutrophil function in whole blood and after purification: Changes in receptor expression, oxidase activity and responsiveness to cytokines",,1992-04-01,https://www.semanticscholar.org/paper/b0b66c84f79ca045fa3e2fa9484310b0b894ffaf,Bioscience Reports
2062,Economic Efficiency Analysis of Wafer Fabrication,"Economic efficiency analysis of semiconductor fabrication facilities (fabs) involves tradeoffs among cost, yield, and cycle time. Due to the disparate units involved, direct evaluation and comparison is difficult. This article employs data envelopment analysis (DEA) to determine relative efficiencies among fabs over time on the basis of empirical data, whereby cycle time performance is transformed into monetary value according to an estimated price decline rate. Two alternative DEA models are formulated to evaluate the influence of cycle time and other performance attributes. The results show that cycle time and yield follow increasing returns to scale, just as do cost and resource utilization. Statistical analyses are performed to investigate the DEA results, leading to specific improvement directions and opportunities for relatively inefficient fabs. Note to Practitioners-Speed of manufacturing is an important metric of factory performance, yet it has long been a challenge to integrate its value into overall performance evaluation. However, for many semiconductor products, a predictable rate of decline in selling prices makes it possible to transform time value into monetary value. This study employs a novel method to incorporate a speed metric into economic efficiency evaluation and thereby provide a guideline for improving fab efficiency in manufacturing practice. Furthermore, this study integrates factory productivity and cycle time into a relative efficiency analysis model that jointly evaluates the impact of these two factors in manufacturing performance. In particular, we validate this approach with data from ten leading wafer fabs obtained by the Competitive Semiconductor Manufacturing Program and we discuss managerial implications.",2007-10-08,https://www.semanticscholar.org/paper/1e7f3ac68c32fb3cb80a615bdfd49102206fa006,IEEE Transactions on Automation Science and Engineering
3489,Improved Bounds on Relaxations of a Parallel Machine Scheduling Problem,,1998-12-01,https://www.semanticscholar.org/paper/695837342d3b39aa1db775e5988c16488fc3d45d,Journal of combinatorial optimization
2905,Novel Computational Models of Evoked Dopamine Release In Vivo Measured by Fast Scan Cyclic Voltammetry Quantify the Regulation of Presynaptic Kinetics by Synucleins,"Dopamine neurotransmission in the striatum is central to many normal and disease functions. Ventral midbrain dopamine neurons exhibit ongoing tonic firing that produce low extrasynaptic levels of dopamine below the detection of extrasynaptic electrochemical recordings (∼10 – 20 nanomolar), with superimposed bursts that can saturate the dopamine uptake transporter and produce transient micromolar concentrations. The bursts have previously been shown to lead to presynaptic plasticity via multiple mechanisms, but analysis methods for these kinetic parameters are limited. To provide a deeper understanding of the mechanics of dopamine neurotransmission, we present three computational models of dopamine release with different levels of spatiotemporal complexity to analyze in vivo fast-scan cyclic voltammetry recordings from the dorsal striatum of mice. The models accurately fit to the cyclic voltammetry data and provide estimates of presynaptic dopamine facilitation/depression kinetics and dopamine transporter reuptake kinetics. We use the models to analyze the role of synuclein proteins in neurotransmission and quantify recent findings linking presynaptic protein α-synuclein to the short-term facilitation and long-term depression of dopamine release.",2022-05-05,https://www.semanticscholar.org/paper/63715dddc876973c251a416eb12eac9c9b496ae0,bioRxiv
510,On graph-theoretic lemmata and complexity classes,"Several new complexity classes of search problems that lie between the classes FP and FNP are defined. These classes are contained in the class TFNP of search problems that always have a solution. A problem in each of these new classes is defined in terms of an implicitly given, exponentially large graph, very much like PLS (polynomial local search). The existence of the solution sought is established by means of a simple graph-theoretic lemma with an inefficiently constructive proof. Several class containments and collapses, resulting in the two new classes PDLF contained in PLF are shown; the relation of either class of PLS is open. PLF contains several important problems for which no polynomial-time algorithm is presently known.<<ETX>>",1990-10-22,https://www.semanticscholar.org/paper/14a35370622c069bc62f6f3d1a9ab72562e4b950,Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
3751,"See, Hear, and Read: Deep Aligned Representations","We capitalize on large amounts of readily-available, synchronous data to learn a deep discriminative representations shared across three major natural modalities: vision, sound and language. By leveraging over a year of sound from video and millions of sentences paired with images, we jointly train a deep convolutional network for aligned representation learning. Our experiments suggest that this representation is useful for several tasks, such as cross-modal retrieval or transferring classifiers between modalities. Moreover, although our network is only trained with image+text and image+sound pairs, it can transfer between text and sound as well, a transfer the network never observed during training. Visualizations of our representation reveal many hidden units which automatically emerge to detect concepts, independent of the modality.",2017-06-03,https://www.semanticscholar.org/paper/52ed3b634c302af93ee2e70b7c28e4b2128a5947,arXiv.org
1054,Simulations of events for the LUX-ZEPLIN (LZ) dark matter experiment,,2020-01-25,https://www.semanticscholar.org/paper/bc42e1127b1dcbd11146d8a829cf9f5e0bd1e3a2,Astroparticle physics
122,Generalizing GlOSS to Vector-Space Databases and Broker Hierarchies,"As large numbers of text databases have become available on the Internet, it is getting harder to locate the right sources for given queries. In this paper we present gGlOSS, a generalized Glossary-Of-Servers Server, that keeps statistics on the available databases to estimate which databases are the potentially most useful for a given query. gGlOSS extends our previous work, which focused on databases using the boolean model of document retrieval, to cover databases using the more sophisticated vector-space retrieval model. We evaluate our new techniques using real-user queries and 53 databases. Finally, we further generalize our approach by showing how to build a hierarchy of gGlOSS brokers. The top level of the hierarchy is so small it could be widely replicated, even at end-user workstations.",1995-09-11,https://www.semanticscholar.org/paper/8897aed85e8da828c8331ceca2a1b98e4cd7e763,Very Large Data Bases Conference
2831,Galectin-3 regulates peritoneal B1-cell differentiation into plasma cells.,"Extracellular galectin-3 participates in the control of B2 lymphocyte migration and adhesion and of their differentiation into plasma cells. Here, we analyzed the role of galectin-3 in B1-cell physiology and the balance between B1a and B1b lymphocytes in the peritoneal cavity. In galectin-3(-/-) mice, the total number of B1a lymphocytes was lower, while B1b lymphocyte number was higher as compared to wild-type mice. The differentiation of B1a cells into plasma cells was associated with their abnormal adhesion and location on the mesentery. The B220 and CD43, constitutively expressed by B1 lymphocytes, were respectively up- and downregulated in galectin-3(-/-) mice. Mononuclear cells were strongly adhered to the mesenteric membranes of both CD43(-/-) and galectin-3(-/-) mice, but in contrast to CD43(-/-) mice, the accumulation of B1 cells in peritoneal membranes in galectin-3(-/-) mice was accompanied by their functional differentiation into plasma cells. We have shown that in the absence of galectin-3, B1-cell differentiation into plasma cells is favored and the dynamic equilibrium of B1-cell populations in the peritoneum is maintained through a compensatory increase in B1b lymphocytes.",2009-11-01,https://www.semanticscholar.org/paper/60abdb9e9c04497e05eb948944ed8ad47efccd8e,Glycobiology
2488,Poster: 3D referencing for remote task assistance in augmented reality,"We present a 3D referencing technique tailored for remote maintenance tasks in augmented reality. The goal is to improve the accuracy and efficiency with which a remote expert can point out a real physical object at a local site to a technician at that site. In a typical referencing task, the remote expert instructs the local technician to navigate to a location from which a target object can be viewed, and then to attend to that object. The expert and technician both wear head-tracked, stereo, see-through, head-worn displays, and the expert's hands are tracked by a set of depth cameras. The remote expert first selects one of a set of prerecorded viewpoints of the local site, and a representation of that viewpoint is presented to the technician to help them navigate to the correct position and orientation. The expert then uses hand gestures to indicate the target.",2013-03-16,https://www.semanticscholar.org/paper/c710881910cd118575f8a5f303417a5a46e40628,IEEE Symposium on 3D User Interfaces
3075,WARP: Enabling fast CPU scheduler development and evaluation,"Developing CPU scheduling algorithms and understanding their impact in practice can be difficult and time consuming due to the need to modify and test operating system kernel code and measure the resulting performance on a consistent workload of real applications. To address this problem, we have developed WARP, a trace-driven virtualized scheduler execution environment that can dramatically simplify and speed the development of CPU schedulers. WARP is easy to use as it can run unmodified kernel scheduling code and can be used with standard user-space debugging and performance monitoring tools. It accomplishes this by virtualizing operating system and hardware events to decouple kernel scheduling code from its native operating system and hardware environment. A simple kernel tracing toolkit can be used with WARP to capture traces of all CPU scheduling related events from a real system. WARP can then replay these traces in its virtualized environment with the same timing characteristics as in the real system. Traces can be used with different schedulers to provide accurate comparisons of scheduling performance for a given application workload. We have implemented a WARP Linux prototype. Our results show that WARP can use application traces captured from its toolkit to accurately reflect the scheduling behavior of the real Linux operating system. Furthermore, testing scheduler behavior using WARP with application traces can be two orders of magnitude faster than running the applications using Linux.",2009-04-26,https://www.semanticscholar.org/paper/b5d5921662913507e9085291efa1e306b11a542e,IEEE International Symposium on Performance Analysis of Systems and Software
1976,An intelligent system for wafer bin map defect diagnosis: An empirical study for semiconductor manufacturing,,2013-05-01,https://www.semanticscholar.org/paper/26368ed17e6ecd502ef8a8df065771cf2b728d61,Engineering applications of artificial intelligence
3582,C++ Dynamic Cast in Autonomous Space Systems,"The dynamic cast operation allows flexibility in the design and use of data management facilities in object- oriented programs. Dynamic cast has an important role in the implementation of the data management services (DMS) of the mission data system project (MDS), the jet propulsion laboratory's experimental work for providing a state-based and goal-oriented unified architecture for testing and development of mission software. DMS is responsible for the storage and transport of control and scientific data in a remote autonomous spacecraft. Like similar operators in other languages, the C++ dynamic cast operator does not provide the timing guarantees needed for hard real-time embedded systems. In a recent study, Gibbs and Stroustrup (G&S) devised a dynamic cast implementation strategy that guarantees fast constant-time performance. This paper presents the definition and application of a co-simulation framework to formally verify and evaluate the G&S fast dynamic casting scheme and its applicability in the mission data system DMS application. We describe the systematic process of model-based simulation and analysis that has lead to performance improvement of the G&S algorithm's heuristics by about a factor of 2.",2008-05-05,https://www.semanticscholar.org/paper/67a7239fca42015fa4e7f59f894239e2b40d01b0,IEEE International Symposium on Real-Time Distributed Computing
2467,[POSTER] Interactive Visualizations for Monoscopic Eyewear to Assist in Manually Orienting Objects in 3D,"Assembly or repair tasks often require objects to be held in specific orientations to view or fit together. Research has addressed the use of AR to assist in these tasks, delivered as registered overlaid graphics on stereoscopic head-worn displays. In contrast, we are interested in using monoscopic head-worn displays, such as Google Glass. To accommodate their small monoscopic field of view, off center from the user's line of sight, we are exploring alternatives to registered overlays. We describe four interactive rotation guidance visualizations for tracked objects intended for these displays.",2015-09-29,https://www.semanticscholar.org/paper/469c16953e13f4d4c7b947782de491331c2fd54a,2015 IEEE International Symposium on Mixed and Augmented Reality
738,Temporal Synthesis for Bounded Systems and Environments,"Temporal synthesis is the automated construction of a system from its temporal specification. It is by now realized that requiring the synthesized system to satisfy the specifications against all possible environments may be too demanding, and, dually, allowing all systems may be not demanding enough. In this work we study bounded temporal synthesis, in which bounds on the sizes of the state space of the system and the environment are additional parameters to the synthesis problem. This study is motivated by the fact that such bounds may indeed change the answer to the synthesis problem, as well as the theoretical and computational aspects of the synthesis problem. In particular, a finer analysis of synthesis, which takes system and environment sizes into account, yields deeper insight into the quantificational structure of the synthesis problem and the relationship between strong synthesis -- there exists a system such that for all environments, the specification holds, and weak synthesis -- for all environments there exists a system such that the specification holds. 
 
We first show that unlike the unbounded setting, where determinacy of regular games implies that strong and weak synthesis coincide, these notions do not coincide in the bounded setting. We then turn to study the complexity of deciding strong and weak synthesis. We show that bounding the size of the system or both the system and the environment, turns the synthesis problem into a search problem, and one cannot expect to do better than brute-force search. In particular, the synthesis problem for bounded systems and environment is Sigma^P_2-complete (in terms of the bounds, for a specification given by a deterministic automaton). We also show that while bounding the environment may lead to the synthesis of specifications that are otherwise unrealizable, such relaxation of the problem comes at a high price from a complexity-theoretic point of view.",,https://www.semanticscholar.org/paper/5456e12fbacb18180f052bfc5d5c64db1b92dc8e,Symposium on Theoretical Aspects of Computer Science
816,On the complexity of database queries (extended abstract),"We revisit the issue of the complexity of database queries, in the light of the recent parametric refinement of complexity theory. We show that, if the query size (or the number of variables in the query) is considered as a parameter, then the relational calculus and its fragments (conjunctive queries, positive queries) are classified at appropriate levels of the so-called W hierarchy of Downey and Fellows. These results strongly suggest that the query size is inherently in the exponent of the data complexity of any query evaluation algorithm, with the implication becoming stronger as the expressibility of the query language increases. For recursive languages (fixpoint logic, Datalog) this is provably the case [14]. On the positive side, we show that this exponential dependence can be avoided for the extension of acyclic queries with # (but not <) inequalities.",1997-05-01,https://www.semanticscholar.org/paper/f6d76c42bc47979485b525fb331afae68eced1eb,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems
1133,Results from a low-energy analysis of the CDMS II germanium data.,"We report results from a reanalysis of data from the Cryogenic Dark Matter Search (CDMS II) experiment at the Soudan Underground Laboratory. Data taken between October 2006 and September 2008 using eight germanium detectors are reanalyzed with a lowered, 2 keV recoil-energy threshold, to give increased sensitivity to interactions from weakly interacting massive particles (WIMPs) with masses below ∼10  GeV/c(2). This analysis provides stronger constraints than previous CDMS II results for WIMP masses below 9  GeV/c(2) and excludes parameter space associated with possible low-mass WIMP signals from the DAMA/LIBRA and CoGeNT experiments.",2010-11-10,https://www.semanticscholar.org/paper/847a4c27c7119cb1ece4396714246b23270a9a83,Physical Review Letters
3221,Moving beyond structure to function,,2018-02-01,https://www.semanticscholar.org/paper/2e94ab0313867b358b1e411eddedc99aa1c74a48,Animal Behaviour
3006,DistAI: Data-Driven Automated Invariant Learning for Distributed Protocols,"Distributed systems are notoriously hard to implement correctly due to non-determinism. Finding the inductive invariant of the distributed protocol is a critical step in verifying the correctness of distributed systems, but takes a long time to do even for simple protocols. We present DistAI, a data-driven automated system for learning inductive invariants for distributed protocols. DistAI generates data by simulating the distributed protocol at different instance sizes and recording states as samples. Based on the observation that invariants are often concise in practice, DistAI starts with small invariant formulas and enumerates all strongest possible invariants that hold for all samples. It then feeds those invariants and the desired safety properties to an SMT solver to check if the conjunction of the invariants and the safety properties is inductive. Starting with small invariant formulas and strongest possible invariants avoids large SMT queries, improving SMT solver performance. Because DistAI starts with the strongest possible invariants, if the SMT solver fails, DistAI does not need to discard failed invariants, but knows to monotonically weaken them and try again with the solver, repeating the process until it eventually succeeds. We prove that DistAI is guaranteed to find the ∃-free inductive invariant that proves the desired safety properties in finite time, if one exists. Our evaluation shows that DistAI successfully verifies 13 common distributed protocols automatically and outperforms alternative methods both in the number of protocols it verifies and the speed at which it does so, in some cases by more than two orders of magnitude.",,https://www.semanticscholar.org/paper/1d71a942f457f6b324c42256eed829a8806b77dd,USENIX Symposium on Operating Systems Design and Implementation
424,Algorithmic Approaches to Information Retrieval and Data Mining (Abstract),,1998-08-12,https://www.semanticscholar.org/paper/764f5ab98e6e75485da09e4607cfcccb129e4a25,International Computing and Combinatorics Conference
610,On the complexity of integer programming,A simple proof that integer programming ts in X~ ~s given. The proof also estabhshes that there ~s a pseudopolynomial-tune algorithm for integer programmmg with any (fixed) number of constraints.,1981-10-01,https://www.semanticscholar.org/paper/c0a6a6dca0a5ca42a966595d15d202c8e4450b0d,JACM
2981,Pitman-Yor Diffusion Trees,"We introduce the Pitman Yor Diffusion Tree (PYDT) for hierarchical clustering, a generalization of the Dirichlet Diffusion Tree (Neal, 2001) which removes the restriction to binary branching structure. The generative process is described and shown to result in an exchangeable distribution over data points. We prove some theoretical properties of the model and then present two inference methods: a collapsed MCMC sampler which allows us to model uncertainty over tree structures, and a computationally efficient greedy Bayesian EM search algorithm. Both algorithms use message passing on the tree structure. The utility of the model and algorithms is demonstrated on synthetic and real world data, both continuous and binary.",2011-06-13,https://www.semanticscholar.org/paper/57156f73746d295f5ed73948c8a43dd024048392,Conference on Uncertainty in Artificial Intelligence
2593,Unit: modular development of distributed interaction techniques for highly interactive user interfaces,"The Unit framework uses a dataflow programming language to describe interaction techniques for highly interactive environments, such as augmented, mixed, and virtual reality. Unit places interaction techniques in an abstraction layer between the input devices and the application, which allows the application developer to separate application functionality from interaction techniques and behavior.Unit's modular approach leads to the design of reusable application-independent interaction control components, portions of which can be distributed across different machines. Unit makes it possible at run time to experiment with interaction technique behavior, as well as to switch among different input device configurations. We provide both a visual interface and a programming API for the specification of the dataflow. To demonstrate how Unit works and to show the benefits to the interaction design process, we describe a few interaction techniques implemented using Unit. We also show how Unit's distribution mechanism can offload CPU intensive operations, as well as avoid costly special-purpose hardware in experimental setups.",2004-06-15,https://www.semanticscholar.org/paper/221f4db6e084abaf586a902c5ce6c0ae5057df6e,Conference on Computer Graphics and Interactive Techniques in Australasia and Southeast Asia
550,The synthesis of communication protocols,,1986-11-01,https://www.semanticscholar.org/paper/92d5d014da5dcac7c39e6fc50b81e50bdc3ae625,ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing
3348,"Kinship, Need, and the Distribution of Altruism","The optimal distribution of investment in kin cannot be determined solely on the basis of knowledge of genetic relatedness. The distribution will be affected by the fact that altruists are likely to encounter diminishing returns on their investments in kin. Furthermore, returns to altruism will vary because of phenotypic variation among recipients. Differences in fitness conversion efficiencies may outweigh relatedness considerations so that a distant relative may be more valuable than a close relative. Or else, if all kin are superefficient at converting investment into fitness, an even distribution of investment among kin may be optimal irrespective of differences in relatedness. It is necessary to ascertain whether the principal consequence of investment results in increased fecundity or increased survivorship of recipients because the optimal distribution of investment may differ considerably in each case.",1983-06-01,https://www.semanticscholar.org/paper/4150754cbc0c59da9b34ef158fbb9d3a4bb11e38,American Naturalist
3677,ClimSim: An open large-scale dataset for training high-resolution physics emulators in hybrid multi-scale climate simulators,"Modern climate projections lack adequate spatial and temporal resolution due to computational constraints. A consequence is inaccurate and imprecise predictions of critical processes such as storms. Hybrid methods that combine physics with machine learning (ML) have introduced a new generation of higher fidelity climate simulators that can sidestep Moore's Law by outsourcing compute-hungry, short, high-resolution simulations to ML emulators. However, this hybrid ML-physics simulation approach requires domain-specific treatment and has been inaccessible to ML experts because of lack of training data and relevant, easy-to-use workflows. We present ClimSim, the largest-ever dataset designed for hybrid ML-physics research. It comprises multi-scale climate simulations, developed by a consortium of climate scientists and ML researchers. It consists of 5.7 billion pairs of multivariate input and output vectors that isolate the influence of locally-nested, high-resolution, high-fidelity physics on a host climate simulator's macro-scale physical state. The dataset is global in coverage, spans multiple years at high sampling frequency, and is designed such that resulting emulators are compatible with downstream coupling into operational climate simulators. We implement a range of deterministic and stochastic regression baselines to highlight the ML challenges and their scoring. The data (https://huggingface.co/datasets/LEAP/ClimSim_high-res, https://huggingface.co/datasets/LEAP/ClimSim_low-res, and https://huggingface.co/datasets/LEAP/ClimSim_low-res_aqua-planet) and code (https://leap-stc.github.io/ClimSim) are released openly to support the development of hybrid ML-physics and high-fidelity climate simulations for the benefit of science and society.",2023-06-14,https://www.semanticscholar.org/paper/111ef44f5c3cf2f8a93a124ef152ebc116135def,arXiv.org
2528,Physician Attitudes about Patient-Facing Information Displays at an Urban Emergency Department.,"Hospital information systems have primarily been designed to support physicians and administrators, though recent research has explored the value of patient-facing information displays. Electronic systems can be designed to provide tailored information to patients on their health, their care teams, the status of their hospital stays, and their expected care plans. However, this direct delivery of information from database to patient represents a fundamental change to the traditional flow of clinical information. We therefore explore physician attitudes toward a proposed patient-facing display of information abstracted from a hospital EHR, in the context of an urban emergency department. We find that physicians generally support direct delivery of electronic information to patients, and uncover important concerns to consider in the design of patient-facing information systems.",2010-11-13,https://www.semanticscholar.org/paper/f598f2a5b9bf1ea037a32dc0a4c76ecf14f6a816,AMIA ... Annual Symposium proceedings. AMIA Symposium
2874,"Cell Cycle Regulation by Galectin-12, a New Member of the Galectin Superfamily*","Galectins are a family of β-galactoside-binding animal lectins with conserved carbohydrate recognition domains (CRDs). Here we report the identification and characterization of a new galectin, galectin-12, which contains two domains that are homologous to the galectin CRD. The N-terminal domain contains all of the sequence elements predicted to form the two β-sheets found in other galectins, as well as conserved carbohydrate-interacting residues. The C-terminal domain shows considerable divergence from the consensus sequence, and many of these conserved residues are not present. Nevertheless, the protein has lactose binding activity, most likely due to the contribution of the N-terminal domain. The mRNA for galectin-12 contains features coding for proteins with growth-regulatory functions. These include start codons in a context that are suboptimal for translation initiation and AU-rich motifs in the 3′-untranslated region, which are known to confer instability to mRNA. Galectin-12 mRNA is sparingly expressed or undetectable in many tissues and cell lines tested, but it is up-regulated in cells synchronized at the G1 phase or the G1/S boundary of the cell cycle. Ectopic expression of galectin-12 in cancer cells causes cell cycle arrest at the G1 phase and cell growth suppression. We conclude that galectin-12 is a novel regulator of cellular homeostasis.",2001-06-08,https://www.semanticscholar.org/paper/f8e7b7784940cb2b5fdbfd91be5620f371b492eb,Journal of Biological Chemistry
2980,Message passing algorithms for dirichlet diffusion trees,"We demonstrate efficient approximate inference for the Dirichlet Diffusion Tree (Neal, 2003), a Bayesian nonparametric prior over tree structures. Although DDTs provide a powerful and elegant approach for modeling hierarchies they haven't seen much use to date. One problem is the computational cost of MCMC inference. We provide the first deterministic approximate inference methods for DDT models and show excellent performance compared to the MCMC alternative. We present message passing algorithms to approximate the Bayesian model evidence for a specific tree. This is used to drive sequential tree building and greedy search to find optimal tree structures, corresponding to hierarchical clusterings of the data. We demonstrate appropriate observation models for continuous and binary data. The empirical performance of our method is very close to the computationally expensive MCMC alternative on a density estimation problem, and significantly outperforms kernel density estimators.",2011-06-28,https://www.semanticscholar.org/paper/53ea725425d01dc3ebb884537783f68f8bc724d8,International Conference on Machine Learning
188,Towards a Unified Complexity Theory of Total Functions,"Abstract The class TFNP, of NP search problems where all instances have solutions, appears not to have complete problems. However, TFNP contains various syntactic subclasses and important problems. We introduce a syntactic class of problems that contains these known subclasses, for the purpose of understanding and classifying TFNP problems. This class is defined in terms of the search for an error in a concisely-represented formal proof. Finally, the known complexity subclasses are based on existence theorems that hold for finite structures; from Herbrand's Theorem, we note that such theorems must apply specifically to finite structures, and not infinite ones.",2017-12-01,https://www.semanticscholar.org/paper/40a8b56f5afe8b9ba82cfa855f9f87f148fca980,Information Technology Convergence and Services
2112,Using Bayesian Network for Fault Location on Distribution Feeder,"The Bayesian network is a probabilistic graphical model in which a problem is structured as a set of variables (parameters) and probabilistic relationships among them. The Bayesian network has been effectively used to incorporate expert knowledge and historical data for revising the prior belief in the light of new evidence in many fields. However, little research has been done to apply the Bayesian network for fault location in power delivery systems. We construct a Bayesian network on the basis of expert knowledge and historical data for fault diagnosis on a distribution feeder in Taiwan. The experimental results validate the practical viability of the proposed approach.",2002-11-07,https://www.semanticscholar.org/paper/96f57edd19ad252639e4003cec6da87ce3b37e07,IEEE Power Engineering Review
2896,Cas13d-mediated isoform-specific RNA knockdown with a unified computational and experimental toolbox,"Alternative splicing is an essential mechanism for diversifying proteins, in which mature RNA isoforms produce proteins with potentially distinct functions. Two major challenges in characterizing the cellular function of isoforms are the lack of experimental methods to specifically and efficiently modulate isoform expression and computational tools for complex experimental design. To address these gaps, we developed and methodically tested a strategy which pairs the RNA-targeting CRISPR/Cas13d system with guide RNAs that span exon-exon junctions in the mature RNA. We performed a high-throughput essentiality screen, quantitative RT-PCR assays, and PacBio long read sequencing to affirm our ability to specifically target and robustly knockdown individual RNA isoforms. In parallel, we provide computational tools for experimental design and screen analysis. Considering all possible splice junctions annotated in GENCODE for multi-isoform genes and our gRNA efficacy predictions, we estimate that our junction-centric strategy can uniquely target up to 89% of human RNA isoforms, including 50,066 protein-coding and 11,415 lncRNA isoforms. Importantly, this specificity spans all splicing and transcriptional events, including exon skipping and inclusion, alternative 5’ and 3’ splice sites, and alternative starts and ends.",2023-09-13,https://www.semanticscholar.org/paper/cfb0b138430322512df115dfd0f51f230f70b072,bioRxiv
2941,Therapeutic reduction of ataxin 2 extends lifespan and reduces pathology in TDP-43 mice,,2017-03-17,https://www.semanticscholar.org/paper/65ad7bddc0133047cc9f830eeb5b97b8bcbf9e8e,Nature
706,"Tarski's Theorem, Supermodular Games, and the Complexity of Equilibria","The use of monotonicity and Tarski's theorem in existence proofs of equilibria is very widespread in economics, while Tarski's theorem is also often used for similar purposes in the context of verification. However, there has been relatively little in the way of analysis of the complexity of finding the fixed points and equilibria guaranteed by this result. We study a computational formalism based on monotone functions on the $d$-dimensional grid with sides of length $N$, and their fixed points, as well as the closely connected subject of supermodular games and their equilibria. It is known that finding some (any) fixed point of a monotone function can be done in time $\log^d N$, and we show it requires at least $\log^2 N$ function evaluations already on the 2-dimensional grid, even for randomized algorithms. We show that the general Tarski problem of finding some fixed point, when the monotone function is given succinctly (by a boolean circuit), is in the class PLS of problems solvable by local search and, rather surprisingly, also in the class PPAD. Finding the greatest or least fixed point guaranteed by Tarski's theorem, however, requires $d\cdot N$ steps, and is NP-hard in the white box model. For supermodular games, we show that finding an equilibrium in such games is essentially computationally equivalent to the Tarski problem, and finding the maximum or minimum equilibrium is similarly harder. Interestingly, two-player supermodular games where the strategy space of one player is one-dimensional can be solved in $O(\log N)$ steps. We also observe that computing (approximating) the value of Condon's (Shapley's) stochastic games reduces to the Tarski problem. An important open problem highlighted by this work is proving a $\Omega(\log^d N)$ lower bound for small fixed dimension $d \geq 3$.",2019-09-07,https://www.semanticscholar.org/paper/a41a42b359497ef929f288ed06bac3fb2b6cbbc4,Information Technology Convergence and Services
3695,RESIN-11: Schema-guided Event Prediction for 11 Newsworthy Scenarios,"We introduce RESIN-11, a new schema-guided event extraction&prediction framework that can be applied to a large variety of newsworthy scenarios. The framework consists of two parts: (1) an open-domain end-to-end multimedia multilingual information extraction system with weak-supervision and zero-shot learningbased techniques. (2) schema matching and schema-guided event prediction based on our curated schema library. We build a demo website based on our dockerized system and schema library publicly available for installation (https://github.com/RESIN-KAIROS/RESIN-11). We also include a video demonstrating the system.",,https://www.semanticscholar.org/paper/396833983f6d7d77957e12c3839e5da05feb053a,North American Chapter of the Association for Computational Linguistics
1150,Search for weakly interacting massive particles with the first five-tower data from the cryogenic dark matter search at the soudan underground laboratory.,"We report results from the Cryogenic Dark Matter Search at the Soudan Underground Laboratory (CDMS II) featuring the full complement of 30 detectors. A blind analysis of data taken between October 2006 and July 2007 sets an upper limit on the weakly interacting massive particle (WIMP) nucleon spin-independent cross section of 6.6x10;{-44} cm;{2} (4.6x10;{-44} cm;{2} when combined with previous CDMS II data) at the 90% confidence level for a WIMP mass of 60 GeV/c;{2}. This achieves the best sensitivity for dark matter WIMPs with masses above 44 GeV/c;{2}, and significantly restricts the parameter space for some favored supersymmetric models.",2009-01-05,https://www.semanticscholar.org/paper/3a62e29c0d7f3489a89a8fa181427ef05476f5f7,Physical Review Letters
3676,SURFSUP: Learning Fluid Simulation for Novel Surfaces,"Modeling the mechanics of fluid in complex scenes is vital to applications in design, graphics, and robotics. Learning-based methods provide fast and differentiable fluid simulators, however most prior work is unable to accurately model how fluids interact with genuinely novel surfaces not seen during training. We introduce SURFSUP, a framework that represents objects implicitly using signed distance functions (SDFs), rather than an explicit representation of meshes or particles. This continuous representation of geometry enables more accurate simulation of fluid-object interactions over long time periods while simultaneously making computation more efficient. Moreover, SURFSUP trained on simple shape primitives generalizes considerably out-of-distribution, even to complex real-world scenes and objects. Finally, we show we can invert our model to design simple objects to manipulate fluid flow.",2023-04-13,https://www.semanticscholar.org/paper/0c1a64d547a93b206f0488366ca765ee2a711593,arXiv.org
734,Model Checking of Recursive Probabilistic Systems,"Recursive Markov Chains (RMCs) are a natural abstract model of procedural probabilistic programs and related systems involving recursion and probability. They succinctly define a class of denumerable Markov chains that generalize several other stochastic models, and they are equivalent in a precise sense to probabilistic Pushdown Systems. In this article, we study the problem of model checking an RMC against an ω-regular specification, given in terms of a Büchi automaton or a Linear Temporal Logic (LTL) formula. Namely, given an RMC A and a property, we wish to know the probability that an execution of A satisfies the property. We establish a number of strong upper bounds, as well as lower bounds, both for qualitative problems (is the probability = 1, or = 0?), and for quantitative problems (is the probability ≥ p?, or, approximate the probability to within a desired precision). The complexity upper bounds we obtain for automata and LTL properties are similar, although the algorithms are different.
 We present algorithms for the qualitative model checking problem that run in polynomial space in the size |A| of the RMC and exponential time in the size of the property (the automaton or the LTL formula). For several classes of RMCs, including single-exit RMCs (a class that encompasses some well-studied stochastic models, for instance, stochastic context-free grammars) the algorithm runs in polynomial time in |A|. For the quantitative model checking problem, we present algorithms that run in polynomial space in the RMC and exponential space in the property. For the class of linearly recursive RMCs we can compute the exact probability in time polynomial in the RMC and exponential in the property. For deterministic automata specifications, all our complexities in the specification come down by one exponential.
 For lower bounds, we show that the qualitative model checking problem, even for a fixed RMC, is already EXPTIME-complete. On the other hand, even for simple reachability analysis, we know from our prior work that our PSPACE upper bounds in A can not be improved substantially without a breakthrough on a well-known open problem in the complexity of numerical computation.",2012-04-01,https://www.semanticscholar.org/paper/5c8117e4b3a6c0002d9ac45e70d7f2faf07a5595,TOCL
994,Lamina depth and thickness correlate with glaucoma severity,"Purpose: To evaluate the correlation between lamina cribrosa (LC) morphology and glaucoma severity in patients with primary forms of open-angle glaucoma (OAG) using enhanced depth imaging spectral-domain optical coherence tomography (SD-OCT) and Humphrey visual field test (HVF). Subjects and Methods: Patients with OAG (n = 166), divided into normal-tension glaucoma (NTG) and high-tension glaucoma (HTG) groups (n = 66 and n = 100), were imaged using SD-OCT to obtain horizontal B-scan images of the optic nerve head (ONH). Laminar depth (LD) and laminar thickness (LT) were measured at the center of ONH. Results: The mean (±standard deviation) values of LD, LT, and visual field mean deviation (MD) were 555.4 ± 142.3 μm, 179.9 ± 49.7 μm, and − 5.7 ± 6.4 dB, respectively. In the multivariate linear regression analysis, LD, LT, and intraocular pressure (IOP) were significantly correlated with MD (P = 0.007, P = 0.037, and P = 0.004, respectively). In the subgroup analyses, only LD was associated with MD in the NTG group (n = 66), whereas LT and IOP were correlated with MD in the HTG group (n = 100). Neither axial length nor central corneal thickness was associated with LD or LT. Conclusions: Glaucoma severity, as measured by HVF MD, shows significant correlations with LD and LT, with greater severity associated with increasing LD and decreasing LT. Normal- and high-tension OAG patients have different associations with LD and LT, which implies that the pathogenesis of these two entities might be different.",2016-05-01,https://www.semanticscholar.org/paper/d202f2d4ae93b8afa2b7f82131b8f311819e5b86,Indian Journal of Ophthalmology
100,Letter from the Special Issue Editor,,,https://www.semanticscholar.org/paper/7a6e96acb393774175ccc1c49c26a73372e646dc,IEEE Data Engineering Bulletin
2503,Directing attention and influencing memory with visual saliency modulation,"In augmented reality, it is often necessary to draw the user's attention to particular objects in the real world without distracting her from her task. We explore the effectiveness of directing a user's attention by imperceptibly modifying existing features of a video. We present three user studies of the effects of applying a saliency modulation technique to video; evaluating modulation awareness, attention, and memory. Our results validate the saliency modulation technique as an alternative means to convey information to the user, suggesting attention shifts and influencing recall of selected regions without perceptible changes to visual input.",2011-05-07,https://www.semanticscholar.org/paper/0ca83c9f0ff452cbd72cb4d18d24d300022c8414,International Conference on Human Factors in Computing Systems
1016,Pertussis toxin lesioning of the nucleus caudate-putamen attenuates adenylate cyclase inhibition and alters neuronal electrophysiological activity,,1989-08-21,https://www.semanticscholar.org/paper/00fad007255b3c960f1e29cc6a23a190ef6f7bdb,Brain Research
1968,Data Mining for Optimizing IC Feature Designs to Enhance Overall Wafer Effectiveness,"As global competition continues to strengthen in semiconductor industry, semiconductor companies have to continuously advance manufacturing technology and improve productivity to maintain competitive advantages. Die cost is significantly influenced by wafer productivity that is determined by yield rate and the number of gross dies per wafer. However, little research has been done on design for manufacturing and productivity enhancement through increasing the gross die number per wafer and decreasing the required shot number for exposure. This paper aims to propose a novel approach to improve overall wafer effectiveness via data mining to generate the optimal IC feature designs that can bridge the gap between integrated circuit (IC) design and wafer fabrication by providing chip designer with the optimal IC feature size in the design phase to increase gross dies and reduce the required shots. An empirical study was conducted in a leading semiconductor company for validation. The results have shown that the proposed approach can effectively enhance wafer productivity. Indeed, the developed solution has been implemented in the company to provide desired IC features to IC designers to enhance overall wafer effectiveness.",2014-02-01,https://www.semanticscholar.org/paper/a54202cfaec90b2e02855d8c67954399a235b8ed,IEEE transactions on semiconductor manufacturing
407,On the Floyd-Warshall Algorithm for Logic Programs,,1999-10-01,https://www.semanticscholar.org/paper/727add3d6621891fbb1387777488f2a18ec773dc,The Journal of Logic Programming
1530,Probabilistic Conformal Prediction Using Conditional Random Samples,"This paper proposes probabilistic conformal prediction (PCP), a predictive inference algorithm that estimates a target variable by a discontinuous predictive set. Given inputs, PCP construct the predictive set based on random samples from an estimated generative model. It is efficient and compatible with either explicit or implicit conditional generative models. Theoretically, we show that PCP guarantees correct marginal coverage with finite samples. Empirically, we study PCP on a variety of simulated and real datasets. Compared to existing methods for conformal inference, PCP provides sharper predictive sets.",2022-06-14,https://www.semanticscholar.org/paper/caf9b1c5458ee90a6351f6438c3a705e4f2086df,International Conference on Artificial Intelligence and Statistics
3112,Reducing Downtime Due to System Maintenance and Upgrades (Awarded Best Student Paper!),"Patching, upgrading, and maintaining operating system software is a growing management complexity problem that can result in unacceptable system downtime. We introduce AutoPod, a system that enables unscheduled operating system updates while preserving application service availability. AutoPod provides a group of processes and associated users with an isolated machine-independent virtualized environment that is decoupled from the underlying operating system instance. This virtualized environment is integrated with a novel checkpoint-restart mechanism which allows processes to be suspended, resumed, and migrated across operating system kernel versions with different security and maintenance patches. 
 
AutoPod incorporates a system status service to determine when operating system patches need to be applied to the current host, then automatically migrates application services to another host to preserve their availability while the current host is updated and rebooted. We have implemented AutoPod on Linux without requiring any application or operating system kernel changes. Our measurements on real world desktop and server applications demonstrate that AutoPod imposes little overhead and provides sub-second suspend and resume times that can be an order of magnitude faster than starting applications after a system reboot. AutoPod enables systems to autonomically stay updated with relevant maintenance and security patches, while ensuring no loss of data and minimizing service disruption.",2005-12-04,https://www.semanticscholar.org/paper/d4101710a4aa68e904d056147806d4a745867eec,LiSA
1515,Variational Inference with Gaussian Score Matching,"Variational inference (VI) is a method to approximate the computationally intractable posterior distributions that arise in Bayesian statistics. Typically, VI fits a simple parametric distribution to the target posterior by minimizing an appropriate objective such as the evidence lower bound (ELBO). In this work, we present a new approach to VI based on the principle of score matching, that if two distributions are equal then their score functions (i.e., gradients of the log density) are equal at every point on their support. With this, we develop score matching VI, an iterative algorithm that seeks to match the scores between the variational approximation and the exact posterior. At each iteration, score matching VI solves an inner optimization, one that minimally adjusts the current variational estimate to match the scores at a newly sampled value of the latent variables. We show that when the variational family is a Gaussian, this inner optimization enjoys a closed form solution, which we call Gaussian score matching VI (GSM-VI). GSM-VI is also a ``black box'' variational algorithm in that it only requires a differentiable joint distribution, and as such it can be applied to a wide class of models. We compare GSM-VI to black box variational inference (BBVI), which has similar requirements but instead optimizes the ELBO. We study how GSM-VI behaves as a function of the problem dimensionality, the condition number of the target covariance matrix (when the target is Gaussian), and the degree of mismatch between the approximating and exact posterior distribution. We also study GSM-VI on a collection of real-world Bayesian inference problems from the posteriorDB database of datasets and models. In all of our studies we find that GSM-VI is faster than BBVI, but without sacrificing accuracy. It requires 10-100x fewer gradient evaluations to obtain a comparable quality of approximation.",2023-07-15,https://www.semanticscholar.org/paper/4707587fc67fd590c0f8c767869bb7ba73f3e56e,arXiv.org
2078,A novel method for determining machine subgroups and backups with an empirical study for semiconductor manufacturing,,2006-08-01,https://www.semanticscholar.org/paper/898e6d08e1e5a6b8156f8b0cebc77a3ebab79740,Journal of Intelligent Manufacturing
3746,A Large Scale Video Dataset for Event Recognition,,2018-09-01,https://www.semanticscholar.org/paper/b263ccf4c5bb19a578165ed731ca24f9ea3653cf,Journal of Vision
2486,Subtle cueing for visual search in head-tracked head worn displays,"Goal-oriented visual search in augmented reality can be facilitated by using visual cues to call attention to a target. However, traditional use of explicit cues can degrade visual search performance due to scene distortion, occlusion and addition of visual clutter. In contrast, Subtle Cueing has been previously proposed as an alter-native to explicit cueing, but little is known about how well it works for head-tracked head worn displays (HWDs). We investigated the effect of Subtle Cueing for head-tracked head worn displays, using visual search research methods in simulated augmented reality environments. Our user study found that Subtle Cueing improves visual search performance, and serves as a feasible cueing mechanism for AR environments using HWDs.",2013-10-01,https://www.semanticscholar.org/paper/6f7f4b48357b104fb3ddf8778f70a0be33a421a6,International Symposium on Mixed and Augmented Reality
3271,"An Extra Dimension to Decision-Making in Animals: The Three-way Trade-off between Speed, Effort per-Unit-Time and Accuracy","The standard view in biology is that all animals, from bumblebees to human beings, face a trade-off between speed and accuracy as they search for resources and mates, and attempt to avoid predators. For example, the more time a forager spends out of cover gathering information about potential food sources the more likely it is to make accurate decisions about which sources are most rewarding. However, when the cost of time spent out of cover rises (e.g. in the presence of a predator) the optimal strategy is for the forager to spend less time gathering information and to accept a corresponding decline in the accuracy of its decisions. We suggest that this familiar picture is missing a crucial dimension: the amount of effort an animal expends on gathering information in each unit of time. This is important because an animal that can respond to changing time costs by modulating its level of effort per-unit-time does not have to accept the same decrease in accuracy that an animal limited to a simple speed-accuracy trade-off must bear in the same situation. Instead, it can direct additional effort towards (i) reducing the frequency of perceptual errors in the samples it gathers or (ii) increasing the number of samples it gathers per-unit-time. Both of these have the effect of allowing it to gather more accurate information within a given period of time. We use a modified version of a canonical model of decision-making (the sequential probability ratio test) to show that this ability to substitute effort for time confers a fitness advantage in the face of changing time costs. We predict that the ability to modulate effort levels will therefore be widespread in nature, and we lay out testable predictions that could be used to detect adaptive modulation of effort levels in laboratory and field studies. Our understanding of decision-making in all species, including our own, will be improved by this more ecologically-complete picture of the three-way tradeoff between time, effort per-unit-time and accuracy.",2014-12-01,https://www.semanticscholar.org/paper/2e85c130f289abb8530b7176c490778ae8aefa32,PLoS Comput. Biol.
191,"8th Innovations in Theoretical Computer Science Conference, ITCS 2017, January 9-11, 2017, Berkeley, CA, USA","For undirected graphs G = (V,E) and G0 = (V0, E0), say that G is a region intersection graph over G0 if there is a family of connected subsets {Ru ⊆ V0 : u ∈ V } of G0 such that {u, v} ∈ E ⇐⇒ Ru ∩Rv 6= ∅. We show if G0 excludes the complete graph Kh as a minor for some h ≥ 1, then every region intersection graph G over G0 with m edges has a balanced separator with at most ch √ m nodes, where ch is a constant depending only on h. If G additionally has uniformly bounded vertex degrees, then such a separator is found by spectral partitioning. A string graph is the intersection graph of continuous arcs in the plane. String graphs are precisely region intersection graphs over planar graphs. Thus the preceding result implies that every string graph with m edges has a balanced separator of size O( √ m). This bound is optimal, as it generalizes the planar separator theorem. It confirms a conjecture of Fox and Pach (2010), and improves over the O( √ m logm) bound of Matoušek (2013). 1998 ACM Subject Classification F.2.2 Nonnumerical Algorithms and Problems, G.1.6 Optimization, G.2.2 Graph Theory",,https://www.semanticscholar.org/paper/cdf6cc8a0fdbff8e266439547b64d8e957bd7c64,Information Technology Convergence and Services
204,Cortical Computation via Iterative Constructions,"We study Boolean functions of an arbitrary number of input variables that can be realized by simple iterative constructions based on constant-size primitives. This restricted type of construction needs little global coordination or control and thus is a candidate for neurally feasible computation. Valiant's construction of a majority function can be realized in this manner and, as we show, can be generalized to any uniform threshold function. We study the rate of convergence, finding that while linear convergence to the correct function can be achieved for any threshold using a fixed set of primitives, for quadratic convergence, the size of the primitives must grow as the threshold approaches 0 or 1. We also study finite realizations of this process and the learnability of the functions realized. We show that the constructions realized are accurate outside a small interval near the target threshold, where the size of the construction grows as the inverse square of the interval width. This phenomenon, that errors are higher closer to thresholds (and thresholds closer to the boundary are harder to represent), is a well-known cognitive finding.",2016-02-26,https://www.semanticscholar.org/paper/87c6bc4a1c2be24f89b6a2d542bbe7866644c563,Annual Conference Computational Learning Theory
3162,Volume rendering on scalable shared-memory MIMD architectures,"Volume rendering is a useful visualization technique for understanding the large amounts of data generated in a variety of scientific disciplines. Routine use of this technique is currently limited by its computational expense. We have designed a parallel volume rendering algorithm for MIMD architectures based on ray tracing and a novel task queue image partitioning technique. The combination of ray tracing and MIMD architectures allows us to employ algorithmic optimizations such as hierarchical opacity enumeration, early ray termination, and adaptive image sampling. The use of task queue image partitioning makes these optimizations efficient in a parallel framework. We have implemented our algorithm on the Stanford DASH Multiprocessor, a scalable shared-memory MIMD machine. Its single address-space and coherent caches provide programming ease and good performance for our algorithm. With only a few days of programming effort, we have obtained nearly linear speedups and near real-time frame update rates on a 48 processor machine. Since DASH is constructed from Silicon Graphics multiprocessors, our code runs on any Silicon Graphics workstation without modification.",1992-12-01,https://www.semanticscholar.org/paper/f797376771e2ea038ba1ff496371dc7b615fbc3b,Symposium on Volume Visualization
2909,A method to extract slip system dependent information for crystal plasticity models,,2022-06-01,https://www.semanticscholar.org/paper/92b4341ff6c92728f13278f3fb7f4643733ece2c,MethodsX
2369,CO-reacting haemoproteins of neutrophils: Evidence for cytochrome b-245 and myeloperoxidase as potential oxidases during the respiratory burst,,1987-03-01,https://www.semanticscholar.org/paper/093c736e7c47eea637ad569ed87bb5f0a618ffa8,Bioscience Reports
1710,Scaling probabilistic models of genetic variation to millions of humans,"A major goal of population genetics is to quantitatively understand variation of genetic polymorphisms among individuals. The aggregated number of genotyped humans is currently on the order of millions of individuals, and existing methods do not scale to data of this size. To solve this problem, we developed TeraStructure, an algorithm to fit Bayesian models of genetic variation in structured human populations on tera-sample-sized data sets (1012 observed genotypes; for example, 1 million individuals at 1 million SNPs). TeraStructure is a scalable approach to Bayesian inference in which subsamples of markers are used to update an estimate of the latent population structure among individuals. We demonstrate that TeraStructure performs as well as existing methods on current globally sampled data, and we show using simulations that TeraStructure continues to be accurate and is the only method that can scale to tera-sample sizes.",2014-12-24,https://www.semanticscholar.org/paper/a46b787c5a9cc883006fd81718d15eaad2fd4197,Nature Genetics
650,Early experience with the Bard XT stent.,"The Bard XT stent is a new generation balloon expandable intracoronary stent. It has several unique design advantages. Between October 1996 and November 1997, 127 Bard XT stents of various length were deployed in 93 patients with 109 lesions. According to the American College of Cardiology (ACC) and American Heart Association (AHA) classifications 7 lesions were type A, 38 were type B1, 43 were type B2 and 21 were type C [Ellis et al.: Circulation 82:1193-1202, 1990]. Stent delivery was successful in 98% of attempts. Angiographic success was achieved in 98% of 109 lesions. Procedural success was achieved in 94% of 93 patients. Minimal luminal diameter (MLD) increased from 0.91+/-0.34 mm to 3.03+/-0.44 mm and percentage diameter stenosis reduced from 69.1+/-11.07 to 9.96+/-6.81. Complications occurred in four patients. One patient had intracranial hemorrhage, one patient had subacute thrombosis and two patients died postprocedure. Patients were followed for a period of 1 to 14 months (average 7+/-4 months) for major cardiac events and clinical restenosis. The Bard XT stent is a user-friendly device which provided excellent angiographic results and short-term clinical outcome in selected cases. Further study is required to evaluate effects on restenosis.",1998-12-01,https://www.semanticscholar.org/paper/7eed1cd3d49226261286d326c5df14b25e4bc1ab,Catheterization and Cardiovascular Diagnosis
25,Beyond Trending Topics: Real-World Event Identification on Twitter,"
 
 User-contributed messages on social media sites such as Twitter have emerged aspowerful, real-time means of information sharing on the Web. These short messages tend to reflect a variety of events in real time, making Twitter particularly well suited as a source of real-time event content. In this paper, we explore approaches for analyzing the stream of Twitter messages to distinguish between messages about real-world events andnon-event messages. Our approach relies on a rich family of aggregatestatistics of topically similar message clusters. Large-scale experiments over millions of Twitter messages show the effectiveness of our approach for surfacing real-world event content on Twitter.
 
",2011-07-05,https://www.semanticscholar.org/paper/bb7965a9dbeab34cec56e19aae949690f2463f20,International Conference on Web and Social Media
985,The Antibiofilm efficacy of nitric oxide on soft contact lenses,,2017-11-21,https://www.semanticscholar.org/paper/30bbef03693997a5c817c65f5f5a2b1f2efd5306,BMC Ophthalmology
2492,3D referencing techniques for physical objects in shared augmented reality,"We introduce an augmented reality referencing technique for shared environments that is designed to improve the accuracy with which one user can point out a real physical object to another user. Our technique, GARDEN (Gesturing in an Augmented Reality Depth-mapped ENvironment), is intended for use in otherwise unmodeled environments in which objects in the environment, and the hand of the user performing a selection, are interactively observed by a depth camera, and users wear tracked see-through displays. We present the results of a user study that compares GARDEN against existing augmented reality referencing techniques, as well as the use of a physical laser pointer. GARDEN performed significantly more accurately than all the comparison techniques when the participating users have sufficiently different views of the scene, and significantly more accurately than one of these techniques when the participating users have similar perspectives.",2012-11-05,https://www.semanticscholar.org/paper/1c5ae751e49b49926c21518c962a7a3dc8696234,International Symposium on Mixed and Augmented Reality
3010,Tap: an app framework for dynamically composable mobile systems,"As smartphones and tablets have become ubiquitous, there is a growing demand for apps that can enable users to collaboratively use multiple mobile systems. We present Tap, a framework that makes it easy for users to dynamically compose collections of mobile systems and developers to write apps that make use of those impromptu collections. Tap users control the composition by simply tapping systems together for discovery and authentication. The physical interaction mimics and supports ephemeral user interactions without the need for tediously exchanging user contact information such as phone numbers or email addresses. Tapping triggers a simple NFC-based mechanism to exchange connectivity information and security credentials that works across heterogeneous networks and requires no user accounts or cloud infrastructure support. Tap makes it possible for apps to use existing mobile platform APIs across multiple mobile systems by virtualizing data sources so that local and remote data sources can be combined together upon tapping. Virtualized data sources can be hardware or software features, including media, clipboard, calendar events, and devices such as cameras and microphones. Leveraging existing mobile platform APIs makes it easy for developers to write apps that use hardware and software features across dynamically composed collections of mobile systems. We have implemented a Tap prototype that allows apps to make use of both unmodified Android and iOS systems. We have modified and implemented various apps using Tap to demonstrate that it is easy to use and can enable apps to provide powerful new functionality by leveraging multiple mobile systems. Our results show that Tap has good performance, even for high-bandwidth features, and is user and developer friendly.",2021-06-24,https://www.semanticscholar.org/paper/95dc1272a354aa822320a39ae0369e7dffa2a892,"ACM SIGMOBILE International Conference on Mobile Systems, Applications, and Services"
2539,Relaxed selection techniques for querying time-series graphs,"Time-series graphs are often used to visualize phenomena that change over time. Common tasks include comparing values at different points in time and searching for specified patterns, either exact or approximate. However, tools that support time-series graphs typically separate query specification from the actual search process, allowing users to adapt the level of similarity only after specifying the pattern. We introduce relaxed selection techniques, in which users implicitly define a level of similarity that can vary across the search pattern, while creating a search query with a single-gesture interaction. Users sketch over part of the graph, establishing the level of similarity through either spatial deviations from the graph, or the speed at which they sketch (temporal deviations). In a user study, participants were significantly faster when using our temporally relaxed selection technique than when using traditional techniques. In addition, they achieved significantly higher precision and recall with our spatially relaxed selection technique compared to traditional techniques.",2009-10-04,https://www.semanticscholar.org/paper/f61ed852eca18bd3d99d68538b0234204eed314b,ACM Symposium on User Interface Software and Technology
29,Session details: Special section on managing information extraction,,,https://www.semanticscholar.org/paper/42ddfcde1bb3d71550318ee79988cb4a9c77e66e,SGMD
433,Decision-making by hierarchies of discordant agents,,1997-12-17,https://www.semanticscholar.org/paper/44e9a124ac393b0396c4f26d34551f90d9f7bba9,Mathematical programming
1717,Deep Exponential Families,"We describe \textit{deep exponential families} (DEFs), a class of latent variable models that are inspired by the hidden structures used in deep neural networks. DEFs capture a hierarchy of dependencies between latent variables, and are easily generalized to many settings through exponential families. We perform inference using recent ""black box"" variational inference techniques. We then evaluate various DEFs on text and combine multiple DEFs into a model for pairwise recommendation data. In an extensive study, we show that going beyond one layer improves predictions for DEFs. We demonstrate that DEFs find interesting exploratory structure in large data sets, and give better predictive performance than state-of-the-art models.",2014-11-10,https://www.semanticscholar.org/paper/d6559f35be0679c6b3371a2e44e3be293704b600,International Conference on Artificial Intelligence and Statistics
3430,Solving Maximum Flow Problems on Real World Bipartite Graphs,,,https://www.semanticscholar.org/paper/609e482fc114ccbf6eb27842197e290326ff4d38,Workshop on Algorithm Engineering and Experimentation
2416,A Location-Triggered Augmented Reality Walking Tour Using Snap Spectacles 2021,"We present an on-site 3D-animated audiovisual tour guide augmented reality application developed for Snap Spectacles 2021. The primary goal of this project is to explore how to use this experimental product to create an augmented reality tour guide. In addition, we present the design considerations for the user interface and the underlying system architecture. We illustrate the workflow of the tour application and discuss our experience working with Spectacles 2021 and its experimental API. We also present our design choices and directions for future work.",2022-03-01,https://www.semanticscholar.org/paper/d40786857480255b5b1cb8d57a5d486dad8e6f9b,2022 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)
1583,Population Predictive Checks,"Bayesian modeling has become a staple for researchers analyzing data. Thanks to recent developments in approximate posterior inference, modern researchers can easily build, use, and revise complicated Bayesian models for large and rich data. These new abilities, however, bring into focus the problem of model assessment. Researchers need tools to diagnose the fitness of their models, to understand where a model falls short, and to guide its revision. In this paper we develop a new method for Bayesian model checking, the population predictive check (Pop-PC). Pop-PCs are built on posterior predictive checks (PPC), a seminal method that checks a model by assessing the posterior predictive distribution on the observed data. Though powerful, PPCs use the data twice---both to calculate the posterior predictive and to evaluate it---which can lead to overconfident assessments. Pop-PCs, in contrast, compare the posterior predictive distribution to the population distribution of the data. This strategy blends Bayesian modeling with frequentist assessment, leading to a robust check that validates the model on its generalization. Of course the population distribution is not usually available; thus we use tools like the bootstrap and cross validation to estimate the Pop-PC. Further, we extend Pop-PCs to hierarchical models. We study Pop-PCs on classical regression and a hierarchical model of text. We show that Pop-PCs are robust to overfitting and can be easily deployed on a broad family of models.",2019-08-02,https://www.semanticscholar.org/paper/eb1b2363fa6cf44dbef9dc3342b3bc0ffac78f46,arXiv.org
3367,The Mechanisms of Filter Feeding: Some Theoretical Considerations,"We enumerate the five basic mechanisms by which any biological or manmade filter can remove particles from a fluid. These mechanisms are: (1) direct interception, (2) inertial impaction, (3) gravitational deposition, (4) motile-particle deposition, and (5) electrostatic attraction. For these mechanisms we present dimensionless indexes that indicate which measurable characteristics of the filter, the particles, and the flow affect the intensity of particle capture. By comparing the magnitudes of these indexes it is possible to determine the main mechanism a filter is using to capture particles. Awareness of these mechanisms and their interrelationships will provide insights for those investigating the efficiency of various modes of filter feeding and the mechanisms of size-selective suspension feeding.",1977-09-01,https://www.semanticscholar.org/paper/af135bbec3b2ee9efe5972e253f8ed525a219dff,American Naturalist
248,Continuous local search,"We introduce CLS, for continuous local search, a class of polynomial-time checkable total functions that lies at the intersection of PPAD and PLS, and captures a particularly benign kind of local optimization in which the domain is continuous, as opposed to combinatorial, and the functions involved are continuous. We show that this class contains several well known intriguing problems which were heretofore known to lie in the intersection of PLS and PPAD but were otherwise unclassifiable: Finding fixpoints of contraction maps, the linear complementarity problem for P matrices, finding a stationary point of a low-degree polynomial objective, the simple stochastic games of Shapley and Condon, and finding a mixed Nash equilibrium in congestion, implicit congestion, and network coordination games. The last four problems belong to CCLS, for convex CLS, another subclass of PPAD ∩ PLS seeking the componentwise local minimum of a componentwise convex function. It is open whether any or all of these problems are complete for the corresponding classes.",2011-01-23,https://www.semanticscholar.org/paper/0422dfac6623506a8b6980e51224982a1c720b7e,ACM-SIAM Symposium on Discrete Algorithms
1008,Changes in Anterior Chamber Configuration after Cataract Surgery as Measured by Anterior Segment Optical Coherence Tomography,"Purpose To evaluate the changes in anterior chamber depth (ACD) and angle width induced by phacoemulsification and intraocular lens (IOL) implantation in normal eyes using anterior segment optical coherence tomography (AS-OCT). Methods Forty-five eyes (45 patients) underwent AS-OCT imaging to evaluate anterior chamber configuration before and 2 days after phacoemulsification and IOL implantation. We analyzed the central ACD and angle width using different methods: anterior chamber angle (ACA), trabecular-iris angle (TIA), angle opening distance (AOD), and trabecular iris surface area (TISA) in the nasal and temporal quadrants. Comparison between preoperative and postoperative measurement was done using paired t-tests and each of the angle parameters was analyzed with Pearson correlation testing. Subgroup analyses according to the IOL and axial length were performed with a general multivariate linear model adjusted for age. Results Before surgery, the mean anterior chamber angle widths were 23.21 ± 6.70° in the nasal quadrant and 24.89 ± 7.66° in the temporal quadrant. The mean central ACD was 2.75 ± 0.43 mm. After phacoemulsification and IOL implantation, the anterior chamber angle width increased significantly to 35.16 ± 4.65° in the nasal quadrant (p = 0.001) and 36.03 ± 4.86° in the temporal quadrant (p = 0.001). Also, central ACD increased to 4.14 ± 0.31 mm (p = 0.001). AOD, TISA, and TIA increased significantly after cataract surgery and showed positive correlation with ACA. Conclusions After cataract surgery, the ACD and angle width significantly increased in eyes with cataract. AS-OCT is a good method for obtaining quantitative data regarding anterior chamber configuration.",2011-03-11,https://www.semanticscholar.org/paper/18caeebafd600bf7330315d8bd4dcf1a37f6be62,Korean Journal of Ophthalmology
3665,An overview of C++,"C++ is a general purpose programming language3 designed to make programming more enjoyable for the serious programmer. Except for minor details, C++ is a superset of the C language*. C++ was designed to [l] be a better C. [2] support data abstraction. [3] support object-oriented programming. This paper describes the features added to C to achieve this. In addition to C, the main influences of the design of C++ were Simula67’ and Algo1684. C++ has been in use for about four years and has been applied to mpst branches of systems programming including compiler construction, data base management, graphics, image processing, music synthesis, networking, numerical software, programming environments, robotics, simulation, and switching. It has a highly portable implementation and there are now at least 1500 installations including AT&T 3B, DEC VAX, Intel 80286, Motorola 68000, and Amdahl machines running UNlXt and other operating systems*.",1986-10-01,https://www.semanticscholar.org/paper/e264da9ced2871d28806b331876c91f76e59f97d,OOPWORK '86
1419,Inclusive jet production in pp(macro) collisions.,"We report a new measurement of the pseudorapidity (eta) and transverse-energy ( E(T)) dependence of the inclusive jet production cross section in pp(macro) collisions at square root of s = 1.8 TeV using 95 pb(-1) of data collected with the D0 detector at the Fermilab Tevatron. The differential cross section d(2)sigma/(dE(T)d eta) is presented up to eta = 3, significantly extending previous measurements. The results are in good overall agreement with next-to-leading order predictions from QCD and indicate a preference for certain parton distribution functions.",2000-11-10,https://www.semanticscholar.org/paper/097590d373bb9d5ff794dc837f04431c57a916a8,Physical Review Letters
58,Optimizing top-k selection queries over multimedia repositories,"Repositories of multimedia objects having multiple types of attributes (e.g., image, text) are becoming increasingly common. A query on these attributes will typically, request not just a set of objects, as in the traditional relational query model (filtering), but also a grade of match associated with each object, which indicates how well the object matches the selection condition (ranking). Furthermore, unlike in the relational model, users may just want the k top-ranked objects for their selection queries for a relatively small k. In addition to the differences in the query model, another peculiarity of multimedia repositories is that they may allow access to the attributes of each object only through indexes. We investigate how to optimize the processing of top-k selection queries over multimedia repositories. The access characteristics of the repositories and the above query model lead to novel issues in query optimization. In particular, the choice of the indexes used to search the repository strongly influences the cost of processing the filtering condition. We define an execution space that is search-minimal, i.e., the set of indexes searched is minimal. Although the general problem of picking an optimal plan in the search-minimal execution space is NP-hard, we present an efficient algorithm that solves the problem optimally with respect to our cost model and execution space when the predicates in the query are independent. We also show that the problem of optimizing top-k selection queries can be viewed, in many cases, as that of evaluating more traditional selection conditions. Thus, both problems can be viewed together as an extended filtering problem to which techniques of query processing and optimization may be adapted.",2004-08-01,https://www.semanticscholar.org/paper/52667eff2fea8af809465233b8667277e1e47ecb,IEEE Transactions on Knowledge and Data Engineering
288,On the complexity of reconfiguration problems,,2008-10-03,https://www.semanticscholar.org/paper/53d06855ade3e9de5efa0c7fb47fd424981c6225,Theoretical Computer Science
3719,We Have So Much In Common: Modeling Semantic Relational Set Abstractions in Videos,,2020-08-12,https://www.semanticscholar.org/paper/33c723f096c3fd156e32295325afc2e6081afac4,European Conference on Computer Vision
1571,Comment: Variational Autoencoders as Empirical Bayes,"We thank Professor Efron for his informative and unifying review of empirical Bayes. In this comment, we discuss the connection between empirical Bayes and the variational autoencoder (VAE), a popular statistical inference framework in the machine learning community. We hope this connection motivates new algorithmic approaches for empirical Bayesians and gives new perspectives on VAEs for machine learners.",2019-05-01,https://www.semanticscholar.org/paper/6a079726a8377b76b5b85a4695492035a77cd8d2,Statistical Science
566,The complexity of facets resolved,Abstract We show that recognizing the facets of the traveling salesman problem polytope is Dp-complete.,1985-10-21,https://www.semanticscholar.org/paper/e68af98db70190f1e0adfb3bf9026d513df312c7,26th Annual Symposium on Foundations of Computer Science (sfcs 1985)
1273,Search for Randall-Sundrum gravitons with 1 fb(-1) of data from pp collisions at sqrt(s)=1.96 TeV.,"We search for decays of Kaluza-Klein excitations of the graviton in the Randall-Sundrum model of extra dimensions to e+ e(-) and gamma gamma in 1 fb(-1) of pp collisions at sqrt(s)=1.96 TeV collected by the D0 detector at the Fermilab Tevatron. We set 95% confidence level upper limits on the production cross section times branching fraction, which translate into lower limits on the mass of the lightest excitation between 300 and 900 GeV for values of the coupling k/MPl between 0.01 and 0.1.",2007-10-17,https://www.semanticscholar.org/paper/35137ab8edfe693010c440dd2fd6101f5c096f7d,Physical Review Letters
3100,On the performance of wide-area thin-client computing,"While many application service providers have proposed using thin-client computing to deliver computational services over the Internet, little work has been done to evaluate the effectiveness of thin-client computing in a wide-area network. To assess the potential of thin-client computing in the context of future commodity high-bandwidth Internet access, we have used a novel, noninvasive slow-motion benchmarking technique to evaluate the performance of several popular thin-client computing platforms in delivering computational services cross-country over Internet2. Our results show that using thin-client computing in a wide-area network environment can deliver acceptable performance over Internet2, even when client and server are located thousands of miles apart on opposite ends of the country. However, performance varies widely among thin-client platforms and not all platforms are suitable for this environment. While many thin-client systems are touted as being bandwidth efficient, we show that network latency is often the key factor in limiting wide-area thin-client performance. Furthermore, we show that the same techniques used to improve bandwidth efficiency often result in worse overall performance in wide-area networks. We characterize and analyze the different design choices in the various thin-client platforms and explain which of these choices should be selected for supporting wide-area computing services.",2006-05-01,https://www.semanticscholar.org/paper/aeb049d0a1869827523689a7b8ccddff43aaec0c,TOCS
571,Updates of Relational Views,"The problem of translating updates of database views is studied. View updates are disambi- guated by requiring that a specified view complement (i.e., a second view that contains all the information omitted from the given view) remain constant during the translation. Some of the computational problems related to the apphcafion of this general methodology in the context of relational databases are studied. Projective views of databases that consist of a single relation and satisfy funcuonal dependencies are emphasized. After characterizing complementary views, the authors show that finding a minimum complement of a given view is NP-complete. The problem of translating the insertion of a tuple into a view is then studied in detail, and the results are extended to the cases of deletion and replacement of a tuple. Finally, the explicit functional dependencies, a new kind of dependency that intuitively states that some part of the database information can be computed from the rest, are defined and studied.",1984-09-20,https://www.semanticscholar.org/paper/2aef3318842c74fa1a9b78f4a0ebcd13a9c2849f,JACM
2211,Mucocutaneous manifestations in a UK national cohort of juvenile-onset systemic lupus erythematosus patients.,"OBJECTIVE
To determine whether mucocutaneous manifestations are associated with major organ involvement in a UK national cohort of juvenile-onset SLE (JSLE) patients.


METHODS
JSLE patients (n = 241) from 15 different centres whose diagnosis fulfilled four or more of the ACR criteria were divided into two groups: those with at least one ACR mucocutaneous criterion (ACR skin feature positive) and those without (ACR skin feature negative) at diagnosis. The relative frequency of skin involvement was described by the paediatric adaptation of the 2004 British Isles Lupus Assessment Group (pBILAG-2004) index.


RESULTS
One hundred and seventy-nine patients (74%) had ACR-defined skin involvement with no significant demographic differences compared with those without. ACR skin feature negative patients showed greater haematological (84% vs 67%), renal (43% vs 26%) (P < 0.05) and neurological (16% vs 4%) involvement (P = 0.001). Forty-two per cent of ACR skin feature negative patients had skin involvement using pBILAG-2004, which included maculopapular rash (17%), non-scaring alopecia (15%), cutaneous vasculitis (12%) and RP (12%). ACR skin feature negative patients with moderate to severe skin involvement by pBILAG-2004 showed greater renal and haematological involvement at diagnosis and over the follow-up period (P < 0.05). Higher immunosuppressive drug use in the skin feature negative group was demonstrated.


CONCLUSION
Patients who fulfil the ACR criteria but without any of the mucocutaneous criteria at diagnosis have an increased risk of major organ involvement. The pBILAG-2004 index has shown that other skin lesions may go undetected using the ACR criteria alone, and these lesions show a strong correlation with disease severity and major organ involvement.",2014-08-01,https://www.semanticscholar.org/paper/b5433cb8f1996b7666a4cfd9cec0aa54c6f64425,Rheumatology
2316,The cell biology of phagocytes.,,1995-11-01,https://www.semanticscholar.org/paper/f6ed9047503d7b668ce5112362c20c616683db6a,Immunology today (Amsterdam. Regular ed.)
541,Complexity Characterizations of Attribute Grammar Languages,,,https://www.semanticscholar.org/paper/ec17837a5788dbe3620a29ebc440d0781a074f8d,Information and Computation
2672,Visual task characterization for automated visual discourse synthesis,"To develop a comprehensive and systematic approach to the automated design of visual discourse, we introduce a visual task taxonomy that interfaces high-level presentation intents with low-level visual techniques. In our approach, visual tasks describe presentation intents through their visual accomplishments, and suggest desired visual techniques through their visual implications. Therefore, we can characterize visual tasks by their visual accomplishments and implications. Through this characterization, visual tasks can guide the visual discourse synthesis process by specifying what presentation intents can be achieved and how to achieve them.",1998-01-01,https://www.semanticscholar.org/paper/4eafbddeabb3c1f4143b8ee190680d0bdcd398f3,International Conference on Human Factors in Computing Systems
1529,Heterogeneous Supervised Topic Models,"Abstract Researchers in the social sciences are often interested in the relationship between text and an outcome of interest, where the goal is to both uncover latent patterns in the text and predict outcomes for unseen texts. To this end, this paper develops the heterogeneous supervised topic model (HSTM), a probabilistic approach to text analysis and prediction. HSTMs posit a joint model of text and outcomes to find heterogeneous patterns that help with both text analysis and prediction. The main benefit of HSTMs is that they capture heterogeneity in the relationship between text and the outcome across latent topics. To fit HSTMs, we develop a variational inference algorithm based on the auto-encoding variational Bayes framework. We study the performance of HSTMs on eight datasets and find that they consistently outperform related methods, including fine-tuned black-box models. Finally, we apply HSTMs to analyze news articles labeled with pro- or anti-tone. We find evidence of differing language used to signal a pro- and anti-tone.",2022-06-01,https://www.semanticscholar.org/paper/bfbd8f1cec08f842e65354c0d6598d42b906f74d,Transactions of the Association for Computational Linguistics
3497,Approximation techniques for average completion time scheduling,"We consider the problem of nonpreemptive scheduling to minimize average (weighted) completion time, allowing for release dates, parallel machines, and precedence constraints. Recent work has led to constant-factor approximations for this problem, based on solving a preemptive or linear programming relaxation and then using the solution to get an ordering on the jobs. We introduce several new techniques which generalize this basic paradigm. We use these ideas to obtain improved approximation algorithms for one-machine scheduling to minimize average completion time with release dates. In the process, we obtain an optimal randomized on-line algorithm for the same problem that beats a lower bound for deterministic on-line algorithms. We consider extensions to the case of parallel machine scheduling, and for this we introduce two new ideas: first, we show that a preemptive one-machine relaxation is a powerful tool for designing parallel machine scheduling algorithms that simultaneously produce good approximations and have small running times; second, we show that a non-greedy {open_quotes}rounding{close_quotes} of the relaxation yields better approximations than a greedy one. We also prove a general theorem relating the value of one-machine relaxations to that of the schedules obtained for the original m-machine problems. This theorem applies even when there are precedencemore » constraints on the jobs. We apply this result to precedence graphs such as in-trees, out-trees, and series- parallel graphs; these are of particular interest in compiler applications that partly motivated our work.« less",1997-01-05,https://www.semanticscholar.org/paper/6d1e977ee1b083430af9f15e731b9a3b7b88bc3e,ACM-SIAM Symposium on Discrete Algorithms
1855,Variational methods for the Dirichlet process,"Variational inference methods, including mean field methods and loopy belief propagation, have been widely used for approximate probabilistic inference in graphical models. While often less accurate than MCMC, variational methods provide a fast deterministic approximation to marginal and conditional probabilities. Such approximations can be particularly useful in high dimensional problems where sampling methods are too slow to be effective. A limitation of current methods, however, is that they are restricted to parametric probabilistic models. MCMC does not have such a limitation; indeed, MCMC samplers have been developed for the Dirichlet process (DP), a nonparametric distribution on distributions (Ferguson, 1973) that is the cornerstone of Bayesian nonparametric statistics (Escobar & West, 1995; Neal, 2000). In this paper, we develop a mean-field variational approach to approximate inference for the Dirichlet process, where the approximate posterior is based on the truncated stick-breaking construction (Ishwaran & James, 2001). We compare our approach to DP samplers for Gaussian DP mixture models.",2004-07-04,https://www.semanticscholar.org/paper/42fddb742959e80cefe3932475a3c2cad97cba8d,International Conference on Machine Learning
1001,Intraocular pressure reduction with topical medications and progression of normal‐tension glaucoma: a 12‐year mean follow‐up study,Purpose:  To investigate whether the amount of intraocular pressure (IOP) reduction with topical medications is associated with the progression of normal‐tension glaucoma (NTG) and to identify risk factors for NTG progression.,2013-06-01,https://www.semanticscholar.org/paper/9a6cbd6f379a683cf66dfa16e496181b18346210,Acta ophthalmologica
966,Relationship between Ocular Fatigue and Use of a Virtual Reality Device,"Purpose: To investigate ocular fatigue after the use of a head-mounted display (HMD)-type virtual reality device. Methods: Healthy adult volunteers were examined for ocular fatigue before and after watching videos for 10 min with an HMD-type virtual reality device. Subjective ocular fatigue was measured using a questionnaire. Objective fatigue was measured using the critical flicker fusion frequency (CFF), high frequency component of accommodative microfluctuation, and accommodation amplitude. The accommodation amplitude was measured using the push-up method and the dynamic measurement mode of the autorefractometer. Changes in the spherical equivalent were also measured. Results: The questionnaire-based subjective ocular fatigue increased ( p = 0.020) after use of the HMD device. In the dominant eye, the high frequency component of accommodative microfluctuation increased ( p < 0.05). The accommodation amplitude using the push-up method was decreased in the nondominant eye ( p = 0.007), and temporary myopia was observed ( p < 0.05). However, there was no increase in ocular fatigue in the CFF or the accommodation amplitude using the dynamic measurement mode, which showed no significant difference before and after using the HMD device ( p > 0.05). Conclusions: A subjective test and some objective tests suggested that use of the HMD-type virtual reality display increased ocular fatigue. However, no increase in ocular fatigue was measured using CFF nor in the accommodation amplitude using the dynamic measurement mode which was a limitation of the study. More studies with the aim to alleviate ocular fatigue after using HMD-type virtual reality devices are therefore needed.",2020-02-15,https://www.semanticscholar.org/paper/866dcd57b8fbeef33b67d77ad05a018dbfe5bac8,Journal of the Korean Ophthalmological Society
928,On Minimal Eulerian Graphs,,1981-08-13,https://www.semanticscholar.org/paper/d5cf61d4919c01a7d2e02904820b00a37edfba9f,Information Processing Letters
1300,Search for the standard model Higgs Boson in the pp[over]-->ZH-->nunu[over]bb[over] channel.,"We report a search for the standard model (SM) Higgs boson based on data collected by the D0 experiment at the Fermilab Tevatron Collider, corresponding to an integrated luminosity of 260 pb(-1). We study events with missing transverse energy and two acoplanar b jets, which provide sensitivity to the ZH production cross section in the nunu[over]bb[over] channel, and to WH production when the lepton from the W-->lnu decay is undetected. The data are consistent with the SM background expectation, and we set 95% C.L. upper limits on sigma(pp[over]-->ZH/WH)xB(H-->bb[over]) from 3.4/8.3 to 2.5/6.3 pb, for Higgs-boson masses between 105 and 135 GeV.",2006-07-01,https://www.semanticscholar.org/paper/089ca28a8809809a1652d6bca849fd575ba88a0a,Physical Review Letters
896,How easy is local search?,,1985-10-21,https://www.semanticscholar.org/paper/72d54ba4fed488a31022b6f37333550e041e93bc,26th Annual Symposium on Foundations of Computer Science (sfcs 1985)
1759,Variational inference in nonconjugate models,"Mean-field variational methods are widely used for approximate posterior inference in many probabilistic models. In a typical application, mean-field methods approximately compute the posterior with a coordinate-ascent optimization algorithm. When the model is conditionally conjugate, the coordinate updates are easily derived and in closed form. However, many models of interest--like the correlated topic model and Bayesian logistic regression--are nonconjugate. In these models, mean-field methods cannot be directly applied and practitioners have had to develop variational algorithms on a case-by-case basis. In this paper, we develop two generic methods for nonconjugate models, Laplace variational inference and delta method variational inference. Our methods have several advantages: they allow for easily derived variational algorithms with a wide class of nonconjugate models; they extend and unify some of the existing algorithms that have been derived for specific models; and they work well on real-world data sets. We studied our methods on the correlated topic model, Bayesian logistic regression, and hierarchical Bayesian logistic regression.",2012-09-19,https://www.semanticscholar.org/paper/c9db97118af813a87eb355c5a80671364c9ebbe1,Journal of machine learning research
633,Some Examples of Difficult Traveling Salesman Problems,"We construct instances of the symmetric traveling salesman problem with n = 8k cities that have the following property: There is exactly one optimal tour with cost n, and there are 2k-1k-1! tours that are next-best, have arbitrarily large cost, and cannot be improved by changing fewer than 3k edges. Thus, there are many local optima with arbitrarily high cost. It appears that local search algorithms are ineffective when applied to these problems. Even more catastrophic examples are available in the non-symmetric case.",1978-06-01,https://www.semanticscholar.org/paper/7487ea033b1729fb7695841673ece0e4782fdb88,Operational Research
2184,A robust intracellular metabolite extraction protocol for human neutrophil metabolic profiling,"Neutrophils are phagocytic innate immune cells that play essential roles in host defence, but are also implicated in inflammatory diseases such as rheumatoid arthritis (RA) where they contribute to systemic inflammation and joint damage. Transcriptomic analysis of neutrophils has revealed significant changes in gene expression in neutrophils activated in vitro by cytokines and in vivo during inflammation in RA. However, there are no reports on the global metabolomic changes that occur as a consequence of this activation. The aim of this study was to establish protocols for the study of changes in the metabolome of human neutrophils using 1H NMR spectroscopy. Sample preparation and spectral analysis protocols were optimised using neutrophils isolated by Ficoll-Paque, with decreased washing steps and inclusion of a heat-shock step to quench metabolite turnover. Cells were incubated ± PMA for 15 min in HEPES-free media and samples were analysed by NMR using a 700 MHz NMR Avance IIIHD Bruker NMR spectrometer equipped with a TCI cryoprobe. Chenomx, Bruker TopSpin and AMIX software were used to process spectra and identify metabolites. Principal Component Analysis (PCA) and signalling pathway analysis was carried out using Metaboanalyst. Cell number and number of scans (NS) were optimised as >3.6 million cells and 512 NS. 327 spectral bins were defined in the neutrophil spectra, of which 287 (87.7%) were assigned to 110 metabolites that included: amino acids, peptides and analogues; carbohydrates, carbonyls and alcohols; nucleotides, nucleosides and analogues; lipids and lipid-like molecules; benzenoids; and other organic compounds. 43 metabolites changed at least 1.5 fold (increase or decrease) after the addition of PMA for 5 or 15 min. Pathway analysis revealed that PMA affected nicotinate and nicotinamide metabolism, aminoacyl-tRNA biosynthesis and glycolysis, suggesting a redirection of glucose metabolism from glycolysis to the pentose phosphate pathway and production of NADPH for activation of the NADPH oxidase and subsequent respiratory burst. We have developed protocols for the study of human neutrophils by 1H NMR spectroscopy. Importantly, this methodology has sufficient sensitivity and reproducibility to detect changes in metabolite abundance from cell numbers typically collected from clinical samples or experiments with multiple assay conditions.",2018-12-20,https://www.semanticscholar.org/paper/495c10b25a26f1f3b864b32d55699d072657535b,PLoS ONE
3255,How the zebra got its stripes: a problem with too many solutions,"The adaptive significance of zebra stripes has thus far eluded understanding. Many explanations have been suggested, including social cohesion, thermoregulation, predation evasion and avoidance of biting flies. Identifying the associations between phenotypic and environmental factors is essential for testing these hypotheses and substantiating existing experimental evidence. Plains zebra striping pattern varies regionally, from heavy black and white striping over the entire body in some areas to reduced stripe coverage with thinner and lighter stripes in others. We examined how well 29 environmental variables predict the variation in stripe characteristics of plains zebra across their range in Africa. In contrast to recent findings, we found no evidence that striping may have evolved to escape predators or avoid biting flies. Instead, we found that temperature successfully predicts a substantial amount of the stripe pattern variation observed in plains zebra. As this association between striping and temperature may be indicative of multiple biological processes, we suggest that the selective agents driving zebra striping are probably multifarious and complex.",2015-01-01,https://www.semanticscholar.org/paper/4a943a8ed14bab479b97136cf501f26eea23b92a,Royal Society Open Science
458,An approximation scheme for planar graph TSP,We consider the special case of the traveling salesman problem (TSP) in which the distance metric is the shortest-path metric of a planar unweighted graph. We present a polynomial-time approximation scheme (PTAS) for this problem.,1995-10-23,https://www.semanticscholar.org/paper/ddf784c8e12525143feb73a2fcecf8ef309db1eb,Proceedings of IEEE 36th Annual Foundations of Computer Science
815,Existence of Reduction Hierarchies,,1997-08-23,https://www.semanticscholar.org/paper/cc96794ae18da7d47a1290cf3975502e15b3e26f,Annual Conference for Computer Science Logic
3290,Group structure in a restricted entry system is mediated by both resident and joiner preferences,,2010-03-05,https://www.semanticscholar.org/paper/50ddda1a11259688837e23c26cf2851da689b02a,Behavioral Ecology and Sociobiology
2268,Regulation of neutrophil apoptosis via death receptors,,2003-11-01,https://www.semanticscholar.org/paper/344163612e1068024cd17faa361d8d0b261b6d83,Cellular and Molecular Life Sciences (CMLS)
1999,Mini–max regret strategy for robust capacity expansion decisions in semiconductor manufacturing,,2012-12-01,https://www.semanticscholar.org/paper/5ae103f2e690ea6878482eb269ef926eccd06f12,Journal of Intelligent Manufacturing
2814,Galectin-3 modulates phagocytosis-induced stellate cell activation and liver fibrosis in vivo.,"Hepatic stellate cells (HSC), the key fibrogenic cells of the liver, transdifferentiate into myofibroblasts upon phagocytosis of apoptotic hepatocytes. Galectin-3, a β-galactoside-binding lectin, is a regulator of the phagocytic process. In this study, our aim was to study the mechanism by which extracellular galectin-3 modulates HSC phagocytosis and activation. The role of galectin-3 in engulfment was evaluated by phagocytosis and integrin binding assays in primary HSC. Galectin-3 expression was studied by real-time PCR and enzyme-linked immunosorbent assay, and in vivo studies were done in wild-type and galectin-3(-/-) mice. We found that HSC from galectin-3(-/-) mice displayed decreased phagocytic activity, expression of transforming growth factor-β1, and procollagen α1(I). Recombinant galectin-3 reversed this defect, suggesting that extracellular galectin-3 is required for HSC activation. Galectin-3 facilitated the α(v)β(3) heterodimer-dependent binding, indicating that galectin-3 modulates HSC phagocytosis via cross-linking this integrin and enhancing the tethering of apoptotic cells. Blocking integrin α(v)β(3) resulted in decreased phagocytosis. Galectin-3 expression and release were induced in active HSC engulfing apoptotic cells, and this was mediated by the nuclear factor-κB signaling. The upregulation of galectin-3 in active HSC was further confirmed in vivo in bile duct-ligated (BDL) rats. Galectin-3(-/-) mice displayed significantly decreased fibrosis, with reduced expression of α-smooth muscle actin and procollagen α1(I) following BDL. In summary, extracellular galectin-3 plays a key role in liver fibrosis by mediating HSC phagocytosis, activation, and subsequent autocrine and paracrine signaling by a feedforward mechanism.",2012-02-01,https://www.semanticscholar.org/paper/de1efea3358c809837042dbd0e373993a28be845,American Journal of Physiology - Gastrointestinal and Liver Physiology
1178,Search for resonant diphoton production with the D0 detector.,"We present a search for a narrow resonance in the inclusive diphoton final state using approximately 2.7 fb(-1) of data collected with the D0 detector at the Fermilab Tevatron pp Collider. We observe good agreement between the data and the background prediction, and set the first 95% C.L. upper limits on the production cross section times the branching ratio for decay into a pair of photons for resonance masses between 100 and 150 GeV. This search is also interpreted in the context of several models of electroweak symmetry breaking with a Higgs boson decaying into two photons.",2009-01-13,https://www.semanticscholar.org/paper/bb537fff7f21784e2c3e5170572280d7fbfb1efb,Physical Review Letters
3137,Limits of wide-area thin-client computing,"While many application service providers have proposed using thin-client computing to deliver computational services over the Internet, little work has been done to evaluate the effectiveness of thin-client computing in a wide-area network. To assess the potential of thin-client computing in the context of future commodity high-bandwidth Internet access, we have used a novel, non-invasive slow-motion benchmarking technique to evaluate the performance of several popular thin-client computing platforms in delivering computational services cross-country over Internet2. Our results show that using thin-client computing in a wide-area network environment can deliver acceptable performance over Internet2, even when client and server are located thousands of miles apart on opposite ends of the country. However, performance varies widely among thin-client platforms and not all platforms are suitable for this environment. While many thin-client systems are touted as being bandwidth efficient, we show that network latency is often the key factor in limiting wide-area thin-client performance. Furthermore, we show that the same techniques used to improve bandwidth efficiency often result in worse overall performance in wide-area networks. We characterize and analyze the different design choices in the various thin-client platforms and explain which of these choices should be selected for supporting wide-area computing services.",2002-06-01,https://www.semanticscholar.org/paper/229dbf2af37c45b151801851271559e0123aeb2f,Measurement and Modeling of Computer Systems
957,Moxifloxacin releasing intraocular implant based on a cross-linked hyaluronic acid membrane,,2021-12-01,https://www.semanticscholar.org/paper/b7db2b4410451d42a1628b4cf29b2da4559f9d65,Scientific Reports
476,Incremental Recompilation of Knowledge,"Approximating a general formula from above and below by Horn formulas (its Horn envelope and Horn core, respectively) was proposed in [SK] as a form of ""knowledge compilation,"" supporting rapid approximate reasoning; on the negative side, this scheme is static in that it supports no updates, and has certain complexity drawbacks pointed out in [KPS]. On the other hand, the many frameworks and schemes proposed in the literature for theory update and revision are plagued by serious complexity-theoretic impediments, even in the Horn case, as was pointed out in [EG2] and the present paper. More fundamentally, these schemes are not inductive, in that they lose in a single update any positive properties of the represented sets of formulas (small size, Horn, etc.). In this paper we propose a new scheme, incremental recompilation, combining Horn approximation and model-based updates; this scheme is inductive and very efficient, free of the problems facing its constituents. A set of formulas is represented by an upper and lower Horn approximation. To update, we replace the upper Horn formula by the Horn envelope of its minimum-change update, and similarly the lower one by the Horn core of its update; the key fact is that Horn envelopes and cores are easy to compute when the underlying formula is the result of a minimum-change update of a Horn formula by a clause. We conjecture that efficient algorithms are possible for more complex updates.",1994-10-05,https://www.semanticscholar.org/paper/d6f31aa49b0c5ecfd9bc77b46e37f619c22a5592,AAAI Conference on Artificial Intelligence
3173,More than ponds amid skyscrapers: Urban fisheries as multiscalar human–natural systems,"Although social-ecological fisheries research is growing, comparatively little attention is paid to fisheries in urban environments. We aim to address this imbalance, because as cities expand worldwide, we expect urban fisheries to become more widespread and important in providing food/nutrition security, recreation, community well-being, and other benefits to fisheries stakeholders and urban dwellers across spatiotemporal scales. This paper contains a first analysis of the economic and sociocultural provisions, trade-offs, and dilemmas associated with urban fisheries to yield insights for sustainable management and planning of urban blue space. To address these objectives, we use the metacoupling framework, a method for assessing human–nature interactions within and across adjacent and distant fisheries systems. We use examples from multiple countries and data from the United States to illustrate how urban fisheries encompass flows of people, money, and information across multiple spatiotemporal scales and provide nutritional, recreational, social, and cultural benefits to fisheries stakeholders. Throughout the world, urban fisheries are influenced by wide-ranging human needs (e.g. food provisioning, recreation, aquatic resource education) that generate social-ecological effects within and beyond cities. Our analysis yields insights for developing holistic, metacoupling-informed management approaches that address the diverse social-ecological objectives and trade-offs involved in sustainable development of urban fisheries.",2022-01-01,https://www.semanticscholar.org/paper/2d5494e7702209a3a79cc2f8157f2e17bf405753,Aquatic Ecosystem Health & Management
2678,Of Vampire mirrors and privacy lamps: privacy management in multi-user augmented environments,"We consider the problem of privacy in a 3D multi-user collaborative environment. We assume that information objects are represented by visual icons, and can either be public or private, and that users need effective methods for viewing and manipulating that state. We suggest two methods, which we call vampire mirrorsand privacy lamps, that are unobtrusive, simple, and natural.",1998-11-01,https://www.semanticscholar.org/paper/9ecd146bc3e4df3c39089e3b72ec223667f79076,ACM Symposium on User Interface Software and Technology
2425,MiXR: A Hybrid AR Sheet Music Interface for Live Performance,"Musicians face a number of issues when performing live, including organizing and annotating sheet music. This can be an unwieldy process, as musicians need to simultaneously read and manipulate sheet music and interact with the conductor and other musicians. Augmented Reality can provide a way to ease some of the more cumbersome aspects of live performance and practice. We present MiXR, a novel interactive system that combines an AR headset, a smartphone, and a tablet to allow performers to intuitively and efficiently manage and annotate virtual sheet music in their physical environment. We discuss our underlying motivation, the interaction techniques supported, and the system architecture.",2020-11-01,https://www.semanticscholar.org/paper/c297da731799cdcae3b5204b6cd8ec2d2e5c7d4c,2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)
891,Four pages are necessary and sufficient for planar graphs,,1986-11-01,https://www.semanticscholar.org/paper/a1ae48e8c24cf15c64fe7bbc47c63a64d55efea5,Symposium on the Theory of Computing
3714,Learning the Predictability of the Future,"We introduce a framework for learning from unlabeled video what is predictable in the future. Instead of committing up front to features to predict, our approach learns from data which features are predictable. Based on the observation that hyperbolic geometry naturally and compactly encodes hierarchical structure, we propose a predictive model in hyperbolic space. When the model is most confident, it will predict at a concrete level of the hierarchy, but when the model is not confident, it learns to automatically select a higher level of abstraction. Experiments on two established datasets show the key role of hierarchical representations for action prediction. Although our representation is trained with unlabeled video, visualizations show that action hierarchies emerge in the representation.",2021-01-01,https://www.semanticscholar.org/paper/8ce65937232b5083f0e9da47ce1d6220bc385c49,Computer Vision and Pattern Recognition
3727,Listening to Sounds of Silence for Speech Denoising,"We introduce a deep learning model for speech denoising, a long-standing challenge in audio analysis arising in numerous applications. Our approach is based on a key observation about human speech: there is often a short pause between each sentence or word. In a recorded speech signal, those pauses introduce a series of time periods during which only noise is present. We leverage these incidental silent intervals to learn a model for automatic speech denoising given only mono-channel audio. Detected silent intervals over time expose not just pure noise but its time-varying features, allowing the model to learn noise dynamics and suppress it from the speech signal. Experiments on multiple datasets confirm the pivotal role of silent interval detection for speech denoising, and our method outperforms several state-of-the-art denoising methods, including those that accept only audio input (like ours) and those that denoise based on audiovisual input (and hence require more information). We also show that our method enjoys excellent generalization properties, such as denoising spoken languages not seen during training.",2020-10-22,https://www.semanticscholar.org/paper/f1d16d4a122e3a6d6db5e959dad03055e3955444,Neural Information Processing Systems
37,Answering General Time-Sensitive Queries,"Time is an important dimension of relevance for a large number of searches, such as over blogs and news archives. So far, research on searching over such collections has largely focused on locating topically similar documents for a query. Unfortunately, topic similarity alone is not always sufficient for document ranking. In this paper, we observe that, for an important class of queries that we call time-sensitive queries, the publication time of the documents in a news archive is important and should be considered in conjunction with the topic similarity to derive the final document ranking. Earlier work has focused on improving retrieval for “recency” queries that target recent documents. We propose a more general framework for handling time-sensitive queries and we automatically identify the important time intervals that are likely to be of interest for a query. Then, we build scoring techniques that seamlessly integrate the temporal aspect into the overall ranking mechanism. We present an extensive experimental evaluation using a variety of news article data sets, including TREC data as well as real web data analyzed using the Amazon Mechanical Turk. We examine several techniques for detecting the important time intervals for a query over a news archive and for incorporating this information in the retrieval process. We show that our techniques are robust and significantly improve result quality for time-sensitive queries compared to state-of-the-art retrieval techniques.",2008-10-26,https://www.semanticscholar.org/paper/62baadac0b75c742bb5e63314cc41c4a5aaeddbc,IEEE Transactions on Knowledge and Data Engineering
2500,Poster: Manipulating virtual objects in hand-held augmented reality using stored snapshots,"We describe a set of interaction techniques that allow a user of a magic-lens style augmented reality application to take snapshots of an augmented scene and revisit them virtually for interaction at a later time. By storing a still image of the background along with the camera pose, this approach allows augmentations to remain dynamic and interactive. This makes it possible for the user to manipulate virtual objects from the vantage points of different locations without the overhead of physically traveling between those locations. Preliminary results from a user study show that participants were able to complete an alignment task significantly faster and as accurately when using snapshots as opposed to physical travel. Qualitative questionnaire answers showed that participants preferred using snapshots over walking and found it less demanding.",2012-03-04,https://www.semanticscholar.org/paper/a20cbfea6aad863edeea63b5f61cfac5ee4c3c2b,IEEE Symposium on 3D User Interfaces
3272,African Vultures Don’t Follow Migratory Herds: Scavenger Habitat Use Is Not Mediated by Prey Abundance,"The ongoing global decline in vulture populations raises major conservation concerns, but little is known about the factors that mediate scavenger habitat use, in particular the importance of abundance of live prey versus prey mortality. We test this using data from the Serengeti-Mara ecosystem in East Africa. The two hypotheses that prey abundance or prey mortality are the main drivers of vulture habitat use provide alternative predictions. If vultures select areas based only on prey abundance, we expect tracked vultures to remain close to herds of migratory wildebeest regardless of season. However, if vultures select areas where mortality rates are greatest then we expect vultures to select the driest regions, where animals are more likely to die of starvation, and to be attracted to migratory wildebeest only during the dry season when wildebeest mortality is greatest. We used data from GSM-GPS transmitters to assess the relationship between three vulture species and migratory wildebeest in the Mara-Serengeti ecosystem. Results indicate that vultures preferentially cluster around migratory herds only during the dry season, when herds experience their highest mortality. Additionally during the wet season, Ruppell’s and Lappet-faced vultures select relatively dry areas, based on Normalized Difference Vegetation Index, whereas White-backed vultures preferred wetter areas during the wet season. Differences in habitat use among species may mediate coexistence in this scavenger guild. In general, our results suggest that prey abundance is not the primary driver of avian scavenger habitat use. The apparent reliance of vultures on non-migratory ungulates during the wet season has important conservation implications for vultures in light of on-going declines in non-migratory ungulate species and use of poisons in unprotected areas.",2014-01-08,https://www.semanticscholar.org/paper/33fcacb076caa7cd19b72af12443e2dde9a1ab4f,PLoS ONE
3514,Improved Algorithms for Bipartite Network Flow,"In this paper, network flow algorithms for bipartite networks are studied. A network $G=(V,E)$ is called bipartite if its vertex set $V$ can be partitioned into two subsets $V_1$ and $V_2$ such that all edges have one endpoint in $V_1$ and the other in $V_2$. Let $n=|V|$, $n_1 = |V_1|$, $n_2 = |V_2|$, $m=|E|$ and assume without loss of generality that $n_1 \leq n_2$. A bipartite network is called unbalanced if $n_1 \ll n_2$ and balanced otherwise. (This notion is necessarily imprecise.) It is shown that several maximum flow algorithms can be substantially sped up when applied to unbalanced networks. The basic idea in these improvements is a two-edge push rule that allows one to ""charge"" most computation to vertices in $V_1$, and hence develop algorithms whose running times depend on $n_1$ rather than $n$. For example, it is shown that the two-edge push version of Goldberg and Tarjan's FIFO preflow-push algorithm runs in $O(n_1 m + n_1^3)$ time and that the analogous version of Ahuja and Orlin's excess scaling algorithm runs in $O(n_1 m + n_1^2 log U)$ time, where $U$ is the largest edge capacity. These ideas are also extended to dynamic tree implementations, parametric maximum flows, and minimum-cost flows.",1994-10-01,https://www.semanticscholar.org/paper/8e3bf550416964efd451b33f06d30a6f4c763074,SIAM journal on computing (Print)
1469,Observation of scaling of the photon structure function F2 gamma at low Q2.,"Disclaimer/Complaints regulations If you believe that digital publication of certain material infringes any of your rights or (privacy) interests, please let the Library know, stating your reasons. In case of a legitimate complaint, the Library will make the material inaccessible and/or remove it from the website. Please Ask the Library: http://uba.uva.nl/en/contact, or a letter to: Library of the University of Amsterdam, Secretariat, Singel 425, 1012 WP Amsterdam, The Netherlands. You will be contacted as soon as possible.",1987-01-12,https://www.semanticscholar.org/paper/00bd99b98bb2656feda7e462c31cb7b6eeb9de3b,Physical Review Letters
670,Solving Probability and Statistics Problems by Program Synthesis,"We solve university level probability and statistics questions by program synthesis using OpenAI's Codex, a Transformer trained on text and fine-tuned on code. We transform course problems from MIT's 18.05 Introduction to Probability and Statistics and Harvard's STAT110 Probability into programming tasks. We then execute the generated code to get a solution. Since these course questions are grounded in probability, we often aim to have Codex generate probabilistic programs that simulate a large number of probabilistic dependencies to compute its solution. Our approach requires prompt engineering to transform the question from its original form to an explicit, tractable form that results in a correct program and solution. To estimate the amount of work needed to translate an original question into its tractable form, we measure the similarity between original and transformed questions. Our work is the first to introduce a new dataset of university-level probability and statistics problems and solve these problems in a scalable fashion using the program synthesis capabilities of large language models.",2021-11-16,https://www.semanticscholar.org/paper/f7987fa2aadc0b368c185dc4d2fdb1337a202c32,arXiv.org
1687,Scalable Recommendation with Hierarchical Poisson Factorization,"We develop hierarchical Poisson matrix factorization (HPF), a novel method for providing users with high quality recommendations based on implicit feedback, such as views, clicks, or purchases. In contrast to existing recommendation models, HPF has a number of desirable properties. First, we show that HPF more accurately captures the long-tailed user activity found in most consumption data by explicitly considering the fact that users have finite attention budgets. This leads to better estimates of users' latent preferences, and therefore superior recommendations, compared to competing methods. Second, HPF learns these latent factors by only explicitly considering positive examples, eliminating the often costly step of generating artificial negative examples when fitting to implicit data. Third, HPF is more than just one method— it is the simplest in a class of probabilistic models with these properties, and can easily be extended to include more complex structure and assumptions. We develop a variational algorithm for approximate posterior inference for HPF that scales up to large data sets, and we demonstrate its performance on a wide variety of real-world recommendation problems—users rating movies, listening to songs, reading scientific papers, and reading news articles.",2015-07-12,https://www.semanticscholar.org/paper/b2a35b6693c74abe7d42ba13e8b90c1545b569f7,Conference on Uncertainty in Artificial Intelligence
2867,Regulation of cellular homeostasis by galectins,,,https://www.semanticscholar.org/paper/e9c4946435ddfd6d18f8a7394e3b2661abb8a5f8,Glycoconjugate Journal
2498,Virtual projection: exploring optical projection as a metaphor for multi-device interaction,"Handheld optical projectors provide a simple way to overcome the limited screen real-estate on mobile devices. We present virtual projection (VP), an interaction metaphor inspired by how we intuitively control the position, size, and orientation of a handheld optical projector's image. VP is based on tracking a handheld device without an optical projector and allows selecting a target display on which to position, scale, and orient an item in a single gesture. By relaxing the optical projection metaphor, we can deviate from modeling perspective projection, for example, to constrain scale or orientation, create multiple copies, or offset the image. VP also supports dynamic filtering based on the projection frustum, creating overview and detail applications, and selecting portions of a larger display for zooming and panning. We show exemplary use cases implemented using our optical feature-tracking framework and present the results of a user study demonstrating the effectiveness of VP in complex interactions with large displays.",2012-05-05,https://www.semanticscholar.org/paper/8371b5478710a7cb1ebb7e488d8e6dd7fff2920a,International Conference on Human Factors in Computing Systems
251,Mechanisms for complement-free procurement,"We study procurement auctions when the buyer has complement-free (subadditive) objectives in the budget feasibility model (Singer 2010). For general subadditive functions we give a randomized universally truthful mechanism which is an O(log2 n) approximation, and an O(log3 n) deterministic truthful approximation mechanism; both mechanisms are in the demand oracle model. For cut functions, an interesting case of nonincreasing objectives, we give both randomized and deterministic truthful and budget feasible approximation mechanisms that achieve a constant approximation factor.",2011-06-05,https://www.semanticscholar.org/paper/3fb51b4a41e143c8c664e59b9f62b52f939cb176,ACM Conference on Economics and Computation
3651,Parametrized Types for C++,"Type parameterization is the ability to defrne a type in terms of another, unspecifled, type. Versions of the parameterized type may then be created for several particular parameter types. A language supporting type parameterization allows specification ofgeneral container types such as list, vector, and associative array where the specific type of the elements is left as a parameter. Thus, a parameterized class specifies an unbounded set of related types; for example: list of int, list of name, list of shape, etc. Type parameterization is one way of making a language more extensible. In the context of C++, the problems are 1. Can type parameterization be easy to use? 2. Can objects of a parameterized type be used as efficiently as objects of a ""hand-coded"" type? 3. Can a general form of parameterized types be integrated into C++? 4. Can parameterized types be implemented so that the compilation and linking speed is similar to that achieved by a compilation system that does not support type parameterization? 5. Can such a compilation system be simple and portable? @ Computing Systems,Yol.2. No. I . Winter 1989 55 56 A design is presented for which the answer to all of these questions is yes. The implementation of this scheme is a fairly simple extension of current C++ implementations. v/ARNING: The scheme for providing parameterized types described here is not implemented. It is not part ofthe C++ language, nor is there any guarantee that it ever will be.",,https://www.semanticscholar.org/paper/7386ca6873e002ea50d0f108391f667ab4adbd1a,Computing Systems
2977,Non-conjugate Variational Message Passing for Multinomial and Binary Regression,"Variational Message Passing (VMP) is an algorithmic implementation of the Variational Bayes (VB) method which applies only in the special case of conjugate exponential family models. We propose an extension to VMP, which we refer to as Non-conjugate Variational Message Passing (NCVMP) which aims to alleviate this restriction while maintaining modularity, allowing choice in how expectations are calculated, and integrating into an existing message-passing framework: Infer.NET. We demonstrate NCVMP on logistic binary and multinomial regression. In the multinomial case we introduce a novel variational bound for the soft-max factor which is tighter than other commonly used bounds whilst maintaining computational tractability.",2011-12-12,https://www.semanticscholar.org/paper/15bbea3d58ac3f7c21392eaba72b8b166888f81f,Neural Information Processing Systems
338,The Complexity of Games on Highly Regular Graphs,,2005-10-03,https://www.semanticscholar.org/paper/bb7288850d42303f2282ed7299b24bf8e10adf16,Embedded Systems and Applications
3155,"The design, implementation and evaluation of SMART: a scheduler for multimedia applications","Real-time applications such as multimedia audio and video are increasingly populating the workstation desktop. To support the execution of these applications in conjunction with traditional non-realtime applications, we have created SMART, a Scheduler for Multimedia And Real-Time applications. SMART supports applications with time constraints, and provides dynamic feedback to applications to allow them to adapt to the current load. In addition, the support for real-time applications is inte grated with the support for conventional computations. This allows the user to prioritize across real-time and conventional computations, and dictate how the processor is to be shared among applications of the same priority . As the system load changes, SMART adjusts the allocation of resources dynamically and seamlessly. SMAR T is unique in its ability to automatically shed real-time tasks and re gulate their execution rates when the system is overloaded, while providing better value in underloaded conditions than previously proposed schemes. We have implemented SMART in the Solaris UNIX operating system and measured its performance against other schedulers in e xecuting real-time, interacti ve, and batch applications. Our results demonstrate SMART’s superior performance in supporting multimedia applications.",1997-10-01,https://www.semanticscholar.org/paper/928e56d819a7a9fb64f800bba2c2d23a875d6794,Symposium on Operating Systems Principles
3014,Heterogeneous Multi-Mobile Computing (video),"As smartphones and tablets proliferate, there is a growing demand for multi-mobile computing [1, 2], the ability to combine multiple commodity mobile systems into more capable ones, including using multiple hardware devices such as cameras, displays, speakers, microphones, sensors, GPS, and input. However, the tremendous device, hardware, and software heterogeneity of mobile systems makes this difficult. In this demo, we present M2, a system for multi-mobile computing that enables existing unmodified mobile apps to make use of new ways of sharing and combining multiple devices. M2 introduces a new data-centric approach that leverages higher-level device abstractions and encoding/decoding hardware to efficiently share device data as opposed to low-level device-specific APIs.",2019-06-12,https://www.semanticscholar.org/paper/38cf86b4377c6367545ac0e11113bb19d704c3f8,"ACM SIGMOBILE International Conference on Mobile Systems, Applications, and Services"
1573,Counterfactual inference for consumer choice across many product categories,,2019-06-06,https://www.semanticscholar.org/paper/818d138bb6614f226a6cf73f69404ca917aebfdf,Quantitative Marketing and Economics
2781,Indispensable role of Galectin-3 in promoting quiescence of hematopoietic stem cells,,2021-04-09,https://www.semanticscholar.org/paper/6358b1fa6aebe48ada3db075aa2a8343df9fee26,Nature Communications
3771,Assessing the Quality of Actions,,2014-09-06,https://www.semanticscholar.org/paper/6bb60de0e93c9df260ef3d20dbff0716ed1ec711,European Conference on Computer Vision
355,On certain connectivity properties of the Internet topology,"We show that random graphs in the preferential connectivity model have constant conductance, and hence have worst-case routing congestion that scales logarithmically with the number of nodes. Another immediate implication is constant spectral gap between the first and second eigenvalues of the random walk matrix associated with these graphs. We also show that the expected frugality (overpayment in the Vickrey-Clarke-Groves mechanism for shortest paths) of a random graph is bounded by a small constant.",2003-10-11,https://www.semanticscholar.org/paper/451c97adbbb68ec1a51b9041d7c06d076642df90,"44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings."
2940,Inferring Relevant Cell Types for Complex Traits by Using Single-Cell Gene Expression.,,2017-11-02,https://www.semanticscholar.org/paper/368fce9778dd4680a025b4551e7c137794bf5194,American Journal of Human Genetics
1627,Dynamic Bernoulli Embeddings for Language Evolution,"Word embeddings are a powerful approach for unsupervised analysis of language. Recently, Rudolph et al. (2016) developed exponential family embeddings, which cast word embeddings in a probabilistic framework. Here, we develop dynamic embeddings, building on exponential family embeddings to capture how the meanings of words change over time. We use dynamic embeddings to analyze three large collections of historical texts: the U.S. Senate speeches from 1858 to 2009, the history of computer science ACM abstracts from 1951 to 2014, and machine learning papers on the Arxiv from 2007 to 2015. We find dynamic embeddings provide better fits than classical embeddings and capture interesting patterns about how language changes.",2017-03-23,https://www.semanticscholar.org/paper/67c04e949cc98c7dcc48dea0c94043d4ebf542db,arXiv.org
3083,"Proceedings of the 1st ACM Workshop on Virtual Machine Security, VMSec 2008, Alexandria, VA, USA, October 27, 2008",,,https://www.semanticscholar.org/paper/d64c4eb79f76bcf77d313c4be1605b00be3d6db3,VMSec
49,To search or to crawl?: towards a query optimizer for text-centric tasks,"Text is ubiquitous and, not surprisingly, many important applications rely on textual data for a variety of tasks. As a notable example, information extraction applications derive structured relations from unstructured text; as another example, focused crawlers explore the web to locate pages about specific topics. Execution plans for text-centric tasks follow two general paradigms for processing a text database: either we can scan, or 'crawl,"" the text database or, alternatively, we can exploit search engine indexes and retrieve the documents of interest via carefully crafted queries constructed in task-specific ways. The choice between crawl- and query-based execution plans can have a substantial impact on both execution time and output ""completeness"" (e.g., in terms of recall). Nevertheless, this choice is typically ad-hoc and based on heuristics or plain intuition. In this paper, we present fundamental building blocks to make the choice of execution plans for text-centric tasks in an informed, cost-based way. Towards this goal, we show how to analyze query- and crawl-based plans in terms of both execution time and output completeness. We adapt results from random-graph theory and statistics to develop a rigorous cost model for the execution plans. Our cost model reflects the fact that the performance of the plans depends on fundamental task-specific properties of the underlying text databases. We identify these properties and present efficient techniques for estimating the associated cost-model parameters. Overall, our approach helps predict the most appropriate execution plans for a task, resulting in significant efficiency and output completeness benefits. We complement our results with a large-scale experimental evaluation for three important text-centric tasks and over multiple real-life data sets.",2006-06-27,https://www.semanticscholar.org/paper/f934522657189b4accc47317af2770029999d0eb,SIGMOD Conference
917,The complexity of facets (and some facets of complexity),"Many important combinatorial optimization problems, including the traveling salesman problem (TSP), the clique problem and many others, call for the optimization of a linear functional over some discrete set of vectors.",1982-05-05,https://www.semanticscholar.org/paper/d01d8269be394b9fa5f3916dc82deae147a2dff2,Symposium on the Theory of Computing
920,On the Complexity of Testing Implications of Functional and Join Dependencies,"It iS shown that testing whether a dependency o is unpiled by a set ~ of functional and join dependencies is NP-hard if o is a join dependency, but it reqmres only O(l u Ill ~ II) time ff o Is either a funcuonal or a multivalued dependency ( j U [ is the number of elements in the set of all the attributes U, and II ~ II as the space required to write down ~). The fact that inferring join dependencies is NP-hard follows from the followmg stronger result. It is proved that if ~ is a set of one jom dependency and several funcuonal dependencies, then testing whether Z implies a join dependency o is NP-complete By combming this result with a recent result of Beeri and Var& ~t can be proved that if 2 is a set of one join dependency and several multivalued dependencies, then testing whether ~ unphes a join dependency o Is NP-hard It is also shown that the problem of deciding whether a JD-rule can be applied to a tableau T and the problem of testing whether a relation r does not obey a join dependency are NP-complete. The first problem is NP-complete even if T can be obtamed from a tableau corresponding to a join dependency by applying some FD-rules. As a result, It follows that deciding whether the join of several relations obtained by projecUon from a umversal instance is not equal to the universal instance is NP-complete. Finally, it is proved that there is no umversal constant n such that for every set of multlvalued dependencies ~ and a join dependency o that is not unpiled by ~, there is a relation with no more than n tuples in which holds but o fails.",1981-10-01,https://www.semanticscholar.org/paper/304528d94097dee67455ddf23b70d80d22ac932a,JACM
3464,Approximating disjoint-path problems using packing integer programs,,,https://www.semanticscholar.org/paper/e63fb6dfb7b2b1903e1c64ac75eadb12b037f216,Mathematical programming
3720,Dissecting Image Crops,"The elementary operation of cropping underpins nearly every computer vision system, ranging from data augmentation and translation invariance to computational photography and representation learning. This paper investigates the subtle traces introduced by this operation. For example, despite refinements to camera optics, lenses will leave behind certain clues, notably chromatic aberration and vignetting. Photographers also leave behind other clues relating to image aesthetics and scene composition. We study how to detect these traces, and investigate the impact that cropping has on the image distribution. While our aim is to dissect the fundamental impact of spatial crops, there are also a number of practical implications to our work, such as revealing faulty photojournalism and equipping neural network researchers with a better understanding of shortcut learning. Code is available at https://github.com/basilevh/dissecting-image-crops.",2020-11-24,https://www.semanticscholar.org/paper/5f4cb14efef53e82b116f999b829c108104e8670,IEEE International Conference on Computer Vision
290,Computing correlated equilibria in multi-player games,"We develop polynomial-time algorithms for finding correlated equilibria—a well-studied notion of rationality that generalizes the Nash equilibrium—in a broad class of succinctly representable multiplayer games, encompassing graphical games, anonymous games, polymatrix games, congestion games, scheduling games, local effect games, as well as several generalizations. Our algorithm is based on a variant of the existence proof due to Hart and Schmeidler, and employs linear programming duality, the ellipsoid algorithm, Markov chain steady state computations, as well as application-specific methods for computing multivariate expectations over product distributions.
 For anonymous games and graphical games of bounded tree-width, we provide a different polynomial-time algorithm for optimizing an arbitrary linear function over the set of correlated equilibria of the game. In contrast to our sweeping positive results for computing an arbitrary correlated equilibrium, we prove that optimizing over correlated equilibria is NP-hard in all of the other classes of games that we consider.",2008-07-01,https://www.semanticscholar.org/paper/6ae3ddfb3e59019d6b01d1670bb0133141ce373a,JACM
96,Simplifying data access: the energy data collection (EDC) project,"The massive amount of statistical and text data available from government agencies has created a set of daunting challenges to both research and analysis communities. These problems include heterogeneity, size, distribution, and control of terminology. At the Digital Government Research Center we are investigating solutions to these key problems. In this paper we focus on (1) ontological mappings for terminology standardization, (2) data integration across data bases with high speed query processing, and (3) interfaces for query input and presentation of results. This collaboration between researchers from Columbia University and the Information Sciences Institute of the University of Southern California employs technology developed at both locations, in particular the SENSUS ontology, the SIMS multi-database access planner, the LKB automated dictionary and terminology analysis system, and others. The pilot application targets gasoline data from the Bureau of Labor Statistics, the Energy Information Administration of the Department of Energy, the Census Bureau, and other government agencies.",2000-05-15,https://www.semanticscholar.org/paper/313e69f0df162ac2f791550a35d43d3e7b5eeae7,Digital Government Research
33,Querying text databases and the web: beyond traditional keyword search,"Traditional keyword search---where a query is a list of keywords and query results are a relevance-ordered list of documents---is, of course, a powerful query paradigm for text databases and the Web. However, more expressive query paradigms, where both queries and their results can exhibit a richer structure than in traditional keyword search, are often desirable. Information extraction systems identify and extract intrinsically structured data that is embedded in natural-language text documents, hence enabling these alternative query paradigms. Unfortunately, information extraction is a time-consuming process, often involving complex text analysis, so exhaustively processing all documents in a large text database --or on the Web-- could be prohibitively expensive. Beyond efficiency, query result quality is also important: information extraction is error-prone and not all extracted data is equally likely to be correct, so result quality is an important consideration during query processing. In this talk, I will discuss recent work on cost-based optimization of structured queries in this information extraction scenario, where modeling query result quality--in addition to execution efficiency-- is a distinctive and important challenge.",2009-06-28,https://www.semanticscholar.org/paper/e4521648675cc58d2c2bbe3b479e2085ebdf1a25,KEYS '09
